Job is running on partition: gpu_ss11
Job is running under account: m2616_g
/pscratch/sd/t/tianle/conda/envs/lucid_surp_2024/lib/python3.11/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:09<00:29,  9.98s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:20<00:20, 10.11s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:30<00:10, 10.15s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:32<00:00,  7.10s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:32<00:00,  8.20s/it]
Llama3 has been loaded successfully.
Model:  LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(128256, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)
          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)
          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)
          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)
)
Size of dataset:  160
In grad_steps = 0, loss = 1.400383472442627
In grad_steps = 1, loss = 0.7049850225448608
In grad_steps = 2, loss = 1.1306487321853638
In grad_steps = 3, loss = 0.6889359951019287
In grad_steps = 4, loss = 0.9060465097427368
In grad_steps = 5, loss = 0.8014107942581177
In grad_steps = 6, loss = 0.8144437074661255
In grad_steps = 7, loss = 0.6986751556396484
In grad_steps = 8, loss = 0.8987993597984314
In grad_steps = 9, loss = 0.74196857213974
In grad_steps = 10, loss = 0.7372785210609436
In grad_steps = 11, loss = 0.728527843952179
In grad_steps = 12, loss = 0.7273502349853516
In grad_steps = 13, loss = 1.1860418319702148
In grad_steps = 14, loss = 1.0274806022644043
In grad_steps = 15, loss = 0.5329322814941406
In grad_steps = 16, loss = 0.706036388874054
In grad_steps = 17, loss = 0.7377666234970093
In grad_steps = 18, loss = 0.7003070712089539
In grad_steps = 19, loss = 0.7180029153823853
In grad_steps = 20, loss = 0.6260555982589722
In grad_steps = 21, loss = 0.7103196382522583
In grad_steps = 22, loss = 0.9260563254356384
In grad_steps = 23, loss = 0.911677360534668
In grad_steps = 24, loss = 0.6665027737617493
In grad_steps = 25, loss = 0.6892314553260803
In grad_steps = 26, loss = 0.7155096530914307
In grad_steps = 27, loss = 0.68663489818573
In grad_steps = 28, loss = 0.7625641822814941
In grad_steps = 29, loss = 0.7038435935974121
In grad_steps = 30, loss = 0.9026855230331421
In grad_steps = 31, loss = 0.528775155544281
In grad_steps = 32, loss = 0.5479413866996765
In grad_steps = 33, loss = 1.016403317451477
In grad_steps = 34, loss = 0.44562864303588867
In grad_steps = 35, loss = 0.7286269664764404
In grad_steps = 36, loss = 0.451781690120697
In grad_steps = 37, loss = 0.7306908965110779
In grad_steps = 38, loss = 0.7659636735916138
In grad_steps = 39, loss = 0.7400379180908203
In grad_steps = 40, loss = 0.44265297055244446
In grad_steps = 41, loss = 0.7532161474227905
In grad_steps = 42, loss = 1.056922435760498
In grad_steps = 43, loss = 0.46325504779815674
In grad_steps = 44, loss = 0.9496058225631714
In grad_steps = 45, loss = 0.7301522493362427
In grad_steps = 46, loss = 0.7192825078964233
In grad_steps = 47, loss = 0.7191725969314575
In grad_steps = 48, loss = 0.6789408922195435
In grad_steps = 49, loss = 0.6963324546813965
In grad_steps = 50, loss = 0.711556077003479
In grad_steps = 51, loss = 0.5100743770599365
In grad_steps = 52, loss = 0.6721014380455017
In grad_steps = 53, loss = 0.7647101879119873
In grad_steps = 54, loss = 1.0767898559570312
In grad_steps = 55, loss = 0.7974531054496765
In grad_steps = 56, loss = 1.0074012279510498
In grad_steps = 57, loss = 0.7224571108818054
In grad_steps = 58, loss = 0.8947906494140625
In grad_steps = 59, loss = 0.7211606502532959
In grad_steps = 60, loss = 0.7040427327156067
In grad_steps = 61, loss = 0.6989893913269043
In grad_steps = 62, loss = 0.7560293078422546
In grad_steps = 63, loss = 1.0411100387573242
In grad_steps = 64, loss = 0.9467306137084961
In grad_steps = 65, loss = 0.6778964996337891
In grad_steps = 66, loss = 0.5330805778503418
In grad_steps = 67, loss = 0.6704892516136169
In grad_steps = 68, loss = 0.8301988840103149
In grad_steps = 69, loss = 0.6922482252120972
In grad_steps = 70, loss = 0.7124819755554199
In grad_steps = 71, loss = 0.6786791086196899
In grad_steps = 72, loss = 0.7998223304748535
In grad_steps = 73, loss = 0.7870112657546997
In grad_steps = 74, loss = 0.6998564004898071
In grad_steps = 75, loss = 0.6714354753494263
In grad_steps = 76, loss = 0.668472170829773
In grad_steps = 77, loss = 0.7267451882362366
In grad_steps = 78, loss = 0.7425402402877808
In grad_steps = 79, loss = 0.6438851356506348
In grad_steps = 80, loss = 0.6928147077560425
In grad_steps = 81, loss = 0.7044723033905029
In grad_steps = 82, loss = 0.6896647810935974
In grad_steps = 83, loss = 0.703065037727356
In grad_steps = 84, loss = 0.714301586151123
In grad_steps = 85, loss = 0.6798573732376099
In grad_steps = 86, loss = 0.7523372173309326
In grad_steps = 87, loss = 0.7416015863418579
In grad_steps = 88, loss = 0.7215330600738525
In grad_steps = 89, loss = 0.6784240007400513
In grad_steps = 90, loss = 0.6902492046356201
In grad_steps = 91, loss = 0.6737003326416016
In grad_steps = 92, loss = 0.7159943580627441
In grad_steps = 93, loss = 0.6816931962966919
In grad_steps = 94, loss = 0.780734658241272
In grad_steps = 95, loss = 0.6074017286300659
In grad_steps = 96, loss = 0.6349265575408936
In grad_steps = 97, loss = 0.8659298419952393
In grad_steps = 98, loss = 0.5279184579849243
In grad_steps = 99, loss = 0.7072228193283081
In grad_steps = 100, loss = 0.5230642557144165
In grad_steps = 101, loss = 0.7116093635559082
In grad_steps = 102, loss = 0.718957781791687
In grad_steps = 103, loss = 0.7179242372512817
In grad_steps = 104, loss = 0.4792622923851013
In grad_steps = 105, loss = 0.7279472351074219
In grad_steps = 106, loss = 0.9888155460357666
In grad_steps = 107, loss = 0.4556269645690918
In grad_steps = 108, loss = 0.9906591773033142
In grad_steps = 109, loss = 0.7358378171920776
In grad_steps = 110, loss = 0.7262204885482788
In grad_steps = 111, loss = 0.8748258352279663
In grad_steps = 112, loss = 0.6885333061218262
In grad_steps = 113, loss = 0.6925400495529175
In grad_steps = 114, loss = 0.6912871599197388
In grad_steps = 115, loss = 0.7121273279190063
In grad_steps = 116, loss = 0.6609586477279663
In grad_steps = 117, loss = 0.6876614093780518
In grad_steps = 118, loss = 0.811143159866333
In grad_steps = 119, loss = 0.7144144773483276
In grad_steps = 120, loss = 0.8296425938606262
In grad_steps = 121, loss = 0.684219241142273
In grad_steps = 122, loss = 0.8104567527770996
In grad_steps = 123, loss = 0.7136316895484924
In grad_steps = 124, loss = 0.6867343187332153
In grad_steps = 125, loss = 0.6834665536880493
In grad_steps = 126, loss = 0.7159424424171448
In grad_steps = 127, loss = 0.8462502956390381
In grad_steps = 128, loss = 0.7916136384010315
In grad_steps = 129, loss = 0.6603652238845825
In grad_steps = 130, loss = 0.5861735343933105
In grad_steps = 131, loss = 0.669411301612854
In grad_steps = 132, loss = 0.7896465063095093
In grad_steps = 133, loss = 0.681511402130127
In grad_steps = 134, loss = 0.6894576549530029
In grad_steps = 135, loss = 0.6559813618659973
In grad_steps = 136, loss = 0.6801824569702148
In grad_steps = 137, loss = 0.6845653057098389
In grad_steps = 138, loss = 0.6882004737854004
In grad_steps = 139, loss = 0.6534264087677002
In grad_steps = 140, loss = 0.6761242151260376
In grad_steps = 141, loss = 0.7008617520332336
In grad_steps = 142, loss = 0.7321031093597412
In grad_steps = 143, loss = 0.631807804107666
In grad_steps = 144, loss = 0.6835665702819824
In grad_steps = 145, loss = 0.6733607053756714
In grad_steps = 146, loss = 0.6678814888000488
In grad_steps = 147, loss = 0.6789115071296692
In grad_steps = 148, loss = 0.6737920641899109
In grad_steps = 149, loss = 0.6591324806213379
In grad_steps = 150, loss = 0.7867431640625
In grad_steps = 151, loss = 0.7683842778205872
In grad_steps = 152, loss = 0.6867377758026123
In grad_steps = 153, loss = 0.6397182941436768
In grad_steps = 154, loss = 0.6873059272766113
In grad_steps = 155, loss = 0.658173680305481
In grad_steps = 156, loss = 0.6993141174316406
In grad_steps = 157, loss = 0.6345752477645874
In grad_steps = 158, loss = 0.8028074502944946
In grad_steps = 159, loss = 0.575401246547699
In grad_steps = 160, loss = 0.6062406301498413
In grad_steps = 161, loss = 0.890475869178772
In grad_steps = 162, loss = 0.4951687455177307
In grad_steps = 163, loss = 0.6835954785346985
In grad_steps = 164, loss = 0.45359134674072266
In grad_steps = 165, loss = 0.7196349501609802
In grad_steps = 166, loss = 0.6965092420578003
In grad_steps = 167, loss = 0.7293022274971008
In grad_steps = 168, loss = 0.41527238488197327
In grad_steps = 169, loss = 0.7101125717163086
In grad_steps = 170, loss = 1.0100756883621216
In grad_steps = 171, loss = 0.414348840713501
In grad_steps = 172, loss = 0.913301408290863
In grad_steps = 173, loss = 0.709606409072876
In grad_steps = 174, loss = 0.6920485496520996
In grad_steps = 175, loss = 0.7361108064651489
In grad_steps = 176, loss = 0.655228853225708
In grad_steps = 177, loss = 0.6530084609985352
In grad_steps = 178, loss = 0.6845897436141968
In grad_steps = 179, loss = 0.6103962659835815
In grad_steps = 180, loss = 0.649990439414978
In grad_steps = 181, loss = 0.7288078665733337
In grad_steps = 182, loss = 0.9602549076080322
In grad_steps = 183, loss = 0.6938605308532715
In grad_steps = 184, loss = 0.9596076011657715
In grad_steps = 185, loss = 0.6258697509765625
In grad_steps = 186, loss = 0.8189710378646851
In grad_steps = 187, loss = 0.6408951878547668
In grad_steps = 188, loss = 0.6771876215934753
In grad_steps = 189, loss = 0.6699145436286926
In grad_steps = 190, loss = 0.7060897350311279
In grad_steps = 191, loss = 0.9632603526115417
In grad_steps = 192, loss = 0.9001150727272034
In grad_steps = 193, loss = 0.6639195680618286
In grad_steps = 194, loss = 0.5480375289916992
In grad_steps = 195, loss = 0.65803462266922
In grad_steps = 196, loss = 0.7761749029159546
In grad_steps = 197, loss = 0.6637803316116333
In grad_steps = 198, loss = 0.6669837236404419
In grad_steps = 199, loss = 0.6165761947631836
In grad_steps = 200, loss = 0.718539834022522
In grad_steps = 201, loss = 0.7315990924835205
In grad_steps = 202, loss = 0.671043872833252
In grad_steps = 203, loss = 0.6167594194412231
In grad_steps = 204, loss = 0.6504685282707214
In grad_steps = 205, loss = 0.5672951340675354
In grad_steps = 206, loss = 0.6754902601242065
In grad_steps = 207, loss = 0.5918288826942444
In grad_steps = 208, loss = 0.6229040622711182
In grad_steps = 209, loss = 0.5730518698692322
In grad_steps = 210, loss = 0.62378990650177
In grad_steps = 211, loss = 0.5506601333618164
In grad_steps = 212, loss = 0.5949273109436035
In grad_steps = 213, loss = 0.47499018907546997
In grad_steps = 214, loss = 0.7912114858627319
In grad_steps = 215, loss = 0.736490786075592
In grad_steps = 216, loss = 0.696021556854248
In grad_steps = 217, loss = 0.45639270544052124
In grad_steps = 218, loss = 0.43103158473968506
In grad_steps = 219, loss = 0.435070276260376
In grad_steps = 220, loss = 0.6703632473945618
In grad_steps = 221, loss = 0.1869237720966339
In grad_steps = 222, loss = 1.350144863128662
In grad_steps = 223, loss = 0.36549296975135803
In grad_steps = 224, loss = 0.6251903772354126
In grad_steps = 225, loss = 0.2298528105020523
In grad_steps = 226, loss = 0.5466872453689575
In grad_steps = 227, loss = 0.475173681974411
In grad_steps = 228, loss = 0.22787785530090332
In grad_steps = 229, loss = 0.48547157645225525
In grad_steps = 230, loss = 0.1760322004556656
In grad_steps = 231, loss = 0.7275869846343994
In grad_steps = 232, loss = 0.23723700642585754
In grad_steps = 233, loss = 0.1897026151418686
In grad_steps = 234, loss = 0.15771564841270447
In grad_steps = 235, loss = 0.512283444404602
In grad_steps = 236, loss = 0.00387529656291008
In grad_steps = 237, loss = 1.9854944944381714
In grad_steps = 238, loss = 1.5415208339691162
In grad_steps = 239, loss = 0.21558083593845367
In grad_steps = 240, loss = 0.6279993057250977
In grad_steps = 241, loss = 0.9401090741157532
In grad_steps = 242, loss = 0.7393221855163574
In grad_steps = 243, loss = 0.8769402503967285
In grad_steps = 244, loss = 0.5351520776748657
In grad_steps = 245, loss = 0.7293673753738403
In grad_steps = 246, loss = 0.8552597761154175
In grad_steps = 247, loss = 0.5117882490158081
In grad_steps = 248, loss = 0.747635006904602
In grad_steps = 249, loss = 0.4457610845565796
In grad_steps = 250, loss = 0.593808114528656
In grad_steps = 251, loss = 0.3746201992034912
In grad_steps = 252, loss = 0.6539645195007324
In grad_steps = 253, loss = 0.6647363305091858
In grad_steps = 254, loss = 0.8564995527267456
In grad_steps = 255, loss = 1.4147365093231201
i = 0, Test ensemble probabilities = 
[array([[0.27423537, 0.72576463],
       [0.25977612, 0.7402239 ],
       [0.28192466, 0.71807534],
       [0.24212478, 0.75787526],
       [0.32004422, 0.6799558 ],
       [0.22683299, 0.773167  ],
       [0.2593551 , 0.74064493],
       [0.27376398, 0.72623605],
       [0.2301524 , 0.7698476 ],
       [0.2238466 , 0.7761533 ],
       [0.24065049, 0.75934947],
       [0.2518196 , 0.7481804 ],
       [0.2364422 , 0.7635578 ],
       [0.2542261 , 0.7457739 ],
       [0.22643733, 0.77356267],
       [0.25850233, 0.74149764]], dtype=float32)]
i = 0, Test true class= 
[0 0 0 1 0 1 0 0 0 1 1 0 1 1 0 0]
In grad_steps = 0, loss = 1.400383472442627
In grad_steps = 1, loss = 0.6985325813293457
In grad_steps = 2, loss = 1.1145555973052979
In grad_steps = 3, loss = 0.6792974472045898
In grad_steps = 4, loss = 0.9233527183532715
In grad_steps = 5, loss = 0.8039684295654297
In grad_steps = 6, loss = 0.8232927322387695
In grad_steps = 7, loss = 0.6979274749755859
In grad_steps = 8, loss = 0.9003580808639526
In grad_steps = 9, loss = 0.7666846513748169
In grad_steps = 10, loss = 0.7276240587234497
In grad_steps = 11, loss = 0.7359632253646851
In grad_steps = 12, loss = 0.7343488931655884
In grad_steps = 13, loss = 1.173398494720459
In grad_steps = 14, loss = 1.0357484817504883
In grad_steps = 15, loss = 0.5210391283035278
In grad_steps = 16, loss = 0.7060277462005615
In grad_steps = 17, loss = 0.7387393712997437
In grad_steps = 18, loss = 0.6978054046630859
In grad_steps = 19, loss = 0.7142459750175476
In grad_steps = 20, loss = 0.6226863265037537
In grad_steps = 21, loss = 0.7007802724838257
In grad_steps = 22, loss = 0.9447365403175354
In grad_steps = 23, loss = 0.9225989580154419
In grad_steps = 24, loss = 0.6619867086410522
In grad_steps = 25, loss = 0.6924874782562256
In grad_steps = 26, loss = 0.7175084352493286
In grad_steps = 27, loss = 0.6908489465713501
In grad_steps = 28, loss = 0.7486832141876221
In grad_steps = 29, loss = 0.6958556175231934
In grad_steps = 30, loss = 0.9003162980079651
In grad_steps = 31, loss = 0.5297172665596008
In grad_steps = 32, loss = 0.5520197749137878
In grad_steps = 33, loss = 1.0137174129486084
In grad_steps = 34, loss = 0.44420844316482544
In grad_steps = 35, loss = 0.7323412895202637
In grad_steps = 36, loss = 0.4464834928512573
In grad_steps = 37, loss = 0.7295284271240234
In grad_steps = 38, loss = 0.7606229782104492
In grad_steps = 39, loss = 0.7397860884666443
In grad_steps = 40, loss = 0.44146767258644104
In grad_steps = 41, loss = 0.7573667764663696
In grad_steps = 42, loss = 1.0673127174377441
In grad_steps = 43, loss = 0.46107757091522217
In grad_steps = 44, loss = 0.9640159010887146
In grad_steps = 45, loss = 0.7336136698722839
In grad_steps = 46, loss = 0.7195947170257568
In grad_steps = 47, loss = 0.7345912456512451
In grad_steps = 48, loss = 0.6802051663398743
In grad_steps = 49, loss = 0.7057323455810547
In grad_steps = 50, loss = 0.7173042297363281
In grad_steps = 51, loss = 0.5191303491592407
In grad_steps = 52, loss = 0.6738151907920837
In grad_steps = 53, loss = 0.7556380033493042
In grad_steps = 54, loss = 1.0784587860107422
In grad_steps = 55, loss = 0.7886533737182617
In grad_steps = 56, loss = 1.0104186534881592
In grad_steps = 57, loss = 0.7235845327377319
In grad_steps = 58, loss = 0.8851039409637451
In grad_steps = 59, loss = 0.7082549929618835
In grad_steps = 60, loss = 0.6992874145507812
In grad_steps = 61, loss = 0.698685884475708
In grad_steps = 62, loss = 0.7599387168884277
In grad_steps = 63, loss = 1.0525273084640503
In grad_steps = 64, loss = 0.9685676097869873
In grad_steps = 65, loss = 0.6803915500640869
In grad_steps = 66, loss = 0.541187047958374
In grad_steps = 67, loss = 0.6779422760009766
In grad_steps = 68, loss = 0.8193589448928833
In grad_steps = 69, loss = 0.690455436706543
In grad_steps = 70, loss = 0.712975263595581
In grad_steps = 71, loss = 0.6756142377853394
In grad_steps = 72, loss = 0.8410735130310059
In grad_steps = 73, loss = 0.8165565133094788
In grad_steps = 74, loss = 0.7024165391921997
In grad_steps = 75, loss = 0.6697169542312622
In grad_steps = 76, loss = 0.6625339984893799
In grad_steps = 77, loss = 0.7298910617828369
In grad_steps = 78, loss = 0.7484450340270996
In grad_steps = 79, loss = 0.6392574310302734
In grad_steps = 80, loss = 0.6958721876144409
In grad_steps = 81, loss = 0.704596996307373
In grad_steps = 82, loss = 0.6981539726257324
In grad_steps = 83, loss = 0.7030894756317139
In grad_steps = 84, loss = 0.7277579307556152
In grad_steps = 85, loss = 0.6760960817337036
In grad_steps = 86, loss = 0.7288607954978943
In grad_steps = 87, loss = 0.7229753732681274
In grad_steps = 88, loss = 0.74103844165802
In grad_steps = 89, loss = 0.6855069398880005
In grad_steps = 90, loss = 0.6831979751586914
In grad_steps = 91, loss = 0.6759375333786011
In grad_steps = 92, loss = 0.7196446657180786
In grad_steps = 93, loss = 0.6752628087997437
In grad_steps = 94, loss = 0.7815502285957336
In grad_steps = 95, loss = 0.6073482632637024
In grad_steps = 96, loss = 0.6339932680130005
In grad_steps = 97, loss = 0.8635317087173462
In grad_steps = 98, loss = 0.5367507338523865
In grad_steps = 99, loss = 0.6977919340133667
In grad_steps = 100, loss = 0.5207442045211792
In grad_steps = 101, loss = 0.7121154069900513
In grad_steps = 102, loss = 0.7128170728683472
In grad_steps = 103, loss = 0.705951452255249
In grad_steps = 104, loss = 0.4817412197589874
In grad_steps = 105, loss = 0.7279813885688782
In grad_steps = 106, loss = 0.9784672260284424
In grad_steps = 107, loss = 0.45994943380355835
In grad_steps = 108, loss = 0.9795500040054321
In grad_steps = 109, loss = 0.7276142835617065
In grad_steps = 110, loss = 0.7231200337409973
In grad_steps = 111, loss = 0.8777593970298767
In grad_steps = 112, loss = 0.6864340305328369
In grad_steps = 113, loss = 0.6950418949127197
In grad_steps = 114, loss = 0.690507173538208
In grad_steps = 115, loss = 0.7202317118644714
In grad_steps = 116, loss = 0.6588975191116333
In grad_steps = 117, loss = 0.6852803826332092
In grad_steps = 118, loss = 0.8170144557952881
In grad_steps = 119, loss = 0.7183494567871094
In grad_steps = 120, loss = 0.8310582637786865
In grad_steps = 121, loss = 0.682153582572937
In grad_steps = 122, loss = 0.8107281923294067
In grad_steps = 123, loss = 0.7031384706497192
In grad_steps = 124, loss = 0.6857882142066956
In grad_steps = 125, loss = 0.6864070892333984
In grad_steps = 126, loss = 0.7121654152870178
In grad_steps = 127, loss = 0.8405643701553345
In grad_steps = 128, loss = 0.7984597682952881
In grad_steps = 129, loss = 0.663444995880127
In grad_steps = 130, loss = 0.5829487442970276
In grad_steps = 131, loss = 0.6781853437423706
In grad_steps = 132, loss = 0.8007932305335999
In grad_steps = 133, loss = 0.6756681203842163
In grad_steps = 134, loss = 0.6993530988693237
In grad_steps = 135, loss = 0.6630425453186035
In grad_steps = 136, loss = 0.6718069911003113
In grad_steps = 137, loss = 0.6695544719696045
In grad_steps = 138, loss = 0.6965578198432922
In grad_steps = 139, loss = 0.6505270004272461
In grad_steps = 140, loss = 0.6678198575973511
In grad_steps = 141, loss = 0.6989732980728149
In grad_steps = 142, loss = 0.7270021438598633
In grad_steps = 143, loss = 0.6371794939041138
In grad_steps = 144, loss = 0.6916202902793884
In grad_steps = 145, loss = 0.6830442547798157
In grad_steps = 146, loss = 0.678545355796814
In grad_steps = 147, loss = 0.6811308860778809
In grad_steps = 148, loss = 0.6944518089294434
In grad_steps = 149, loss = 0.6460036039352417
In grad_steps = 150, loss = 0.7637981176376343
In grad_steps = 151, loss = 0.7484357357025146
In grad_steps = 152, loss = 0.6921498775482178
In grad_steps = 153, loss = 0.6498622894287109
In grad_steps = 154, loss = 0.6904860734939575
In grad_steps = 155, loss = 0.6684513688087463
In grad_steps = 156, loss = 0.6843651533126831
In grad_steps = 157, loss = 0.6409357786178589
In grad_steps = 158, loss = 0.7612583637237549
In grad_steps = 159, loss = 0.5918231010437012
In grad_steps = 160, loss = 0.6185795068740845
In grad_steps = 161, loss = 0.8810021877288818
In grad_steps = 162, loss = 0.5091371536254883
In grad_steps = 163, loss = 0.6832778453826904
In grad_steps = 164, loss = 0.47036880254745483
In grad_steps = 165, loss = 0.7195330262184143
In grad_steps = 166, loss = 0.6933323740959167
In grad_steps = 167, loss = 0.7315567135810852
In grad_steps = 168, loss = 0.4210032522678375
In grad_steps = 169, loss = 0.7006092667579651
In grad_steps = 170, loss = 1.024701476097107
In grad_steps = 171, loss = 0.4081041216850281
In grad_steps = 172, loss = 0.9297958612442017
In grad_steps = 173, loss = 0.7158396244049072
In grad_steps = 174, loss = 0.690878689289093
In grad_steps = 175, loss = 0.7544357776641846
In grad_steps = 176, loss = 0.6507800817489624
In grad_steps = 177, loss = 0.6563922166824341
In grad_steps = 178, loss = 0.6945957541465759
In grad_steps = 179, loss = 0.6246280670166016
In grad_steps = 180, loss = 0.6490440368652344
In grad_steps = 181, loss = 0.7216548919677734
In grad_steps = 182, loss = 0.9428696632385254
In grad_steps = 183, loss = 0.6958981156349182
In grad_steps = 184, loss = 0.9352831244468689
In grad_steps = 185, loss = 0.6253963708877563
In grad_steps = 186, loss = 0.8150269389152527
In grad_steps = 187, loss = 0.6386747360229492
In grad_steps = 188, loss = 0.6676971912384033
In grad_steps = 189, loss = 0.6752392053604126
In grad_steps = 190, loss = 0.7105872631072998
In grad_steps = 191, loss = 0.9637744426727295
In grad_steps = 192, loss = 0.9097430109977722
In grad_steps = 193, loss = 0.6675909757614136
In grad_steps = 194, loss = 0.5504475235939026
In grad_steps = 195, loss = 0.6599942445755005
In grad_steps = 196, loss = 0.7626802921295166
In grad_steps = 197, loss = 0.6588693857192993
In grad_steps = 198, loss = 0.6643083095550537
In grad_steps = 199, loss = 0.6259770393371582
In grad_steps = 200, loss = 0.7144584655761719
In grad_steps = 201, loss = 0.7216962575912476
In grad_steps = 202, loss = 0.6820108890533447
In grad_steps = 203, loss = 0.6126121282577515
In grad_steps = 204, loss = 0.6338294744491577
In grad_steps = 205, loss = 0.5629483461380005
In grad_steps = 206, loss = 0.6555641889572144
In grad_steps = 207, loss = 0.5778497457504272
In grad_steps = 208, loss = 0.6138669848442078
In grad_steps = 209, loss = 0.5834758281707764
In grad_steps = 210, loss = 0.6215195059776306
In grad_steps = 211, loss = 0.5345791578292847
In grad_steps = 212, loss = 0.6327707171440125
In grad_steps = 213, loss = 0.4177718758583069
In grad_steps = 214, loss = 0.8118114471435547
In grad_steps = 215, loss = 0.7619670629501343
In grad_steps = 216, loss = 0.7268750667572021
In grad_steps = 217, loss = 0.447870135307312
In grad_steps = 218, loss = 0.46208399534225464
In grad_steps = 219, loss = 0.45018982887268066
In grad_steps = 220, loss = 0.6582641005516052
In grad_steps = 221, loss = 0.20901352167129517
In grad_steps = 222, loss = 1.5616600513458252
In grad_steps = 223, loss = 0.3135458528995514
In grad_steps = 224, loss = 0.687537431716919
In grad_steps = 225, loss = 0.2533824145793915
In grad_steps = 226, loss = 0.6152122020721436
In grad_steps = 227, loss = 0.4828110933303833
In grad_steps = 228, loss = 0.39915186166763306
In grad_steps = 229, loss = 0.44902703166007996
In grad_steps = 230, loss = 0.24741068482398987
In grad_steps = 231, loss = 0.7960328459739685
In grad_steps = 232, loss = 0.27806949615478516
In grad_steps = 233, loss = 0.3900103271007538
In grad_steps = 234, loss = 0.4302991032600403
In grad_steps = 235, loss = 0.276352196931839
In grad_steps = 236, loss = 0.01987602189183235
In grad_steps = 237, loss = 1.355865240097046
In grad_steps = 238, loss = 0.9329889416694641
In grad_steps = 239, loss = 0.09570468217134476
In grad_steps = 240, loss = 1.076635718345642
In grad_steps = 241, loss = 0.36008208990097046
In grad_steps = 242, loss = 0.6785752773284912
In grad_steps = 243, loss = 0.5525332689285278
In grad_steps = 244, loss = 0.48605746030807495
In grad_steps = 245, loss = 0.7277588844299316
In grad_steps = 246, loss = 0.5952200293540955
In grad_steps = 247, loss = 0.43350690603256226
In grad_steps = 248, loss = 0.5040208101272583
In grad_steps = 249, loss = 0.3696342706680298
In grad_steps = 250, loss = 0.427280068397522
In grad_steps = 251, loss = 0.09786030650138855
In grad_steps = 252, loss = 0.5771727561950684
In grad_steps = 253, loss = 0.6661779284477234
In grad_steps = 254, loss = 1.1303545236587524
In grad_steps = 255, loss = 1.7817726135253906
i = 1, Test ensemble probabilities = 
[array([[0.27423537, 0.72576463],
       [0.25977612, 0.7402239 ],
       [0.28192466, 0.71807534],
       [0.24212478, 0.75787526],
       [0.32004422, 0.6799558 ],
       [0.22683299, 0.773167  ],
       [0.2593551 , 0.74064493],
       [0.27376398, 0.72623605],
       [0.2301524 , 0.7698476 ],
       [0.2238466 , 0.7761533 ],
       [0.24065049, 0.75934947],
       [0.2518196 , 0.7481804 ],
       [0.2364422 , 0.7635578 ],
       [0.2542261 , 0.7457739 ],
       [0.22643733, 0.77356267],
       [0.25850233, 0.74149764]], dtype=float32), array([[0.24502255, 0.75497746],
       [0.23063603, 0.769364  ],
       [0.34460974, 0.6553903 ],
       [0.1717802 , 0.82821983],
       [0.41938832, 0.5806117 ],
       [0.2570326 , 0.74296737],
       [0.2765226 , 0.7234774 ],
       [0.29743448, 0.7025655 ],
       [0.15905035, 0.84094965],
       [0.15455276, 0.84544724],
       [0.31955835, 0.6804416 ],
       [0.24798138, 0.7520186 ],
       [0.23535131, 0.7646487 ],
       [0.17042601, 0.829574  ],
       [0.33915058, 0.6608494 ],
       [0.5898202 , 0.4101798 ]], dtype=float32)]
i = 1, Test true class= 
[0 0 0 1 0 1 0 0 0 1 1 0 1 1 0 0]
In grad_steps = 0, loss = 1.400383472442627
In grad_steps = 1, loss = 0.7026429772377014
In grad_steps = 2, loss = 1.154126524925232
In grad_steps = 3, loss = 0.671347975730896
In grad_steps = 4, loss = 0.8721093535423279
In grad_steps = 5, loss = 0.8010281324386597
In grad_steps = 6, loss = 0.8200798034667969
In grad_steps = 7, loss = 0.7034518718719482
In grad_steps = 8, loss = 0.9033037424087524
In grad_steps = 9, loss = 0.7544345259666443
In grad_steps = 10, loss = 0.7350813746452332
In grad_steps = 11, loss = 0.7263771891593933
In grad_steps = 12, loss = 0.7344837188720703
In grad_steps = 13, loss = 1.2480568885803223
In grad_steps = 14, loss = 1.0585598945617676
In grad_steps = 15, loss = 0.5259984731674194
In grad_steps = 16, loss = 0.7123844623565674
In grad_steps = 17, loss = 0.7444314360618591
In grad_steps = 18, loss = 0.7134225368499756
In grad_steps = 19, loss = 0.7178028225898743
In grad_steps = 20, loss = 0.6435070633888245
In grad_steps = 21, loss = 0.6998617649078369
In grad_steps = 22, loss = 0.8925950527191162
In grad_steps = 23, loss = 0.8957207798957825
In grad_steps = 24, loss = 0.6861186027526855
In grad_steps = 25, loss = 0.6808964610099792
In grad_steps = 26, loss = 0.6894824504852295
In grad_steps = 27, loss = 0.6952877044677734
In grad_steps = 28, loss = 0.7501320838928223
In grad_steps = 29, loss = 0.6971951127052307
In grad_steps = 30, loss = 0.9044120907783508
In grad_steps = 31, loss = 0.5367929339408875
In grad_steps = 32, loss = 0.558372437953949
In grad_steps = 33, loss = 0.9979819059371948
In grad_steps = 34, loss = 0.4651097059249878
In grad_steps = 35, loss = 0.7378943562507629
In grad_steps = 36, loss = 0.45323339104652405
In grad_steps = 37, loss = 0.7214881181716919
In grad_steps = 38, loss = 0.7556644678115845
In grad_steps = 39, loss = 0.72902911901474
In grad_steps = 40, loss = 0.4526653289794922
In grad_steps = 41, loss = 0.7566760778427124
In grad_steps = 42, loss = 1.0517979860305786
In grad_steps = 43, loss = 0.4687670171260834
In grad_steps = 44, loss = 0.9394860863685608
In grad_steps = 45, loss = 0.7312743663787842
In grad_steps = 46, loss = 0.7147914171218872
In grad_steps = 47, loss = 0.7299196720123291
In grad_steps = 48, loss = 0.6791883111000061
In grad_steps = 49, loss = 0.7035273313522339
In grad_steps = 50, loss = 0.7052391767501831
In grad_steps = 51, loss = 0.5235447883605957
In grad_steps = 52, loss = 0.6683453321456909
In grad_steps = 53, loss = 0.7572464942932129
In grad_steps = 54, loss = 1.0458967685699463
In grad_steps = 55, loss = 0.7978321313858032
In grad_steps = 56, loss = 1.0096251964569092
In grad_steps = 57, loss = 0.7200087904930115
In grad_steps = 58, loss = 0.9020407199859619
In grad_steps = 59, loss = 0.7394628524780273
In grad_steps = 60, loss = 0.692156195640564
In grad_steps = 61, loss = 0.7019253969192505
In grad_steps = 62, loss = 0.7475161552429199
In grad_steps = 63, loss = 1.000528335571289
In grad_steps = 64, loss = 0.9256501197814941
In grad_steps = 65, loss = 0.6944305896759033
In grad_steps = 66, loss = 0.5406403541564941
In grad_steps = 67, loss = 0.6826930046081543
In grad_steps = 68, loss = 0.8402966260910034
In grad_steps = 69, loss = 0.7010756731033325
In grad_steps = 70, loss = 0.7075279951095581
In grad_steps = 71, loss = 0.6808582544326782
In grad_steps = 72, loss = 0.7752947807312012
In grad_steps = 73, loss = 0.7624526619911194
In grad_steps = 74, loss = 0.701780378818512
In grad_steps = 75, loss = 0.6679112911224365
In grad_steps = 76, loss = 0.6638792753219604
In grad_steps = 77, loss = 0.7397099733352661
In grad_steps = 78, loss = 0.7400422096252441
In grad_steps = 79, loss = 0.6572864055633545
In grad_steps = 80, loss = 0.7014713287353516
In grad_steps = 81, loss = 0.7009510397911072
In grad_steps = 82, loss = 0.6942712068557739
In grad_steps = 83, loss = 0.6995753049850464
In grad_steps = 84, loss = 0.7046750783920288
In grad_steps = 85, loss = 0.6856256723403931
In grad_steps = 86, loss = 0.7495837807655334
In grad_steps = 87, loss = 0.7490620017051697
In grad_steps = 88, loss = 0.7237494587898254
In grad_steps = 89, loss = 0.6740458011627197
In grad_steps = 90, loss = 0.6927752494812012
In grad_steps = 91, loss = 0.6737352013587952
In grad_steps = 92, loss = 0.7214266061782837
In grad_steps = 93, loss = 0.675263524055481
In grad_steps = 94, loss = 0.7728564143180847
In grad_steps = 95, loss = 0.6043803691864014
In grad_steps = 96, loss = 0.6380215883255005
In grad_steps = 97, loss = 0.8643014430999756
In grad_steps = 98, loss = 0.5316277742385864
In grad_steps = 99, loss = 0.6985906362533569
In grad_steps = 100, loss = 0.5196095108985901
In grad_steps = 101, loss = 0.7017980217933655
In grad_steps = 102, loss = 0.7138991355895996
In grad_steps = 103, loss = 0.7090326547622681
In grad_steps = 104, loss = 0.48258447647094727
In grad_steps = 105, loss = 0.7316150665283203
In grad_steps = 106, loss = 0.9775159955024719
In grad_steps = 107, loss = 0.4561605453491211
In grad_steps = 108, loss = 0.9829976558685303
In grad_steps = 109, loss = 0.7302252054214478
In grad_steps = 110, loss = 0.7274668216705322
In grad_steps = 111, loss = 0.8757410049438477
In grad_steps = 112, loss = 0.6963273286819458
In grad_steps = 113, loss = 0.6944324970245361
In grad_steps = 114, loss = 0.691670298576355
In grad_steps = 115, loss = 0.7188380360603333
In grad_steps = 116, loss = 0.6488832235336304
In grad_steps = 117, loss = 0.6896414756774902
In grad_steps = 118, loss = 0.8079580664634705
In grad_steps = 119, loss = 0.7171352505683899
In grad_steps = 120, loss = 0.8260643482208252
In grad_steps = 121, loss = 0.6886643171310425
In grad_steps = 122, loss = 0.8051067590713501
In grad_steps = 123, loss = 0.7046093940734863
In grad_steps = 124, loss = 0.6854109168052673
In grad_steps = 125, loss = 0.6814365386962891
In grad_steps = 126, loss = 0.7120649814605713
In grad_steps = 127, loss = 0.8366371393203735
In grad_steps = 128, loss = 0.7917286157608032
In grad_steps = 129, loss = 0.6531137228012085
In grad_steps = 130, loss = 0.588184118270874
In grad_steps = 131, loss = 0.6713180541992188
In grad_steps = 132, loss = 0.7878984212875366
In grad_steps = 133, loss = 0.6820328235626221
In grad_steps = 134, loss = 0.6929295063018799
In grad_steps = 135, loss = 0.6582597494125366
In grad_steps = 136, loss = 0.6880356073379517
In grad_steps = 137, loss = 0.6861141920089722
In grad_steps = 138, loss = 0.6946774125099182
In grad_steps = 139, loss = 0.6459957957267761
In grad_steps = 140, loss = 0.6640448570251465
In grad_steps = 141, loss = 0.692217230796814
In grad_steps = 142, loss = 0.7222175598144531
In grad_steps = 143, loss = 0.6411963701248169
In grad_steps = 144, loss = 0.700304388999939
In grad_steps = 145, loss = 0.6758584976196289
In grad_steps = 146, loss = 0.6734450459480286
In grad_steps = 147, loss = 0.681618332862854
In grad_steps = 148, loss = 0.6777033805847168
In grad_steps = 149, loss = 0.6517965197563171
In grad_steps = 150, loss = 0.7861213684082031
In grad_steps = 151, loss = 0.7675052881240845
In grad_steps = 152, loss = 0.6943936347961426
In grad_steps = 153, loss = 0.6302949786186218
In grad_steps = 154, loss = 0.6854956150054932
In grad_steps = 155, loss = 0.6573538780212402
In grad_steps = 156, loss = 0.7016893029212952
In grad_steps = 157, loss = 0.6329467296600342
In grad_steps = 158, loss = 0.76631760597229
In grad_steps = 159, loss = 0.5803697109222412
In grad_steps = 160, loss = 0.6091570258140564
In grad_steps = 161, loss = 0.9019483923912048
In grad_steps = 162, loss = 0.4888359308242798
In grad_steps = 163, loss = 0.6827179789543152
In grad_steps = 164, loss = 0.4551863372325897
In grad_steps = 165, loss = 0.7165273427963257
In grad_steps = 166, loss = 0.697907567024231
In grad_steps = 167, loss = 0.7327510714530945
In grad_steps = 168, loss = 0.4036414325237274
In grad_steps = 169, loss = 0.7149409055709839
In grad_steps = 170, loss = 1.0407419204711914
In grad_steps = 171, loss = 0.4091030955314636
In grad_steps = 172, loss = 0.9094828963279724
In grad_steps = 173, loss = 0.7150246500968933
In grad_steps = 174, loss = 0.6910789608955383
In grad_steps = 175, loss = 0.7265723943710327
In grad_steps = 176, loss = 0.6511666774749756
In grad_steps = 177, loss = 0.6545277833938599
In grad_steps = 178, loss = 0.6815682649612427
In grad_steps = 179, loss = 0.5837843418121338
In grad_steps = 180, loss = 0.6456925868988037
In grad_steps = 181, loss = 0.730113685131073
In grad_steps = 182, loss = 0.9660862684249878
In grad_steps = 183, loss = 0.7004722952842712
In grad_steps = 184, loss = 0.9590110778808594
In grad_steps = 185, loss = 0.6355102062225342
In grad_steps = 186, loss = 0.8136330246925354
In grad_steps = 187, loss = 0.6283366680145264
In grad_steps = 188, loss = 0.6691174507141113
In grad_steps = 189, loss = 0.684391975402832
In grad_steps = 190, loss = 0.718113899230957
In grad_steps = 191, loss = 1.0033888816833496
In grad_steps = 192, loss = 0.9349936842918396
In grad_steps = 193, loss = 0.6620771884918213
In grad_steps = 194, loss = 0.5451594591140747
In grad_steps = 195, loss = 0.655838668346405
In grad_steps = 196, loss = 0.7633169889450073
In grad_steps = 197, loss = 0.6614612340927124
In grad_steps = 198, loss = 0.6758198738098145
In grad_steps = 199, loss = 0.6226733922958374
In grad_steps = 200, loss = 0.7255687713623047
In grad_steps = 201, loss = 0.744886577129364
In grad_steps = 202, loss = 0.6765327453613281
In grad_steps = 203, loss = 0.6157644987106323
In grad_steps = 204, loss = 0.631420373916626
In grad_steps = 205, loss = 0.5805996656417847
In grad_steps = 206, loss = 0.66766357421875
In grad_steps = 207, loss = 0.5922213196754456
In grad_steps = 208, loss = 0.6293718814849854
In grad_steps = 209, loss = 0.5782614946365356
In grad_steps = 210, loss = 0.6365149021148682
In grad_steps = 211, loss = 0.5579980611801147
In grad_steps = 212, loss = 0.6188416481018066
In grad_steps = 213, loss = 0.4681529402732849
In grad_steps = 214, loss = 0.7478350400924683
In grad_steps = 215, loss = 0.7511716485023499
In grad_steps = 216, loss = 0.6491052508354187
In grad_steps = 217, loss = 0.4217382073402405
In grad_steps = 218, loss = 0.4517775774002075
In grad_steps = 219, loss = 0.4494408369064331
In grad_steps = 220, loss = 0.6685826182365417
In grad_steps = 221, loss = 0.20598721504211426
In grad_steps = 222, loss = 1.348746418952942
In grad_steps = 223, loss = 0.29927900433540344
In grad_steps = 224, loss = 0.6771217584609985
In grad_steps = 225, loss = 0.19695213437080383
In grad_steps = 226, loss = 0.548592209815979
In grad_steps = 227, loss = 0.5106785297393799
In grad_steps = 228, loss = 0.2800526022911072
In grad_steps = 229, loss = 0.47895577549934387
In grad_steps = 230, loss = 0.19787488877773285
In grad_steps = 231, loss = 0.6360895037651062
In grad_steps = 232, loss = 0.16192710399627686
In grad_steps = 233, loss = 0.23958620429039001
In grad_steps = 234, loss = 0.26320725679397583
In grad_steps = 235, loss = 0.7354263663291931
In grad_steps = 236, loss = 0.0036769742146134377
In grad_steps = 237, loss = 2.2496373653411865
In grad_steps = 238, loss = 1.1608576774597168
In grad_steps = 239, loss = 0.13693872094154358
In grad_steps = 240, loss = 0.6847612261772156
In grad_steps = 241, loss = 0.5384463667869568
In grad_steps = 242, loss = 0.7472595572471619
In grad_steps = 243, loss = 0.5377368330955505
In grad_steps = 244, loss = 0.5904320478439331
In grad_steps = 245, loss = 0.611994206905365
In grad_steps = 246, loss = 0.779879093170166
In grad_steps = 247, loss = 0.46572113037109375
In grad_steps = 248, loss = 0.7617201805114746
In grad_steps = 249, loss = 0.42568275332450867
In grad_steps = 250, loss = 0.7907893061637878
In grad_steps = 251, loss = 0.3190153241157532
In grad_steps = 252, loss = 0.565372884273529
In grad_steps = 253, loss = 0.5373347401618958
In grad_steps = 254, loss = 0.9007444381713867
In grad_steps = 255, loss = 1.409374713897705
i = 2, Test ensemble probabilities = 
[array([[0.27423537, 0.72576463],
       [0.25977612, 0.7402239 ],
       [0.28192466, 0.71807534],
       [0.24212478, 0.75787526],
       [0.32004422, 0.6799558 ],
       [0.22683299, 0.773167  ],
       [0.2593551 , 0.74064493],
       [0.27376398, 0.72623605],
       [0.2301524 , 0.7698476 ],
       [0.2238466 , 0.7761533 ],
       [0.24065049, 0.75934947],
       [0.2518196 , 0.7481804 ],
       [0.2364422 , 0.7635578 ],
       [0.2542261 , 0.7457739 ],
       [0.22643733, 0.77356267],
       [0.25850233, 0.74149764]], dtype=float32), array([[0.24502255, 0.75497746],
       [0.23063603, 0.769364  ],
       [0.34460974, 0.6553903 ],
       [0.1717802 , 0.82821983],
       [0.41938832, 0.5806117 ],
       [0.2570326 , 0.74296737],
       [0.2765226 , 0.7234774 ],
       [0.29743448, 0.7025655 ],
       [0.15905035, 0.84094965],
       [0.15455276, 0.84544724],
       [0.31955835, 0.6804416 ],
       [0.24798138, 0.7520186 ],
       [0.23535131, 0.7646487 ],
       [0.17042601, 0.829574  ],
       [0.33915058, 0.6608494 ],
       [0.5898202 , 0.4101798 ]], dtype=float32), array([[0.25132602, 0.74867404],
       [0.2513931 , 0.74860686],
       [0.2896903 , 0.71030974],
       [0.20258367, 0.7974164 ],
       [0.29343137, 0.7065686 ],
       [0.1844537 , 0.8155463 ],
       [0.192857  , 0.807143  ],
       [0.28377095, 0.7162291 ],
       [0.19494712, 0.8050529 ],
       [0.18430713, 0.81569284],
       [0.19259807, 0.8074019 ],
       [0.18899707, 0.8110029 ],
       [0.20273067, 0.79726934],
       [0.23009902, 0.769901  ],
       [0.25002906, 0.749971  ],
       [0.37403834, 0.6259616 ]], dtype=float32)]
i = 2, Test true class= 
[0 0 0 1 0 1 0 0 0 1 1 0 1 1 0 0]
In grad_steps = 0, loss = 1.400383472442627
In grad_steps = 1, loss = 0.7008229494094849
In grad_steps = 2, loss = 1.0944578647613525
In grad_steps = 3, loss = 0.674807071685791
In grad_steps = 4, loss = 0.9095436930656433
In grad_steps = 5, loss = 0.7876491546630859
In grad_steps = 6, loss = 0.8096271753311157
In grad_steps = 7, loss = 0.7067273855209351
In grad_steps = 8, loss = 0.9289198517799377
In grad_steps = 9, loss = 0.7785568237304688
In grad_steps = 10, loss = 0.7353399991989136
In grad_steps = 11, loss = 0.7362266182899475
In grad_steps = 12, loss = 0.7284984588623047
In grad_steps = 13, loss = 1.2341957092285156
In grad_steps = 14, loss = 1.0593335628509521
In grad_steps = 15, loss = 0.5173582434654236
In grad_steps = 16, loss = 0.7027266621589661
In grad_steps = 17, loss = 0.7489877343177795
In grad_steps = 18, loss = 0.7062438130378723
In grad_steps = 19, loss = 0.7181854248046875
In grad_steps = 20, loss = 0.6497893929481506
In grad_steps = 21, loss = 0.7030253410339355
In grad_steps = 22, loss = 0.9328741431236267
In grad_steps = 23, loss = 0.9054508209228516
In grad_steps = 24, loss = 0.6837952136993408
In grad_steps = 25, loss = 0.690434455871582
In grad_steps = 26, loss = 0.6980769038200378
In grad_steps = 27, loss = 0.6951702833175659
In grad_steps = 28, loss = 0.7537721991539001
In grad_steps = 29, loss = 0.7032620310783386
In grad_steps = 30, loss = 0.9239692687988281
In grad_steps = 31, loss = 0.5272833108901978
In grad_steps = 32, loss = 0.550112783908844
In grad_steps = 33, loss = 1.0077064037322998
In grad_steps = 34, loss = 0.4559594392776489
In grad_steps = 35, loss = 0.7313960790634155
In grad_steps = 36, loss = 0.46426528692245483
In grad_steps = 37, loss = 0.7251654863357544
In grad_steps = 38, loss = 0.7592672109603882
In grad_steps = 39, loss = 0.7306196689605713
In grad_steps = 40, loss = 0.45213958621025085
In grad_steps = 41, loss = 0.7531906366348267
In grad_steps = 42, loss = 1.047905683517456
In grad_steps = 43, loss = 0.4683758020401001
In grad_steps = 44, loss = 0.9517080783843994
In grad_steps = 45, loss = 0.7349185347557068
In grad_steps = 46, loss = 0.7204439043998718
In grad_steps = 47, loss = 0.7298955321311951
In grad_steps = 48, loss = 0.6813226938247681
In grad_steps = 49, loss = 0.706281304359436
In grad_steps = 50, loss = 0.7082864046096802
In grad_steps = 51, loss = 0.5202423334121704
In grad_steps = 52, loss = 0.6798990964889526
In grad_steps = 53, loss = 0.7572001814842224
In grad_steps = 54, loss = 1.076767921447754
In grad_steps = 55, loss = 0.7813307046890259
In grad_steps = 56, loss = 1.0046601295471191
In grad_steps = 57, loss = 0.7230206727981567
In grad_steps = 58, loss = 0.9054830074310303
In grad_steps = 59, loss = 0.728783905506134
In grad_steps = 60, loss = 0.6807619333267212
In grad_steps = 61, loss = 0.6935473680496216
In grad_steps = 62, loss = 0.7481520771980286
In grad_steps = 63, loss = 1.0100712776184082
In grad_steps = 64, loss = 0.9153136014938354
In grad_steps = 65, loss = 0.675767183303833
In grad_steps = 66, loss = 0.5446093082427979
In grad_steps = 67, loss = 0.6849457025527954
In grad_steps = 68, loss = 0.850923478603363
In grad_steps = 69, loss = 0.7007898688316345
In grad_steps = 70, loss = 0.7144187688827515
In grad_steps = 71, loss = 0.6726285219192505
In grad_steps = 72, loss = 0.7847028970718384
In grad_steps = 73, loss = 0.7626448273658752
In grad_steps = 74, loss = 0.705520749092102
In grad_steps = 75, loss = 0.6677781343460083
In grad_steps = 76, loss = 0.6652404069900513
In grad_steps = 77, loss = 0.7444126009941101
In grad_steps = 78, loss = 0.7509307861328125
In grad_steps = 79, loss = 0.6448116302490234
In grad_steps = 80, loss = 0.6971151828765869
In grad_steps = 81, loss = 0.706195592880249
In grad_steps = 82, loss = 0.6984747648239136
In grad_steps = 83, loss = 0.7008917331695557
In grad_steps = 84, loss = 0.7088112235069275
In grad_steps = 85, loss = 0.6816044449806213
In grad_steps = 86, loss = 0.7537505030632019
In grad_steps = 87, loss = 0.7541087865829468
In grad_steps = 88, loss = 0.7260242700576782
In grad_steps = 89, loss = 0.6790750026702881
In grad_steps = 90, loss = 0.6958500146865845
In grad_steps = 91, loss = 0.6741208434104919
In grad_steps = 92, loss = 0.7176756858825684
In grad_steps = 93, loss = 0.6803234815597534
In grad_steps = 94, loss = 0.7736042737960815
In grad_steps = 95, loss = 0.6057254076004028
In grad_steps = 96, loss = 0.6404439210891724
In grad_steps = 97, loss = 0.868178129196167
In grad_steps = 98, loss = 0.5292436480522156
In grad_steps = 99, loss = 0.7006032466888428
In grad_steps = 100, loss = 0.5168840289115906
In grad_steps = 101, loss = 0.712899923324585
In grad_steps = 102, loss = 0.7210697531700134
In grad_steps = 103, loss = 0.7139480113983154
In grad_steps = 104, loss = 0.481136292219162
In grad_steps = 105, loss = 0.7330116629600525
In grad_steps = 106, loss = 0.985182523727417
In grad_steps = 107, loss = 0.4598175287246704
In grad_steps = 108, loss = 0.9802514314651489
In grad_steps = 109, loss = 0.7322824001312256
In grad_steps = 110, loss = 0.7242090702056885
In grad_steps = 111, loss = 0.8770259618759155
In grad_steps = 112, loss = 0.6872699856758118
In grad_steps = 113, loss = 0.6912024617195129
In grad_steps = 114, loss = 0.6959875226020813
In grad_steps = 115, loss = 0.7234374284744263
In grad_steps = 116, loss = 0.659121036529541
In grad_steps = 117, loss = 0.6888620853424072
In grad_steps = 118, loss = 0.80649733543396
In grad_steps = 119, loss = 0.715914249420166
In grad_steps = 120, loss = 0.8251392245292664
In grad_steps = 121, loss = 0.6820404529571533
In grad_steps = 122, loss = 0.8061437606811523
In grad_steps = 123, loss = 0.7114468812942505
In grad_steps = 124, loss = 0.6878074407577515
In grad_steps = 125, loss = 0.690654993057251
In grad_steps = 126, loss = 0.7089983224868774
In grad_steps = 127, loss = 0.8397967219352722
In grad_steps = 128, loss = 0.7996388077735901
In grad_steps = 129, loss = 0.6640143394470215
In grad_steps = 130, loss = 0.5916861891746521
In grad_steps = 131, loss = 0.6743545532226562
In grad_steps = 132, loss = 0.7985047101974487
In grad_steps = 133, loss = 0.6822687387466431
In grad_steps = 134, loss = 0.6919566988945007
In grad_steps = 135, loss = 0.6584810614585876
In grad_steps = 136, loss = 0.6894486546516418
In grad_steps = 137, loss = 0.6811025142669678
In grad_steps = 138, loss = 0.6927107572555542
In grad_steps = 139, loss = 0.6509998440742493
In grad_steps = 140, loss = 0.6703951954841614
In grad_steps = 141, loss = 0.7069224715232849
In grad_steps = 142, loss = 0.7329713106155396
In grad_steps = 143, loss = 0.6398657560348511
In grad_steps = 144, loss = 0.6810186505317688
In grad_steps = 145, loss = 0.6826201677322388
In grad_steps = 146, loss = 0.6812012195587158
In grad_steps = 147, loss = 0.6825401186943054
In grad_steps = 148, loss = 0.6857429146766663
In grad_steps = 149, loss = 0.6676908731460571
In grad_steps = 150, loss = 0.7700657844543457
In grad_steps = 151, loss = 0.7557576298713684
In grad_steps = 152, loss = 0.6967317461967468
In grad_steps = 153, loss = 0.6486006379127502
In grad_steps = 154, loss = 0.6931724548339844
In grad_steps = 155, loss = 0.6570373773574829
In grad_steps = 156, loss = 0.6956541538238525
In grad_steps = 157, loss = 0.6430381536483765
In grad_steps = 158, loss = 0.751532793045044
In grad_steps = 159, loss = 0.6073652505874634
In grad_steps = 160, loss = 0.631454586982727
In grad_steps = 161, loss = 0.861111044883728
In grad_steps = 162, loss = 0.5174897909164429
In grad_steps = 163, loss = 0.6922677755355835
In grad_steps = 164, loss = 0.48900753259658813
In grad_steps = 165, loss = 0.7036163210868835
In grad_steps = 166, loss = 0.6987528800964355
In grad_steps = 167, loss = 0.7291549444198608
In grad_steps = 168, loss = 0.4345172047615051
In grad_steps = 169, loss = 0.7182184457778931
In grad_steps = 170, loss = 1.0141825675964355
In grad_steps = 171, loss = 0.4201764464378357
In grad_steps = 172, loss = 0.9555672407150269
In grad_steps = 173, loss = 0.7166918516159058
In grad_steps = 174, loss = 0.7095704674720764
In grad_steps = 175, loss = 0.8034828901290894
In grad_steps = 176, loss = 0.6615771055221558
In grad_steps = 177, loss = 0.667807936668396
In grad_steps = 178, loss = 0.6980047225952148
In grad_steps = 179, loss = 0.6646209955215454
In grad_steps = 180, loss = 0.6488886475563049
In grad_steps = 181, loss = 0.7008435726165771
In grad_steps = 182, loss = 0.8643518090248108
In grad_steps = 183, loss = 0.7038690447807312
In grad_steps = 184, loss = 0.8878117203712463
In grad_steps = 185, loss = 0.6417999863624573
In grad_steps = 186, loss = 0.8043960332870483
In grad_steps = 187, loss = 0.654863715171814
In grad_steps = 188, loss = 0.6686117649078369
In grad_steps = 189, loss = 0.6798011064529419
In grad_steps = 190, loss = 0.6987919807434082
In grad_steps = 191, loss = 0.9356902837753296
In grad_steps = 192, loss = 0.8776140213012695
In grad_steps = 193, loss = 0.6653400659561157
In grad_steps = 194, loss = 0.5575065612792969
In grad_steps = 195, loss = 0.6579176783561707
In grad_steps = 196, loss = 0.7730357646942139
In grad_steps = 197, loss = 0.6610833406448364
In grad_steps = 198, loss = 0.666386604309082
In grad_steps = 199, loss = 0.6233506202697754
In grad_steps = 200, loss = 0.698418140411377
In grad_steps = 201, loss = 0.7047982811927795
In grad_steps = 202, loss = 0.6794430017471313
In grad_steps = 203, loss = 0.611886203289032
In grad_steps = 204, loss = 0.6398643255233765
In grad_steps = 205, loss = 0.5760177373886108
In grad_steps = 206, loss = 0.6677718162536621
In grad_steps = 207, loss = 0.6149015426635742
In grad_steps = 208, loss = 0.6286500096321106
In grad_steps = 209, loss = 0.5928876399993896
In grad_steps = 210, loss = 0.6378139853477478
In grad_steps = 211, loss = 0.5799611806869507
In grad_steps = 212, loss = 0.6174517869949341
In grad_steps = 213, loss = 0.4771488606929779
In grad_steps = 214, loss = 0.8311456441879272
In grad_steps = 215, loss = 0.7807009816169739
In grad_steps = 216, loss = 0.6947612762451172
In grad_steps = 217, loss = 0.4542747735977173
In grad_steps = 218, loss = 0.4729679524898529
In grad_steps = 219, loss = 0.46233904361724854
In grad_steps = 220, loss = 0.657302737236023
In grad_steps = 221, loss = 0.27364346385002136
In grad_steps = 222, loss = 1.380920648574829
In grad_steps = 223, loss = 0.3333361744880676
In grad_steps = 224, loss = 0.5497218370437622
In grad_steps = 225, loss = 0.34364092350006104
In grad_steps = 226, loss = 0.5434714555740356
In grad_steps = 227, loss = 0.5438346266746521
In grad_steps = 228, loss = 0.3207974433898926
In grad_steps = 229, loss = 0.5961785912513733
In grad_steps = 230, loss = 0.24113737046718597
In grad_steps = 231, loss = 0.6805559396743774
In grad_steps = 232, loss = 0.42600345611572266
In grad_steps = 233, loss = 0.37033969163894653
In grad_steps = 234, loss = 0.21869604289531708
In grad_steps = 235, loss = 0.42842674255371094
In grad_steps = 236, loss = 0.004435636568814516
In grad_steps = 237, loss = 0.8019798398017883
In grad_steps = 238, loss = 1.3534353971481323
In grad_steps = 239, loss = 1.413666844367981
In grad_steps = 240, loss = 0.4252391755580902
In grad_steps = 241, loss = 0.6917305588722229
In grad_steps = 242, loss = 0.595657467842102
In grad_steps = 243, loss = 0.2607186734676361
In grad_steps = 244, loss = 1.0517466068267822
In grad_steps = 245, loss = 1.1416929960250854
In grad_steps = 246, loss = 1.2036973237991333
In grad_steps = 247, loss = 0.5766972303390503
In grad_steps = 248, loss = 0.5136932730674744
In grad_steps = 249, loss = 0.6302868127822876
In grad_steps = 250, loss = 0.325908899307251
In grad_steps = 251, loss = 0.26491379737854004
In grad_steps = 252, loss = 0.942108154296875
In grad_steps = 253, loss = 0.966895341873169
In grad_steps = 254, loss = 0.9984678626060486
In grad_steps = 255, loss = 1.6235240697860718
i = 3, Test ensemble probabilities = 
[array([[0.27423537, 0.72576463],
       [0.25977612, 0.7402239 ],
       [0.28192466, 0.71807534],
       [0.24212478, 0.75787526],
       [0.32004422, 0.6799558 ],
       [0.22683299, 0.773167  ],
       [0.2593551 , 0.74064493],
       [0.27376398, 0.72623605],
       [0.2301524 , 0.7698476 ],
       [0.2238466 , 0.7761533 ],
       [0.24065049, 0.75934947],
       [0.2518196 , 0.7481804 ],
       [0.2364422 , 0.7635578 ],
       [0.2542261 , 0.7457739 ],
       [0.22643733, 0.77356267],
       [0.25850233, 0.74149764]], dtype=float32), array([[0.24502255, 0.75497746],
       [0.23063603, 0.769364  ],
       [0.34460974, 0.6553903 ],
       [0.1717802 , 0.82821983],
       [0.41938832, 0.5806117 ],
       [0.2570326 , 0.74296737],
       [0.2765226 , 0.7234774 ],
       [0.29743448, 0.7025655 ],
       [0.15905035, 0.84094965],
       [0.15455276, 0.84544724],
       [0.31955835, 0.6804416 ],
       [0.24798138, 0.7520186 ],
       [0.23535131, 0.7646487 ],
       [0.17042601, 0.829574  ],
       [0.33915058, 0.6608494 ],
       [0.5898202 , 0.4101798 ]], dtype=float32), array([[0.25132602, 0.74867404],
       [0.2513931 , 0.74860686],
       [0.2896903 , 0.71030974],
       [0.20258367, 0.7974164 ],
       [0.29343137, 0.7065686 ],
       [0.1844537 , 0.8155463 ],
       [0.192857  , 0.807143  ],
       [0.28377095, 0.7162291 ],
       [0.19494712, 0.8050529 ],
       [0.18430713, 0.81569284],
       [0.19259807, 0.8074019 ],
       [0.18899707, 0.8110029 ],
       [0.20273067, 0.79726934],
       [0.23009902, 0.769901  ],
       [0.25002906, 0.749971  ],
       [0.37403834, 0.6259616 ]], dtype=float32), array([[0.2639196 , 0.7360804 ],
       [0.2711339 , 0.7288661 ],
       [0.27655637, 0.7234436 ],
       [0.26204956, 0.7379505 ],
       [0.27285838, 0.7271416 ],
       [0.2682997 , 0.7317003 ],
       [0.2732436 , 0.7267564 ],
       [0.25833437, 0.7416656 ],
       [0.26334977, 0.7366503 ],
       [0.26348206, 0.73651797],
       [0.25679284, 0.74320716],
       [0.26316145, 0.7368386 ],
       [0.26405376, 0.73594624],
       [0.27341464, 0.7265853 ],
       [0.26675683, 0.73324317],
       [0.26793572, 0.73206425]], dtype=float32)]
i = 3, Test true class= 
[0 0 0 1 0 1 0 0 0 1 1 0 1 1 0 0]
In grad_steps = 0, loss = 1.400383472442627
In grad_steps = 1, loss = 0.6952738761901855
In grad_steps = 2, loss = 1.1041512489318848
In grad_steps = 3, loss = 0.6794590950012207
In grad_steps = 4, loss = 0.9291453957557678
In grad_steps = 5, loss = 0.8020820617675781
In grad_steps = 6, loss = 0.8263190984725952
In grad_steps = 7, loss = 0.686365008354187
In grad_steps = 8, loss = 0.8959790468215942
In grad_steps = 9, loss = 0.7602729797363281
In grad_steps = 10, loss = 0.7416015267372131
In grad_steps = 11, loss = 0.7327231168746948
In grad_steps = 12, loss = 0.7344757318496704
In grad_steps = 13, loss = 1.1968798637390137
In grad_steps = 14, loss = 1.0300860404968262
In grad_steps = 15, loss = 0.5314626097679138
In grad_steps = 16, loss = 0.7162724137306213
In grad_steps = 17, loss = 0.7354413270950317
In grad_steps = 18, loss = 0.7070950865745544
In grad_steps = 19, loss = 0.7101346850395203
In grad_steps = 20, loss = 0.6256377696990967
In grad_steps = 21, loss = 0.7023360729217529
In grad_steps = 22, loss = 0.9404222965240479
In grad_steps = 23, loss = 0.9170454740524292
In grad_steps = 24, loss = 0.670574426651001
In grad_steps = 25, loss = 0.6968188285827637
In grad_steps = 26, loss = 0.7097212076187134
In grad_steps = 27, loss = 0.6945643424987793
In grad_steps = 28, loss = 0.7503186464309692
In grad_steps = 29, loss = 0.700410783290863
In grad_steps = 30, loss = 0.8977067470550537
In grad_steps = 31, loss = 0.5278836488723755
In grad_steps = 32, loss = 0.5566011071205139
In grad_steps = 33, loss = 1.0111817121505737
In grad_steps = 34, loss = 0.45198971033096313
In grad_steps = 35, loss = 0.7274777293205261
In grad_steps = 36, loss = 0.45771023631095886
In grad_steps = 37, loss = 0.728645384311676
In grad_steps = 38, loss = 0.7532050609588623
In grad_steps = 39, loss = 0.733352541923523
In grad_steps = 40, loss = 0.44451794028282166
In grad_steps = 41, loss = 0.7552065849304199
In grad_steps = 42, loss = 1.0528388023376465
In grad_steps = 43, loss = 0.46382492780685425
In grad_steps = 44, loss = 0.959747314453125
In grad_steps = 45, loss = 0.7354868650436401
In grad_steps = 46, loss = 0.7190794944763184
In grad_steps = 47, loss = 0.7403163313865662
In grad_steps = 48, loss = 0.6791675686836243
In grad_steps = 49, loss = 0.7029781341552734
In grad_steps = 50, loss = 0.7079000473022461
In grad_steps = 51, loss = 0.52083420753479
In grad_steps = 52, loss = 0.6673762202262878
In grad_steps = 53, loss = 0.7581815719604492
In grad_steps = 54, loss = 1.0866434574127197
In grad_steps = 55, loss = 0.7782652974128723
In grad_steps = 56, loss = 1.0077054500579834
In grad_steps = 57, loss = 0.7209436297416687
In grad_steps = 58, loss = 0.8887497186660767
In grad_steps = 59, loss = 0.7242724895477295
In grad_steps = 60, loss = 0.6921148300170898
In grad_steps = 61, loss = 0.70758056640625
In grad_steps = 62, loss = 0.7449747323989868
In grad_steps = 63, loss = 1.0186493396759033
In grad_steps = 64, loss = 0.940427839756012
In grad_steps = 65, loss = 0.6752628087997437
In grad_steps = 66, loss = 0.5431753396987915
In grad_steps = 67, loss = 0.6796863675117493
In grad_steps = 68, loss = 0.8283281922340393
In grad_steps = 69, loss = 0.7006562948226929
In grad_steps = 70, loss = 0.709202766418457
In grad_steps = 71, loss = 0.6794853210449219
In grad_steps = 72, loss = 0.8147814273834229
In grad_steps = 73, loss = 0.796160101890564
In grad_steps = 74, loss = 0.7073251605033875
In grad_steps = 75, loss = 0.6706833243370056
In grad_steps = 76, loss = 0.6660479307174683
In grad_steps = 77, loss = 0.7377288937568665
In grad_steps = 78, loss = 0.7544114589691162
In grad_steps = 79, loss = 0.6468108296394348
In grad_steps = 80, loss = 0.6943860054016113
In grad_steps = 81, loss = 0.6956951022148132
In grad_steps = 82, loss = 0.6892958283424377
In grad_steps = 83, loss = 0.7016482353210449
In grad_steps = 84, loss = 0.7221657037734985
In grad_steps = 85, loss = 0.6813703775405884
In grad_steps = 86, loss = 0.7406719923019409
In grad_steps = 87, loss = 0.7343832850456238
In grad_steps = 88, loss = 0.734967827796936
In grad_steps = 89, loss = 0.6777335405349731
In grad_steps = 90, loss = 0.6853761672973633
In grad_steps = 91, loss = 0.6783881187438965
In grad_steps = 92, loss = 0.7102522850036621
In grad_steps = 93, loss = 0.6805667877197266
In grad_steps = 94, loss = 0.7798674702644348
In grad_steps = 95, loss = 0.6014629602432251
In grad_steps = 96, loss = 0.6312410831451416
In grad_steps = 97, loss = 0.8628728985786438
In grad_steps = 98, loss = 0.5339899063110352
In grad_steps = 99, loss = 0.6995604038238525
In grad_steps = 100, loss = 0.5176513195037842
In grad_steps = 101, loss = 0.7117679715156555
In grad_steps = 102, loss = 0.7190344333648682
In grad_steps = 103, loss = 0.715161919593811
In grad_steps = 104, loss = 0.4783347547054291
In grad_steps = 105, loss = 0.7318000197410583
In grad_steps = 106, loss = 0.9860606789588928
In grad_steps = 107, loss = 0.45911580324172974
In grad_steps = 108, loss = 0.9887568950653076
In grad_steps = 109, loss = 0.7282257676124573
In grad_steps = 110, loss = 0.7249881029129028
In grad_steps = 111, loss = 0.8745297193527222
In grad_steps = 112, loss = 0.6878397464752197
In grad_steps = 113, loss = 0.692564070224762
In grad_steps = 114, loss = 0.6949862241744995
In grad_steps = 115, loss = 0.7248148322105408
In grad_steps = 116, loss = 0.6659327745437622
In grad_steps = 117, loss = 0.6890860795974731
In grad_steps = 118, loss = 0.8008285760879517
In grad_steps = 119, loss = 0.7154572606086731
In grad_steps = 120, loss = 0.8262308835983276
In grad_steps = 121, loss = 0.6791213154792786
In grad_steps = 122, loss = 0.8009213209152222
In grad_steps = 123, loss = 0.7104815244674683
In grad_steps = 124, loss = 0.6856566071510315
In grad_steps = 125, loss = 0.6842511892318726
In grad_steps = 126, loss = 0.7119169235229492
In grad_steps = 127, loss = 0.8410260081291199
In grad_steps = 128, loss = 0.7982696294784546
In grad_steps = 129, loss = 0.6667420864105225
In grad_steps = 130, loss = 0.5858534574508667
In grad_steps = 131, loss = 0.6720640659332275
In grad_steps = 132, loss = 0.7965654730796814
In grad_steps = 133, loss = 0.6810093522071838
In grad_steps = 134, loss = 0.6815934777259827
In grad_steps = 135, loss = 0.658251941204071
In grad_steps = 136, loss = 0.6605069637298584
In grad_steps = 137, loss = 0.6714543104171753
In grad_steps = 138, loss = 0.6968125700950623
In grad_steps = 139, loss = 0.6493249535560608
In grad_steps = 140, loss = 0.6781779527664185
In grad_steps = 141, loss = 0.6975237131118774
In grad_steps = 142, loss = 0.7302989959716797
In grad_steps = 143, loss = 0.6271563172340393
In grad_steps = 144, loss = 0.6853348612785339
In grad_steps = 145, loss = 0.6871182918548584
In grad_steps = 146, loss = 0.6788978576660156
In grad_steps = 147, loss = 0.68055260181427
In grad_steps = 148, loss = 0.6792880296707153
In grad_steps = 149, loss = 0.6594290733337402
In grad_steps = 150, loss = 0.7864343523979187
In grad_steps = 151, loss = 0.7540256381034851
In grad_steps = 152, loss = 0.6900973320007324
In grad_steps = 153, loss = 0.6245376467704773
In grad_steps = 154, loss = 0.6939359903335571
In grad_steps = 155, loss = 0.6664862632751465
In grad_steps = 156, loss = 0.6917548775672913
In grad_steps = 157, loss = 0.6323256492614746
In grad_steps = 158, loss = 0.7812747955322266
In grad_steps = 159, loss = 0.5836074352264404
In grad_steps = 160, loss = 0.6089242100715637
In grad_steps = 161, loss = 0.8893939256668091
In grad_steps = 162, loss = 0.5048131942749023
In grad_steps = 163, loss = 0.6733511090278625
In grad_steps = 164, loss = 0.4649866819381714
In grad_steps = 165, loss = 0.7213613390922546
In grad_steps = 166, loss = 0.692716121673584
In grad_steps = 167, loss = 0.7314729690551758
In grad_steps = 168, loss = 0.42535167932510376
In grad_steps = 169, loss = 0.7001660466194153
In grad_steps = 170, loss = 1.021010160446167
In grad_steps = 171, loss = 0.41442668437957764
In grad_steps = 172, loss = 0.9337368011474609
In grad_steps = 173, loss = 0.7060691118240356
In grad_steps = 174, loss = 0.6903862953186035
In grad_steps = 175, loss = 0.7478479146957397
In grad_steps = 176, loss = 0.657724142074585
In grad_steps = 177, loss = 0.6579263210296631
In grad_steps = 178, loss = 0.7082830667495728
In grad_steps = 179, loss = 0.6199662685394287
In grad_steps = 180, loss = 0.6492596864700317
In grad_steps = 181, loss = 0.7231780290603638
In grad_steps = 182, loss = 0.9309217929840088
In grad_steps = 183, loss = 0.7025132179260254
In grad_steps = 184, loss = 0.9333430528640747
In grad_steps = 185, loss = 0.6311986446380615
In grad_steps = 186, loss = 0.8036289215087891
In grad_steps = 187, loss = 0.6358517408370972
In grad_steps = 188, loss = 0.6776766180992126
In grad_steps = 189, loss = 0.6812931299209595
In grad_steps = 190, loss = 0.7135823965072632
In grad_steps = 191, loss = 0.9783284068107605
In grad_steps = 192, loss = 0.9178961515426636
In grad_steps = 193, loss = 0.6759451627731323
In grad_steps = 194, loss = 0.5500139594078064
In grad_steps = 195, loss = 0.663677453994751
In grad_steps = 196, loss = 0.7673840522766113
In grad_steps = 197, loss = 0.6668795347213745
In grad_steps = 198, loss = 0.6722660660743713
In grad_steps = 199, loss = 0.6297599077224731
In grad_steps = 200, loss = 0.7207116484642029
In grad_steps = 201, loss = 0.7325475215911865
In grad_steps = 202, loss = 0.6813886165618896
In grad_steps = 203, loss = 0.6216163039207458
In grad_steps = 204, loss = 0.6538383364677429
In grad_steps = 205, loss = 0.5926559567451477
In grad_steps = 206, loss = 0.6717933416366577
In grad_steps = 207, loss = 0.6022090315818787
In grad_steps = 208, loss = 0.6262409687042236
In grad_steps = 209, loss = 0.602515459060669
In grad_steps = 210, loss = 0.6376392841339111
In grad_steps = 211, loss = 0.570722758769989
In grad_steps = 212, loss = 0.634872317314148
In grad_steps = 213, loss = 0.5003881454467773
In grad_steps = 214, loss = 0.781289279460907
In grad_steps = 215, loss = 0.7099876999855042
In grad_steps = 216, loss = 0.7011476755142212
In grad_steps = 217, loss = 0.4436159133911133
In grad_steps = 218, loss = 0.4860672950744629
In grad_steps = 219, loss = 0.5119726657867432
In grad_steps = 220, loss = 0.6202605962753296
In grad_steps = 221, loss = 0.2546382248401642
In grad_steps = 222, loss = 1.2252933979034424
In grad_steps = 223, loss = 0.32007336616516113
In grad_steps = 224, loss = 0.6594051718711853
In grad_steps = 225, loss = 0.3187146782875061
In grad_steps = 226, loss = 0.5165141820907593
In grad_steps = 227, loss = 0.4855632185935974
In grad_steps = 228, loss = 0.3089500069618225
In grad_steps = 229, loss = 0.4922679662704468
In grad_steps = 230, loss = 0.2498752325773239
In grad_steps = 231, loss = 0.6758909821510315
In grad_steps = 232, loss = 0.26309043169021606
In grad_steps = 233, loss = 0.15682633221149445
In grad_steps = 234, loss = 0.18859559297561646
In grad_steps = 235, loss = 1.086952805519104
In grad_steps = 236, loss = 0.0042754667811095715
In grad_steps = 237, loss = 1.8118866682052612
In grad_steps = 238, loss = 1.4820066690444946
In grad_steps = 239, loss = 0.7952966094017029
In grad_steps = 240, loss = 0.3408138155937195
In grad_steps = 241, loss = 0.5315757393836975
In grad_steps = 242, loss = 0.3941827416419983
In grad_steps = 243, loss = 0.5659949779510498
In grad_steps = 244, loss = 0.6612298488616943
In grad_steps = 245, loss = 1.0320168733596802
In grad_steps = 246, loss = 1.1216181516647339
In grad_steps = 247, loss = 0.6861535310745239
In grad_steps = 248, loss = 0.9786850214004517
In grad_steps = 249, loss = 0.4504946172237396
In grad_steps = 250, loss = 0.5912038087844849
In grad_steps = 251, loss = 0.3621165156364441
In grad_steps = 252, loss = 0.720665454864502
In grad_steps = 253, loss = 0.7864993810653687
In grad_steps = 254, loss = 0.8116025328636169
In grad_steps = 255, loss = 1.39682137966156
i = 4, Test ensemble probabilities = 
[array([[0.27423537, 0.72576463],
       [0.25977612, 0.7402239 ],
       [0.28192466, 0.71807534],
       [0.24212478, 0.75787526],
       [0.32004422, 0.6799558 ],
       [0.22683299, 0.773167  ],
       [0.2593551 , 0.74064493],
       [0.27376398, 0.72623605],
       [0.2301524 , 0.7698476 ],
       [0.2238466 , 0.7761533 ],
       [0.24065049, 0.75934947],
       [0.2518196 , 0.7481804 ],
       [0.2364422 , 0.7635578 ],
       [0.2542261 , 0.7457739 ],
       [0.22643733, 0.77356267],
       [0.25850233, 0.74149764]], dtype=float32), array([[0.24502255, 0.75497746],
       [0.23063603, 0.769364  ],
       [0.34460974, 0.6553903 ],
       [0.1717802 , 0.82821983],
       [0.41938832, 0.5806117 ],
       [0.2570326 , 0.74296737],
       [0.2765226 , 0.7234774 ],
       [0.29743448, 0.7025655 ],
       [0.15905035, 0.84094965],
       [0.15455276, 0.84544724],
       [0.31955835, 0.6804416 ],
       [0.24798138, 0.7520186 ],
       [0.23535131, 0.7646487 ],
       [0.17042601, 0.829574  ],
       [0.33915058, 0.6608494 ],
       [0.5898202 , 0.4101798 ]], dtype=float32), array([[0.25132602, 0.74867404],
       [0.2513931 , 0.74860686],
       [0.2896903 , 0.71030974],
       [0.20258367, 0.7974164 ],
       [0.29343137, 0.7065686 ],
       [0.1844537 , 0.8155463 ],
       [0.192857  , 0.807143  ],
       [0.28377095, 0.7162291 ],
       [0.19494712, 0.8050529 ],
       [0.18430713, 0.81569284],
       [0.19259807, 0.8074019 ],
       [0.18899707, 0.8110029 ],
       [0.20273067, 0.79726934],
       [0.23009902, 0.769901  ],
       [0.25002906, 0.749971  ],
       [0.37403834, 0.6259616 ]], dtype=float32), array([[0.2639196 , 0.7360804 ],
       [0.2711339 , 0.7288661 ],
       [0.27655637, 0.7234436 ],
       [0.26204956, 0.7379505 ],
       [0.27285838, 0.7271416 ],
       [0.2682997 , 0.7317003 ],
       [0.2732436 , 0.7267564 ],
       [0.25833437, 0.7416656 ],
       [0.26334977, 0.7366503 ],
       [0.26348206, 0.73651797],
       [0.25679284, 0.74320716],
       [0.26316145, 0.7368386 ],
       [0.26405376, 0.73594624],
       [0.27341464, 0.7265853 ],
       [0.26675683, 0.73324317],
       [0.26793572, 0.73206425]], dtype=float32), array([[0.2596771 , 0.74032295],
       [0.25413135, 0.7458687 ],
       [0.28321943, 0.7167806 ],
       [0.24999812, 0.7500019 ],
       [0.2821987 , 0.71780133],
       [0.2605956 , 0.7394044 ],
       [0.27060425, 0.7293957 ],
       [0.26460287, 0.7353971 ],
       [0.2623331 , 0.7376669 ],
       [0.23187077, 0.7681292 ],
       [0.25045687, 0.74954313],
       [0.25691265, 0.7430874 ],
       [0.266567  , 0.733433  ],
       [0.26681802, 0.733182  ],
       [0.27308947, 0.7269105 ],
       [0.2711599 , 0.72884005]], dtype=float32)]
i = 4, Test true class= 
[0 0 0 1 0 1 0 0 0 1 1 0 1 1 0 0]
Final, Test average ensemble probabilities = 
[[0.25883612 0.7411639 ]
 [0.2534141  0.7465859 ]
 [0.2952001  0.70479995]
 [0.22570726 0.7742928 ]
 [0.31758422 0.68241584]
 [0.23944291 0.7605571 ]
 [0.25451654 0.74548346]
 [0.27558133 0.72441864]
 [0.22196655 0.77803344]
 [0.21161187 0.78838813]
 [0.2520113  0.74798864]
 [0.24177441 0.75822556]
 [0.24102898 0.758971  ]
 [0.23899677 0.76100326]
 [0.27109265 0.7289073 ]
 [0.3522913  0.64770865]]
Accuracy: 0.3750
MCC: -0.1291
AUC: 0.9000
Confusion Matrix:
tensor([[ 0, 10],
        [ 0,  6]])
Specificity: 0.0000
Precision (Macro): 0.1875
F1 Score (Macro): 0.2727
Expected Calibration Error (ECE): 0.3656
NLL loss: 0.9143
Main task is done! Can finish
