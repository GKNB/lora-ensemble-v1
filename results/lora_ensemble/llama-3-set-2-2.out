Job is running on partition: gpu_ss11
Job is running under account: m2616_g
/pscratch/sd/t/tianle/conda/envs/lucid_surp_2024/lib/python3.11/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:10<00:30, 10.30s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:20<00:20, 10.20s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:30<00:10, 10.11s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:32<00:00,  7.04s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:32<00:00,  8.19s/it]
Llama3 has been loaded successfully.
Model:  LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(128256, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)
          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)
          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)
          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)
)
Size of dataset:  198
In grad_steps = 0, loss = 0.19905029237270355
In grad_steps = 1, loss = 3.175783634185791
In grad_steps = 2, loss = 2.0018696784973145
In grad_steps = 3, loss = 0.2787349224090576
In grad_steps = 4, loss = 0.43959444761276245
In grad_steps = 5, loss = 0.4778178930282593
In grad_steps = 6, loss = 0.2508496940135956
In grad_steps = 7, loss = 0.16894063353538513
In grad_steps = 8, loss = 0.11486200988292694
In grad_steps = 9, loss = 0.070343978703022
In grad_steps = 10, loss = 3.870476007461548
In grad_steps = 11, loss = 0.02557063102722168
In grad_steps = 12, loss = 0.036960139870643616
In grad_steps = 13, loss = 2.294285774230957
In grad_steps = 14, loss = 2.2730329036712646
In grad_steps = 15, loss = 0.18501606583595276
In grad_steps = 16, loss = 0.28429490327835083
In grad_steps = 17, loss = 1.189378261566162
In grad_steps = 18, loss = 0.8446006178855896
In grad_steps = 19, loss = 0.644075334072113
In grad_steps = 20, loss = 0.7634496092796326
In grad_steps = 21, loss = 0.662912130355835
In grad_steps = 22, loss = 0.7435535192489624
In grad_steps = 23, loss = 0.7154986262321472
In grad_steps = 24, loss = 0.7110423445701599
In grad_steps = 25, loss = 0.5237085819244385
In grad_steps = 26, loss = 0.48517102003097534
In grad_steps = 27, loss = 0.4294150769710541
In grad_steps = 28, loss = 0.3293096423149109
In grad_steps = 29, loss = 1.7700347900390625
In grad_steps = 30, loss = 1.822596549987793
In grad_steps = 31, loss = 1.6329221725463867
In grad_steps = 32, loss = 1.2967422008514404
In grad_steps = 33, loss = 0.46398359537124634
In grad_steps = 34, loss = 0.9035100936889648
In grad_steps = 35, loss = 0.7834299206733704
In grad_steps = 36, loss = 0.5486512184143066
In grad_steps = 37, loss = 0.8951826095581055
In grad_steps = 38, loss = 1.1137609481811523
In grad_steps = 39, loss = 0.402829647064209
In grad_steps = 40, loss = 0.44690579175949097
In grad_steps = 41, loss = 1.0636509656906128
In grad_steps = 42, loss = 0.393603652715683
In grad_steps = 43, loss = 1.1154290437698364
In grad_steps = 44, loss = 1.0187135934829712
In grad_steps = 45, loss = 1.0376482009887695
In grad_steps = 46, loss = 0.9683336019515991
In grad_steps = 47, loss = 0.7025195360183716
In grad_steps = 48, loss = 0.7303595542907715
In grad_steps = 49, loss = 0.8179567456245422
In grad_steps = 50, loss = 0.6544845700263977
In grad_steps = 51, loss = 0.5812366604804993
In grad_steps = 52, loss = 0.47962677478790283
In grad_steps = 53, loss = 0.901861846446991
In grad_steps = 54, loss = 0.46247029304504395
In grad_steps = 55, loss = 0.45925790071487427
In grad_steps = 56, loss = 0.4523555338382721
In grad_steps = 57, loss = 1.127434253692627
In grad_steps = 58, loss = 0.4009939134120941
In grad_steps = 59, loss = 0.3370024859905243
In grad_steps = 60, loss = 1.2966139316558838
In grad_steps = 61, loss = 0.3720521330833435
In grad_steps = 62, loss = 1.2507826089859009
In grad_steps = 63, loss = 1.3381578922271729
In grad_steps = 64, loss = 1.1329585313796997
In grad_steps = 65, loss = 1.0865869522094727
In grad_steps = 66, loss = 0.520409345626831
In grad_steps = 67, loss = 0.5492249727249146
In grad_steps = 68, loss = 0.7339508533477783
In grad_steps = 69, loss = 0.6717312335968018
In grad_steps = 70, loss = 0.7324048280715942
In grad_steps = 71, loss = 0.6768353581428528
In grad_steps = 72, loss = 0.5989013910293579
In grad_steps = 73, loss = 0.5652157068252563
In grad_steps = 74, loss = 0.5456371307373047
In grad_steps = 75, loss = 0.983530580997467
In grad_steps = 76, loss = 1.0115561485290527
In grad_steps = 77, loss = 0.49579358100891113
In grad_steps = 78, loss = 1.0007801055908203
In grad_steps = 79, loss = 0.37160560488700867
In grad_steps = 80, loss = 1.035828948020935
In grad_steps = 81, loss = 1.0446869134902954
In grad_steps = 82, loss = 0.45283663272857666
In grad_steps = 83, loss = 0.4999930262565613
In grad_steps = 84, loss = 1.0345239639282227
In grad_steps = 85, loss = 0.5532524585723877
In grad_steps = 86, loss = 0.47696420550346375
In grad_steps = 87, loss = 0.42514491081237793
In grad_steps = 88, loss = 1.014591932296753
In grad_steps = 89, loss = 0.9464148283004761
In grad_steps = 90, loss = 0.9216371774673462
In grad_steps = 91, loss = 0.563303530216217
In grad_steps = 92, loss = 0.8728439211845398
In grad_steps = 93, loss = 0.7879143953323364
In grad_steps = 94, loss = 0.7061877250671387
In grad_steps = 95, loss = 0.6633708477020264
In grad_steps = 96, loss = 0.6671730875968933
In grad_steps = 97, loss = 0.6394171714782715
In grad_steps = 98, loss = 0.6802095174789429
In grad_steps = 99, loss = 0.5722379684448242
In grad_steps = 100, loss = 0.8055403828620911
In grad_steps = 101, loss = 0.796813428401947
In grad_steps = 102, loss = 0.5525744557380676
In grad_steps = 103, loss = 0.8799582719802856
In grad_steps = 104, loss = 0.8273319602012634
In grad_steps = 105, loss = 0.5460855960845947
In grad_steps = 106, loss = 0.8000221848487854
In grad_steps = 107, loss = 0.7725837230682373
In grad_steps = 108, loss = 0.6627199053764343
In grad_steps = 109, loss = 0.6025153994560242
In grad_steps = 110, loss = 0.6315355896949768
In grad_steps = 111, loss = 0.7926678657531738
In grad_steps = 112, loss = 0.7856661677360535
In grad_steps = 113, loss = 0.7176931500434875
In grad_steps = 114, loss = 0.7300581932067871
In grad_steps = 115, loss = 0.6709114909172058
In grad_steps = 116, loss = 0.7295898199081421
In grad_steps = 117, loss = 0.5695316791534424
In grad_steps = 118, loss = 0.7926334142684937
In grad_steps = 119, loss = 0.7749470472335815
In grad_steps = 120, loss = 0.5729888677597046
In grad_steps = 121, loss = 0.8609326481819153
In grad_steps = 122, loss = 0.8557623624801636
In grad_steps = 123, loss = 0.6301729679107666
In grad_steps = 124, loss = 0.7388515472412109
In grad_steps = 125, loss = 0.6071377992630005
In grad_steps = 126, loss = 0.8380807042121887
In grad_steps = 127, loss = 0.6666857004165649
In grad_steps = 128, loss = 0.726048469543457
In grad_steps = 129, loss = 0.7978687286376953
In grad_steps = 130, loss = 0.7092546224594116
In grad_steps = 131, loss = 0.7298575639724731
In grad_steps = 132, loss = 0.6992348432540894
In grad_steps = 133, loss = 0.5566749572753906
In grad_steps = 134, loss = 0.5932444930076599
In grad_steps = 135, loss = 0.4550665020942688
In grad_steps = 136, loss = 0.454314649105072
In grad_steps = 137, loss = 1.0619606971740723
In grad_steps = 138, loss = 0.41120949387550354
In grad_steps = 139, loss = 0.3413635790348053
In grad_steps = 140, loss = 0.3636442720890045
In grad_steps = 141, loss = 1.2771289348602295
In grad_steps = 142, loss = 1.330136775970459
In grad_steps = 143, loss = 0.3358951508998871
In grad_steps = 144, loss = 1.1748671531677246
In grad_steps = 145, loss = 1.2524479627609253
In grad_steps = 146, loss = 0.355695903301239
In grad_steps = 147, loss = 1.1536242961883545
In grad_steps = 148, loss = 1.0795598030090332
In grad_steps = 149, loss = 0.5010287761688232
In grad_steps = 150, loss = 0.9609725475311279
In grad_steps = 151, loss = 0.8824440240859985
In grad_steps = 152, loss = 0.814605712890625
In grad_steps = 153, loss = 0.7242935299873352
In grad_steps = 154, loss = 0.5514295697212219
In grad_steps = 155, loss = 1.0017151832580566
In grad_steps = 156, loss = 1.1639196872711182
In grad_steps = 157, loss = 1.1438336372375488
In grad_steps = 158, loss = 1.024072289466858
In grad_steps = 159, loss = 0.5030004382133484
In grad_steps = 160, loss = 0.5262824892997742
In grad_steps = 161, loss = 0.8412927389144897
In grad_steps = 162, loss = 0.8085403442382812
In grad_steps = 163, loss = 0.7711854577064514
In grad_steps = 164, loss = 0.6410892009735107
In grad_steps = 165, loss = 0.5651029944419861
In grad_steps = 166, loss = 0.536561131477356
In grad_steps = 167, loss = 0.5042811036109924
In grad_steps = 168, loss = 1.06546151638031
In grad_steps = 169, loss = 0.3651062250137329
In grad_steps = 170, loss = 0.34983354806900024
In grad_steps = 171, loss = 1.2225643396377563
In grad_steps = 172, loss = 1.3234246969223022
In grad_steps = 173, loss = 0.3175834119319916
In grad_steps = 174, loss = 0.31653812527656555
In grad_steps = 175, loss = 1.2746940851211548
In grad_steps = 176, loss = 1.1766645908355713
In grad_steps = 177, loss = 0.3804797828197479
In grad_steps = 178, loss = 0.41650158166885376
In grad_steps = 179, loss = 1.051001787185669
In grad_steps = 180, loss = 0.46813666820526123
In grad_steps = 181, loss = 0.9650585055351257
In grad_steps = 182, loss = 0.9090590476989746
In grad_steps = 183, loss = 0.8134542107582092
In grad_steps = 184, loss = 0.7161802649497986
In grad_steps = 185, loss = 0.6179293394088745
In grad_steps = 186, loss = 0.5484482645988464
In grad_steps = 187, loss = 1.050696611404419
In grad_steps = 188, loss = 1.1972026824951172
In grad_steps = 189, loss = 1.1519016027450562
In grad_steps = 190, loss = 1.1037206649780273
In grad_steps = 191, loss = 0.4377170205116272
In grad_steps = 192, loss = 0.9499362111091614
In grad_steps = 193, loss = 0.8665503263473511
In grad_steps = 194, loss = 0.7523950934410095
In grad_steps = 195, loss = 0.6821966767311096
In grad_steps = 196, loss = 0.7523890733718872
In grad_steps = 197, loss = 0.579984188079834
In grad_steps = 198, loss = 0.6031078100204468
In grad_steps = 199, loss = 0.8255438208580017
In grad_steps = 200, loss = 0.5463289022445679
In grad_steps = 201, loss = 0.9208334684371948
In grad_steps = 202, loss = 0.8619493246078491
In grad_steps = 203, loss = 0.9321845173835754
In grad_steps = 204, loss = 0.9332662224769592
In grad_steps = 205, loss = 0.5648369789123535
In grad_steps = 206, loss = 0.5482599139213562
In grad_steps = 207, loss = 0.5772331953048706
In grad_steps = 208, loss = 0.8021625876426697
In grad_steps = 209, loss = 0.8100769519805908
In grad_steps = 210, loss = 0.7649930715560913
In grad_steps = 211, loss = 0.6081774830818176
In grad_steps = 212, loss = 0.7356413006782532
In grad_steps = 213, loss = 0.7342593669891357
In grad_steps = 214, loss = 0.7266630530357361
In grad_steps = 215, loss = 0.6938636302947998
In grad_steps = 216, loss = 0.6461250185966492
In grad_steps = 217, loss = 0.5974133014678955
In grad_steps = 218, loss = 0.804157555103302
In grad_steps = 219, loss = 0.5420969724655151
In grad_steps = 220, loss = 0.8712451457977295
In grad_steps = 221, loss = 0.9381179809570312
In grad_steps = 222, loss = 0.8556231260299683
In grad_steps = 223, loss = 0.8894034624099731
In grad_steps = 224, loss = 0.5464143753051758
In grad_steps = 225, loss = 0.558215856552124
In grad_steps = 226, loss = 0.805540919303894
In grad_steps = 227, loss = 0.5993732810020447
In grad_steps = 228, loss = 0.8134223222732544
In grad_steps = 229, loss = 0.7632861733436584
In grad_steps = 230, loss = 0.7251165509223938
In grad_steps = 231, loss = 0.7011448740959167
In grad_steps = 232, loss = 0.6940779089927673
In grad_steps = 233, loss = 0.7343745231628418
In grad_steps = 234, loss = 0.7477594017982483
In grad_steps = 235, loss = 0.6489361524581909
In grad_steps = 236, loss = 0.7723085880279541
In grad_steps = 237, loss = 0.5519277453422546
In grad_steps = 238, loss = 0.8059350252151489
In grad_steps = 239, loss = 0.7965337038040161
In grad_steps = 240, loss = 0.5763620138168335
In grad_steps = 241, loss = 0.5975257158279419
In grad_steps = 242, loss = 0.8358875513076782
In grad_steps = 243, loss = 0.6144267916679382
In grad_steps = 244, loss = 0.5665494203567505
In grad_steps = 245, loss = 0.5225338339805603
In grad_steps = 246, loss = 0.8889473080635071
In grad_steps = 247, loss = 0.846780002117157
In grad_steps = 248, loss = 0.8628706932067871
In grad_steps = 249, loss = 0.5661889314651489
In grad_steps = 250, loss = 0.8296387195587158
In grad_steps = 251, loss = 0.8242484927177429
In grad_steps = 252, loss = 0.6357852220535278
In grad_steps = 253, loss = 0.6005367040634155
In grad_steps = 254, loss = 0.5947070121765137
In grad_steps = 255, loss = 0.5866460204124451
In grad_steps = 256, loss = 0.6117064356803894
In grad_steps = 257, loss = 0.5263308882713318
In grad_steps = 258, loss = 0.8318158984184265
In grad_steps = 259, loss = 0.802476167678833
In grad_steps = 260, loss = 0.5550401210784912
In grad_steps = 261, loss = 0.8691806197166443
In grad_steps = 262, loss = 0.8426745533943176
In grad_steps = 263, loss = 0.527794361114502
In grad_steps = 264, loss = 0.8017343282699585
In grad_steps = 265, loss = 0.7732634544372559
In grad_steps = 266, loss = 0.6207666397094727
In grad_steps = 267, loss = 0.585823655128479
In grad_steps = 268, loss = 0.6032520532608032
In grad_steps = 269, loss = 0.813724935054779
In grad_steps = 270, loss = 0.795954704284668
In grad_steps = 271, loss = 0.759380578994751
In grad_steps = 272, loss = 0.7521974444389343
In grad_steps = 273, loss = 0.6243544816970825
In grad_steps = 274, loss = 0.7830430269241333
In grad_steps = 275, loss = 0.6343730092048645
In grad_steps = 276, loss = 0.7282620668411255
In grad_steps = 277, loss = 0.6998922228813171
In grad_steps = 278, loss = 0.6344921588897705
In grad_steps = 279, loss = 0.7425110340118408
In grad_steps = 280, loss = 0.7393178343772888
In grad_steps = 281, loss = 0.6866074204444885
In grad_steps = 282, loss = 0.6662996411323547
In grad_steps = 283, loss = 0.6614204049110413
In grad_steps = 284, loss = 0.7531234622001648
In grad_steps = 285, loss = 0.6776911616325378
In grad_steps = 286, loss = 0.6979824900627136
In grad_steps = 287, loss = 0.7634215950965881
In grad_steps = 288, loss = 0.6981348991394043
In grad_steps = 289, loss = 0.7172917723655701
In grad_steps = 290, loss = 0.7007961273193359
In grad_steps = 291, loss = 0.5700403451919556
In grad_steps = 292, loss = 0.6296318769454956
In grad_steps = 293, loss = 0.5178396701812744
In grad_steps = 294, loss = 0.5229228734970093
In grad_steps = 295, loss = 0.9262439012527466
In grad_steps = 296, loss = 0.4894258975982666
In grad_steps = 297, loss = 0.40865078568458557
In grad_steps = 298, loss = 0.4338335394859314
In grad_steps = 299, loss = 1.06364107131958
In grad_steps = 300, loss = 1.1001085042953491
In grad_steps = 301, loss = 0.40476104617118835
In grad_steps = 302, loss = 0.9746291041374207
In grad_steps = 303, loss = 1.0952026844024658
In grad_steps = 304, loss = 0.40191471576690674
In grad_steps = 305, loss = 1.0349568128585815
In grad_steps = 306, loss = 0.9825590252876282
In grad_steps = 307, loss = 0.5046786069869995
In grad_steps = 308, loss = 0.9396999478340149
In grad_steps = 309, loss = 0.8851009011268616
In grad_steps = 310, loss = 0.8287904858589172
In grad_steps = 311, loss = 0.7929142713546753
In grad_steps = 312, loss = 0.6992510557174683
In grad_steps = 313, loss = 0.7118885517120361
In grad_steps = 314, loss = 0.7865456342697144
In grad_steps = 315, loss = 0.7885047197341919
In grad_steps = 316, loss = 0.7632077932357788
In grad_steps = 317, loss = 0.6076648235321045
In grad_steps = 318, loss = 0.5849345326423645
In grad_steps = 319, loss = 0.7732028961181641
In grad_steps = 320, loss = 0.768326461315155
In grad_steps = 321, loss = 0.7687565684318542
In grad_steps = 322, loss = 0.6858351826667786
In grad_steps = 323, loss = 0.6198475956916809
In grad_steps = 324, loss = 0.6036158800125122
In grad_steps = 325, loss = 0.5929338932037354
In grad_steps = 326, loss = 0.8759319186210632
In grad_steps = 327, loss = 0.4684690833091736
In grad_steps = 328, loss = 0.46419742703437805
In grad_steps = 329, loss = 0.982028067111969
In grad_steps = 330, loss = 1.075688123703003
In grad_steps = 331, loss = 0.4048610031604767
In grad_steps = 332, loss = 0.39884620904922485
In grad_steps = 333, loss = 1.1206952333450317
In grad_steps = 334, loss = 1.031014084815979
In grad_steps = 335, loss = 0.4196699559688568
In grad_steps = 336, loss = 0.4397871494293213
In grad_steps = 337, loss = 1.0354875326156616
In grad_steps = 338, loss = 0.45198357105255127
In grad_steps = 339, loss = 0.9966820478439331
In grad_steps = 340, loss = 0.9724055528640747
In grad_steps = 341, loss = 0.8952475190162659
In grad_steps = 342, loss = 0.830775260925293
In grad_steps = 343, loss = 0.7688496112823486
In grad_steps = 344, loss = 0.7244055271148682
In grad_steps = 345, loss = 0.7292101383209229
In grad_steps = 346, loss = 0.821637749671936
In grad_steps = 347, loss = 0.7756312489509583
In grad_steps = 348, loss = 0.7981350421905518
In grad_steps = 349, loss = 0.5690010190010071
In grad_steps = 350, loss = 0.7812397480010986
In grad_steps = 351, loss = 0.747805118560791
In grad_steps = 352, loss = 0.6871717572212219
In grad_steps = 353, loss = 0.7036603093147278
In grad_steps = 354, loss = 0.73517245054245
In grad_steps = 355, loss = 0.6046909689903259
In grad_steps = 356, loss = 0.6440033316612244
In grad_steps = 357, loss = 0.7272418737411499
In grad_steps = 358, loss = 0.6232157349586487
In grad_steps = 359, loss = 0.8424345254898071
In grad_steps = 360, loss = 0.754635751247406
In grad_steps = 361, loss = 0.8440943360328674
In grad_steps = 362, loss = 0.8380731344223022
In grad_steps = 363, loss = 0.6238909959793091
In grad_steps = 364, loss = 0.5917288064956665
In grad_steps = 365, loss = 0.6060846447944641
In grad_steps = 366, loss = 0.7246096134185791
In grad_steps = 367, loss = 0.7469843029975891
In grad_steps = 368, loss = 0.6957675218582153
In grad_steps = 369, loss = 0.627426028251648
In grad_steps = 370, loss = 0.6900577545166016
In grad_steps = 371, loss = 0.7230032086372375
In grad_steps = 372, loss = 0.6989029049873352
In grad_steps = 373, loss = 0.654887318611145
In grad_steps = 374, loss = 0.6363404393196106
In grad_steps = 375, loss = 0.5839080810546875
In grad_steps = 376, loss = 0.7700300812721252
In grad_steps = 377, loss = 0.5156195163726807
In grad_steps = 378, loss = 0.8332181572914124
In grad_steps = 379, loss = 0.932884931564331
In grad_steps = 380, loss = 0.8254237771034241
In grad_steps = 381, loss = 0.8877274394035339
In grad_steps = 382, loss = 0.5252853631973267
In grad_steps = 383, loss = 0.552288293838501
In grad_steps = 384, loss = 0.7911622524261475
In grad_steps = 385, loss = 0.5799463987350464
In grad_steps = 386, loss = 0.8490055203437805
In grad_steps = 387, loss = 0.7480522394180298
In grad_steps = 388, loss = 0.6461572647094727
In grad_steps = 389, loss = 0.6650064587593079
In grad_steps = 390, loss = 0.6325646042823792
In grad_steps = 391, loss = 0.759141206741333
In grad_steps = 392, loss = 0.805631160736084
In grad_steps = 393, loss = 0.5829477906227112
In grad_steps = 394, loss = 0.8320281505584717
In grad_steps = 395, loss = 0.4466803967952728
In grad_steps = 396, loss = 0.8329819440841675
In grad_steps = 397, loss = 0.8387291431427002
In grad_steps = 398, loss = 0.4888267517089844
In grad_steps = 399, loss = 0.5631178021430969
In grad_steps = 400, loss = 0.8545230627059937
In grad_steps = 401, loss = 0.603299617767334
In grad_steps = 402, loss = 0.5550214648246765
In grad_steps = 403, loss = 0.456015020608902
In grad_steps = 404, loss = 0.966947615146637
In grad_steps = 405, loss = 0.804291844367981
In grad_steps = 406, loss = 0.8839620351791382
In grad_steps = 407, loss = 0.5551238656044006
In grad_steps = 408, loss = 0.6461386680603027
In grad_steps = 409, loss = 0.7286617159843445
In grad_steps = 410, loss = 0.689638078212738
In grad_steps = 411, loss = 0.6012905240058899
In grad_steps = 412, loss = 0.5960692763328552
In grad_steps = 413, loss = 0.5305155515670776
In grad_steps = 414, loss = 0.5983334183692932
In grad_steps = 415, loss = 0.45402079820632935
In grad_steps = 416, loss = 0.7985147833824158
In grad_steps = 417, loss = 0.6966800689697266
In grad_steps = 418, loss = 0.5537154674530029
In grad_steps = 419, loss = 0.7233169674873352
In grad_steps = 420, loss = 0.6957406997680664
In grad_steps = 421, loss = 0.40090954303741455
In grad_steps = 422, loss = 0.6052398681640625
In grad_steps = 423, loss = 0.5979230403900146
In grad_steps = 424, loss = 0.7967746257781982
In grad_steps = 425, loss = 1.0355311632156372
In grad_steps = 426, loss = 0.5678820610046387
In grad_steps = 427, loss = 1.0901504755020142
In grad_steps = 428, loss = 0.9894214868545532
In grad_steps = 429, loss = 1.0040581226348877
In grad_steps = 430, loss = 0.7172039151191711
In grad_steps = 431, loss = 0.6122145056724548
In grad_steps = 432, loss = 0.9066267013549805
In grad_steps = 433, loss = 0.48491549491882324
In grad_steps = 434, loss = 0.80448317527771
In grad_steps = 435, loss = 0.7302519083023071
In grad_steps = 436, loss = 0.548053503036499
In grad_steps = 437, loss = 0.8321348428726196
In grad_steps = 438, loss = 0.7465803623199463
In grad_steps = 439, loss = 0.6258327960968018
In grad_steps = 440, loss = 0.6280920505523682
In grad_steps = 441, loss = 0.6413830518722534
In grad_steps = 442, loss = 0.7192508578300476
In grad_steps = 443, loss = 0.7119439840316772
In grad_steps = 444, loss = 0.639008641242981
In grad_steps = 445, loss = 0.6883777976036072
In grad_steps = 446, loss = 0.6151805520057678
In grad_steps = 447, loss = 0.6241278052330017
In grad_steps = 448, loss = 0.5941108465194702
In grad_steps = 449, loss = 0.4614671468734741
In grad_steps = 450, loss = 0.45577511191368103
In grad_steps = 451, loss = 0.36063051223754883
In grad_steps = 452, loss = 0.3388792872428894
In grad_steps = 453, loss = 1.4000065326690674
In grad_steps = 454, loss = 0.29346445202827454
In grad_steps = 455, loss = 0.23116090893745422
In grad_steps = 456, loss = 0.2284426987171173
In grad_steps = 457, loss = 1.6566792726516724
In grad_steps = 458, loss = 1.6026124954223633
In grad_steps = 459, loss = 0.2392522692680359
In grad_steps = 460, loss = 1.32456636428833
In grad_steps = 461, loss = 1.4130866527557373
In grad_steps = 462, loss = 0.33199143409729004
In grad_steps = 463, loss = 1.1252024173736572
In grad_steps = 464, loss = 0.9999461770057678
In grad_steps = 465, loss = 0.47415149211883545
In grad_steps = 466, loss = 0.9213254451751709
In grad_steps = 467, loss = 0.8444439768791199
In grad_steps = 468, loss = 0.7840983271598816
In grad_steps = 469, loss = 0.7421941757202148
In grad_steps = 470, loss = 0.6627698540687561
In grad_steps = 471, loss = 0.751793622970581
In grad_steps = 472, loss = 0.7863447070121765
In grad_steps = 473, loss = 0.8060865998268127
In grad_steps = 474, loss = 0.8054016828536987
In grad_steps = 475, loss = 0.5788614153862
In grad_steps = 476, loss = 0.5575653910636902
In grad_steps = 477, loss = 0.9057076573371887
In grad_steps = 478, loss = 0.7795193195343018
In grad_steps = 479, loss = 0.8131738901138306
In grad_steps = 480, loss = 0.7837268114089966
In grad_steps = 481, loss = 0.656069815158844
In grad_steps = 482, loss = 0.6347023248672485
In grad_steps = 483, loss = 0.5931600332260132
In grad_steps = 484, loss = 0.8635340332984924
In grad_steps = 485, loss = 0.44963204860687256
In grad_steps = 486, loss = 0.420949250459671
In grad_steps = 487, loss = 1.0437710285186768
In grad_steps = 488, loss = 1.2027299404144287
In grad_steps = 489, loss = 0.3588433861732483
In grad_steps = 490, loss = 0.3657955229282379
In grad_steps = 491, loss = 1.18576180934906
In grad_steps = 492, loss = 1.0559457540512085
In grad_steps = 493, loss = 0.4046156704425812
In grad_steps = 494, loss = 0.4679712951183319
In grad_steps = 495, loss = 1.0076069831848145
In grad_steps = 496, loss = 0.4637867212295532
In grad_steps = 497, loss = 0.9462584257125854
In grad_steps = 498, loss = 0.8946902751922607
In grad_steps = 499, loss = 0.7990809679031372
In grad_steps = 500, loss = 0.7479004859924316
In grad_steps = 501, loss = 0.7230027914047241
In grad_steps = 502, loss = 0.6471840739250183
In grad_steps = 503, loss = 0.7792198657989502
In grad_steps = 504, loss = 0.8587203621864319
In grad_steps = 505, loss = 0.8217483758926392
In grad_steps = 506, loss = 0.8017685413360596
In grad_steps = 507, loss = 0.5175309777259827
In grad_steps = 508, loss = 0.8400829434394836
In grad_steps = 509, loss = 0.8181895017623901
In grad_steps = 510, loss = 0.7407686114311218
In grad_steps = 511, loss = 0.6215870380401611
In grad_steps = 512, loss = 0.6712476015090942
In grad_steps = 513, loss = 0.6690510511398315
In grad_steps = 514, loss = 0.6590781807899475
In grad_steps = 515, loss = 0.7058250308036804
In grad_steps = 516, loss = 0.6368497014045715
In grad_steps = 517, loss = 0.777766227722168
In grad_steps = 518, loss = 0.7078399658203125
In grad_steps = 519, loss = 0.7816782593727112
In grad_steps = 520, loss = 0.7723000049591064
In grad_steps = 521, loss = 0.6428561806678772
In grad_steps = 522, loss = 0.6078524589538574
In grad_steps = 523, loss = 0.5923692584037781
In grad_steps = 524, loss = 0.7225674390792847
In grad_steps = 525, loss = 0.771220862865448
In grad_steps = 526, loss = 0.6643053889274597
In grad_steps = 527, loss = 0.6364432573318481
In grad_steps = 528, loss = 0.7077877521514893
In grad_steps = 529, loss = 0.7116879820823669
In grad_steps = 530, loss = 0.7167621850967407
In grad_steps = 531, loss = 0.6384265422821045
In grad_steps = 532, loss = 0.6421776413917542
In grad_steps = 533, loss = 0.5982128381729126
In grad_steps = 534, loss = 0.6064978837966919
In grad_steps = 535, loss = 0.513876736164093
In grad_steps = 536, loss = 0.6273398399353027
In grad_steps = 537, loss = 0.9105180501937866
In grad_steps = 538, loss = 0.692111074924469
In grad_steps = 539, loss = 0.7671222686767578
In grad_steps = 540, loss = 0.45616936683654785
In grad_steps = 541, loss = 0.44678792357444763
In grad_steps = 542, loss = 0.7705898880958557
In grad_steps = 543, loss = 0.4789302945137024
In grad_steps = 544, loss = 0.8849390745162964
In grad_steps = 545, loss = 0.7215346097946167
In grad_steps = 546, loss = 0.39617204666137695
In grad_steps = 547, loss = 0.4286501705646515
In grad_steps = 548, loss = 0.13963036239147186
In grad_steps = 549, loss = 1.1284700632095337
In grad_steps = 550, loss = 1.037406325340271
In grad_steps = 551, loss = 0.24131901562213898
In grad_steps = 552, loss = 0.841241717338562
In grad_steps = 553, loss = 0.17341727018356323
In grad_steps = 554, loss = 0.27680501341819763
In grad_steps = 555, loss = 0.13089439272880554
In grad_steps = 556, loss = 0.11599680036306381
In grad_steps = 557, loss = 2.100743532180786
In grad_steps = 558, loss = 0.18225325644016266
In grad_steps = 559, loss = 0.6780536770820618
In grad_steps = 560, loss = 0.6271785497665405
In grad_steps = 561, loss = 0.3697926998138428
In grad_steps = 562, loss = 1.2146211862564087
In grad_steps = 563, loss = 0.8892456293106079
In grad_steps = 564, loss = 1.0689395666122437
In grad_steps = 565, loss = 0.4518268406391144
In grad_steps = 566, loss = 0.5084149241447449
In grad_steps = 567, loss = 0.7397754788398743
In grad_steps = 568, loss = 0.558466374874115
In grad_steps = 569, loss = 0.5845929384231567
In grad_steps = 570, loss = 0.5749350190162659
In grad_steps = 571, loss = 0.2802298665046692
In grad_steps = 572, loss = 0.5557428598403931
In grad_steps = 573, loss = 0.30983832478523254
In grad_steps = 574, loss = 0.9021865725517273
In grad_steps = 575, loss = 0.659167468547821
In grad_steps = 576, loss = 0.45674848556518555
In grad_steps = 577, loss = 0.5991390347480774
In grad_steps = 578, loss = 0.43605995178222656
In grad_steps = 579, loss = 0.29361823201179504
In grad_steps = 580, loss = 0.5754598379135132
In grad_steps = 581, loss = 0.3511855900287628
In grad_steps = 582, loss = 0.18981781601905823
In grad_steps = 583, loss = 0.05255763232707977
In grad_steps = 584, loss = 0.23285141587257385
In grad_steps = 585, loss = 0.08518768846988678
In grad_steps = 586, loss = 0.0847056657075882
In grad_steps = 587, loss = 0.018311498686671257
In grad_steps = 588, loss = 0.004242230672389269
In grad_steps = 589, loss = 0.46802616119384766
In grad_steps = 590, loss = 6.949002742767334
In grad_steps = 591, loss = 0.6692841649055481
In grad_steps = 592, loss = 0.022487521171569824
In grad_steps = 593, loss = 0.38203126192092896
In grad_steps = 594, loss = 0.023365404456853867
In grad_steps = 595, loss = 0.07442466914653778
In grad_steps = 596, loss = 0.03537072241306305
In grad_steps = 597, loss = 0.056497927755117416
In grad_steps = 598, loss = 0.03303514048457146
In grad_steps = 599, loss = 2.8745462894439697
In grad_steps = 600, loss = 0.0895800068974495
In grad_steps = 601, loss = 0.619552493095398
In grad_steps = 602, loss = 1.7554986476898193
In grad_steps = 603, loss = 1.3422608375549316
In grad_steps = 604, loss = 0.9426007866859436
In grad_steps = 605, loss = 0.7492915987968445
In grad_steps = 606, loss = 0.8979125022888184
In grad_steps = 607, loss = 0.34877416491508484
In grad_steps = 608, loss = 0.39449256658554077
In grad_steps = 609, loss = 0.33191972970962524
In grad_steps = 610, loss = 0.3289732336997986
In grad_steps = 611, loss = 1.6933965682983398
In grad_steps = 612, loss = 0.2512381970882416
In grad_steps = 613, loss = 0.21213336288928986
In grad_steps = 614, loss = 0.18751494586467743
In grad_steps = 615, loss = 1.7356631755828857
In grad_steps = 616, loss = 1.6449620723724365
In grad_steps = 617, loss = 0.235007181763649
In grad_steps = 618, loss = 1.35849130153656
In grad_steps = 619, loss = 1.2497739791870117
In grad_steps = 620, loss = 0.43595606088638306
In grad_steps = 621, loss = 0.9657092690467834
In grad_steps = 622, loss = 0.8372323513031006
In grad_steps = 623, loss = 0.652670681476593
In grad_steps = 624, loss = 0.6742490530014038
In grad_steps = 625, loss = 0.6113015413284302
In grad_steps = 626, loss = 0.5572439432144165
In grad_steps = 627, loss = 0.48697876930236816
In grad_steps = 628, loss = 0.4017086923122406
In grad_steps = 629, loss = 1.2520039081573486
In grad_steps = 630, loss = 1.3279539346694946
In grad_steps = 631, loss = 1.29446280002594
i = 0, Test ensemble probabilities = 
[array([[0.6967262 , 0.3032738 ],
       [0.7028291 , 0.29717088],
       [0.70460814, 0.2953919 ],
       [0.68939936, 0.31060067],
       [0.7036712 , 0.29632884],
       [0.7036262 , 0.29637375],
       [0.6958931 , 0.30410686],
       [0.68907434, 0.31092563],
       [0.6974445 , 0.30255547],
       [0.6990177 , 0.3009823 ],
       [0.6981735 , 0.3018265 ],
       [0.7090719 , 0.29092813],
       [0.7018059 , 0.2981941 ],
       [0.70384294, 0.29615703],
       [0.69626576, 0.30373424],
       [0.6943978 , 0.3056022 ],
       [0.69633776, 0.3036622 ],
       [0.70439136, 0.29560867],
       [0.70563054, 0.2943695 ],
       [0.6988664 , 0.30113357],
       [0.69604003, 0.30395997]], dtype=float32)]
i = 0, Test true class= 
[0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 1 0 1 1 0 0]
In grad_steps = 0, loss = 0.19905029237270355
In grad_steps = 1, loss = 3.1284587383270264
In grad_steps = 2, loss = 1.9893747568130493
In grad_steps = 3, loss = 0.28657588362693787
In grad_steps = 4, loss = 0.45112842321395874
In grad_steps = 5, loss = 0.4937196969985962
In grad_steps = 6, loss = 0.2602875530719757
In grad_steps = 7, loss = 0.17717543244361877
In grad_steps = 8, loss = 0.11606845259666443
In grad_steps = 9, loss = 0.0719088762998581
In grad_steps = 10, loss = 3.7703542709350586
In grad_steps = 11, loss = 0.026308923959732056
In grad_steps = 12, loss = 0.0388963520526886
In grad_steps = 13, loss = 2.333733081817627
In grad_steps = 14, loss = 2.2364234924316406
In grad_steps = 15, loss = 0.18236908316612244
In grad_steps = 16, loss = 0.29433754086494446
In grad_steps = 17, loss = 1.2056128978729248
In grad_steps = 18, loss = 0.8111188411712646
In grad_steps = 19, loss = 0.5907803773880005
In grad_steps = 20, loss = 0.7614796757698059
In grad_steps = 21, loss = 0.6525061130523682
In grad_steps = 22, loss = 0.7738804817199707
In grad_steps = 23, loss = 0.7057058215141296
In grad_steps = 24, loss = 0.6949383616447449
In grad_steps = 25, loss = 0.5346663594245911
In grad_steps = 26, loss = 0.4812861680984497
In grad_steps = 27, loss = 0.4183397591114044
In grad_steps = 28, loss = 0.308833509683609
In grad_steps = 29, loss = 1.883673906326294
In grad_steps = 30, loss = 1.8849518299102783
In grad_steps = 31, loss = 1.732421636581421
In grad_steps = 32, loss = 1.3790501356124878
In grad_steps = 33, loss = 0.48796454071998596
In grad_steps = 34, loss = 0.8984211683273315
In grad_steps = 35, loss = 0.7564425468444824
In grad_steps = 36, loss = 0.5157605409622192
In grad_steps = 37, loss = 0.9197961688041687
In grad_steps = 38, loss = 1.141601324081421
In grad_steps = 39, loss = 0.3902084529399872
In grad_steps = 40, loss = 0.43267205357551575
In grad_steps = 41, loss = 1.0871469974517822
In grad_steps = 42, loss = 0.38674744963645935
In grad_steps = 43, loss = 1.110719919204712
In grad_steps = 44, loss = 0.9678710699081421
In grad_steps = 45, loss = 1.0289089679718018
In grad_steps = 46, loss = 0.96746426820755
In grad_steps = 47, loss = 0.7142506837844849
In grad_steps = 48, loss = 0.7453615665435791
In grad_steps = 49, loss = 0.7635601162910461
In grad_steps = 50, loss = 0.6625581979751587
In grad_steps = 51, loss = 0.6176963448524475
In grad_steps = 52, loss = 0.5519601106643677
In grad_steps = 53, loss = 0.8922535181045532
In grad_steps = 54, loss = 0.4897168278694153
In grad_steps = 55, loss = 0.5044685006141663
In grad_steps = 56, loss = 0.47871842980384827
In grad_steps = 57, loss = 1.1122286319732666
In grad_steps = 58, loss = 0.3607904016971588
In grad_steps = 59, loss = 0.32304733991622925
In grad_steps = 60, loss = 1.3361015319824219
In grad_steps = 61, loss = 0.32080817222595215
In grad_steps = 62, loss = 1.3854773044586182
In grad_steps = 63, loss = 1.3468763828277588
In grad_steps = 64, loss = 1.0995054244995117
In grad_steps = 65, loss = 1.0566561222076416
In grad_steps = 66, loss = 0.5324608683586121
In grad_steps = 67, loss = 0.5868830680847168
In grad_steps = 68, loss = 0.7199872136116028
In grad_steps = 69, loss = 0.7642641067504883
In grad_steps = 70, loss = 0.6699107885360718
In grad_steps = 71, loss = 0.6103564500808716
In grad_steps = 72, loss = 0.5268297791481018
In grad_steps = 73, loss = 0.5098090171813965
In grad_steps = 74, loss = 0.4620495140552521
In grad_steps = 75, loss = 1.0861927270889282
In grad_steps = 76, loss = 1.1274781227111816
In grad_steps = 77, loss = 0.422639399766922
In grad_steps = 78, loss = 1.1348201036453247
In grad_steps = 79, loss = 0.3638618588447571
In grad_steps = 80, loss = 1.1906981468200684
In grad_steps = 81, loss = 1.1189528703689575
In grad_steps = 82, loss = 0.40440118312835693
In grad_steps = 83, loss = 0.446138471364975
In grad_steps = 84, loss = 1.0369353294372559
In grad_steps = 85, loss = 0.503897488117218
In grad_steps = 86, loss = 0.4586106836795807
In grad_steps = 87, loss = 0.4474242925643921
In grad_steps = 88, loss = 1.0264850854873657
In grad_steps = 89, loss = 0.9587845206260681
In grad_steps = 90, loss = 0.9031827449798584
In grad_steps = 91, loss = 0.5465529561042786
In grad_steps = 92, loss = 0.8512358069419861
In grad_steps = 93, loss = 0.7846131324768066
In grad_steps = 94, loss = 0.6798321604728699
In grad_steps = 95, loss = 0.6535552740097046
In grad_steps = 96, loss = 0.6889208555221558
In grad_steps = 97, loss = 0.6727039217948914
In grad_steps = 98, loss = 0.6772904396057129
In grad_steps = 99, loss = 0.565471887588501
In grad_steps = 100, loss = 0.8210709691047668
In grad_steps = 101, loss = 0.7997480630874634
In grad_steps = 102, loss = 0.5843451619148254
In grad_steps = 103, loss = 0.8656275272369385
In grad_steps = 104, loss = 0.8598420023918152
In grad_steps = 105, loss = 0.566756546497345
In grad_steps = 106, loss = 0.7941763401031494
In grad_steps = 107, loss = 0.7739291787147522
In grad_steps = 108, loss = 0.6489989757537842
In grad_steps = 109, loss = 0.6050518751144409
In grad_steps = 110, loss = 0.6425455808639526
In grad_steps = 111, loss = 0.7964048385620117
In grad_steps = 112, loss = 0.7725077867507935
In grad_steps = 113, loss = 0.743450939655304
In grad_steps = 114, loss = 0.7085195183753967
In grad_steps = 115, loss = 0.6664508581161499
In grad_steps = 116, loss = 0.6906278729438782
In grad_steps = 117, loss = 0.5880618095397949
In grad_steps = 118, loss = 0.8317770957946777
In grad_steps = 119, loss = 0.7905149459838867
In grad_steps = 120, loss = 0.5730770230293274
In grad_steps = 121, loss = 0.8467690944671631
In grad_steps = 122, loss = 0.8495348691940308
In grad_steps = 123, loss = 0.5841848254203796
In grad_steps = 124, loss = 0.7630031108856201
In grad_steps = 125, loss = 0.6150153875350952
In grad_steps = 126, loss = 0.8188073635101318
In grad_steps = 127, loss = 0.6318625807762146
In grad_steps = 128, loss = 0.7435771226882935
In grad_steps = 129, loss = 0.767673134803772
In grad_steps = 130, loss = 0.6986544728279114
In grad_steps = 131, loss = 0.7036723494529724
In grad_steps = 132, loss = 0.6701332926750183
In grad_steps = 133, loss = 0.5793917775154114
In grad_steps = 134, loss = 0.5955949425697327
In grad_steps = 135, loss = 0.4803299307823181
In grad_steps = 136, loss = 0.4670260548591614
In grad_steps = 137, loss = 1.1035001277923584
In grad_steps = 138, loss = 0.42135560512542725
In grad_steps = 139, loss = 0.3578352928161621
In grad_steps = 140, loss = 0.3518439531326294
In grad_steps = 141, loss = 1.2663592100143433
In grad_steps = 142, loss = 1.33573317527771
In grad_steps = 143, loss = 0.34076881408691406
In grad_steps = 144, loss = 1.2032251358032227
In grad_steps = 145, loss = 1.2726032733917236
In grad_steps = 146, loss = 0.33390891551971436
In grad_steps = 147, loss = 1.1662204265594482
In grad_steps = 148, loss = 1.0912675857543945
In grad_steps = 149, loss = 0.47035831212997437
In grad_steps = 150, loss = 1.003507137298584
In grad_steps = 151, loss = 0.9188351631164551
In grad_steps = 152, loss = 0.8497073650360107
In grad_steps = 153, loss = 0.7572110295295715
In grad_steps = 154, loss = 0.6371539235115051
In grad_steps = 155, loss = 0.8824596405029297
In grad_steps = 156, loss = 0.9918400049209595
In grad_steps = 157, loss = 0.9992163181304932
In grad_steps = 158, loss = 0.9388890266418457
In grad_steps = 159, loss = 0.5399956107139587
In grad_steps = 160, loss = 0.5295259952545166
In grad_steps = 161, loss = 0.8427973985671997
In grad_steps = 162, loss = 0.8152403235435486
In grad_steps = 163, loss = 0.7926355004310608
In grad_steps = 164, loss = 0.68149334192276
In grad_steps = 165, loss = 0.6057145595550537
In grad_steps = 166, loss = 0.569169819355011
In grad_steps = 167, loss = 0.528887152671814
In grad_steps = 168, loss = 1.0487353801727295
In grad_steps = 169, loss = 0.37991878390312195
In grad_steps = 170, loss = 0.35433539748191833
In grad_steps = 171, loss = 1.241202473640442
In grad_steps = 172, loss = 1.2890225648880005
In grad_steps = 173, loss = 0.32202085852622986
In grad_steps = 174, loss = 0.3281650245189667
In grad_steps = 175, loss = 1.2595415115356445
In grad_steps = 176, loss = 1.1857213973999023
In grad_steps = 177, loss = 0.39365020394325256
In grad_steps = 178, loss = 0.4282878637313843
In grad_steps = 179, loss = 1.0358362197875977
In grad_steps = 180, loss = 0.4823644161224365
In grad_steps = 181, loss = 0.9464403986930847
In grad_steps = 182, loss = 0.9050204157829285
In grad_steps = 183, loss = 0.8503058552742004
In grad_steps = 184, loss = 0.7251460552215576
In grad_steps = 185, loss = 0.6543261408805847
In grad_steps = 186, loss = 0.5805476307868958
In grad_steps = 187, loss = 0.994591236114502
In grad_steps = 188, loss = 1.11099112033844
In grad_steps = 189, loss = 1.0882320404052734
In grad_steps = 190, loss = 1.0322962999343872
In grad_steps = 191, loss = 0.4572945833206177
In grad_steps = 192, loss = 0.9235333204269409
In grad_steps = 193, loss = 0.8411450386047363
In grad_steps = 194, loss = 0.7409327626228333
In grad_steps = 195, loss = 0.6848997473716736
In grad_steps = 196, loss = 0.7633381485939026
In grad_steps = 197, loss = 0.5942137837409973
In grad_steps = 198, loss = 0.6075049638748169
In grad_steps = 199, loss = 0.8084156513214111
In grad_steps = 200, loss = 0.5597378015518188
In grad_steps = 201, loss = 0.9045036435127258
In grad_steps = 202, loss = 0.8418288230895996
In grad_steps = 203, loss = 0.9172197580337524
In grad_steps = 204, loss = 0.9204535484313965
In grad_steps = 205, loss = 0.5817724466323853
In grad_steps = 206, loss = 0.5756922960281372
In grad_steps = 207, loss = 0.5770073533058167
In grad_steps = 208, loss = 0.8064132332801819
In grad_steps = 209, loss = 0.7968940138816833
In grad_steps = 210, loss = 0.7550830245018005
In grad_steps = 211, loss = 0.6202719807624817
In grad_steps = 212, loss = 0.7218223810195923
In grad_steps = 213, loss = 0.7405575513839722
In grad_steps = 214, loss = 0.723526120185852
In grad_steps = 215, loss = 0.6845574975013733
In grad_steps = 216, loss = 0.6604371070861816
In grad_steps = 217, loss = 0.6088173389434814
In grad_steps = 218, loss = 0.7647697329521179
In grad_steps = 219, loss = 0.5594059228897095
In grad_steps = 220, loss = 0.840521514415741
In grad_steps = 221, loss = 0.89911949634552
In grad_steps = 222, loss = 0.8312785625457764
In grad_steps = 223, loss = 0.866946816444397
In grad_steps = 224, loss = 0.5501589179039001
In grad_steps = 225, loss = 0.5662137269973755
In grad_steps = 226, loss = 0.7987550497055054
In grad_steps = 227, loss = 0.6173465251922607
In grad_steps = 228, loss = 0.7963116765022278
In grad_steps = 229, loss = 0.7745810151100159
In grad_steps = 230, loss = 0.7209979295730591
In grad_steps = 231, loss = 0.7214890122413635
In grad_steps = 232, loss = 0.6937758922576904
In grad_steps = 233, loss = 0.715487003326416
In grad_steps = 234, loss = 0.7400916814804077
In grad_steps = 235, loss = 0.6572350859642029
In grad_steps = 236, loss = 0.7516540288925171
In grad_steps = 237, loss = 0.5780566930770874
In grad_steps = 238, loss = 0.7984843850135803
In grad_steps = 239, loss = 0.7873395085334778
In grad_steps = 240, loss = 0.5881025195121765
In grad_steps = 241, loss = 0.5995082855224609
In grad_steps = 242, loss = 0.8145228028297424
In grad_steps = 243, loss = 0.6198877096176147
In grad_steps = 244, loss = 0.5686112642288208
In grad_steps = 245, loss = 0.5535987019538879
In grad_steps = 246, loss = 0.8721211552619934
In grad_steps = 247, loss = 0.84163498878479
In grad_steps = 248, loss = 0.8435242176055908
In grad_steps = 249, loss = 0.5728615522384644
In grad_steps = 250, loss = 0.7979285717010498
In grad_steps = 251, loss = 0.798546314239502
In grad_steps = 252, loss = 0.614380419254303
In grad_steps = 253, loss = 0.595098614692688
In grad_steps = 254, loss = 0.5913639068603516
In grad_steps = 255, loss = 0.5972940921783447
In grad_steps = 256, loss = 0.609221875667572
In grad_steps = 257, loss = 0.5230681300163269
In grad_steps = 258, loss = 0.8551976680755615
In grad_steps = 259, loss = 0.8052893877029419
In grad_steps = 260, loss = 0.564523458480835
In grad_steps = 261, loss = 0.861542820930481
In grad_steps = 262, loss = 0.8688428997993469
In grad_steps = 263, loss = 0.535446047782898
In grad_steps = 264, loss = 0.8177415132522583
In grad_steps = 265, loss = 0.7901241183280945
In grad_steps = 266, loss = 0.6100115180015564
In grad_steps = 267, loss = 0.5830700397491455
In grad_steps = 268, loss = 0.6040490865707397
In grad_steps = 269, loss = 0.8122273683547974
In grad_steps = 270, loss = 0.7932379245758057
In grad_steps = 271, loss = 0.7724965214729309
In grad_steps = 272, loss = 0.7508239150047302
In grad_steps = 273, loss = 0.6185053586959839
In grad_steps = 274, loss = 0.7460041642189026
In grad_steps = 275, loss = 0.6476677060127258
In grad_steps = 276, loss = 0.7390767931938171
In grad_steps = 277, loss = 0.6958057284355164
In grad_steps = 278, loss = 0.6475977301597595
In grad_steps = 279, loss = 0.7365496158599854
In grad_steps = 280, loss = 0.7403519153594971
In grad_steps = 281, loss = 0.6556872129440308
In grad_steps = 282, loss = 0.6820005178451538
In grad_steps = 283, loss = 0.6664007902145386
In grad_steps = 284, loss = 0.7390318512916565
In grad_steps = 285, loss = 0.6589198708534241
In grad_steps = 286, loss = 0.6957952380180359
In grad_steps = 287, loss = 0.7347933053970337
In grad_steps = 288, loss = 0.6831657886505127
In grad_steps = 289, loss = 0.6917605400085449
In grad_steps = 290, loss = 0.6867362856864929
In grad_steps = 291, loss = 0.5880753397941589
In grad_steps = 292, loss = 0.63353031873703
In grad_steps = 293, loss = 0.5271017551422119
In grad_steps = 294, loss = 0.5366784334182739
In grad_steps = 295, loss = 0.9466481804847717
In grad_steps = 296, loss = 0.49688082933425903
In grad_steps = 297, loss = 0.4172382652759552
In grad_steps = 298, loss = 0.41829732060432434
In grad_steps = 299, loss = 1.0763285160064697
In grad_steps = 300, loss = 1.1106700897216797
In grad_steps = 301, loss = 0.409808874130249
In grad_steps = 302, loss = 1.0064513683319092
In grad_steps = 303, loss = 1.0991137027740479
In grad_steps = 304, loss = 0.3966841697692871
In grad_steps = 305, loss = 1.0488505363464355
In grad_steps = 306, loss = 0.9968931674957275
In grad_steps = 307, loss = 0.4916509687900543
In grad_steps = 308, loss = 0.9488499164581299
In grad_steps = 309, loss = 0.9011582136154175
In grad_steps = 310, loss = 0.8487600088119507
In grad_steps = 311, loss = 0.7921772599220276
In grad_steps = 312, loss = 0.7269486784934998
In grad_steps = 313, loss = 0.6847900748252869
In grad_steps = 314, loss = 0.7518309354782104
In grad_steps = 315, loss = 0.759376585483551
In grad_steps = 316, loss = 0.7476032376289368
In grad_steps = 317, loss = 0.6400079727172852
In grad_steps = 318, loss = 0.619451105594635
In grad_steps = 319, loss = 0.7465277314186096
In grad_steps = 320, loss = 0.7485129237174988
In grad_steps = 321, loss = 0.7470927238464355
In grad_steps = 322, loss = 0.6710382699966431
In grad_steps = 323, loss = 0.6264092326164246
In grad_steps = 324, loss = 0.6103312373161316
In grad_steps = 325, loss = 0.5961716771125793
In grad_steps = 326, loss = 0.8816658854484558
In grad_steps = 327, loss = 0.47210532426834106
In grad_steps = 328, loss = 0.4500424861907959
In grad_steps = 329, loss = 1.0361475944519043
In grad_steps = 330, loss = 1.0742740631103516
In grad_steps = 331, loss = 0.39949312806129456
In grad_steps = 332, loss = 0.3909556567668915
In grad_steps = 333, loss = 1.0989069938659668
In grad_steps = 334, loss = 1.068098783493042
In grad_steps = 335, loss = 0.4227764904499054
In grad_steps = 336, loss = 0.4435807466506958
In grad_steps = 337, loss = 1.026782512664795
In grad_steps = 338, loss = 0.4723724126815796
In grad_steps = 339, loss = 0.9664919972419739
In grad_steps = 340, loss = 0.957019567489624
In grad_steps = 341, loss = 0.8984698057174683
In grad_steps = 342, loss = 0.8114919662475586
In grad_steps = 343, loss = 0.7657530307769775
In grad_steps = 344, loss = 0.7144332528114319
In grad_steps = 345, loss = 0.7374695539474487
In grad_steps = 346, loss = 0.8203607797622681
In grad_steps = 347, loss = 0.7874486446380615
In grad_steps = 348, loss = 0.7832377552986145
In grad_steps = 349, loss = 0.5604522824287415
In grad_steps = 350, loss = 0.7830451726913452
In grad_steps = 351, loss = 0.7426604628562927
In grad_steps = 352, loss = 0.6973636150360107
In grad_steps = 353, loss = 0.7040603160858154
In grad_steps = 354, loss = 0.7627633810043335
In grad_steps = 355, loss = 0.6111785769462585
In grad_steps = 356, loss = 0.6233997344970703
In grad_steps = 357, loss = 0.7379477024078369
In grad_steps = 358, loss = 0.6161542534828186
In grad_steps = 359, loss = 0.8328126668930054
In grad_steps = 360, loss = 0.7612767815589905
In grad_steps = 361, loss = 0.8392707109451294
In grad_steps = 362, loss = 0.8207841515541077
In grad_steps = 363, loss = 0.6196925044059753
In grad_steps = 364, loss = 0.6098390817642212
In grad_steps = 365, loss = 0.6003180146217346
In grad_steps = 366, loss = 0.757785975933075
In grad_steps = 367, loss = 0.7561976313591003
In grad_steps = 368, loss = 0.7145141363143921
In grad_steps = 369, loss = 0.6389968395233154
In grad_steps = 370, loss = 0.6797550320625305
In grad_steps = 371, loss = 0.7194206118583679
In grad_steps = 372, loss = 0.7139158844947815
In grad_steps = 373, loss = 0.6738225817680359
In grad_steps = 374, loss = 0.6341772079467773
In grad_steps = 375, loss = 0.5980865955352783
In grad_steps = 376, loss = 0.7232727408409119
In grad_steps = 377, loss = 0.524643063545227
In grad_steps = 378, loss = 0.8236446976661682
In grad_steps = 379, loss = 0.9003903865814209
In grad_steps = 380, loss = 0.815739631652832
In grad_steps = 381, loss = 0.8705237507820129
In grad_steps = 382, loss = 0.5209722518920898
In grad_steps = 383, loss = 0.5407994985580444
In grad_steps = 384, loss = 0.8105652928352356
In grad_steps = 385, loss = 0.5897970199584961
In grad_steps = 386, loss = 0.8344836235046387
In grad_steps = 387, loss = 0.7620740532875061
In grad_steps = 388, loss = 0.6883260607719421
In grad_steps = 389, loss = 0.709322988986969
In grad_steps = 390, loss = 0.6648563146591187
In grad_steps = 391, loss = 0.7095996141433716
In grad_steps = 392, loss = 0.7452353239059448
In grad_steps = 393, loss = 0.6278153657913208
In grad_steps = 394, loss = 0.7781029343605042
In grad_steps = 395, loss = 0.5231547355651855
In grad_steps = 396, loss = 0.8135773539543152
In grad_steps = 397, loss = 0.7928004264831543
In grad_steps = 398, loss = 0.5396467447280884
In grad_steps = 399, loss = 0.5756810903549194
In grad_steps = 400, loss = 0.8368673920631409
In grad_steps = 401, loss = 0.5983317494392395
In grad_steps = 402, loss = 0.5516535639762878
In grad_steps = 403, loss = 0.5288699269294739
In grad_steps = 404, loss = 0.9453228116035461
In grad_steps = 405, loss = 0.81722491979599
In grad_steps = 406, loss = 0.878577709197998
In grad_steps = 407, loss = 0.5435774922370911
In grad_steps = 408, loss = 0.6063673496246338
In grad_steps = 409, loss = 0.7648365497589111
In grad_steps = 410, loss = 0.6189898252487183
In grad_steps = 411, loss = 0.567305862903595
In grad_steps = 412, loss = 0.5617878437042236
In grad_steps = 413, loss = 0.5830666422843933
In grad_steps = 414, loss = 0.5754448175430298
In grad_steps = 415, loss = 0.4324576258659363
In grad_steps = 416, loss = 0.8600433468818665
In grad_steps = 417, loss = 0.7531648278236389
In grad_steps = 418, loss = 0.5440226793289185
In grad_steps = 419, loss = 0.7723773717880249
In grad_steps = 420, loss = 0.8041790723800659
In grad_steps = 421, loss = 0.4680667817592621
In grad_steps = 422, loss = 0.7287386655807495
In grad_steps = 423, loss = 0.6675650477409363
In grad_steps = 424, loss = 0.7064592242240906
In grad_steps = 425, loss = 0.7631385326385498
In grad_steps = 426, loss = 0.7397834658622742
In grad_steps = 427, loss = 0.6882411241531372
In grad_steps = 428, loss = 0.6388154625892639
In grad_steps = 429, loss = 0.6162726879119873
In grad_steps = 430, loss = 0.5145050883293152
In grad_steps = 431, loss = 0.837870717048645
In grad_steps = 432, loss = 0.6942979693412781
In grad_steps = 433, loss = 0.2790631651878357
In grad_steps = 434, loss = 1.7165563106536865
In grad_steps = 435, loss = 0.9036853313446045
In grad_steps = 436, loss = 0.4721326231956482
In grad_steps = 437, loss = 0.8684520721435547
In grad_steps = 438, loss = 0.6890918016433716
In grad_steps = 439, loss = 0.7184407114982605
In grad_steps = 440, loss = 0.47579464316368103
In grad_steps = 441, loss = 0.7730616927146912
In grad_steps = 442, loss = 0.5884231328964233
In grad_steps = 443, loss = 0.7925853133201599
In grad_steps = 444, loss = 0.5628756284713745
In grad_steps = 445, loss = 0.6243607401847839
In grad_steps = 446, loss = 0.5333516597747803
In grad_steps = 447, loss = 0.5315064191818237
In grad_steps = 448, loss = 0.5532107353210449
In grad_steps = 449, loss = 0.4547395706176758
In grad_steps = 450, loss = 0.49154990911483765
In grad_steps = 451, loss = 0.35660520195961
In grad_steps = 452, loss = 0.38405361771583557
In grad_steps = 453, loss = 1.2733023166656494
In grad_steps = 454, loss = 0.34177088737487793
In grad_steps = 455, loss = 0.2378147840499878
In grad_steps = 456, loss = 0.2192254364490509
In grad_steps = 457, loss = 1.4242944717407227
In grad_steps = 458, loss = 1.4098564386367798
In grad_steps = 459, loss = 0.27691155672073364
In grad_steps = 460, loss = 1.105597972869873
In grad_steps = 461, loss = 1.2283645868301392
In grad_steps = 462, loss = 0.3234289586544037
In grad_steps = 463, loss = 1.0915545225143433
In grad_steps = 464, loss = 0.9677817821502686
In grad_steps = 465, loss = 0.4743863642215729
In grad_steps = 466, loss = 0.9332956075668335
In grad_steps = 467, loss = 0.8565434813499451
In grad_steps = 468, loss = 0.7859636545181274
In grad_steps = 469, loss = 0.7226773500442505
In grad_steps = 470, loss = 0.6724429726600647
In grad_steps = 471, loss = 0.6800759434700012
In grad_steps = 472, loss = 0.7287021279335022
In grad_steps = 473, loss = 0.774785041809082
In grad_steps = 474, loss = 0.7718684673309326
In grad_steps = 475, loss = 0.5926822423934937
In grad_steps = 476, loss = 0.5805174112319946
In grad_steps = 477, loss = 0.8498198390007019
In grad_steps = 478, loss = 0.7520870566368103
In grad_steps = 479, loss = 0.7757229208946228
In grad_steps = 480, loss = 0.7155751585960388
In grad_steps = 481, loss = 0.6273828744888306
In grad_steps = 482, loss = 0.6131319403648376
In grad_steps = 483, loss = 0.5683172941207886
In grad_steps = 484, loss = 0.8530855178833008
In grad_steps = 485, loss = 0.4360477924346924
In grad_steps = 486, loss = 0.4014342427253723
In grad_steps = 487, loss = 1.0792878866195679
In grad_steps = 488, loss = 1.197392463684082
In grad_steps = 489, loss = 0.3246558904647827
In grad_steps = 490, loss = 0.34357261657714844
In grad_steps = 491, loss = 1.1692184209823608
In grad_steps = 492, loss = 1.073472261428833
In grad_steps = 493, loss = 0.38618162274360657
In grad_steps = 494, loss = 0.4078369438648224
In grad_steps = 495, loss = 1.0277211666107178
In grad_steps = 496, loss = 0.4708426594734192
In grad_steps = 497, loss = 0.9425867795944214
In grad_steps = 498, loss = 0.9054388999938965
In grad_steps = 499, loss = 0.8409807085990906
In grad_steps = 500, loss = 0.750611424446106
In grad_steps = 501, loss = 0.7215522527694702
In grad_steps = 502, loss = 0.6858137845993042
In grad_steps = 503, loss = 0.7307615280151367
In grad_steps = 504, loss = 0.808669924736023
In grad_steps = 505, loss = 0.7717484831809998
In grad_steps = 506, loss = 0.7732368111610413
In grad_steps = 507, loss = 0.516362190246582
In grad_steps = 508, loss = 0.7971146702766418
In grad_steps = 509, loss = 0.7530609369277954
In grad_steps = 510, loss = 0.7123296856880188
In grad_steps = 511, loss = 0.6764423847198486
In grad_steps = 512, loss = 0.7256763577461243
In grad_steps = 513, loss = 0.6516883969306946
In grad_steps = 514, loss = 0.6247559189796448
In grad_steps = 515, loss = 0.7250526547431946
In grad_steps = 516, loss = 0.6436570286750793
In grad_steps = 517, loss = 0.811205267906189
In grad_steps = 518, loss = 0.7510988712310791
In grad_steps = 519, loss = 0.8028250336647034
In grad_steps = 520, loss = 0.8046903014183044
In grad_steps = 521, loss = 0.6311021447181702
In grad_steps = 522, loss = 0.5950191020965576
In grad_steps = 523, loss = 0.5991708636283875
In grad_steps = 524, loss = 0.7059259414672852
In grad_steps = 525, loss = 0.7253299951553345
In grad_steps = 526, loss = 0.6749757528305054
In grad_steps = 527, loss = 0.6489375829696655
In grad_steps = 528, loss = 0.6404520273208618
In grad_steps = 529, loss = 0.6999076008796692
In grad_steps = 530, loss = 0.6854678988456726
In grad_steps = 531, loss = 0.6553347110748291
In grad_steps = 532, loss = 0.585207462310791
In grad_steps = 533, loss = 0.5671601295471191
In grad_steps = 534, loss = 0.5257893800735474
In grad_steps = 535, loss = 0.4269934296607971
In grad_steps = 536, loss = 0.7005866765975952
In grad_steps = 537, loss = 0.9935867786407471
In grad_steps = 538, loss = 0.7206844687461853
In grad_steps = 539, loss = 0.8692631721496582
In grad_steps = 540, loss = 0.4279063045978546
In grad_steps = 541, loss = 0.4408751428127289
In grad_steps = 542, loss = 0.7989627718925476
In grad_steps = 543, loss = 0.4820502996444702
In grad_steps = 544, loss = 0.9043680429458618
In grad_steps = 545, loss = 0.6518240571022034
In grad_steps = 546, loss = 0.3772539496421814
In grad_steps = 547, loss = 0.5453792810440063
In grad_steps = 548, loss = 0.2908437252044678
In grad_steps = 549, loss = 0.9488008618354797
In grad_steps = 550, loss = 1.126194715499878
In grad_steps = 551, loss = 0.3205151855945587
In grad_steps = 552, loss = 1.0329914093017578
In grad_steps = 553, loss = 0.1757073998451233
In grad_steps = 554, loss = 0.6062887907028198
In grad_steps = 555, loss = 0.6104166507720947
In grad_steps = 556, loss = 0.14390553534030914
In grad_steps = 557, loss = 0.5885427594184875
In grad_steps = 558, loss = 0.4703661799430847
In grad_steps = 559, loss = 0.6749579310417175
In grad_steps = 560, loss = 0.8254907727241516
In grad_steps = 561, loss = 0.5585858821868896
In grad_steps = 562, loss = 1.3064253330230713
In grad_steps = 563, loss = 0.6135044693946838
In grad_steps = 564, loss = 0.7443768382072449
In grad_steps = 565, loss = 0.62623131275177
In grad_steps = 566, loss = 0.1251145452260971
In grad_steps = 567, loss = 0.43514153361320496
In grad_steps = 568, loss = 0.8334483504295349
In grad_steps = 569, loss = 0.6789532899856567
In grad_steps = 570, loss = 0.5243192315101624
In grad_steps = 571, loss = 0.3273833096027374
In grad_steps = 572, loss = 0.44885414838790894
In grad_steps = 573, loss = 0.07100165635347366
In grad_steps = 574, loss = 1.7336864471435547
In grad_steps = 575, loss = 0.6658414602279663
In grad_steps = 576, loss = 0.2672792375087738
In grad_steps = 577, loss = 0.4252703785896301
In grad_steps = 578, loss = 0.19503732025623322
In grad_steps = 579, loss = 0.39475470781326294
In grad_steps = 580, loss = 0.7560873627662659
In grad_steps = 581, loss = 0.3112729489803314
In grad_steps = 582, loss = 0.23009668290615082
In grad_steps = 583, loss = 0.7059434056282043
In grad_steps = 584, loss = 0.41767314076423645
In grad_steps = 585, loss = 0.07342579960823059
In grad_steps = 586, loss = 0.04685376212000847
In grad_steps = 587, loss = 0.0159185528755188
In grad_steps = 588, loss = 0.008153486996889114
In grad_steps = 589, loss = 0.1874624788761139
In grad_steps = 590, loss = 1.185530662536621
In grad_steps = 591, loss = 0.005401066038757563
In grad_steps = 592, loss = 0.4616607427597046
In grad_steps = 593, loss = 2.3191070556640625
In grad_steps = 594, loss = 0.01266896165907383
In grad_steps = 595, loss = 0.491705060005188
In grad_steps = 596, loss = 0.1675325483083725
In grad_steps = 597, loss = 1.5797046422958374
In grad_steps = 598, loss = 0.02261747606098652
In grad_steps = 599, loss = 1.5994102954864502
In grad_steps = 600, loss = 0.23922468721866608
In grad_steps = 601, loss = 1.0134007930755615
In grad_steps = 602, loss = 0.573748767375946
In grad_steps = 603, loss = 0.6505602598190308
In grad_steps = 604, loss = 0.4759557545185089
In grad_steps = 605, loss = 0.6581193804740906
In grad_steps = 606, loss = 1.058138132095337
In grad_steps = 607, loss = 0.4503903090953827
In grad_steps = 608, loss = 0.5782091617584229
In grad_steps = 609, loss = 0.30796924233436584
In grad_steps = 610, loss = 0.3743116557598114
In grad_steps = 611, loss = 1.3290265798568726
In grad_steps = 612, loss = 0.32608136534690857
In grad_steps = 613, loss = 0.1199636161327362
In grad_steps = 614, loss = 0.190508171916008
In grad_steps = 615, loss = 1.6485021114349365
In grad_steps = 616, loss = 1.4615955352783203
In grad_steps = 617, loss = 0.2377217411994934
In grad_steps = 618, loss = 0.9397696256637573
In grad_steps = 619, loss = 1.1934163570404053
In grad_steps = 620, loss = 0.3967660367488861
In grad_steps = 621, loss = 0.8757354021072388
In grad_steps = 622, loss = 0.745380163192749
In grad_steps = 623, loss = 0.5931084156036377
In grad_steps = 624, loss = 0.8311611413955688
In grad_steps = 625, loss = 0.7469456791877747
In grad_steps = 626, loss = 0.6045745611190796
In grad_steps = 627, loss = 0.6473777294158936
In grad_steps = 628, loss = 0.46684393286705017
In grad_steps = 629, loss = 0.9728100895881653
In grad_steps = 630, loss = 0.9404182434082031
In grad_steps = 631, loss = 0.9169110655784607
i = 1, Test ensemble probabilities = 
[array([[0.6967262 , 0.3032738 ],
       [0.7028291 , 0.29717088],
       [0.70460814, 0.2953919 ],
       [0.68939936, 0.31060067],
       [0.7036712 , 0.29632884],
       [0.7036262 , 0.29637375],
       [0.6958931 , 0.30410686],
       [0.68907434, 0.31092563],
       [0.6974445 , 0.30255547],
       [0.6990177 , 0.3009823 ],
       [0.6981735 , 0.3018265 ],
       [0.7090719 , 0.29092813],
       [0.7018059 , 0.2981941 ],
       [0.70384294, 0.29615703],
       [0.69626576, 0.30373424],
       [0.6943978 , 0.3056022 ],
       [0.69633776, 0.3036622 ],
       [0.70439136, 0.29560867],
       [0.70563054, 0.2943695 ],
       [0.6988664 , 0.30113357],
       [0.69604003, 0.30395997]], dtype=float32), array([[0.6744076 , 0.32559237],
       [0.63860315, 0.36139688],
       [0.654905  , 0.34509492],
       [0.56939536, 0.43060464],
       [0.681396  , 0.318604  ],
       [0.60607004, 0.39392996],
       [0.6339681 , 0.36603186],
       [0.608821  , 0.39117903],
       [0.6456442 , 0.35435578],
       [0.63895434, 0.3610457 ],
       [0.59237045, 0.4076295 ],
       [0.6148586 , 0.38514137],
       [0.6200046 , 0.3799954 ],
       [0.62975216, 0.37024784],
       [0.61797065, 0.38202932],
       [0.60365945, 0.39634055],
       [0.6642158 , 0.3357842 ],
       [0.6495719 , 0.35042813],
       [0.62240934, 0.37759066],
       [0.6177311 , 0.3822689 ],
       [0.54644036, 0.45355964]], dtype=float32)]
i = 1, Test true class= 
[0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 1 0 1 1 0 0]
In grad_steps = 0, loss = 0.19905029237270355
In grad_steps = 1, loss = 3.2109432220458984
In grad_steps = 2, loss = 2.0222856998443604
In grad_steps = 3, loss = 0.28422412276268005
In grad_steps = 4, loss = 0.4552708864212036
In grad_steps = 5, loss = 0.4881729483604431
In grad_steps = 6, loss = 0.2706438899040222
In grad_steps = 7, loss = 0.18914435803890228
In grad_steps = 8, loss = 0.13191251456737518
In grad_steps = 9, loss = 0.08171460032463074
In grad_steps = 10, loss = 3.672419548034668
In grad_steps = 11, loss = 0.0280561875551939
In grad_steps = 12, loss = 0.040593162178993225
In grad_steps = 13, loss = 2.292293071746826
In grad_steps = 14, loss = 2.3341939449310303
In grad_steps = 15, loss = 0.18884871900081635
In grad_steps = 16, loss = 0.29493746161460876
In grad_steps = 17, loss = 1.1827640533447266
In grad_steps = 18, loss = 0.8176935315132141
In grad_steps = 19, loss = 0.6363306045532227
In grad_steps = 20, loss = 0.7488411068916321
In grad_steps = 21, loss = 0.6409592032432556
In grad_steps = 22, loss = 0.7574065327644348
In grad_steps = 23, loss = 0.7170418500900269
In grad_steps = 24, loss = 0.6859833002090454
In grad_steps = 25, loss = 0.5290008187294006
In grad_steps = 26, loss = 0.47687777876853943
In grad_steps = 27, loss = 0.4452446699142456
In grad_steps = 28, loss = 0.3360733687877655
In grad_steps = 29, loss = 1.798397421836853
In grad_steps = 30, loss = 1.8636926412582397
In grad_steps = 31, loss = 1.658436894416809
In grad_steps = 32, loss = 1.373233437538147
In grad_steps = 33, loss = 0.44292306900024414
In grad_steps = 34, loss = 0.8763125538825989
In grad_steps = 35, loss = 0.7997332811355591
In grad_steps = 36, loss = 0.56833416223526
In grad_steps = 37, loss = 0.8675523996353149
In grad_steps = 38, loss = 1.0647203922271729
In grad_steps = 39, loss = 0.4197363257408142
In grad_steps = 40, loss = 0.4581681489944458
In grad_steps = 41, loss = 1.0485135316848755
In grad_steps = 42, loss = 0.4028599262237549
In grad_steps = 43, loss = 1.0859400033950806
In grad_steps = 44, loss = 0.982257068157196
In grad_steps = 45, loss = 1.0617547035217285
In grad_steps = 46, loss = 0.976096510887146
In grad_steps = 47, loss = 0.7099698781967163
In grad_steps = 48, loss = 0.7338358163833618
In grad_steps = 49, loss = 0.7651735544204712
In grad_steps = 50, loss = 0.6804542541503906
In grad_steps = 51, loss = 0.639304518699646
In grad_steps = 52, loss = 0.5633955001831055
In grad_steps = 53, loss = 0.8477085828781128
In grad_steps = 54, loss = 0.5136180520057678
In grad_steps = 55, loss = 0.5115756988525391
In grad_steps = 56, loss = 0.5038944482803345
In grad_steps = 57, loss = 1.0766692161560059
In grad_steps = 58, loss = 0.3736686706542969
In grad_steps = 59, loss = 0.3351128101348877
In grad_steps = 60, loss = 1.2915414571762085
In grad_steps = 61, loss = 0.32322898507118225
In grad_steps = 62, loss = 1.32086181640625
In grad_steps = 63, loss = 1.348305106163025
In grad_steps = 64, loss = 1.1040750741958618
In grad_steps = 65, loss = 1.0587215423583984
In grad_steps = 66, loss = 0.5208955407142639
In grad_steps = 67, loss = 0.5705708861351013
In grad_steps = 68, loss = 0.7381005883216858
In grad_steps = 69, loss = 0.7508621215820312
In grad_steps = 70, loss = 0.6859614849090576
In grad_steps = 71, loss = 0.6232153177261353
In grad_steps = 72, loss = 0.5357608795166016
In grad_steps = 73, loss = 0.5176908373832703
In grad_steps = 74, loss = 0.46978604793548584
In grad_steps = 75, loss = 1.083938479423523
In grad_steps = 76, loss = 1.1013762950897217
In grad_steps = 77, loss = 0.4312736988067627
In grad_steps = 78, loss = 1.1306930780410767
In grad_steps = 79, loss = 0.37201663851737976
In grad_steps = 80, loss = 1.1878259181976318
In grad_steps = 81, loss = 1.1127498149871826
In grad_steps = 82, loss = 0.4140157997608185
In grad_steps = 83, loss = 0.4460362195968628
In grad_steps = 84, loss = 1.0362510681152344
In grad_steps = 85, loss = 0.5024750232696533
In grad_steps = 86, loss = 0.4573688209056854
In grad_steps = 87, loss = 0.4499364495277405
In grad_steps = 88, loss = 1.0118790864944458
In grad_steps = 89, loss = 0.969697117805481
In grad_steps = 90, loss = 0.9008988738059998
In grad_steps = 91, loss = 0.552278995513916
In grad_steps = 92, loss = 0.8495697975158691
In grad_steps = 93, loss = 0.7788302898406982
In grad_steps = 94, loss = 0.6952885389328003
In grad_steps = 95, loss = 0.6676486134529114
In grad_steps = 96, loss = 0.7068525552749634
In grad_steps = 97, loss = 0.6775137186050415
In grad_steps = 98, loss = 0.6897255182266235
In grad_steps = 99, loss = 0.5774121880531311
In grad_steps = 100, loss = 0.8130093216896057
In grad_steps = 101, loss = 0.7912957668304443
In grad_steps = 102, loss = 0.588759183883667
In grad_steps = 103, loss = 0.8875159025192261
In grad_steps = 104, loss = 0.8676983714103699
In grad_steps = 105, loss = 0.5602915287017822
In grad_steps = 106, loss = 0.809602677822113
In grad_steps = 107, loss = 0.7748637199401855
In grad_steps = 108, loss = 0.6484382152557373
In grad_steps = 109, loss = 0.606330931186676
In grad_steps = 110, loss = 0.6376474499702454
In grad_steps = 111, loss = 0.7951728701591492
In grad_steps = 112, loss = 0.7713720798492432
In grad_steps = 113, loss = 0.7380209565162659
In grad_steps = 114, loss = 0.7172390818595886
In grad_steps = 115, loss = 0.6759127974510193
In grad_steps = 116, loss = 0.6758875250816345
In grad_steps = 117, loss = 0.5960564613342285
In grad_steps = 118, loss = 0.8344317674636841
In grad_steps = 119, loss = 0.797761857509613
In grad_steps = 120, loss = 0.5747212171554565
In grad_steps = 121, loss = 0.8474687337875366
In grad_steps = 122, loss = 0.8410388231277466
In grad_steps = 123, loss = 0.5859784483909607
In grad_steps = 124, loss = 0.7706893682479858
In grad_steps = 125, loss = 0.6128889918327332
In grad_steps = 126, loss = 0.8113070130348206
In grad_steps = 127, loss = 0.6313025951385498
In grad_steps = 128, loss = 0.7247171998023987
In grad_steps = 129, loss = 0.7600618004798889
In grad_steps = 130, loss = 0.6886636018753052
In grad_steps = 131, loss = 0.6990678310394287
In grad_steps = 132, loss = 0.6669418215751648
In grad_steps = 133, loss = 0.5802435874938965
In grad_steps = 134, loss = 0.5925986766815186
In grad_steps = 135, loss = 0.4785667061805725
In grad_steps = 136, loss = 0.4705251455307007
In grad_steps = 137, loss = 1.0903011560440063
In grad_steps = 138, loss = 0.39587417244911194
In grad_steps = 139, loss = 0.347112238407135
In grad_steps = 140, loss = 0.3473881185054779
In grad_steps = 141, loss = 1.2936598062515259
In grad_steps = 142, loss = 1.3556655645370483
In grad_steps = 143, loss = 0.33858034014701843
In grad_steps = 144, loss = 1.2065989971160889
In grad_steps = 145, loss = 1.2738077640533447
In grad_steps = 146, loss = 0.3477610647678375
In grad_steps = 147, loss = 1.1821238994598389
In grad_steps = 148, loss = 1.0925463438034058
In grad_steps = 149, loss = 0.4820331931114197
In grad_steps = 150, loss = 0.9844335913658142
In grad_steps = 151, loss = 0.8908345699310303
In grad_steps = 152, loss = 0.829473078250885
In grad_steps = 153, loss = 0.7130908370018005
In grad_steps = 154, loss = 0.5937311053276062
In grad_steps = 155, loss = 0.9505936503410339
In grad_steps = 156, loss = 1.0810351371765137
In grad_steps = 157, loss = 1.0737730264663696
In grad_steps = 158, loss = 1.000974416732788
In grad_steps = 159, loss = 0.5147544145584106
In grad_steps = 160, loss = 0.5173348784446716
In grad_steps = 161, loss = 0.8417038917541504
In grad_steps = 162, loss = 0.8199808597564697
In grad_steps = 163, loss = 0.7866359949111938
In grad_steps = 164, loss = 0.6608935594558716
In grad_steps = 165, loss = 0.5980698466300964
In grad_steps = 166, loss = 0.5554513931274414
In grad_steps = 167, loss = 0.5182538032531738
In grad_steps = 168, loss = 1.0595426559448242
In grad_steps = 169, loss = 0.37244221568107605
In grad_steps = 170, loss = 0.34403368830680847
In grad_steps = 171, loss = 1.2686805725097656
In grad_steps = 172, loss = 1.2961345911026
In grad_steps = 173, loss = 0.31591737270355225
In grad_steps = 174, loss = 0.32469895482063293
In grad_steps = 175, loss = 1.2695767879486084
In grad_steps = 176, loss = 1.1712185144424438
In grad_steps = 177, loss = 0.3963838815689087
In grad_steps = 178, loss = 0.4298356771469116
In grad_steps = 179, loss = 1.0305631160736084
In grad_steps = 180, loss = 0.4922357499599457
In grad_steps = 181, loss = 0.9391782283782959
In grad_steps = 182, loss = 0.8867620825767517
In grad_steps = 183, loss = 0.8270518779754639
In grad_steps = 184, loss = 0.7024771571159363
In grad_steps = 185, loss = 0.616323709487915
In grad_steps = 186, loss = 0.539816677570343
In grad_steps = 187, loss = 1.0648703575134277
In grad_steps = 188, loss = 1.197523593902588
In grad_steps = 189, loss = 1.1666104793548584
In grad_steps = 190, loss = 1.095337152481079
In grad_steps = 191, loss = 0.4330419600009918
In grad_steps = 192, loss = 0.9496169090270996
In grad_steps = 193, loss = 0.842656135559082
In grad_steps = 194, loss = 0.739298939704895
In grad_steps = 195, loss = 0.6878389120101929
In grad_steps = 196, loss = 0.7757846117019653
In grad_steps = 197, loss = 0.5806299448013306
In grad_steps = 198, loss = 0.5936474204063416
In grad_steps = 199, loss = 0.8258740305900574
In grad_steps = 200, loss = 0.5447729825973511
In grad_steps = 201, loss = 0.9200795292854309
In grad_steps = 202, loss = 0.8563932180404663
In grad_steps = 203, loss = 0.9338159561157227
In grad_steps = 204, loss = 0.9259501695632935
In grad_steps = 205, loss = 0.5720394253730774
In grad_steps = 206, loss = 0.5744611024856567
In grad_steps = 207, loss = 0.5663208961486816
In grad_steps = 208, loss = 0.8169757127761841
In grad_steps = 209, loss = 0.8099541664123535
In grad_steps = 210, loss = 0.7602019309997559
In grad_steps = 211, loss = 0.6190061569213867
In grad_steps = 212, loss = 0.7287386655807495
In grad_steps = 213, loss = 0.7430949211120605
In grad_steps = 214, loss = 0.723063588142395
In grad_steps = 215, loss = 0.6894755959510803
In grad_steps = 216, loss = 0.6554405689239502
In grad_steps = 217, loss = 0.6084461212158203
In grad_steps = 218, loss = 0.7623375058174133
In grad_steps = 219, loss = 0.5628534555435181
In grad_steps = 220, loss = 0.8469520807266235
In grad_steps = 221, loss = 0.9163028001785278
In grad_steps = 222, loss = 0.8416811227798462
In grad_steps = 223, loss = 0.8707865476608276
In grad_steps = 224, loss = 0.5539262890815735
In grad_steps = 225, loss = 0.567924976348877
In grad_steps = 226, loss = 0.7948800325393677
In grad_steps = 227, loss = 0.619666576385498
In grad_steps = 228, loss = 0.7953779697418213
In grad_steps = 229, loss = 0.7642213106155396
In grad_steps = 230, loss = 0.7169071435928345
In grad_steps = 231, loss = 0.714275062084198
In grad_steps = 232, loss = 0.6792678833007812
In grad_steps = 233, loss = 0.7250752449035645
In grad_steps = 234, loss = 0.771033763885498
In grad_steps = 235, loss = 0.6391369104385376
In grad_steps = 236, loss = 0.7739056348800659
In grad_steps = 237, loss = 0.569425642490387
In grad_steps = 238, loss = 0.8219706416130066
In grad_steps = 239, loss = 0.7998540997505188
In grad_steps = 240, loss = 0.5794873833656311
In grad_steps = 241, loss = 0.5875107645988464
In grad_steps = 242, loss = 0.820932149887085
In grad_steps = 243, loss = 0.6091530919075012
In grad_steps = 244, loss = 0.5621461868286133
In grad_steps = 245, loss = 0.546072781085968
In grad_steps = 246, loss = 0.8817005753517151
In grad_steps = 247, loss = 0.8463616967201233
In grad_steps = 248, loss = 0.8518327474594116
In grad_steps = 249, loss = 0.5637069344520569
In grad_steps = 250, loss = 0.7932139039039612
In grad_steps = 251, loss = 0.800130307674408
In grad_steps = 252, loss = 0.6158736348152161
In grad_steps = 253, loss = 0.5957353711128235
In grad_steps = 254, loss = 0.5983326435089111
In grad_steps = 255, loss = 0.5960906744003296
In grad_steps = 256, loss = 0.6073687076568604
In grad_steps = 257, loss = 0.5242764353752136
In grad_steps = 258, loss = 0.8556910753250122
In grad_steps = 259, loss = 0.8207152485847473
In grad_steps = 260, loss = 0.5621109008789062
In grad_steps = 261, loss = 0.8650499582290649
In grad_steps = 262, loss = 0.8725163340568542
In grad_steps = 263, loss = 0.545124888420105
In grad_steps = 264, loss = 0.8191454410552979
In grad_steps = 265, loss = 0.7911716103553772
In grad_steps = 266, loss = 0.6069276928901672
In grad_steps = 267, loss = 0.5823919177055359
In grad_steps = 268, loss = 0.6082347631454468
In grad_steps = 269, loss = 0.8199626207351685
In grad_steps = 270, loss = 0.7820098996162415
In grad_steps = 271, loss = 0.7741214036941528
In grad_steps = 272, loss = 0.7456412315368652
In grad_steps = 273, loss = 0.6185598969459534
In grad_steps = 274, loss = 0.7383435368537903
In grad_steps = 275, loss = 0.6561845541000366
In grad_steps = 276, loss = 0.744086503982544
In grad_steps = 277, loss = 0.7020778059959412
In grad_steps = 278, loss = 0.6425491571426392
In grad_steps = 279, loss = 0.7387977838516235
In grad_steps = 280, loss = 0.7353829741477966
In grad_steps = 281, loss = 0.6694536209106445
In grad_steps = 282, loss = 0.6820892095565796
In grad_steps = 283, loss = 0.668136477470398
In grad_steps = 284, loss = 0.7389600872993469
In grad_steps = 285, loss = 0.6598849892616272
In grad_steps = 286, loss = 0.7067781686782837
In grad_steps = 287, loss = 0.7247546315193176
In grad_steps = 288, loss = 0.6770267486572266
In grad_steps = 289, loss = 0.6890254020690918
In grad_steps = 290, loss = 0.6826419234275818
In grad_steps = 291, loss = 0.5997551679611206
In grad_steps = 292, loss = 0.6318304538726807
In grad_steps = 293, loss = 0.5212976336479187
In grad_steps = 294, loss = 0.5300734639167786
In grad_steps = 295, loss = 0.9493526220321655
In grad_steps = 296, loss = 0.49733883142471313
In grad_steps = 297, loss = 0.42126643657684326
In grad_steps = 298, loss = 0.42206311225891113
In grad_steps = 299, loss = 1.065091609954834
In grad_steps = 300, loss = 1.0960190296173096
In grad_steps = 301, loss = 0.4107756018638611
In grad_steps = 302, loss = 1.006520390510559
In grad_steps = 303, loss = 1.089125156402588
In grad_steps = 304, loss = 0.3978341817855835
In grad_steps = 305, loss = 1.0477319955825806
In grad_steps = 306, loss = 0.9919372200965881
In grad_steps = 307, loss = 0.49714604020118713
In grad_steps = 308, loss = 0.9510489702224731
In grad_steps = 309, loss = 0.8978440761566162
In grad_steps = 310, loss = 0.8426669836044312
In grad_steps = 311, loss = 0.7803657650947571
In grad_steps = 312, loss = 0.7168135046958923
In grad_steps = 313, loss = 0.7027496099472046
In grad_steps = 314, loss = 0.76326584815979
In grad_steps = 315, loss = 0.7737041711807251
In grad_steps = 316, loss = 0.7648604512214661
In grad_steps = 317, loss = 0.6276113986968994
In grad_steps = 318, loss = 0.6094930171966553
In grad_steps = 319, loss = 0.7544226050376892
In grad_steps = 320, loss = 0.7508133053779602
In grad_steps = 321, loss = 0.7532303929328918
In grad_steps = 322, loss = 0.6745054125785828
In grad_steps = 323, loss = 0.6256833672523499
In grad_steps = 324, loss = 0.6055313348770142
In grad_steps = 325, loss = 0.5881758332252502
In grad_steps = 326, loss = 0.8847789764404297
In grad_steps = 327, loss = 0.4624931812286377
In grad_steps = 328, loss = 0.43949100375175476
In grad_steps = 329, loss = 1.0512324571609497
In grad_steps = 330, loss = 1.1060657501220703
In grad_steps = 331, loss = 0.387721985578537
In grad_steps = 332, loss = 0.37625613808631897
In grad_steps = 333, loss = 1.1125097274780273
In grad_steps = 334, loss = 1.0783827304840088
In grad_steps = 335, loss = 0.41276681423187256
In grad_steps = 336, loss = 0.43913665413856506
In grad_steps = 337, loss = 1.034152626991272
In grad_steps = 338, loss = 0.4692468047142029
In grad_steps = 339, loss = 0.9643378257751465
In grad_steps = 340, loss = 0.9350221157073975
In grad_steps = 341, loss = 0.8960182666778564
In grad_steps = 342, loss = 0.7921465039253235
In grad_steps = 343, loss = 0.7376993894577026
In grad_steps = 344, loss = 0.6862034201622009
In grad_steps = 345, loss = 0.7682257294654846
In grad_steps = 346, loss = 0.8453143239021301
In grad_steps = 347, loss = 0.82773357629776
In grad_steps = 348, loss = 0.8056759834289551
In grad_steps = 349, loss = 0.535373330116272
In grad_steps = 350, loss = 0.8099255561828613
In grad_steps = 351, loss = 0.759515643119812
In grad_steps = 352, loss = 0.7079144716262817
In grad_steps = 353, loss = 0.7032253742218018
In grad_steps = 354, loss = 0.751929759979248
In grad_steps = 355, loss = 0.6020468473434448
In grad_steps = 356, loss = 0.6112169623374939
In grad_steps = 357, loss = 0.75422602891922
In grad_steps = 358, loss = 0.6015819907188416
In grad_steps = 359, loss = 0.8673299551010132
In grad_steps = 360, loss = 0.7829601764678955
In grad_steps = 361, loss = 0.8676186800003052
In grad_steps = 362, loss = 0.8392327427864075
In grad_steps = 363, loss = 0.6064635515213013
In grad_steps = 364, loss = 0.5917497873306274
In grad_steps = 365, loss = 0.5938848853111267
In grad_steps = 366, loss = 0.7562199234962463
In grad_steps = 367, loss = 0.7475495338439941
In grad_steps = 368, loss = 0.7045390605926514
In grad_steps = 369, loss = 0.6434193849563599
In grad_steps = 370, loss = 0.6758518815040588
In grad_steps = 371, loss = 0.7227101922035217
In grad_steps = 372, loss = 0.7152676582336426
In grad_steps = 373, loss = 0.669399619102478
In grad_steps = 374, loss = 0.6270558834075928
In grad_steps = 375, loss = 0.5888814330101013
In grad_steps = 376, loss = 0.714910089969635
In grad_steps = 377, loss = 0.5068762302398682
In grad_steps = 378, loss = 0.841170072555542
In grad_steps = 379, loss = 0.9051254987716675
In grad_steps = 380, loss = 0.811001718044281
In grad_steps = 381, loss = 0.8826345801353455
In grad_steps = 382, loss = 0.5205146074295044
In grad_steps = 383, loss = 0.5289757251739502
In grad_steps = 384, loss = 0.8244605660438538
In grad_steps = 385, loss = 0.5874624848365784
In grad_steps = 386, loss = 0.8354381322860718
In grad_steps = 387, loss = 0.7662511467933655
In grad_steps = 388, loss = 0.6761040091514587
In grad_steps = 389, loss = 0.7133359909057617
In grad_steps = 390, loss = 0.6432719230651855
In grad_steps = 391, loss = 0.7157852053642273
In grad_steps = 392, loss = 0.7469732761383057
In grad_steps = 393, loss = 0.6203963160514832
In grad_steps = 394, loss = 0.7885353565216064
In grad_steps = 395, loss = 0.510168731212616
In grad_steps = 396, loss = 0.8317124843597412
In grad_steps = 397, loss = 0.8062453866004944
In grad_steps = 398, loss = 0.5384066104888916
In grad_steps = 399, loss = 0.5483677983283997
In grad_steps = 400, loss = 0.8406299352645874
In grad_steps = 401, loss = 0.5826672911643982
In grad_steps = 402, loss = 0.5457566380500793
In grad_steps = 403, loss = 0.5226759314537048
In grad_steps = 404, loss = 0.9698653221130371
In grad_steps = 405, loss = 0.8491954207420349
In grad_steps = 406, loss = 0.8942826390266418
In grad_steps = 407, loss = 0.5451191663742065
In grad_steps = 408, loss = 0.5259298086166382
In grad_steps = 409, loss = 0.746177613735199
In grad_steps = 410, loss = 0.6310615539550781
In grad_steps = 411, loss = 0.5868111848831177
In grad_steps = 412, loss = 0.570961594581604
In grad_steps = 413, loss = 0.5824314951896667
In grad_steps = 414, loss = 0.5840422511100769
In grad_steps = 415, loss = 0.4046975076198578
In grad_steps = 416, loss = 0.8619844913482666
In grad_steps = 417, loss = 0.7493957281112671
In grad_steps = 418, loss = 0.5324317812919617
In grad_steps = 419, loss = 0.7871575951576233
In grad_steps = 420, loss = 0.7742951512336731
In grad_steps = 421, loss = 0.4462531507015228
In grad_steps = 422, loss = 0.7100601196289062
In grad_steps = 423, loss = 0.6713347434997559
In grad_steps = 424, loss = 0.6943046450614929
In grad_steps = 425, loss = 0.7989921569824219
In grad_steps = 426, loss = 0.7637964487075806
In grad_steps = 427, loss = 0.6832460165023804
In grad_steps = 428, loss = 0.5879218578338623
In grad_steps = 429, loss = 0.6234586238861084
In grad_steps = 430, loss = 0.4794822335243225
In grad_steps = 431, loss = 0.8657203912734985
In grad_steps = 432, loss = 0.6726600527763367
In grad_steps = 433, loss = 0.23208752274513245
In grad_steps = 434, loss = 1.943582534790039
In grad_steps = 435, loss = 0.9297417402267456
In grad_steps = 436, loss = 0.4718875288963318
In grad_steps = 437, loss = 0.8662452697753906
In grad_steps = 438, loss = 0.672234296798706
In grad_steps = 439, loss = 0.7283813953399658
In grad_steps = 440, loss = 0.4426995813846588
In grad_steps = 441, loss = 0.7957159280776978
In grad_steps = 442, loss = 0.5838270783424377
In grad_steps = 443, loss = 0.794479489326477
In grad_steps = 444, loss = 0.5656276345252991
In grad_steps = 445, loss = 0.6348950862884521
In grad_steps = 446, loss = 0.5346304774284363
In grad_steps = 447, loss = 0.5345980525016785
In grad_steps = 448, loss = 0.5460386872291565
In grad_steps = 449, loss = 0.4488562047481537
In grad_steps = 450, loss = 0.47931915521621704
In grad_steps = 451, loss = 0.33515629172325134
In grad_steps = 452, loss = 0.36097538471221924
In grad_steps = 453, loss = 1.344376564025879
In grad_steps = 454, loss = 0.3144107460975647
In grad_steps = 455, loss = 0.20462724566459656
In grad_steps = 456, loss = 0.18212120234966278
In grad_steps = 457, loss = 1.5420548915863037
In grad_steps = 458, loss = 1.576516032218933
In grad_steps = 459, loss = 0.25974708795547485
In grad_steps = 460, loss = 1.1603952646255493
In grad_steps = 461, loss = 1.2086282968521118
In grad_steps = 462, loss = 0.3399841785430908
In grad_steps = 463, loss = 1.0783209800720215
In grad_steps = 464, loss = 0.9772053956985474
In grad_steps = 465, loss = 0.4869507849216461
In grad_steps = 466, loss = 0.9245778322219849
In grad_steps = 467, loss = 0.8288921117782593
In grad_steps = 468, loss = 0.7726956605911255
In grad_steps = 469, loss = 0.7267812490463257
In grad_steps = 470, loss = 0.6628326177597046
In grad_steps = 471, loss = 0.7094067335128784
In grad_steps = 472, loss = 0.7390732169151306
In grad_steps = 473, loss = 0.7889037728309631
In grad_steps = 474, loss = 0.790998637676239
In grad_steps = 475, loss = 0.600737452507019
In grad_steps = 476, loss = 0.5688549280166626
In grad_steps = 477, loss = 0.8652603626251221
In grad_steps = 478, loss = 0.7536827325820923
In grad_steps = 479, loss = 0.7945390939712524
In grad_steps = 480, loss = 0.7478041052818298
In grad_steps = 481, loss = 0.6435234546661377
In grad_steps = 482, loss = 0.6189303994178772
In grad_steps = 483, loss = 0.5615845322608948
In grad_steps = 484, loss = 0.8772663474082947
In grad_steps = 485, loss = 0.4270510673522949
In grad_steps = 486, loss = 0.3989582061767578
In grad_steps = 487, loss = 1.1312775611877441
In grad_steps = 488, loss = 1.3095049858093262
In grad_steps = 489, loss = 0.3054358959197998
In grad_steps = 490, loss = 0.3325220048427582
In grad_steps = 491, loss = 1.1649483442306519
In grad_steps = 492, loss = 1.021501064300537
In grad_steps = 493, loss = 0.38335520029067993
In grad_steps = 494, loss = 0.43114975094795227
In grad_steps = 495, loss = 1.0054563283920288
In grad_steps = 496, loss = 0.5039585828781128
In grad_steps = 497, loss = 0.8956514596939087
In grad_steps = 498, loss = 0.854949951171875
In grad_steps = 499, loss = 0.8136235475540161
In grad_steps = 500, loss = 0.6829246282577515
In grad_steps = 501, loss = 0.6469265222549438
In grad_steps = 502, loss = 0.6042419075965881
In grad_steps = 503, loss = 0.8495922088623047
In grad_steps = 504, loss = 0.9324760437011719
In grad_steps = 505, loss = 0.8848729133605957
In grad_steps = 506, loss = 0.8674748539924622
In grad_steps = 507, loss = 0.46731722354888916
In grad_steps = 508, loss = 0.8437910079956055
In grad_steps = 509, loss = 0.7795491814613342
In grad_steps = 510, loss = 0.7356025576591492
In grad_steps = 511, loss = 0.6886877417564392
In grad_steps = 512, loss = 0.7272990345954895
In grad_steps = 513, loss = 0.6197652220726013
In grad_steps = 514, loss = 0.6084677577018738
In grad_steps = 515, loss = 0.7619066834449768
In grad_steps = 516, loss = 0.6089756488800049
In grad_steps = 517, loss = 0.8687255382537842
In grad_steps = 518, loss = 0.7798274755477905
In grad_steps = 519, loss = 0.8466400504112244
In grad_steps = 520, loss = 0.8248466849327087
In grad_steps = 521, loss = 0.6144259572029114
In grad_steps = 522, loss = 0.5832338929176331
In grad_steps = 523, loss = 0.588605523109436
In grad_steps = 524, loss = 0.7295759320259094
In grad_steps = 525, loss = 0.7453314661979675
In grad_steps = 526, loss = 0.6854522824287415
In grad_steps = 527, loss = 0.6494794487953186
In grad_steps = 528, loss = 0.6591417193412781
In grad_steps = 529, loss = 0.7068853974342346
In grad_steps = 530, loss = 0.7218115329742432
In grad_steps = 531, loss = 0.6409167647361755
In grad_steps = 532, loss = 0.6268150806427002
In grad_steps = 533, loss = 0.591266393661499
In grad_steps = 534, loss = 0.4788047671318054
In grad_steps = 535, loss = 0.4895867705345154
In grad_steps = 536, loss = 0.7060829401016235
In grad_steps = 537, loss = 0.8962711691856384
In grad_steps = 538, loss = 0.7149919271469116
In grad_steps = 539, loss = 0.7786836624145508
In grad_steps = 540, loss = 0.47812989354133606
In grad_steps = 541, loss = 0.4957484304904938
In grad_steps = 542, loss = 0.7973519563674927
In grad_steps = 543, loss = 0.5427533388137817
In grad_steps = 544, loss = 0.8515522480010986
In grad_steps = 545, loss = 0.6779902577400208
In grad_steps = 546, loss = 0.4768267869949341
In grad_steps = 547, loss = 0.620593786239624
In grad_steps = 548, loss = 0.4909835457801819
In grad_steps = 549, loss = 0.7991234064102173
In grad_steps = 550, loss = 0.9201563000679016
In grad_steps = 551, loss = 0.46190202236175537
In grad_steps = 552, loss = 0.8870885372161865
In grad_steps = 553, loss = 0.21405476331710815
In grad_steps = 554, loss = 0.7152208685874939
In grad_steps = 555, loss = 0.634780764579773
In grad_steps = 556, loss = 0.2848648428916931
In grad_steps = 557, loss = 0.4726995825767517
In grad_steps = 558, loss = 0.5274643898010254
In grad_steps = 559, loss = 0.3590899109840393
In grad_steps = 560, loss = 0.5586112141609192
In grad_steps = 561, loss = 0.21748420596122742
In grad_steps = 562, loss = 3.328218698501587
In grad_steps = 563, loss = 0.27120140194892883
In grad_steps = 564, loss = 0.5013189911842346
In grad_steps = 565, loss = 0.9131829142570496
In grad_steps = 566, loss = 0.04331662878394127
In grad_steps = 567, loss = 0.33001917600631714
In grad_steps = 568, loss = 0.7767469882965088
In grad_steps = 569, loss = 0.743718683719635
In grad_steps = 570, loss = 0.5687481760978699
In grad_steps = 571, loss = 0.25023505091667175
In grad_steps = 572, loss = 0.6419087648391724
In grad_steps = 573, loss = 0.13007475435733795
In grad_steps = 574, loss = 0.8866050243377686
In grad_steps = 575, loss = 0.7096388936042786
In grad_steps = 576, loss = 0.42359161376953125
In grad_steps = 577, loss = 0.6654850244522095
In grad_steps = 578, loss = 0.2771667540073395
In grad_steps = 579, loss = 0.39122551679611206
In grad_steps = 580, loss = 0.5903491377830505
In grad_steps = 581, loss = 0.4736429452896118
In grad_steps = 582, loss = 0.47655993700027466
In grad_steps = 583, loss = 0.42619726061820984
In grad_steps = 584, loss = 0.1630753129720688
In grad_steps = 585, loss = 0.1487971693277359
In grad_steps = 586, loss = 0.04232286661863327
In grad_steps = 587, loss = 0.011988021433353424
In grad_steps = 588, loss = 0.005037117283791304
In grad_steps = 589, loss = 0.05918847769498825
In grad_steps = 590, loss = 1.8744715452194214
In grad_steps = 591, loss = 0.006026075221598148
In grad_steps = 592, loss = 0.862113356590271
In grad_steps = 593, loss = 2.0699546337127686
In grad_steps = 594, loss = 0.014471924863755703
In grad_steps = 595, loss = 0.46814024448394775
In grad_steps = 596, loss = 0.2631664574146271
In grad_steps = 597, loss = 0.4587664306163788
In grad_steps = 598, loss = 0.12845703959465027
In grad_steps = 599, loss = 1.0063135623931885
In grad_steps = 600, loss = 0.3150310218334198
In grad_steps = 601, loss = 1.6637133359909058
In grad_steps = 602, loss = 0.2878054678440094
In grad_steps = 603, loss = 0.4170837700366974
In grad_steps = 604, loss = 0.28982022404670715
In grad_steps = 605, loss = 0.306317538022995
In grad_steps = 606, loss = 0.5254462957382202
In grad_steps = 607, loss = 0.1860017627477646
In grad_steps = 608, loss = 0.2656162977218628
In grad_steps = 609, loss = 0.1395140439271927
In grad_steps = 610, loss = 0.16794593632221222
In grad_steps = 611, loss = 2.666245937347412
In grad_steps = 612, loss = 0.19727098941802979
In grad_steps = 613, loss = 0.06481033563613892
In grad_steps = 614, loss = 0.10173774510622025
In grad_steps = 615, loss = 1.7717314958572388
In grad_steps = 616, loss = 1.3400551080703735
In grad_steps = 617, loss = 0.33390191197395325
In grad_steps = 618, loss = 0.871013879776001
In grad_steps = 619, loss = 1.1102901697158813
In grad_steps = 620, loss = 0.5140526294708252
In grad_steps = 621, loss = 0.7300699949264526
In grad_steps = 622, loss = 0.7037229537963867
In grad_steps = 623, loss = 0.7016329765319824
In grad_steps = 624, loss = 0.5860329866409302
In grad_steps = 625, loss = 0.5694886445999146
In grad_steps = 626, loss = 0.45087045431137085
In grad_steps = 627, loss = 0.43363383412361145
In grad_steps = 628, loss = 0.36418598890304565
In grad_steps = 629, loss = 1.3420979976654053
In grad_steps = 630, loss = 1.3813185691833496
In grad_steps = 631, loss = 1.2588237524032593
i = 2, Test ensemble probabilities = 
[array([[0.6967262 , 0.3032738 ],
       [0.7028291 , 0.29717088],
       [0.70460814, 0.2953919 ],
       [0.68939936, 0.31060067],
       [0.7036712 , 0.29632884],
       [0.7036262 , 0.29637375],
       [0.6958931 , 0.30410686],
       [0.68907434, 0.31092563],
       [0.6974445 , 0.30255547],
       [0.6990177 , 0.3009823 ],
       [0.6981735 , 0.3018265 ],
       [0.7090719 , 0.29092813],
       [0.7018059 , 0.2981941 ],
       [0.70384294, 0.29615703],
       [0.69626576, 0.30373424],
       [0.6943978 , 0.3056022 ],
       [0.69633776, 0.3036622 ],
       [0.70439136, 0.29560867],
       [0.70563054, 0.2943695 ],
       [0.6988664 , 0.30113357],
       [0.69604003, 0.30395997]], dtype=float32), array([[0.6744076 , 0.32559237],
       [0.63860315, 0.36139688],
       [0.654905  , 0.34509492],
       [0.56939536, 0.43060464],
       [0.681396  , 0.318604  ],
       [0.60607004, 0.39392996],
       [0.6339681 , 0.36603186],
       [0.608821  , 0.39117903],
       [0.6456442 , 0.35435578],
       [0.63895434, 0.3610457 ],
       [0.59237045, 0.4076295 ],
       [0.6148586 , 0.38514137],
       [0.6200046 , 0.3799954 ],
       [0.62975216, 0.37024784],
       [0.61797065, 0.38202932],
       [0.60365945, 0.39634055],
       [0.6642158 , 0.3357842 ],
       [0.6495719 , 0.35042813],
       [0.62240934, 0.37759066],
       [0.6177311 , 0.3822689 ],
       [0.54644036, 0.45355964]], dtype=float32), array([[0.7289234 , 0.2710766 ],
       [0.6688579 , 0.33114213],
       [0.7037342 , 0.29626578],
       [0.6693629 , 0.33063707],
       [0.727387  , 0.27261305],
       [0.6707304 , 0.32926956],
       [0.68790483, 0.31209517],
       [0.6725088 , 0.32749122],
       [0.6767822 , 0.32321778],
       [0.6714226 , 0.3285774 ],
       [0.6862493 , 0.31375068],
       [0.6909219 , 0.3090781 ],
       [0.7119528 , 0.28804722],
       [0.70750237, 0.2924976 ],
       [0.65528005, 0.34472   ],
       [0.6819204 , 0.31807956],
       [0.71274483, 0.2872552 ],
       [0.7397543 , 0.26024565],
       [0.7187024 , 0.28129762],
       [0.7101613 , 0.28983867],
       [0.65661806, 0.3433819 ]], dtype=float32)]
i = 2, Test true class= 
[0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 1 0 1 1 0 0]
In grad_steps = 0, loss = 0.19905029237270355
In grad_steps = 1, loss = 3.0646278858184814
In grad_steps = 2, loss = 2.004409074783325
In grad_steps = 3, loss = 0.2717498540878296
In grad_steps = 4, loss = 0.4261201322078705
In grad_steps = 5, loss = 0.4693750739097595
In grad_steps = 6, loss = 0.2364945411682129
In grad_steps = 7, loss = 0.16164575517177582
In grad_steps = 8, loss = 0.10423855483531952
In grad_steps = 9, loss = 0.060957565903663635
In grad_steps = 10, loss = 3.983362913131714
In grad_steps = 11, loss = 0.021226145327091217
In grad_steps = 12, loss = 0.0347621813416481
In grad_steps = 13, loss = 2.340264320373535
In grad_steps = 14, loss = 2.2895302772521973
In grad_steps = 15, loss = 0.18668358027935028
In grad_steps = 16, loss = 0.29670774936676025
In grad_steps = 17, loss = 1.201911449432373
In grad_steps = 18, loss = 0.8156704902648926
In grad_steps = 19, loss = 0.641202986240387
In grad_steps = 20, loss = 0.7649170160293579
In grad_steps = 21, loss = 0.6723248362541199
In grad_steps = 22, loss = 0.7816441655158997
In grad_steps = 23, loss = 0.6862545013427734
In grad_steps = 24, loss = 0.6955289840698242
In grad_steps = 25, loss = 0.5311644077301025
In grad_steps = 26, loss = 0.48928967118263245
In grad_steps = 27, loss = 0.4227675795555115
In grad_steps = 28, loss = 0.3174247145652771
In grad_steps = 29, loss = 1.8058929443359375
In grad_steps = 30, loss = 1.7967774868011475
In grad_steps = 31, loss = 1.7044029235839844
In grad_steps = 32, loss = 1.354367733001709
In grad_steps = 33, loss = 0.4491974413394928
In grad_steps = 34, loss = 0.9135855436325073
In grad_steps = 35, loss = 0.7663776874542236
In grad_steps = 36, loss = 0.5380088090896606
In grad_steps = 37, loss = 0.913001298904419
In grad_steps = 38, loss = 1.1057989597320557
In grad_steps = 39, loss = 0.3894825279712677
In grad_steps = 40, loss = 0.43960466980934143
In grad_steps = 41, loss = 1.0857088565826416
In grad_steps = 42, loss = 0.3891768157482147
In grad_steps = 43, loss = 1.1424980163574219
In grad_steps = 44, loss = 0.997192919254303
In grad_steps = 45, loss = 1.0759614706039429
In grad_steps = 46, loss = 1.0098991394042969
In grad_steps = 47, loss = 0.6883379817008972
In grad_steps = 48, loss = 0.7070227265357971
In grad_steps = 49, loss = 0.8143824934959412
In grad_steps = 50, loss = 0.6729719638824463
In grad_steps = 51, loss = 0.5999892354011536
In grad_steps = 52, loss = 0.484091579914093
In grad_steps = 53, loss = 0.9125103950500488
In grad_steps = 54, loss = 0.46137097477912903
In grad_steps = 55, loss = 0.4456849694252014
In grad_steps = 56, loss = 0.4398875832557678
In grad_steps = 57, loss = 1.1403131484985352
In grad_steps = 58, loss = 0.3672889173030853
In grad_steps = 59, loss = 0.3367689251899719
In grad_steps = 60, loss = 1.3233015537261963
In grad_steps = 61, loss = 0.35760989785194397
In grad_steps = 62, loss = 1.2732913494110107
In grad_steps = 63, loss = 1.3632991313934326
In grad_steps = 64, loss = 1.1547905206680298
In grad_steps = 65, loss = 1.0844371318817139
In grad_steps = 66, loss = 0.5308147072792053
In grad_steps = 67, loss = 0.5501484274864197
In grad_steps = 68, loss = 0.7321988344192505
In grad_steps = 69, loss = 0.6819401383399963
In grad_steps = 70, loss = 0.7226552963256836
In grad_steps = 71, loss = 0.6717796921730042
In grad_steps = 72, loss = 0.5882152915000916
In grad_steps = 73, loss = 0.5523995757102966
In grad_steps = 74, loss = 0.5330013036727905
In grad_steps = 75, loss = 1.015242576599121
In grad_steps = 76, loss = 1.0382838249206543
In grad_steps = 77, loss = 0.4774470329284668
In grad_steps = 78, loss = 1.0448613166809082
In grad_steps = 79, loss = 0.3695678412914276
In grad_steps = 80, loss = 1.0674490928649902
In grad_steps = 81, loss = 1.0723748207092285
In grad_steps = 82, loss = 0.4418971538543701
In grad_steps = 83, loss = 0.4933816194534302
In grad_steps = 84, loss = 1.0154359340667725
In grad_steps = 85, loss = 0.5498660206794739
In grad_steps = 86, loss = 0.4843946099281311
In grad_steps = 87, loss = 0.4299163520336151
In grad_steps = 88, loss = 1.030371904373169
In grad_steps = 89, loss = 0.9579226970672607
In grad_steps = 90, loss = 0.9231413006782532
In grad_steps = 91, loss = 0.573863685131073
In grad_steps = 92, loss = 0.8489553332328796
In grad_steps = 93, loss = 0.774827778339386
In grad_steps = 94, loss = 0.7134361863136292
In grad_steps = 95, loss = 0.6731355786323547
In grad_steps = 96, loss = 0.6859104037284851
In grad_steps = 97, loss = 0.6453600525856018
In grad_steps = 98, loss = 0.701097846031189
In grad_steps = 99, loss = 0.5758801698684692
In grad_steps = 100, loss = 0.8099838495254517
In grad_steps = 101, loss = 0.7965483665466309
In grad_steps = 102, loss = 0.5612395405769348
In grad_steps = 103, loss = 0.8702155351638794
In grad_steps = 104, loss = 0.8315520286560059
In grad_steps = 105, loss = 0.5475406646728516
In grad_steps = 106, loss = 0.7865259051322937
In grad_steps = 107, loss = 0.7776698470115662
In grad_steps = 108, loss = 0.6651212573051453
In grad_steps = 109, loss = 0.6025093793869019
In grad_steps = 110, loss = 0.635057806968689
In grad_steps = 111, loss = 0.7910871505737305
In grad_steps = 112, loss = 0.795615553855896
In grad_steps = 113, loss = 0.7243170738220215
In grad_steps = 114, loss = 0.7323433756828308
In grad_steps = 115, loss = 0.6634752750396729
In grad_steps = 116, loss = 0.7431899309158325
In grad_steps = 117, loss = 0.5824239253997803
In grad_steps = 118, loss = 0.7964457273483276
In grad_steps = 119, loss = 0.7760872840881348
In grad_steps = 120, loss = 0.576239287853241
In grad_steps = 121, loss = 0.8525941371917725
In grad_steps = 122, loss = 0.8567904233932495
In grad_steps = 123, loss = 0.6279083490371704
In grad_steps = 124, loss = 0.7293369770050049
In grad_steps = 125, loss = 0.6147788763046265
In grad_steps = 126, loss = 0.8287980556488037
In grad_steps = 127, loss = 0.6644294261932373
In grad_steps = 128, loss = 0.7313267588615417
In grad_steps = 129, loss = 0.7974745631217957
In grad_steps = 130, loss = 0.7077432870864868
In grad_steps = 131, loss = 0.7256990671157837
In grad_steps = 132, loss = 0.6936308145523071
In grad_steps = 133, loss = 0.5557155609130859
In grad_steps = 134, loss = 0.5938447713851929
In grad_steps = 135, loss = 0.45670896768569946
In grad_steps = 136, loss = 0.45323556661605835
In grad_steps = 137, loss = 1.056902527809143
In grad_steps = 138, loss = 0.4000226557254791
In grad_steps = 139, loss = 0.34567326307296753
In grad_steps = 140, loss = 0.3642733693122864
In grad_steps = 141, loss = 1.2630984783172607
In grad_steps = 142, loss = 1.3169691562652588
In grad_steps = 143, loss = 0.34779465198516846
In grad_steps = 144, loss = 1.1689718961715698
In grad_steps = 145, loss = 1.2338838577270508
In grad_steps = 146, loss = 0.3532436192035675
In grad_steps = 147, loss = 1.146118402481079
In grad_steps = 148, loss = 1.078545093536377
In grad_steps = 149, loss = 0.5054409503936768
In grad_steps = 150, loss = 0.9696521162986755
In grad_steps = 151, loss = 0.8971706628799438
In grad_steps = 152, loss = 0.8203448057174683
In grad_steps = 153, loss = 0.7347227931022644
In grad_steps = 154, loss = 0.573135256767273
In grad_steps = 155, loss = 0.9864537715911865
In grad_steps = 156, loss = 1.1503640413284302
In grad_steps = 157, loss = 1.1073992252349854
In grad_steps = 158, loss = 0.9979045391082764
In grad_steps = 159, loss = 0.5096307992935181
In grad_steps = 160, loss = 0.5254182815551758
In grad_steps = 161, loss = 0.8353505730628967
In grad_steps = 162, loss = 0.8087551593780518
In grad_steps = 163, loss = 0.7856640815734863
In grad_steps = 164, loss = 0.6523855328559875
In grad_steps = 165, loss = 0.5716090798377991
In grad_steps = 166, loss = 0.5354611277580261
In grad_steps = 167, loss = 0.5014445781707764
In grad_steps = 168, loss = 1.0623375177383423
In grad_steps = 169, loss = 0.37220513820648193
In grad_steps = 170, loss = 0.35204198956489563
In grad_steps = 171, loss = 1.2325444221496582
In grad_steps = 172, loss = 1.3130232095718384
In grad_steps = 173, loss = 0.31989794969558716
In grad_steps = 174, loss = 0.32513654232025146
In grad_steps = 175, loss = 1.271071434020996
In grad_steps = 176, loss = 1.1632966995239258
In grad_steps = 177, loss = 0.3877505660057068
In grad_steps = 178, loss = 0.43406790494918823
In grad_steps = 179, loss = 1.0389567613601685
In grad_steps = 180, loss = 0.47296762466430664
In grad_steps = 181, loss = 0.9591550827026367
In grad_steps = 182, loss = 0.907444953918457
In grad_steps = 183, loss = 0.8251022696495056
In grad_steps = 184, loss = 0.7181591391563416
In grad_steps = 185, loss = 0.6201080083847046
In grad_steps = 186, loss = 0.5426667928695679
In grad_steps = 187, loss = 1.0780001878738403
In grad_steps = 188, loss = 1.198951244354248
In grad_steps = 189, loss = 1.1731606721878052
In grad_steps = 190, loss = 1.1073927879333496
In grad_steps = 191, loss = 0.4461432695388794
In grad_steps = 192, loss = 0.9587751626968384
In grad_steps = 193, loss = 0.8616429567337036
In grad_steps = 194, loss = 0.7452462911605835
In grad_steps = 195, loss = 0.6862986087799072
In grad_steps = 196, loss = 0.767970621585846
In grad_steps = 197, loss = 0.5707359910011292
In grad_steps = 198, loss = 0.6054649949073792
In grad_steps = 199, loss = 0.8168192505836487
In grad_steps = 200, loss = 0.5455785989761353
In grad_steps = 201, loss = 0.9233102798461914
In grad_steps = 202, loss = 0.8701767325401306
In grad_steps = 203, loss = 0.9359509944915771
In grad_steps = 204, loss = 0.9404240846633911
In grad_steps = 205, loss = 0.5632731318473816
In grad_steps = 206, loss = 0.5344669818878174
In grad_steps = 207, loss = 0.5673591494560242
In grad_steps = 208, loss = 0.8095952868461609
In grad_steps = 209, loss = 0.8166807889938354
In grad_steps = 210, loss = 0.7601256966590881
In grad_steps = 211, loss = 0.5991584062576294
In grad_steps = 212, loss = 0.7379631996154785
In grad_steps = 213, loss = 0.7412092685699463
In grad_steps = 214, loss = 0.7137115001678467
In grad_steps = 215, loss = 0.675595223903656
In grad_steps = 216, loss = 0.6603116989135742
In grad_steps = 217, loss = 0.6032892465591431
In grad_steps = 218, loss = 0.7826247215270996
In grad_steps = 219, loss = 0.5488767623901367
In grad_steps = 220, loss = 0.8601677417755127
In grad_steps = 221, loss = 0.9192628860473633
In grad_steps = 222, loss = 0.8414880037307739
In grad_steps = 223, loss = 0.8772987127304077
In grad_steps = 224, loss = 0.5474947094917297
In grad_steps = 225, loss = 0.5657380819320679
In grad_steps = 226, loss = 0.793005108833313
In grad_steps = 227, loss = 0.6051211953163147
In grad_steps = 228, loss = 0.8102485537528992
In grad_steps = 229, loss = 0.7567411661148071
In grad_steps = 230, loss = 0.7162202000617981
In grad_steps = 231, loss = 0.7034623026847839
In grad_steps = 232, loss = 0.6783438920974731
In grad_steps = 233, loss = 0.7393976449966431
In grad_steps = 234, loss = 0.7604517936706543
In grad_steps = 235, loss = 0.6353187561035156
In grad_steps = 236, loss = 0.7746149301528931
In grad_steps = 237, loss = 0.5483428835868835
In grad_steps = 238, loss = 0.8081502318382263
In grad_steps = 239, loss = 0.8061957359313965
In grad_steps = 240, loss = 0.5710894465446472
In grad_steps = 241, loss = 0.5929961800575256
In grad_steps = 242, loss = 0.8299947381019592
In grad_steps = 243, loss = 0.6145537495613098
In grad_steps = 244, loss = 0.5583394169807434
In grad_steps = 245, loss = 0.5169374942779541
In grad_steps = 246, loss = 0.8966894745826721
In grad_steps = 247, loss = 0.850832462310791
In grad_steps = 248, loss = 0.8673465251922607
In grad_steps = 249, loss = 0.5700461864471436
In grad_steps = 250, loss = 0.8152701258659363
In grad_steps = 251, loss = 0.8121637105941772
In grad_steps = 252, loss = 0.63570636510849
In grad_steps = 253, loss = 0.5940232276916504
In grad_steps = 254, loss = 0.5938788652420044
In grad_steps = 255, loss = 0.5803989171981812
In grad_steps = 256, loss = 0.6119813919067383
In grad_steps = 257, loss = 0.5096757411956787
In grad_steps = 258, loss = 0.8453839421272278
In grad_steps = 259, loss = 0.8109837770462036
In grad_steps = 260, loss = 0.5567921996116638
In grad_steps = 261, loss = 0.8692216277122498
In grad_steps = 262, loss = 0.846780002117157
In grad_steps = 263, loss = 0.5317823886871338
In grad_steps = 264, loss = 0.8068673610687256
In grad_steps = 265, loss = 0.7815298438072205
In grad_steps = 266, loss = 0.6261278986930847
In grad_steps = 267, loss = 0.5802754759788513
In grad_steps = 268, loss = 0.6011078953742981
In grad_steps = 269, loss = 0.8185898065567017
In grad_steps = 270, loss = 0.8094703555107117
In grad_steps = 271, loss = 0.7626275420188904
In grad_steps = 272, loss = 0.7474200129508972
In grad_steps = 273, loss = 0.6255608797073364
In grad_steps = 274, loss = 0.7783422470092773
In grad_steps = 275, loss = 0.6350963115692139
In grad_steps = 276, loss = 0.7341840267181396
In grad_steps = 277, loss = 0.708759605884552
In grad_steps = 278, loss = 0.6287314295768738
In grad_steps = 279, loss = 0.7313277721405029
In grad_steps = 280, loss = 0.7572516202926636
In grad_steps = 281, loss = 0.6850908398628235
In grad_steps = 282, loss = 0.6550940275192261
In grad_steps = 283, loss = 0.6539424061775208
In grad_steps = 284, loss = 0.7523719072341919
In grad_steps = 285, loss = 0.6675399541854858
In grad_steps = 286, loss = 0.7032802700996399
In grad_steps = 287, loss = 0.7656877040863037
In grad_steps = 288, loss = 0.6913949251174927
In grad_steps = 289, loss = 0.7082783579826355
In grad_steps = 290, loss = 0.7004626393318176
In grad_steps = 291, loss = 0.5775812268257141
In grad_steps = 292, loss = 0.6315615773200989
In grad_steps = 293, loss = 0.502832293510437
In grad_steps = 294, loss = 0.5123758316040039
In grad_steps = 295, loss = 0.946843147277832
In grad_steps = 296, loss = 0.4731767773628235
In grad_steps = 297, loss = 0.3934876024723053
In grad_steps = 298, loss = 0.4123219847679138
In grad_steps = 299, loss = 1.1020386219024658
In grad_steps = 300, loss = 1.1403586864471436
In grad_steps = 301, loss = 0.3915371000766754
In grad_steps = 302, loss = 1.0116132497787476
In grad_steps = 303, loss = 1.103844404220581
In grad_steps = 304, loss = 0.3926618695259094
In grad_steps = 305, loss = 1.0328378677368164
In grad_steps = 306, loss = 0.9715405702590942
In grad_steps = 307, loss = 0.5127452611923218
In grad_steps = 308, loss = 0.9398545622825623
In grad_steps = 309, loss = 0.8722010850906372
In grad_steps = 310, loss = 0.8218926191329956
In grad_steps = 311, loss = 0.7710306644439697
In grad_steps = 312, loss = 0.6770896315574646
In grad_steps = 313, loss = 0.7476478815078735
In grad_steps = 314, loss = 0.8267728686332703
In grad_steps = 315, loss = 0.8355331420898438
In grad_steps = 316, loss = 0.8034668564796448
In grad_steps = 317, loss = 0.5894121527671814
In grad_steps = 318, loss = 0.5722978711128235
In grad_steps = 319, loss = 0.798272430896759
In grad_steps = 320, loss = 0.7844306230545044
In grad_steps = 321, loss = 0.7760862112045288
In grad_steps = 322, loss = 0.6957416534423828
In grad_steps = 323, loss = 0.6148244142532349
In grad_steps = 324, loss = 0.5877188444137573
In grad_steps = 325, loss = 0.5726902484893799
In grad_steps = 326, loss = 0.9161093235015869
In grad_steps = 327, loss = 0.43835195899009705
In grad_steps = 328, loss = 0.43052881956100464
In grad_steps = 329, loss = 1.0550776720046997
In grad_steps = 330, loss = 1.1323833465576172
In grad_steps = 331, loss = 0.37819361686706543
In grad_steps = 332, loss = 0.3746572434902191
In grad_steps = 333, loss = 1.1604220867156982
In grad_steps = 334, loss = 1.070361614227295
In grad_steps = 335, loss = 0.40611547231674194
In grad_steps = 336, loss = 0.4369994103908539
In grad_steps = 337, loss = 1.0325983762741089
In grad_steps = 338, loss = 0.4448525309562683
In grad_steps = 339, loss = 0.9746379852294922
In grad_steps = 340, loss = 0.9425839185714722
In grad_steps = 341, loss = 0.867455005645752
In grad_steps = 342, loss = 0.784650444984436
In grad_steps = 343, loss = 0.7094072103500366
In grad_steps = 344, loss = 0.6435352563858032
In grad_steps = 345, loss = 0.8340541124343872
In grad_steps = 346, loss = 0.9663578867912292
In grad_steps = 347, loss = 0.8919777274131775
In grad_steps = 348, loss = 0.8886559009552002
In grad_steps = 349, loss = 0.5196908712387085
In grad_steps = 350, loss = 0.8479238748550415
In grad_steps = 351, loss = 0.7704162001609802
In grad_steps = 352, loss = 0.7101531624794006
In grad_steps = 353, loss = 0.708124041557312
In grad_steps = 354, loss = 0.7483516335487366
In grad_steps = 355, loss = 0.5806156396865845
In grad_steps = 356, loss = 0.6111589074134827
In grad_steps = 357, loss = 0.7626143097877502
In grad_steps = 358, loss = 0.5848425030708313
In grad_steps = 359, loss = 0.8943886160850525
In grad_steps = 360, loss = 0.8078172206878662
In grad_steps = 361, loss = 0.8876854181289673
In grad_steps = 362, loss = 0.8797930479049683
In grad_steps = 363, loss = 0.5807635188102722
In grad_steps = 364, loss = 0.5475398302078247
In grad_steps = 365, loss = 0.5697801113128662
In grad_steps = 366, loss = 0.7492719888687134
In grad_steps = 367, loss = 0.784651517868042
In grad_steps = 368, loss = 0.7228250503540039
In grad_steps = 369, loss = 0.5988696217536926
In grad_steps = 370, loss = 0.7085253000259399
In grad_steps = 371, loss = 0.7422630786895752
In grad_steps = 372, loss = 0.717385470867157
In grad_steps = 373, loss = 0.6358885169029236
In grad_steps = 374, loss = 0.6643312573432922
In grad_steps = 375, loss = 0.6163894534111023
In grad_steps = 376, loss = 0.7006279230117798
In grad_steps = 377, loss = 0.5381255149841309
In grad_steps = 378, loss = 0.7926667332649231
In grad_steps = 379, loss = 0.8977723121643066
In grad_steps = 380, loss = 0.7716448903083801
In grad_steps = 381, loss = 0.849981427192688
In grad_steps = 382, loss = 0.5444926619529724
In grad_steps = 383, loss = 0.5603165030479431
In grad_steps = 384, loss = 0.7802634835243225
In grad_steps = 385, loss = 0.572721004486084
In grad_steps = 386, loss = 0.8468780517578125
In grad_steps = 387, loss = 0.754835307598114
In grad_steps = 388, loss = 0.6528297662734985
In grad_steps = 389, loss = 0.686988353729248
In grad_steps = 390, loss = 0.6180427074432373
In grad_steps = 391, loss = 0.7868947386741638
In grad_steps = 392, loss = 0.7937948703765869
In grad_steps = 393, loss = 0.5942094922065735
In grad_steps = 394, loss = 0.8227501511573792
In grad_steps = 395, loss = 0.4443209171295166
In grad_steps = 396, loss = 0.833532989025116
In grad_steps = 397, loss = 0.8266473412513733
In grad_steps = 398, loss = 0.47917062044143677
In grad_steps = 399, loss = 0.5714920163154602
In grad_steps = 400, loss = 0.849808931350708
In grad_steps = 401, loss = 0.6008476614952087
In grad_steps = 402, loss = 0.5552489757537842
In grad_steps = 403, loss = 0.47963768243789673
In grad_steps = 404, loss = 0.9802432060241699
In grad_steps = 405, loss = 0.8182060718536377
In grad_steps = 406, loss = 0.8895933628082275
In grad_steps = 407, loss = 0.5572744607925415
In grad_steps = 408, loss = 0.6715428233146667
In grad_steps = 409, loss = 0.7344260811805725
In grad_steps = 410, loss = 0.685263991355896
In grad_steps = 411, loss = 0.5963939428329468
In grad_steps = 412, loss = 0.5932795405387878
In grad_steps = 413, loss = 0.5601164698600769
In grad_steps = 414, loss = 0.614657998085022
In grad_steps = 415, loss = 0.4345233738422394
In grad_steps = 416, loss = 0.806373119354248
In grad_steps = 417, loss = 0.6898823976516724
In grad_steps = 418, loss = 0.5727218389511108
In grad_steps = 419, loss = 0.7172185182571411
In grad_steps = 420, loss = 0.6771055459976196
In grad_steps = 421, loss = 0.41161927580833435
In grad_steps = 422, loss = 0.5909450650215149
In grad_steps = 423, loss = 0.6155763268470764
In grad_steps = 424, loss = 0.7932996153831482
In grad_steps = 425, loss = 1.0308637619018555
In grad_steps = 426, loss = 0.6028521656990051
In grad_steps = 427, loss = 0.9516857266426086
In grad_steps = 428, loss = 0.918798565864563
In grad_steps = 429, loss = 0.897891640663147
In grad_steps = 430, loss = 0.6983275413513184
In grad_steps = 431, loss = 0.6490809321403503
In grad_steps = 432, loss = 0.9278366565704346
In grad_steps = 433, loss = 0.4214094281196594
In grad_steps = 434, loss = 0.8653690814971924
In grad_steps = 435, loss = 0.7491834759712219
In grad_steps = 436, loss = 0.5218363404273987
In grad_steps = 437, loss = 0.8705580830574036
In grad_steps = 438, loss = 0.7470595836639404
In grad_steps = 439, loss = 0.6521633863449097
In grad_steps = 440, loss = 0.5616459846496582
In grad_steps = 441, loss = 0.6535342335700989
In grad_steps = 442, loss = 0.6722772121429443
In grad_steps = 443, loss = 0.727053165435791
In grad_steps = 444, loss = 0.612377405166626
In grad_steps = 445, loss = 0.6974460482597351
In grad_steps = 446, loss = 0.5872913599014282
In grad_steps = 447, loss = 0.6079692840576172
In grad_steps = 448, loss = 0.5777879953384399
In grad_steps = 449, loss = 0.4306662976741791
In grad_steps = 450, loss = 0.4699426293373108
In grad_steps = 451, loss = 0.3431529998779297
In grad_steps = 452, loss = 0.3367222845554352
In grad_steps = 453, loss = 1.4933230876922607
In grad_steps = 454, loss = 0.280307412147522
In grad_steps = 455, loss = 0.19699780642986298
In grad_steps = 456, loss = 0.2185995727777481
In grad_steps = 457, loss = 1.6145421266555786
In grad_steps = 458, loss = 1.6363575458526611
In grad_steps = 459, loss = 0.25453388690948486
In grad_steps = 460, loss = 1.2040340900421143
In grad_steps = 461, loss = 1.2863826751708984
In grad_steps = 462, loss = 0.35327449440956116
In grad_steps = 463, loss = 1.0554544925689697
In grad_steps = 464, loss = 0.9419257044792175
In grad_steps = 465, loss = 0.5016205906867981
In grad_steps = 466, loss = 0.8881837129592896
In grad_steps = 467, loss = 0.8197587728500366
In grad_steps = 468, loss = 0.7695630788803101
In grad_steps = 469, loss = 0.7127452492713928
In grad_steps = 470, loss = 0.6427015066146851
In grad_steps = 471, loss = 0.7690348625183105
In grad_steps = 472, loss = 0.8113133311271667
In grad_steps = 473, loss = 0.8282774686813354
In grad_steps = 474, loss = 0.8300355672836304
In grad_steps = 475, loss = 0.5610794425010681
In grad_steps = 476, loss = 0.550617516040802
In grad_steps = 477, loss = 0.895635724067688
In grad_steps = 478, loss = 0.7978121042251587
In grad_steps = 479, loss = 0.8309792876243591
In grad_steps = 480, loss = 0.7958081364631653
In grad_steps = 481, loss = 0.679940402507782
In grad_steps = 482, loss = 0.6517633199691772
In grad_steps = 483, loss = 0.622081995010376
In grad_steps = 484, loss = 0.8259683847427368
In grad_steps = 485, loss = 0.49069350957870483
In grad_steps = 486, loss = 0.4578321576118469
In grad_steps = 487, loss = 0.9944782853126526
In grad_steps = 488, loss = 1.0847938060760498
In grad_steps = 489, loss = 0.3807152509689331
In grad_steps = 490, loss = 0.3734353184700012
In grad_steps = 491, loss = 1.1628344058990479
In grad_steps = 492, loss = 1.0542958974838257
In grad_steps = 493, loss = 0.3952174484729767
In grad_steps = 494, loss = 0.48311150074005127
In grad_steps = 495, loss = 1.018287181854248
In grad_steps = 496, loss = 0.45508813858032227
In grad_steps = 497, loss = 0.9551467895507812
In grad_steps = 498, loss = 0.9085568189620972
In grad_steps = 499, loss = 0.8024840354919434
In grad_steps = 500, loss = 0.7372327446937561
In grad_steps = 501, loss = 0.6750019192695618
In grad_steps = 502, loss = 0.6096053123474121
In grad_steps = 503, loss = 0.8645989298820496
In grad_steps = 504, loss = 0.9240617752075195
In grad_steps = 505, loss = 0.8597919344902039
In grad_steps = 506, loss = 0.8045732975006104
In grad_steps = 507, loss = 0.4957876205444336
In grad_steps = 508, loss = 0.8222678303718567
In grad_steps = 509, loss = 0.7592796683311462
In grad_steps = 510, loss = 0.7058892846107483
In grad_steps = 511, loss = 0.7367658019065857
In grad_steps = 512, loss = 0.7795910835266113
In grad_steps = 513, loss = 0.592336893081665
In grad_steps = 514, loss = 0.5788534283638
In grad_steps = 515, loss = 0.7920228242874146
In grad_steps = 516, loss = 0.5555956363677979
In grad_steps = 517, loss = 0.9380890130996704
In grad_steps = 518, loss = 0.8321493864059448
In grad_steps = 519, loss = 0.9097157120704651
In grad_steps = 520, loss = 0.8905731439590454
In grad_steps = 521, loss = 0.5764176249504089
In grad_steps = 522, loss = 0.5804312825202942
In grad_steps = 523, loss = 0.5617833137512207
In grad_steps = 524, loss = 0.7442561984062195
In grad_steps = 525, loss = 0.7625369429588318
In grad_steps = 526, loss = 0.6771116256713867
In grad_steps = 527, loss = 0.6458141207695007
In grad_steps = 528, loss = 0.6666164994239807
In grad_steps = 529, loss = 0.6795899271965027
In grad_steps = 530, loss = 0.673086941242218
In grad_steps = 531, loss = 0.6886976957321167
In grad_steps = 532, loss = 0.5931540727615356
In grad_steps = 533, loss = 0.5565414428710938
In grad_steps = 534, loss = 0.6730602979660034
In grad_steps = 535, loss = 0.4377237856388092
In grad_steps = 536, loss = 0.8225415945053101
In grad_steps = 537, loss = 1.0285272598266602
In grad_steps = 538, loss = 0.7882430553436279
In grad_steps = 539, loss = 0.8774675130844116
In grad_steps = 540, loss = 0.4806973934173584
In grad_steps = 541, loss = 0.47560441493988037
In grad_steps = 542, loss = 0.795264482498169
In grad_steps = 543, loss = 0.5075731873512268
In grad_steps = 544, loss = 0.8119887709617615
In grad_steps = 545, loss = 0.6435397863388062
In grad_steps = 546, loss = 0.4273053705692291
In grad_steps = 547, loss = 0.4757787883281708
In grad_steps = 548, loss = 0.23519489169120789
In grad_steps = 549, loss = 1.1575825214385986
In grad_steps = 550, loss = 1.1703698635101318
In grad_steps = 551, loss = 0.2582690715789795
In grad_steps = 552, loss = 0.8177383542060852
In grad_steps = 553, loss = 0.2478075921535492
In grad_steps = 554, loss = 0.2009124457836151
In grad_steps = 555, loss = 0.13045315444469452
In grad_steps = 556, loss = 0.3989563584327698
In grad_steps = 557, loss = 1.6077430248260498
In grad_steps = 558, loss = 0.2865774929523468
In grad_steps = 559, loss = 0.6579731702804565
In grad_steps = 560, loss = 0.4492760896682739
In grad_steps = 561, loss = 0.3508979082107544
In grad_steps = 562, loss = 1.411790370941162
In grad_steps = 563, loss = 1.0550341606140137
In grad_steps = 564, loss = 1.1838127374649048
In grad_steps = 565, loss = 0.4802360534667969
In grad_steps = 566, loss = 0.4590591788291931
In grad_steps = 567, loss = 0.7365376353263855
In grad_steps = 568, loss = 0.6013948917388916
In grad_steps = 569, loss = 0.6198498010635376
In grad_steps = 570, loss = 0.6686301827430725
In grad_steps = 571, loss = 0.5697698593139648
In grad_steps = 572, loss = 0.6744132041931152
In grad_steps = 573, loss = 0.38848066329956055
In grad_steps = 574, loss = 0.7702497839927673
In grad_steps = 575, loss = 0.6299384832382202
In grad_steps = 576, loss = 0.5835239291191101
In grad_steps = 577, loss = 0.6041100025177002
In grad_steps = 578, loss = 0.5091196298599243
In grad_steps = 579, loss = 0.4607146978378296
In grad_steps = 580, loss = 0.5905213952064514
In grad_steps = 581, loss = 0.23515193164348602
In grad_steps = 582, loss = 0.3941618502140045
In grad_steps = 583, loss = 0.12830427289009094
In grad_steps = 584, loss = 0.4612892270088196
In grad_steps = 585, loss = 0.2954649329185486
In grad_steps = 586, loss = 0.11826621741056442
In grad_steps = 587, loss = 0.026733694598078728
In grad_steps = 588, loss = 0.020346147939562798
In grad_steps = 589, loss = 0.02110079675912857
In grad_steps = 590, loss = 4.257697582244873
In grad_steps = 591, loss = 0.013158979825675488
In grad_steps = 592, loss = 3.989973545074463
In grad_steps = 593, loss = 1.2381348609924316
In grad_steps = 594, loss = 0.056607428938150406
In grad_steps = 595, loss = 1.0914950370788574
In grad_steps = 596, loss = 0.5853899121284485
In grad_steps = 597, loss = 0.70099937915802
In grad_steps = 598, loss = 0.2944943606853485
In grad_steps = 599, loss = 1.2077053785324097
In grad_steps = 600, loss = 0.33653396368026733
In grad_steps = 601, loss = 1.1359643936157227
In grad_steps = 602, loss = 0.33875298500061035
In grad_steps = 603, loss = 0.4291307330131531
In grad_steps = 604, loss = 0.3384056091308594
In grad_steps = 605, loss = 0.36582791805267334
In grad_steps = 606, loss = 0.4450017809867859
In grad_steps = 607, loss = 0.2060583084821701
In grad_steps = 608, loss = 0.2622997462749481
In grad_steps = 609, loss = 0.1904299408197403
In grad_steps = 610, loss = 0.1393069624900818
In grad_steps = 611, loss = 2.258105516433716
In grad_steps = 612, loss = 0.15278981626033783
In grad_steps = 613, loss = 0.07213594019412994
In grad_steps = 614, loss = 0.06316053122282028
In grad_steps = 615, loss = 2.0712220668792725
In grad_steps = 616, loss = 1.5909522771835327
In grad_steps = 617, loss = 0.26162660121917725
In grad_steps = 618, loss = 1.4387283325195312
In grad_steps = 619, loss = 1.4010097980499268
In grad_steps = 620, loss = 0.37122687697410583
In grad_steps = 621, loss = 0.8765519261360168
In grad_steps = 622, loss = 0.6979216933250427
In grad_steps = 623, loss = 0.7088022232055664
In grad_steps = 624, loss = 0.5923841595649719
In grad_steps = 625, loss = 0.5071185827255249
In grad_steps = 626, loss = 0.4653981924057007
In grad_steps = 627, loss = 0.36368438601493835
In grad_steps = 628, loss = 0.27478930354118347
In grad_steps = 629, loss = 1.3230202198028564
In grad_steps = 630, loss = 1.4384337663650513
In grad_steps = 631, loss = 1.3938716650009155
i = 3, Test ensemble probabilities = 
[array([[0.6967262 , 0.3032738 ],
       [0.7028291 , 0.29717088],
       [0.70460814, 0.2953919 ],
       [0.68939936, 0.31060067],
       [0.7036712 , 0.29632884],
       [0.7036262 , 0.29637375],
       [0.6958931 , 0.30410686],
       [0.68907434, 0.31092563],
       [0.6974445 , 0.30255547],
       [0.6990177 , 0.3009823 ],
       [0.6981735 , 0.3018265 ],
       [0.7090719 , 0.29092813],
       [0.7018059 , 0.2981941 ],
       [0.70384294, 0.29615703],
       [0.69626576, 0.30373424],
       [0.6943978 , 0.3056022 ],
       [0.69633776, 0.3036622 ],
       [0.70439136, 0.29560867],
       [0.70563054, 0.2943695 ],
       [0.6988664 , 0.30113357],
       [0.69604003, 0.30395997]], dtype=float32), array([[0.6744076 , 0.32559237],
       [0.63860315, 0.36139688],
       [0.654905  , 0.34509492],
       [0.56939536, 0.43060464],
       [0.681396  , 0.318604  ],
       [0.60607004, 0.39392996],
       [0.6339681 , 0.36603186],
       [0.608821  , 0.39117903],
       [0.6456442 , 0.35435578],
       [0.63895434, 0.3610457 ],
       [0.59237045, 0.4076295 ],
       [0.6148586 , 0.38514137],
       [0.6200046 , 0.3799954 ],
       [0.62975216, 0.37024784],
       [0.61797065, 0.38202932],
       [0.60365945, 0.39634055],
       [0.6642158 , 0.3357842 ],
       [0.6495719 , 0.35042813],
       [0.62240934, 0.37759066],
       [0.6177311 , 0.3822689 ],
       [0.54644036, 0.45355964]], dtype=float32), array([[0.7289234 , 0.2710766 ],
       [0.6688579 , 0.33114213],
       [0.7037342 , 0.29626578],
       [0.6693629 , 0.33063707],
       [0.727387  , 0.27261305],
       [0.6707304 , 0.32926956],
       [0.68790483, 0.31209517],
       [0.6725088 , 0.32749122],
       [0.6767822 , 0.32321778],
       [0.6714226 , 0.3285774 ],
       [0.6862493 , 0.31375068],
       [0.6909219 , 0.3090781 ],
       [0.7119528 , 0.28804722],
       [0.70750237, 0.2924976 ],
       [0.65528005, 0.34472   ],
       [0.6819204 , 0.31807956],
       [0.71274483, 0.2872552 ],
       [0.7397543 , 0.26024565],
       [0.7187024 , 0.28129762],
       [0.7101613 , 0.28983867],
       [0.65661806, 0.3433819 ]], dtype=float32), array([[0.77815   , 0.22184995],
       [0.73417705, 0.26582295],
       [0.7476291 , 0.25237092],
       [0.686922  , 0.313078  ],
       [0.7559023 , 0.24409774],
       [0.7241618 , 0.2758382 ],
       [0.7486356 , 0.2513644 ],
       [0.7102997 , 0.28970027],
       [0.7148219 , 0.2851781 ],
       [0.73546785, 0.2645322 ],
       [0.7297    , 0.2703    ],
       [0.744603  , 0.25539696],
       [0.7260263 , 0.27397373],
       [0.7261355 , 0.27386445],
       [0.73964065, 0.26035935],
       [0.7617074 , 0.23829253],
       [0.73603404, 0.26396593],
       [0.7548001 , 0.2451999 ],
       [0.7320205 , 0.2679795 ],
       [0.7642733 , 0.2357267 ],
       [0.7227531 , 0.27724686]], dtype=float32)]
i = 3, Test true class= 
[0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 1 0 1 1 0 0]
In grad_steps = 0, loss = 0.19905029237270355
In grad_steps = 1, loss = 3.0864193439483643
In grad_steps = 2, loss = 1.984773874282837
In grad_steps = 3, loss = 0.2888457775115967
In grad_steps = 4, loss = 0.46197259426116943
In grad_steps = 5, loss = 0.49832916259765625
In grad_steps = 6, loss = 0.26152297854423523
In grad_steps = 7, loss = 0.1784316897392273
In grad_steps = 8, loss = 0.1196749210357666
In grad_steps = 9, loss = 0.07051941007375717
In grad_steps = 10, loss = 3.90303635597229
In grad_steps = 11, loss = 0.023188257589936256
In grad_steps = 12, loss = 0.03688029199838638
In grad_steps = 13, loss = 2.4014978408813477
In grad_steps = 14, loss = 2.3291378021240234
In grad_steps = 15, loss = 0.18470324575901031
In grad_steps = 16, loss = 0.28162074089050293
In grad_steps = 17, loss = 1.2261323928833008
In grad_steps = 18, loss = 0.8432368040084839
In grad_steps = 19, loss = 0.6317048668861389
In grad_steps = 20, loss = 0.7393468022346497
In grad_steps = 21, loss = 0.690767765045166
In grad_steps = 22, loss = 0.7440289258956909
In grad_steps = 23, loss = 0.6968284249305725
In grad_steps = 24, loss = 0.7192444205284119
In grad_steps = 25, loss = 0.5518901348114014
In grad_steps = 26, loss = 0.5085160136222839
In grad_steps = 27, loss = 0.4504244923591614
In grad_steps = 28, loss = 0.3740580379962921
In grad_steps = 29, loss = 1.7769660949707031
In grad_steps = 30, loss = 1.7174739837646484
In grad_steps = 31, loss = 1.6654412746429443
In grad_steps = 32, loss = 1.3515390157699585
In grad_steps = 33, loss = 0.4403728246688843
In grad_steps = 34, loss = 0.9185469746589661
In grad_steps = 35, loss = 0.8044341206550598
In grad_steps = 36, loss = 0.561264157295227
In grad_steps = 37, loss = 0.8683579564094543
In grad_steps = 38, loss = 1.0679434537887573
In grad_steps = 39, loss = 0.4122118055820465
In grad_steps = 40, loss = 0.4543606638908386
In grad_steps = 41, loss = 1.0595053434371948
In grad_steps = 42, loss = 0.3948788046836853
In grad_steps = 43, loss = 1.1200512647628784
In grad_steps = 44, loss = 0.9971110820770264
In grad_steps = 45, loss = 1.024067997932434
In grad_steps = 46, loss = 0.9882105588912964
In grad_steps = 47, loss = 0.6851314306259155
In grad_steps = 48, loss = 0.7100232243537903
In grad_steps = 49, loss = 0.8217120170593262
In grad_steps = 50, loss = 0.5887863039970398
In grad_steps = 51, loss = 0.5749879479408264
In grad_steps = 52, loss = 0.49198001623153687
In grad_steps = 53, loss = 0.8937641382217407
In grad_steps = 54, loss = 0.47954899072647095
In grad_steps = 55, loss = 0.46507570147514343
In grad_steps = 56, loss = 0.4801589846611023
In grad_steps = 57, loss = 1.0611222982406616
In grad_steps = 58, loss = 0.37956079840660095
In grad_steps = 59, loss = 0.3267492949962616
In grad_steps = 60, loss = 1.2975025177001953
In grad_steps = 61, loss = 0.358527272939682
In grad_steps = 62, loss = 1.3307290077209473
In grad_steps = 63, loss = 1.3125171661376953
In grad_steps = 64, loss = 1.1492146253585815
In grad_steps = 65, loss = 1.1052751541137695
In grad_steps = 66, loss = 0.5124285817146301
In grad_steps = 67, loss = 0.5409967303276062
In grad_steps = 68, loss = 0.7383385896682739
In grad_steps = 69, loss = 0.6647849678993225
In grad_steps = 70, loss = 0.7409427165985107
In grad_steps = 71, loss = 0.6818373799324036
In grad_steps = 72, loss = 0.5972375273704529
In grad_steps = 73, loss = 0.5574048757553101
In grad_steps = 74, loss = 0.554572343826294
In grad_steps = 75, loss = 0.9828275442123413
In grad_steps = 76, loss = 1.0283955335617065
In grad_steps = 77, loss = 0.4914214015007019
In grad_steps = 78, loss = 1.0191158056259155
In grad_steps = 79, loss = 0.3731325566768646
In grad_steps = 80, loss = 1.0436499118804932
In grad_steps = 81, loss = 1.04597806930542
In grad_steps = 82, loss = 0.44424766302108765
In grad_steps = 83, loss = 0.5026650428771973
In grad_steps = 84, loss = 1.0036859512329102
In grad_steps = 85, loss = 0.5604535341262817
In grad_steps = 86, loss = 0.48507314920425415
In grad_steps = 87, loss = 0.42592960596084595
In grad_steps = 88, loss = 1.0092346668243408
In grad_steps = 89, loss = 0.9398393630981445
In grad_steps = 90, loss = 0.9099794626235962
In grad_steps = 91, loss = 0.5676647424697876
In grad_steps = 92, loss = 0.8660718202590942
In grad_steps = 93, loss = 0.7872958183288574
In grad_steps = 94, loss = 0.706282377243042
In grad_steps = 95, loss = 0.661328136920929
In grad_steps = 96, loss = 0.6782180070877075
In grad_steps = 97, loss = 0.6360885500907898
In grad_steps = 98, loss = 0.6882791519165039
In grad_steps = 99, loss = 0.563562273979187
In grad_steps = 100, loss = 0.8084357380867004
In grad_steps = 101, loss = 0.7863607406616211
In grad_steps = 102, loss = 0.5576828122138977
In grad_steps = 103, loss = 0.8769344091415405
In grad_steps = 104, loss = 0.8395253419876099
In grad_steps = 105, loss = 0.545661985874176
In grad_steps = 106, loss = 0.7889703512191772
In grad_steps = 107, loss = 0.7669169306755066
In grad_steps = 108, loss = 0.6616004109382629
In grad_steps = 109, loss = 0.5952569246292114
In grad_steps = 110, loss = 0.6263436079025269
In grad_steps = 111, loss = 0.7986459136009216
In grad_steps = 112, loss = 0.7924121022224426
In grad_steps = 113, loss = 0.7295937538146973
In grad_steps = 114, loss = 0.7345600724220276
In grad_steps = 115, loss = 0.6623234152793884
In grad_steps = 116, loss = 0.7346354722976685
In grad_steps = 117, loss = 0.5734906792640686
In grad_steps = 118, loss = 0.8027602434158325
In grad_steps = 119, loss = 0.7801002264022827
In grad_steps = 120, loss = 0.5841561555862427
In grad_steps = 121, loss = 0.8483901619911194
In grad_steps = 122, loss = 0.8539735078811646
In grad_steps = 123, loss = 0.6312606334686279
In grad_steps = 124, loss = 0.7265241146087646
In grad_steps = 125, loss = 0.6251360774040222
In grad_steps = 126, loss = 0.8316598534584045
In grad_steps = 127, loss = 0.6609923243522644
In grad_steps = 128, loss = 0.7279029488563538
In grad_steps = 129, loss = 0.7874049544334412
In grad_steps = 130, loss = 0.7009603381156921
In grad_steps = 131, loss = 0.7142390608787537
In grad_steps = 132, loss = 0.6913853883743286
In grad_steps = 133, loss = 0.5267595648765564
In grad_steps = 134, loss = 0.5915504097938538
In grad_steps = 135, loss = 0.4565936028957367
In grad_steps = 136, loss = 0.45703208446502686
In grad_steps = 137, loss = 1.0545680522918701
In grad_steps = 138, loss = 0.4114172160625458
In grad_steps = 139, loss = 0.33933374285697937
In grad_steps = 140, loss = 0.35839906334877014
In grad_steps = 141, loss = 1.2635458707809448
In grad_steps = 142, loss = 1.322060465812683
In grad_steps = 143, loss = 0.34254786372184753
In grad_steps = 144, loss = 1.1796813011169434
In grad_steps = 145, loss = 1.2319904565811157
In grad_steps = 146, loss = 0.35636842250823975
In grad_steps = 147, loss = 1.14168381690979
In grad_steps = 148, loss = 1.0723786354064941
In grad_steps = 149, loss = 0.5049213171005249
In grad_steps = 150, loss = 0.9688708186149597
In grad_steps = 151, loss = 0.9017655253410339
In grad_steps = 152, loss = 0.8152849674224854
In grad_steps = 153, loss = 0.7327455878257751
In grad_steps = 154, loss = 0.5640102624893188
In grad_steps = 155, loss = 0.9914177656173706
In grad_steps = 156, loss = 1.1617597341537476
In grad_steps = 157, loss = 1.126703143119812
In grad_steps = 158, loss = 1.0049816370010376
In grad_steps = 159, loss = 0.5018711686134338
In grad_steps = 160, loss = 0.5234923958778381
In grad_steps = 161, loss = 0.8461885452270508
In grad_steps = 162, loss = 0.7978236675262451
In grad_steps = 163, loss = 0.7687979936599731
In grad_steps = 164, loss = 0.6473240852355957
In grad_steps = 165, loss = 0.5608644485473633
In grad_steps = 166, loss = 0.5213425159454346
In grad_steps = 167, loss = 0.4849683344364166
In grad_steps = 168, loss = 1.1185624599456787
In grad_steps = 169, loss = 0.3492562472820282
In grad_steps = 170, loss = 0.3321682810783386
In grad_steps = 171, loss = 1.3099784851074219
In grad_steps = 172, loss = 1.3804845809936523
In grad_steps = 173, loss = 0.3056804835796356
In grad_steps = 174, loss = 0.3138844966888428
In grad_steps = 175, loss = 1.2864274978637695
In grad_steps = 176, loss = 1.1775963306427002
In grad_steps = 177, loss = 0.39082083106040955
In grad_steps = 178, loss = 0.43282005190849304
In grad_steps = 179, loss = 1.0290303230285645
In grad_steps = 180, loss = 0.4794182777404785
In grad_steps = 181, loss = 0.9412753582000732
In grad_steps = 182, loss = 0.8950811624526978
In grad_steps = 183, loss = 0.8125668168067932
In grad_steps = 184, loss = 0.6936842799186707
In grad_steps = 185, loss = 0.6087538003921509
In grad_steps = 186, loss = 0.536212682723999
In grad_steps = 187, loss = 1.0486326217651367
In grad_steps = 188, loss = 1.1523113250732422
In grad_steps = 189, loss = 1.104967713356018
In grad_steps = 190, loss = 1.021016001701355
In grad_steps = 191, loss = 0.4746302366256714
In grad_steps = 192, loss = 0.8734970688819885
In grad_steps = 193, loss = 0.7710501551628113
In grad_steps = 194, loss = 0.6603854894638062
In grad_steps = 195, loss = 0.792863130569458
In grad_steps = 196, loss = 0.8901621699333191
In grad_steps = 197, loss = 0.5079832673072815
In grad_steps = 198, loss = 0.517069935798645
In grad_steps = 199, loss = 0.9408391714096069
In grad_steps = 200, loss = 0.4693686366081238
In grad_steps = 201, loss = 1.0520987510681152
In grad_steps = 202, loss = 0.9585352540016174
In grad_steps = 203, loss = 1.0332444906234741
In grad_steps = 204, loss = 0.9892669320106506
In grad_steps = 205, loss = 0.5344732999801636
In grad_steps = 206, loss = 0.5521762371063232
In grad_steps = 207, loss = 0.5539036393165588
In grad_steps = 208, loss = 0.8234659433364868
In grad_steps = 209, loss = 0.8115943074226379
In grad_steps = 210, loss = 0.7623355388641357
In grad_steps = 211, loss = 0.6347278952598572
In grad_steps = 212, loss = 0.7029826641082764
In grad_steps = 213, loss = 0.7126455903053284
In grad_steps = 214, loss = 0.6915929317474365
In grad_steps = 215, loss = 0.7366997003555298
In grad_steps = 216, loss = 0.6085433959960938
In grad_steps = 217, loss = 0.5631153583526611
In grad_steps = 218, loss = 0.8446099162101746
In grad_steps = 219, loss = 0.49907854199409485
In grad_steps = 220, loss = 0.9266481995582581
In grad_steps = 221, loss = 0.9854953289031982
In grad_steps = 222, loss = 0.9086456894874573
In grad_steps = 223, loss = 0.9258389472961426
In grad_steps = 224, loss = 0.5174003839492798
In grad_steps = 225, loss = 0.5311278700828552
In grad_steps = 226, loss = 0.8385618925094604
In grad_steps = 227, loss = 0.5900201201438904
In grad_steps = 228, loss = 0.8336333632469177
In grad_steps = 229, loss = 0.7825367450714111
In grad_steps = 230, loss = 0.7404208183288574
In grad_steps = 231, loss = 0.713411808013916
In grad_steps = 232, loss = 0.6727113723754883
In grad_steps = 233, loss = 0.7420707941055298
In grad_steps = 234, loss = 0.7739662528038025
In grad_steps = 235, loss = 0.6111537218093872
In grad_steps = 236, loss = 0.8048300743103027
In grad_steps = 237, loss = 0.5484893321990967
In grad_steps = 238, loss = 0.8484103083610535
In grad_steps = 239, loss = 0.8403376340866089
In grad_steps = 240, loss = 0.5519666075706482
In grad_steps = 241, loss = 0.5611600875854492
In grad_steps = 242, loss = 0.8563904166221619
In grad_steps = 243, loss = 0.5745598077774048
In grad_steps = 244, loss = 0.5378350019454956
In grad_steps = 245, loss = 0.5227946639060974
In grad_steps = 246, loss = 0.9091160297393799
In grad_steps = 247, loss = 0.8811385631561279
In grad_steps = 248, loss = 0.8828314542770386
In grad_steps = 249, loss = 0.5452835559844971
In grad_steps = 250, loss = 0.8319985270500183
In grad_steps = 251, loss = 0.8410019278526306
In grad_steps = 252, loss = 0.6049549579620361
In grad_steps = 253, loss = 0.590769350528717
In grad_steps = 254, loss = 0.5948194265365601
In grad_steps = 255, loss = 0.5865198969841003
In grad_steps = 256, loss = 0.6052290797233582
In grad_steps = 257, loss = 0.5321924090385437
In grad_steps = 258, loss = 0.845873236656189
In grad_steps = 259, loss = 0.8349931240081787
In grad_steps = 260, loss = 0.5510799884796143
In grad_steps = 261, loss = 0.8669428825378418
In grad_steps = 262, loss = 0.8676341772079468
In grad_steps = 263, loss = 0.5431513786315918
In grad_steps = 264, loss = 0.8259319067001343
In grad_steps = 265, loss = 0.8046185970306396
In grad_steps = 266, loss = 0.6133798360824585
In grad_steps = 267, loss = 0.5871490836143494
In grad_steps = 268, loss = 0.6071438789367676
In grad_steps = 269, loss = 0.8097853660583496
In grad_steps = 270, loss = 0.789495587348938
In grad_steps = 271, loss = 0.7605629563331604
In grad_steps = 272, loss = 0.7375605702400208
In grad_steps = 273, loss = 0.631103515625
In grad_steps = 274, loss = 0.7186704277992249
In grad_steps = 275, loss = 0.6524919867515564
In grad_steps = 276, loss = 0.740772008895874
In grad_steps = 277, loss = 0.7081467509269714
In grad_steps = 278, loss = 0.6342018842697144
In grad_steps = 279, loss = 0.7506406307220459
In grad_steps = 280, loss = 0.7481616735458374
In grad_steps = 281, loss = 0.6484514474868774
In grad_steps = 282, loss = 0.6963121891021729
In grad_steps = 283, loss = 0.6643224954605103
In grad_steps = 284, loss = 0.7512227892875671
In grad_steps = 285, loss = 0.6608267426490784
In grad_steps = 286, loss = 0.7136263847351074
In grad_steps = 287, loss = 0.7395601272583008
In grad_steps = 288, loss = 0.6949474215507507
In grad_steps = 289, loss = 0.7008311152458191
In grad_steps = 290, loss = 0.6836138963699341
In grad_steps = 291, loss = 0.61524498462677
In grad_steps = 292, loss = 0.6343066692352295
In grad_steps = 293, loss = 0.5406926870346069
In grad_steps = 294, loss = 0.5361751317977905
In grad_steps = 295, loss = 0.9244887232780457
In grad_steps = 296, loss = 0.5083597898483276
In grad_steps = 297, loss = 0.43481290340423584
In grad_steps = 298, loss = 0.4434276223182678
In grad_steps = 299, loss = 1.0325074195861816
In grad_steps = 300, loss = 1.0704373121261597
In grad_steps = 301, loss = 0.4198971688747406
In grad_steps = 302, loss = 1.0009013414382935
In grad_steps = 303, loss = 1.0783462524414062
In grad_steps = 304, loss = 0.4094441831111908
In grad_steps = 305, loss = 1.0508124828338623
In grad_steps = 306, loss = 1.008821964263916
In grad_steps = 307, loss = 0.48263019323349
In grad_steps = 308, loss = 0.9779263138771057
In grad_steps = 309, loss = 0.9188988208770752
In grad_steps = 310, loss = 0.8702903389930725
In grad_steps = 311, loss = 0.8101733326911926
In grad_steps = 312, loss = 0.7426024675369263
In grad_steps = 313, loss = 0.6765652298927307
In grad_steps = 314, loss = 0.7307570576667786
In grad_steps = 315, loss = 0.7439504265785217
In grad_steps = 316, loss = 0.7313327789306641
In grad_steps = 317, loss = 0.6500617861747742
In grad_steps = 318, loss = 0.6110302805900574
In grad_steps = 319, loss = 0.7383579611778259
In grad_steps = 320, loss = 0.738843560218811
In grad_steps = 321, loss = 0.7459153532981873
In grad_steps = 322, loss = 0.6761996150016785
In grad_steps = 323, loss = 0.63364177942276
In grad_steps = 324, loss = 0.6256096363067627
In grad_steps = 325, loss = 0.6135635375976562
In grad_steps = 326, loss = 0.8335492014884949
In grad_steps = 327, loss = 0.5012441277503967
In grad_steps = 328, loss = 0.47808200120925903
In grad_steps = 329, loss = 0.9382463693618774
In grad_steps = 330, loss = 1.0032178163528442
In grad_steps = 331, loss = 0.4256593883037567
In grad_steps = 332, loss = 0.4196925461292267
In grad_steps = 333, loss = 1.0490010976791382
In grad_steps = 334, loss = 0.9924749135971069
In grad_steps = 335, loss = 0.4282626211643219
In grad_steps = 336, loss = 0.4475172162055969
In grad_steps = 337, loss = 1.0174124240875244
In grad_steps = 338, loss = 0.4667891263961792
In grad_steps = 339, loss = 0.9817381501197815
In grad_steps = 340, loss = 0.9804874658584595
In grad_steps = 341, loss = 0.9192805886268616
In grad_steps = 342, loss = 0.8352647423744202
In grad_steps = 343, loss = 0.7912024259567261
In grad_steps = 344, loss = 0.7585011124610901
In grad_steps = 345, loss = 0.682161271572113
In grad_steps = 346, loss = 0.7723394632339478
In grad_steps = 347, loss = 0.7501633763313293
In grad_steps = 348, loss = 0.7466205358505249
In grad_steps = 349, loss = 0.5901756286621094
In grad_steps = 350, loss = 0.7561161518096924
In grad_steps = 351, loss = 0.7245451211929321
In grad_steps = 352, loss = 0.6867291331291199
In grad_steps = 353, loss = 0.7049509882926941
In grad_steps = 354, loss = 0.7395979762077332
In grad_steps = 355, loss = 0.6199982762336731
In grad_steps = 356, loss = 0.6306037902832031
In grad_steps = 357, loss = 0.7205713391304016
In grad_steps = 358, loss = 0.62142014503479
In grad_steps = 359, loss = 0.8113817572593689
In grad_steps = 360, loss = 0.7411394119262695
In grad_steps = 361, loss = 0.8192387223243713
In grad_steps = 362, loss = 0.7854970097541809
In grad_steps = 363, loss = 0.6331191658973694
In grad_steps = 364, loss = 0.6205743551254272
In grad_steps = 365, loss = 0.6203516721725464
In grad_steps = 366, loss = 0.7305128574371338
In grad_steps = 367, loss = 0.740153431892395
In grad_steps = 368, loss = 0.6939963102340698
In grad_steps = 369, loss = 0.651037871837616
In grad_steps = 370, loss = 0.6714206337928772
In grad_steps = 371, loss = 0.698854923248291
In grad_steps = 372, loss = 0.6855246424674988
In grad_steps = 373, loss = 0.6863805651664734
In grad_steps = 374, loss = 0.6097450256347656
In grad_steps = 375, loss = 0.5660954117774963
In grad_steps = 376, loss = 0.7651582956314087
In grad_steps = 377, loss = 0.4788522720336914
In grad_steps = 378, loss = 0.8713676929473877
In grad_steps = 379, loss = 0.9621607661247253
In grad_steps = 380, loss = 0.8379936218261719
In grad_steps = 381, loss = 0.9012500047683716
In grad_steps = 382, loss = 0.5170074701309204
In grad_steps = 383, loss = 0.5213654637336731
In grad_steps = 384, loss = 0.8138975501060486
In grad_steps = 385, loss = 0.5878472924232483
In grad_steps = 386, loss = 0.8238517642021179
In grad_steps = 387, loss = 0.7306670546531677
In grad_steps = 388, loss = 0.6407904028892517
In grad_steps = 389, loss = 0.6774719953536987
In grad_steps = 390, loss = 0.6184169054031372
In grad_steps = 391, loss = 0.7413719892501831
In grad_steps = 392, loss = 0.7725400924682617
In grad_steps = 393, loss = 0.586337149143219
In grad_steps = 394, loss = 0.8288722038269043
In grad_steps = 395, loss = 0.472137987613678
In grad_steps = 396, loss = 0.8524409532546997
In grad_steps = 397, loss = 0.830303430557251
In grad_steps = 398, loss = 0.5015446543693542
In grad_steps = 399, loss = 0.5352032780647278
In grad_steps = 400, loss = 0.8951340913772583
In grad_steps = 401, loss = 0.5793447494506836
In grad_steps = 402, loss = 0.5382505655288696
In grad_steps = 403, loss = 0.5049034357070923
In grad_steps = 404, loss = 0.9929323196411133
In grad_steps = 405, loss = 0.8421778082847595
In grad_steps = 406, loss = 0.895026445388794
In grad_steps = 407, loss = 0.5411111116409302
In grad_steps = 408, loss = 0.6022008657455444
In grad_steps = 409, loss = 0.7292776703834534
In grad_steps = 410, loss = 0.643619954586029
In grad_steps = 411, loss = 0.5950024127960205
In grad_steps = 412, loss = 0.5855176448822021
In grad_steps = 413, loss = 0.5927453637123108
In grad_steps = 414, loss = 0.5982741713523865
In grad_steps = 415, loss = 0.4466102123260498
In grad_steps = 416, loss = 0.8056860566139221
In grad_steps = 417, loss = 0.7282472252845764
In grad_steps = 418, loss = 0.5426735877990723
In grad_steps = 419, loss = 0.7445365190505981
In grad_steps = 420, loss = 0.7487481236457825
In grad_steps = 421, loss = 0.44865140318870544
In grad_steps = 422, loss = 0.7158159613609314
In grad_steps = 423, loss = 0.676807165145874
In grad_steps = 424, loss = 0.7251794934272766
In grad_steps = 425, loss = 0.7884615659713745
In grad_steps = 426, loss = 0.7486225366592407
In grad_steps = 427, loss = 0.6803451180458069
In grad_steps = 428, loss = 0.6208826899528503
In grad_steps = 429, loss = 0.6433039903640747
In grad_steps = 430, loss = 0.4525384306907654
In grad_steps = 431, loss = 0.85807204246521
In grad_steps = 432, loss = 0.6807047724723816
In grad_steps = 433, loss = 0.268568754196167
In grad_steps = 434, loss = 1.9536652565002441
In grad_steps = 435, loss = 0.7878499627113342
In grad_steps = 436, loss = 0.4660312533378601
In grad_steps = 437, loss = 0.7860098481178284
In grad_steps = 438, loss = 0.6942116618156433
In grad_steps = 439, loss = 0.7100038528442383
In grad_steps = 440, loss = 0.409069687128067
In grad_steps = 441, loss = 0.7705909609794617
In grad_steps = 442, loss = 0.5573612451553345
In grad_steps = 443, loss = 0.8290533423423767
In grad_steps = 444, loss = 0.5531454682350159
In grad_steps = 445, loss = 0.6316710114479065
In grad_steps = 446, loss = 0.5144054293632507
In grad_steps = 447, loss = 0.5008806586265564
In grad_steps = 448, loss = 0.5433628559112549
In grad_steps = 449, loss = 0.40786951780319214
In grad_steps = 450, loss = 0.45643940567970276
In grad_steps = 451, loss = 0.20004500448703766
In grad_steps = 452, loss = 0.2788263261318207
In grad_steps = 453, loss = 1.8525958061218262
In grad_steps = 454, loss = 0.25330477952957153
In grad_steps = 455, loss = 0.14217232167720795
In grad_steps = 456, loss = 0.17683866620063782
In grad_steps = 457, loss = 1.4865820407867432
In grad_steps = 458, loss = 1.339464545249939
In grad_steps = 459, loss = 0.31408870220184326
In grad_steps = 460, loss = 0.9662485718727112
In grad_steps = 461, loss = 1.0858685970306396
In grad_steps = 462, loss = 0.37387537956237793
In grad_steps = 463, loss = 0.9712703824043274
In grad_steps = 464, loss = 0.8800883293151855
In grad_steps = 465, loss = 0.538694441318512
In grad_steps = 466, loss = 0.8560590744018555
In grad_steps = 467, loss = 0.7921767234802246
In grad_steps = 468, loss = 0.7675831317901611
In grad_steps = 469, loss = 0.7097784876823425
In grad_steps = 470, loss = 0.6441636681556702
In grad_steps = 471, loss = 0.7153109908103943
In grad_steps = 472, loss = 0.770859956741333
In grad_steps = 473, loss = 0.8041169047355652
In grad_steps = 474, loss = 0.7589306831359863
In grad_steps = 475, loss = 0.5740294456481934
In grad_steps = 476, loss = 0.5225175619125366
In grad_steps = 477, loss = 0.8515353202819824
In grad_steps = 478, loss = 0.7618176937103271
In grad_steps = 479, loss = 0.8027850389480591
In grad_steps = 480, loss = 0.7395920157432556
In grad_steps = 481, loss = 0.6556684374809265
In grad_steps = 482, loss = 0.6488219499588013
In grad_steps = 483, loss = 0.6627938151359558
In grad_steps = 484, loss = 0.7701616883277893
In grad_steps = 485, loss = 0.49101388454437256
In grad_steps = 486, loss = 0.4771842360496521
In grad_steps = 487, loss = 0.9117834568023682
In grad_steps = 488, loss = 1.0216255187988281
In grad_steps = 489, loss = 0.38161322474479675
In grad_steps = 490, loss = 0.3965056240558624
In grad_steps = 491, loss = 1.0638364553451538
In grad_steps = 492, loss = 0.9694823026657104
In grad_steps = 493, loss = 0.41998255252838135
In grad_steps = 494, loss = 0.4132753312587738
In grad_steps = 495, loss = 1.01558256149292
In grad_steps = 496, loss = 0.4537907838821411
In grad_steps = 497, loss = 0.977267861366272
In grad_steps = 498, loss = 0.9064504504203796
In grad_steps = 499, loss = 0.873808741569519
In grad_steps = 500, loss = 0.7446893453598022
In grad_steps = 501, loss = 0.7805585265159607
In grad_steps = 502, loss = 0.7081917524337769
In grad_steps = 503, loss = 0.691770076751709
In grad_steps = 504, loss = 0.7518118023872375
In grad_steps = 505, loss = 0.7446527481079102
In grad_steps = 506, loss = 0.7056193351745605
In grad_steps = 507, loss = 0.5042451620101929
In grad_steps = 508, loss = 0.7760121822357178
In grad_steps = 509, loss = 0.7761335968971252
In grad_steps = 510, loss = 0.742006778717041
In grad_steps = 511, loss = 0.6794445514678955
In grad_steps = 512, loss = 0.7083442211151123
In grad_steps = 513, loss = 0.6830666661262512
In grad_steps = 514, loss = 0.614486813545227
In grad_steps = 515, loss = 0.6939534544944763
In grad_steps = 516, loss = 0.6587316393852234
In grad_steps = 517, loss = 0.793893039226532
In grad_steps = 518, loss = 0.737916886806488
In grad_steps = 519, loss = 0.8235259056091309
In grad_steps = 520, loss = 0.7958672642707825
In grad_steps = 521, loss = 0.6409524083137512
In grad_steps = 522, loss = 0.5883913040161133
In grad_steps = 523, loss = 0.5771036148071289
In grad_steps = 524, loss = 0.6819995641708374
In grad_steps = 525, loss = 0.7203431129455566
In grad_steps = 526, loss = 0.6450332403182983
In grad_steps = 527, loss = 0.6223846077919006
In grad_steps = 528, loss = 0.6174489259719849
In grad_steps = 529, loss = 0.710892915725708
In grad_steps = 530, loss = 0.7074254751205444
In grad_steps = 531, loss = 0.5452053546905518
In grad_steps = 532, loss = 0.5896419286727905
In grad_steps = 533, loss = 0.5573033690452576
In grad_steps = 534, loss = 0.5167942047119141
In grad_steps = 535, loss = 0.29235973954200745
In grad_steps = 536, loss = 0.5253185033798218
In grad_steps = 537, loss = 1.0149664878845215
In grad_steps = 538, loss = 0.6831011176109314
In grad_steps = 539, loss = 0.9169358015060425
In grad_steps = 540, loss = 0.2822008430957794
In grad_steps = 541, loss = 0.39461877942085266
In grad_steps = 542, loss = 0.5975463390350342
In grad_steps = 543, loss = 0.23226435482501984
In grad_steps = 544, loss = 0.9011118412017822
In grad_steps = 545, loss = 0.6348973512649536
In grad_steps = 546, loss = 0.08296255022287369
In grad_steps = 547, loss = 0.06604740768671036
In grad_steps = 548, loss = 0.020076284185051918
In grad_steps = 549, loss = 4.466871738433838
In grad_steps = 550, loss = 1.789715051651001
In grad_steps = 551, loss = 0.29106375575065613
In grad_steps = 552, loss = 0.8090603947639465
In grad_steps = 553, loss = 0.4920562505722046
In grad_steps = 554, loss = 0.4952220022678375
In grad_steps = 555, loss = 0.5146018266677856
In grad_steps = 556, loss = 0.5774975419044495
In grad_steps = 557, loss = 0.6651532053947449
In grad_steps = 558, loss = 0.5848707556724548
In grad_steps = 559, loss = 0.7396886944770813
In grad_steps = 560, loss = 0.7939187288284302
In grad_steps = 561, loss = 0.6541956067085266
In grad_steps = 562, loss = 0.7995941638946533
In grad_steps = 563, loss = 0.642151415348053
In grad_steps = 564, loss = 0.6426368951797485
In grad_steps = 565, loss = 0.5160765051841736
In grad_steps = 566, loss = 0.3598143756389618
In grad_steps = 567, loss = 0.6129234433174133
In grad_steps = 568, loss = 0.7269531488418579
In grad_steps = 569, loss = 0.4745798110961914
In grad_steps = 570, loss = 0.5007968544960022
In grad_steps = 571, loss = 0.4662807583808899
In grad_steps = 572, loss = 0.38994455337524414
In grad_steps = 573, loss = 0.055544812232255936
In grad_steps = 574, loss = 2.833538293838501
In grad_steps = 575, loss = 1.448087215423584
In grad_steps = 576, loss = 0.29693010449409485
In grad_steps = 577, loss = 0.5663134455680847
In grad_steps = 578, loss = 0.312256395816803
In grad_steps = 579, loss = 0.5146113634109497
In grad_steps = 580, loss = 0.5323318243026733
In grad_steps = 581, loss = 0.38664525747299194
In grad_steps = 582, loss = 0.7423935532569885
In grad_steps = 583, loss = 0.9453863501548767
In grad_steps = 584, loss = 0.7816731333732605
In grad_steps = 585, loss = 0.23498955368995667
In grad_steps = 586, loss = 0.24188439548015594
In grad_steps = 587, loss = 0.21058157086372375
In grad_steps = 588, loss = 0.12985795736312866
In grad_steps = 589, loss = 0.628831148147583
In grad_steps = 590, loss = 0.4026379883289337
In grad_steps = 591, loss = 0.13417762517929077
In grad_steps = 592, loss = 0.6278119087219238
In grad_steps = 593, loss = 0.5781768560409546
In grad_steps = 594, loss = 0.11463328450918198
In grad_steps = 595, loss = 0.4869239926338196
In grad_steps = 596, loss = 0.28972235321998596
In grad_steps = 597, loss = 0.6023946404457092
In grad_steps = 598, loss = 0.16846148669719696
In grad_steps = 599, loss = 0.25183990597724915
In grad_steps = 600, loss = 0.4338769018650055
In grad_steps = 601, loss = 1.5043413639068604
In grad_steps = 602, loss = 1.9874058961868286
In grad_steps = 603, loss = 2.294999122619629
In grad_steps = 604, loss = 0.15773561596870422
In grad_steps = 605, loss = 0.15665824711322784
In grad_steps = 606, loss = 0.21658197045326233
In grad_steps = 607, loss = 0.12847621738910675
In grad_steps = 608, loss = 0.13808941841125488
In grad_steps = 609, loss = 0.11785455793142319
In grad_steps = 610, loss = 0.10411781817674637
In grad_steps = 611, loss = 2.394648551940918
In grad_steps = 612, loss = 0.11470354348421097
In grad_steps = 613, loss = 0.09986703842878342
In grad_steps = 614, loss = 0.1275203824043274
In grad_steps = 615, loss = 2.0176899433135986
In grad_steps = 616, loss = 1.6979738473892212
In grad_steps = 617, loss = 0.30242180824279785
In grad_steps = 618, loss = 1.2243480682373047
In grad_steps = 619, loss = 0.9678049087524414
In grad_steps = 620, loss = 0.7034310102462769
In grad_steps = 621, loss = 0.4545065760612488
In grad_steps = 622, loss = 0.3326224386692047
In grad_steps = 623, loss = 1.5750939846038818
In grad_steps = 624, loss = 0.25739482045173645
In grad_steps = 625, loss = 0.22360560297966003
In grad_steps = 626, loss = 0.22919192910194397
In grad_steps = 627, loss = 0.16925661265850067
In grad_steps = 628, loss = 0.13782979547977448
In grad_steps = 629, loss = 2.5909459590911865
In grad_steps = 630, loss = 2.157841444015503
In grad_steps = 631, loss = 1.3741849660873413
i = 4, Test ensemble probabilities = 
[array([[0.6967262 , 0.3032738 ],
       [0.7028291 , 0.29717088],
       [0.70460814, 0.2953919 ],
       [0.68939936, 0.31060067],
       [0.7036712 , 0.29632884],
       [0.7036262 , 0.29637375],
       [0.6958931 , 0.30410686],
       [0.68907434, 0.31092563],
       [0.6974445 , 0.30255547],
       [0.6990177 , 0.3009823 ],
       [0.6981735 , 0.3018265 ],
       [0.7090719 , 0.29092813],
       [0.7018059 , 0.2981941 ],
       [0.70384294, 0.29615703],
       [0.69626576, 0.30373424],
       [0.6943978 , 0.3056022 ],
       [0.69633776, 0.3036622 ],
       [0.70439136, 0.29560867],
       [0.70563054, 0.2943695 ],
       [0.6988664 , 0.30113357],
       [0.69604003, 0.30395997]], dtype=float32), array([[0.6744076 , 0.32559237],
       [0.63860315, 0.36139688],
       [0.654905  , 0.34509492],
       [0.56939536, 0.43060464],
       [0.681396  , 0.318604  ],
       [0.60607004, 0.39392996],
       [0.6339681 , 0.36603186],
       [0.608821  , 0.39117903],
       [0.6456442 , 0.35435578],
       [0.63895434, 0.3610457 ],
       [0.59237045, 0.4076295 ],
       [0.6148586 , 0.38514137],
       [0.6200046 , 0.3799954 ],
       [0.62975216, 0.37024784],
       [0.61797065, 0.38202932],
       [0.60365945, 0.39634055],
       [0.6642158 , 0.3357842 ],
       [0.6495719 , 0.35042813],
       [0.62240934, 0.37759066],
       [0.6177311 , 0.3822689 ],
       [0.54644036, 0.45355964]], dtype=float32), array([[0.7289234 , 0.2710766 ],
       [0.6688579 , 0.33114213],
       [0.7037342 , 0.29626578],
       [0.6693629 , 0.33063707],
       [0.727387  , 0.27261305],
       [0.6707304 , 0.32926956],
       [0.68790483, 0.31209517],
       [0.6725088 , 0.32749122],
       [0.6767822 , 0.32321778],
       [0.6714226 , 0.3285774 ],
       [0.6862493 , 0.31375068],
       [0.6909219 , 0.3090781 ],
       [0.7119528 , 0.28804722],
       [0.70750237, 0.2924976 ],
       [0.65528005, 0.34472   ],
       [0.6819204 , 0.31807956],
       [0.71274483, 0.2872552 ],
       [0.7397543 , 0.26024565],
       [0.7187024 , 0.28129762],
       [0.7101613 , 0.28983867],
       [0.65661806, 0.3433819 ]], dtype=float32), array([[0.77815   , 0.22184995],
       [0.73417705, 0.26582295],
       [0.7476291 , 0.25237092],
       [0.686922  , 0.313078  ],
       [0.7559023 , 0.24409774],
       [0.7241618 , 0.2758382 ],
       [0.7486356 , 0.2513644 ],
       [0.7102997 , 0.28970027],
       [0.7148219 , 0.2851781 ],
       [0.73546785, 0.2645322 ],
       [0.7297    , 0.2703    ],
       [0.744603  , 0.25539696],
       [0.7260263 , 0.27397373],
       [0.7261355 , 0.27386445],
       [0.73964065, 0.26035935],
       [0.7617074 , 0.23829253],
       [0.73603404, 0.26396593],
       [0.7548001 , 0.2451999 ],
       [0.7320205 , 0.2679795 ],
       [0.7642733 , 0.2357267 ],
       [0.7227531 , 0.27724686]], dtype=float32), array([[0.6145433 , 0.38545668],
       [0.6168392 , 0.38316086],
       [0.602592  , 0.39740798],
       [0.5936868 , 0.4063132 ],
       [0.6220002 , 0.37799975],
       [0.6135952 , 0.38640478],
       [0.60894704, 0.39105296],
       [0.5869929 , 0.41300702],
       [0.60497266, 0.39502728],
       [0.61844385, 0.38155618],
       [0.60377806, 0.39622194],
       [0.623445  , 0.37655506],
       [0.60973275, 0.39026728],
       [0.62915915, 0.37084088],
       [0.62496006, 0.37503988],
       [0.64198416, 0.35801587],
       [0.6239468 , 0.37605324],
       [0.6109974 , 0.38900262],
       [0.61733735, 0.38266262],
       [0.63340956, 0.3665904 ],
       [0.5919497 , 0.4080503 ]], dtype=float32)]
i = 4, Test true class= 
[0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 1 0 1 1 0 0]
Final, Test average ensemble probabilities = 
[[0.6985501  0.30144987]
 [0.6722613  0.32773873]
 [0.6826937  0.3173063 ]
 [0.6417533  0.3582467 ]
 [0.69807136 0.3019287 ]
 [0.66363674 0.33636323]
 [0.6750697  0.32493025]
 [0.65353936 0.34646064]
 [0.66793305 0.33206686]
 [0.6726613  0.32733876]
 [0.6620543  0.33794576]
 [0.6765801  0.32341993]
 [0.67390454 0.32609552]
 [0.67927843 0.32072157]
 [0.6668235  0.33317655]
 [0.67673385 0.32326615]
 [0.6866558  0.31334415]
 [0.691903   0.30809698]
 [0.67922    0.32077998]
 [0.68488824 0.31511164]
 [0.6427602  0.35723972]]
Accuracy: 0.7143
MCC: 0.2070
AUC: 0.3222
Confusion Matrix:
tensor([[15,  0],
        [ 6,  0]])
Specificity: 1.0000
Precision (Macro): 0.3571
F1 Score (Macro): 0.4167
Expected Calibration Error (ECE): 0.0406
NLL loss: 0.6093
Main task is done! Can finish
