Job is running on partition: gpu_ss11
Job is running under account: m2616_g
/pscratch/sd/t/tianle/conda/envs/lucid_surp_2024/lib/python3.11/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:10<00:30, 10.10s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:20<00:20, 10.10s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:30<00:10, 10.02s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:32<00:00,  6.98s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:32<00:00,  8.11s/it]
Llama3 has been loaded successfully.
Model:  LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(128256, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)
          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)
          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)
          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)
)
Size of dataset:  2222
In grad_steps = 0, loss = 0.7334465384483337
In grad_steps = 1, loss = 0.8182331919670105
In grad_steps = 2, loss = 0.7695340514183044
In grad_steps = 3, loss = 0.7781873345375061
In grad_steps = 4, loss = 0.7190706133842468
In grad_steps = 5, loss = 0.6939585208892822
In grad_steps = 6, loss = 0.6755613684654236
In grad_steps = 7, loss = 0.7780153751373291
In grad_steps = 8, loss = 0.6492263674736023
In grad_steps = 9, loss = 0.6713115572929382
In grad_steps = 10, loss = 0.7229143977165222
In grad_steps = 11, loss = 0.7446827292442322
In grad_steps = 12, loss = 0.7271780967712402
In grad_steps = 13, loss = 0.7459633946418762
In grad_steps = 14, loss = 0.7228260636329651
In grad_steps = 15, loss = 0.7034876346588135
In grad_steps = 16, loss = 0.6864734888076782
In grad_steps = 17, loss = 0.7037312984466553
In grad_steps = 18, loss = 0.7413205504417419
In grad_steps = 19, loss = 0.7410036325454712
In grad_steps = 20, loss = 0.6963307857513428
In grad_steps = 21, loss = 0.7069300413131714
In grad_steps = 22, loss = 0.7100424766540527
In grad_steps = 23, loss = 0.7494601011276245
In grad_steps = 24, loss = 0.6333298087120056
In grad_steps = 25, loss = 0.646141767501831
In grad_steps = 26, loss = 0.6990087628364563
In grad_steps = 27, loss = 0.8518080711364746
In grad_steps = 28, loss = 0.6898232102394104
In grad_steps = 29, loss = 0.7537038922309875
In grad_steps = 30, loss = 0.7106547355651855
In grad_steps = 31, loss = 0.6862991452217102
In grad_steps = 32, loss = 0.7000558972358704
In grad_steps = 33, loss = 0.6968480944633484
In grad_steps = 34, loss = 0.774718165397644
In grad_steps = 35, loss = 0.7180317640304565
In grad_steps = 36, loss = 0.6957454085350037
In grad_steps = 37, loss = 0.6960209012031555
In grad_steps = 38, loss = 0.6913772225379944
In grad_steps = 39, loss = 0.6470423340797424
In grad_steps = 40, loss = 0.670184850692749
In grad_steps = 41, loss = 0.7480705976486206
In grad_steps = 42, loss = 0.6957681775093079
In grad_steps = 43, loss = 0.7280465960502625
In grad_steps = 44, loss = 0.6803357601165771
In grad_steps = 45, loss = 0.7038683891296387
In grad_steps = 46, loss = 0.7341652512550354
In grad_steps = 47, loss = 0.68509840965271
In grad_steps = 48, loss = 0.6860719323158264
In grad_steps = 49, loss = 0.7283679842948914
In grad_steps = 50, loss = 0.7614135146141052
In grad_steps = 51, loss = 0.6691989898681641
In grad_steps = 52, loss = 0.6530150771141052
In grad_steps = 53, loss = 0.7386852502822876
In grad_steps = 54, loss = 0.7005419135093689
In grad_steps = 55, loss = 0.6459230780601501
In grad_steps = 56, loss = 0.6970904469490051
In grad_steps = 57, loss = 0.7282417416572571
In grad_steps = 58, loss = 0.6841534376144409
In grad_steps = 59, loss = 0.7145209312438965
In grad_steps = 60, loss = 0.7206454873085022
In grad_steps = 61, loss = 0.7043278217315674
In grad_steps = 62, loss = 0.7258893251419067
In grad_steps = 63, loss = 0.6996910572052002
In grad_steps = 64, loss = 0.7244178652763367
In grad_steps = 65, loss = 0.6972732543945312
In grad_steps = 66, loss = 0.684054434299469
In grad_steps = 67, loss = 0.7126305103302002
In grad_steps = 68, loss = 0.6937966346740723
In grad_steps = 69, loss = 0.7131280899047852
In grad_steps = 70, loss = 0.7378636598587036
In grad_steps = 71, loss = 0.6744357347488403
In grad_steps = 72, loss = 0.6927726864814758
In grad_steps = 73, loss = 0.699023425579071
In grad_steps = 74, loss = 0.7018190026283264
In grad_steps = 75, loss = 0.6795026063919067
In grad_steps = 76, loss = 0.6992759108543396
In grad_steps = 77, loss = 0.6652154326438904
In grad_steps = 78, loss = 0.7306654453277588
In grad_steps = 79, loss = 0.7412658333778381
In grad_steps = 80, loss = 0.6936647891998291
In grad_steps = 81, loss = 0.6956688165664673
In grad_steps = 82, loss = 0.6977620720863342
In grad_steps = 83, loss = 0.6911707520484924
In grad_steps = 84, loss = 0.6914899349212646
In grad_steps = 85, loss = 0.6966331601142883
In grad_steps = 86, loss = 0.664237380027771
In grad_steps = 87, loss = 0.6956573724746704
In grad_steps = 88, loss = 0.7239174246788025
In grad_steps = 89, loss = 0.670244574546814
In grad_steps = 90, loss = 0.669358491897583
In grad_steps = 91, loss = 0.7498288750648499
In grad_steps = 92, loss = 0.657238245010376
In grad_steps = 93, loss = 0.6386140584945679
In grad_steps = 94, loss = 0.7597268223762512
In grad_steps = 95, loss = 0.7686528563499451
In grad_steps = 96, loss = 0.734811544418335
In grad_steps = 97, loss = 0.7016077637672424
In grad_steps = 98, loss = 0.687680184841156
In grad_steps = 99, loss = 0.6925609707832336
In grad_steps = 100, loss = 0.6839653849601746
In grad_steps = 101, loss = 0.6987931728363037
In grad_steps = 102, loss = 0.6890043020248413
In grad_steps = 103, loss = 0.697461724281311
In grad_steps = 104, loss = 0.6819793581962585
In grad_steps = 105, loss = 0.6664716601371765
In grad_steps = 106, loss = 0.7659680247306824
In grad_steps = 107, loss = 0.6993674039840698
In grad_steps = 108, loss = 0.6709069013595581
In grad_steps = 109, loss = 0.6773119568824768
In grad_steps = 110, loss = 0.7391301989555359
In grad_steps = 111, loss = 0.75642329454422
In grad_steps = 112, loss = 0.6870477199554443
In grad_steps = 113, loss = 0.6930112242698669
In grad_steps = 114, loss = 0.710017204284668
In grad_steps = 115, loss = 0.6403172016143799
In grad_steps = 116, loss = 0.7371129989624023
In grad_steps = 117, loss = 0.6769185066223145
In grad_steps = 118, loss = 0.6279489994049072
In grad_steps = 119, loss = 0.8037630319595337
In grad_steps = 120, loss = 0.6426483392715454
In grad_steps = 121, loss = 0.6562983989715576
In grad_steps = 122, loss = 0.7047923803329468
In grad_steps = 123, loss = 0.711077868938446
In grad_steps = 124, loss = 0.695840060710907
In grad_steps = 125, loss = 0.6797036528587341
In grad_steps = 126, loss = 0.7293831706047058
In grad_steps = 127, loss = 0.6535007953643799
In grad_steps = 128, loss = 0.7015882730484009
In grad_steps = 129, loss = 0.6853253841400146
In grad_steps = 130, loss = 0.7339224219322205
In grad_steps = 131, loss = 0.7175922393798828
In grad_steps = 132, loss = 0.6703755855560303
In grad_steps = 133, loss = 0.6620616912841797
In grad_steps = 134, loss = 0.7000203132629395
In grad_steps = 135, loss = 0.744215726852417
In grad_steps = 136, loss = 0.6124315857887268
In grad_steps = 137, loss = 0.6275138258934021
In grad_steps = 138, loss = 0.6630818843841553
In grad_steps = 139, loss = 0.8152172565460205
In grad_steps = 140, loss = 0.6966224908828735
In grad_steps = 141, loss = 0.7816212177276611
In grad_steps = 142, loss = 0.7325522899627686
In grad_steps = 143, loss = 0.6801425218582153
In grad_steps = 144, loss = 0.6816653609275818
In grad_steps = 145, loss = 0.6702746152877808
In grad_steps = 146, loss = 0.706102192401886
In grad_steps = 147, loss = 0.7014155387878418
In grad_steps = 148, loss = 0.6887406706809998
In grad_steps = 149, loss = 0.685343861579895
In grad_steps = 150, loss = 0.6775708198547363
In grad_steps = 151, loss = 0.6719546914100647
In grad_steps = 152, loss = 0.660757303237915
In grad_steps = 153, loss = 0.697940468788147
In grad_steps = 154, loss = 0.67264324426651
In grad_steps = 155, loss = 0.6870525479316711
In grad_steps = 156, loss = 0.6617335677146912
In grad_steps = 157, loss = 0.6975001096725464
In grad_steps = 158, loss = 0.7067747116088867
In grad_steps = 159, loss = 0.6783415675163269
In grad_steps = 160, loss = 0.6441835165023804
In grad_steps = 161, loss = 0.7018071413040161
In grad_steps = 162, loss = 0.7333198189735413
In grad_steps = 163, loss = 0.6462259292602539
In grad_steps = 164, loss = 0.614378809928894
In grad_steps = 165, loss = 0.7459498643875122
In grad_steps = 166, loss = 0.658450722694397
In grad_steps = 167, loss = 0.6091256141662598
In grad_steps = 168, loss = 0.6974260807037354
In grad_steps = 169, loss = 0.7324885129928589
In grad_steps = 170, loss = 0.6013823747634888
In grad_steps = 171, loss = 0.7372405529022217
In grad_steps = 172, loss = 0.7746085524559021
In grad_steps = 173, loss = 0.7650896310806274
In grad_steps = 174, loss = 0.7072076201438904
In grad_steps = 175, loss = 0.6834976077079773
In grad_steps = 176, loss = 0.7474748492240906
In grad_steps = 177, loss = 0.6983585953712463
In grad_steps = 178, loss = 0.6486982107162476
In grad_steps = 179, loss = 0.7204623222351074
In grad_steps = 180, loss = 0.7059626579284668
In grad_steps = 181, loss = 0.708975076675415
In grad_steps = 182, loss = 0.6956394910812378
In grad_steps = 183, loss = 0.6602057814598083
In grad_steps = 184, loss = 0.6833591461181641
In grad_steps = 185, loss = 0.6883504986763
In grad_steps = 186, loss = 0.7118035554885864
In grad_steps = 187, loss = 0.6660115122795105
In grad_steps = 188, loss = 0.7004464864730835
In grad_steps = 189, loss = 0.6561527848243713
In grad_steps = 190, loss = 0.7357363104820251
In grad_steps = 191, loss = 0.7377757430076599
In grad_steps = 192, loss = 0.6960134506225586
In grad_steps = 193, loss = 0.6916177272796631
In grad_steps = 194, loss = 0.688107967376709
In grad_steps = 195, loss = 0.6790742874145508
In grad_steps = 196, loss = 0.6826793551445007
In grad_steps = 197, loss = 0.707613468170166
In grad_steps = 198, loss = 0.6498871445655823
In grad_steps = 199, loss = 0.6878475546836853
In grad_steps = 200, loss = 0.7256072163581848
In grad_steps = 201, loss = 0.6472498178482056
In grad_steps = 202, loss = 0.6542397737503052
In grad_steps = 203, loss = 0.7340202331542969
In grad_steps = 204, loss = 0.6496461629867554
In grad_steps = 205, loss = 0.6323075890541077
In grad_steps = 206, loss = 0.7378677129745483
In grad_steps = 207, loss = 0.7389331459999084
In grad_steps = 208, loss = 0.7201337218284607
In grad_steps = 209, loss = 0.6972311735153198
In grad_steps = 210, loss = 0.6977710723876953
In grad_steps = 211, loss = 0.723896861076355
In grad_steps = 212, loss = 0.6760052442550659
In grad_steps = 213, loss = 0.7042704224586487
In grad_steps = 214, loss = 0.6801252365112305
In grad_steps = 215, loss = 0.6800931096076965
In grad_steps = 216, loss = 0.6845875382423401
In grad_steps = 217, loss = 0.6746188998222351
In grad_steps = 218, loss = 0.7737886309623718
In grad_steps = 219, loss = 0.6967192888259888
In grad_steps = 220, loss = 0.6620178818702698
In grad_steps = 221, loss = 0.660885751247406
In grad_steps = 222, loss = 0.7391363978385925
In grad_steps = 223, loss = 0.7104445695877075
In grad_steps = 224, loss = 0.6850121021270752
In grad_steps = 225, loss = 0.6887189745903015
In grad_steps = 226, loss = 0.6777952313423157
In grad_steps = 227, loss = 0.638343334197998
In grad_steps = 228, loss = 0.7292219400405884
In grad_steps = 229, loss = 0.6580305695533752
In grad_steps = 230, loss = 0.6312113404273987
In grad_steps = 231, loss = 0.776566743850708
In grad_steps = 232, loss = 0.6352957487106323
In grad_steps = 233, loss = 0.6348485946655273
In grad_steps = 234, loss = 0.7016872763633728
In grad_steps = 235, loss = 0.7166815996170044
In grad_steps = 236, loss = 0.6781368255615234
In grad_steps = 237, loss = 0.6630669236183167
In grad_steps = 238, loss = 0.6496579647064209
In grad_steps = 239, loss = 0.6418003439903259
In grad_steps = 240, loss = 0.6929683089256287
In grad_steps = 241, loss = 0.6396210193634033
In grad_steps = 242, loss = 0.715047299861908
In grad_steps = 243, loss = 0.6292243599891663
In grad_steps = 244, loss = 0.6365596652030945
In grad_steps = 245, loss = 0.6137815713882446
In grad_steps = 246, loss = 0.6893851161003113
In grad_steps = 247, loss = 0.7426789999008179
In grad_steps = 248, loss = 0.5360593795776367
In grad_steps = 249, loss = 0.5749897956848145
In grad_steps = 250, loss = 0.5523467063903809
In grad_steps = 251, loss = 0.6661129593849182
In grad_steps = 252, loss = 0.7121829986572266
In grad_steps = 253, loss = 0.6431671977043152
In grad_steps = 254, loss = 0.7752270698547363
In grad_steps = 255, loss = 0.8119512796401978
In grad_steps = 256, loss = 0.6627832651138306
In grad_steps = 257, loss = 0.60859614610672
In grad_steps = 258, loss = 0.615155816078186
In grad_steps = 259, loss = 0.6781238913536072
In grad_steps = 260, loss = 0.7780406475067139
In grad_steps = 261, loss = 0.6451857089996338
In grad_steps = 262, loss = 0.7743447422981262
In grad_steps = 263, loss = 0.5435168147087097
In grad_steps = 264, loss = 0.6197962760925293
In grad_steps = 265, loss = 0.7460139989852905
In grad_steps = 266, loss = 0.6667697429656982
In grad_steps = 267, loss = 0.6534684300422668
In grad_steps = 268, loss = 0.6281356811523438
In grad_steps = 269, loss = 0.6724688410758972
In grad_steps = 270, loss = 0.6877055168151855
In grad_steps = 271, loss = 0.6457469463348389
In grad_steps = 272, loss = 0.6164707541465759
In grad_steps = 273, loss = 0.7370327115058899
In grad_steps = 274, loss = 0.7320384383201599
In grad_steps = 275, loss = 0.5851439237594604
In grad_steps = 276, loss = 0.591964840888977
In grad_steps = 277, loss = 0.6431678533554077
In grad_steps = 278, loss = 0.5196179747581482
In grad_steps = 279, loss = 0.5181669592857361
In grad_steps = 280, loss = 0.5614285469055176
In grad_steps = 281, loss = 0.589909017086029
In grad_steps = 282, loss = 0.4224244952201843
In grad_steps = 283, loss = 0.4573742747306824
In grad_steps = 284, loss = 0.38897180557250977
In grad_steps = 285, loss = 1.4923228025436401
In grad_steps = 286, loss = 0.6458812952041626
In grad_steps = 287, loss = 0.5881281495094299
In grad_steps = 288, loss = 0.7610301375389099
In grad_steps = 289, loss = 0.5915362238883972
In grad_steps = 290, loss = 0.7372426390647888
In grad_steps = 291, loss = 0.7783012986183167
In grad_steps = 292, loss = 0.6170955896377563
In grad_steps = 293, loss = 0.6918917894363403
In grad_steps = 294, loss = 0.7188661694526672
In grad_steps = 295, loss = 0.6628278493881226
In grad_steps = 296, loss = 0.6742268204689026
In grad_steps = 297, loss = 0.6839590668678284
In grad_steps = 298, loss = 0.6864308714866638
In grad_steps = 299, loss = 0.6953642964363098
In grad_steps = 300, loss = 0.6887601613998413
In grad_steps = 301, loss = 0.6785432696342468
In grad_steps = 302, loss = 0.7130153775215149
In grad_steps = 303, loss = 0.7192308306694031
In grad_steps = 304, loss = 0.6994381546974182
In grad_steps = 305, loss = 0.6945112943649292
In grad_steps = 306, loss = 0.6659263372421265
In grad_steps = 307, loss = 0.642170786857605
In grad_steps = 308, loss = 0.6650019884109497
In grad_steps = 309, loss = 0.7192885279655457
In grad_steps = 310, loss = 0.622573733329773
In grad_steps = 311, loss = 0.6573662757873535
In grad_steps = 312, loss = 0.6920122504234314
In grad_steps = 313, loss = 0.6205218434333801
In grad_steps = 314, loss = 0.6542174816131592
In grad_steps = 315, loss = 0.6856685876846313
In grad_steps = 316, loss = 0.6138476133346558
In grad_steps = 317, loss = 0.6334439516067505
In grad_steps = 318, loss = 0.7028322219848633
In grad_steps = 319, loss = 0.6301807761192322
In grad_steps = 320, loss = 0.6434755325317383
In grad_steps = 321, loss = 0.7187693119049072
In grad_steps = 322, loss = 0.6871456503868103
In grad_steps = 323, loss = 0.9014040231704712
In grad_steps = 324, loss = 0.6740379333496094
In grad_steps = 325, loss = 0.6492759585380554
In grad_steps = 326, loss = 0.6110446453094482
In grad_steps = 327, loss = 0.644464910030365
In grad_steps = 328, loss = 0.646730363368988
In grad_steps = 329, loss = 0.6520328521728516
In grad_steps = 330, loss = 0.9302029609680176
In grad_steps = 331, loss = 0.6305511593818665
In grad_steps = 332, loss = 0.6809529662132263
In grad_steps = 333, loss = 0.6544058918952942
In grad_steps = 334, loss = 0.6709262132644653
In grad_steps = 335, loss = 0.2650217115879059
In grad_steps = 336, loss = 0.6027821898460388
In grad_steps = 337, loss = 0.7034178376197815
In grad_steps = 338, loss = 0.7208513617515564
In grad_steps = 339, loss = 0.5963273644447327
In grad_steps = 340, loss = 0.7708054184913635
In grad_steps = 341, loss = 0.6250116229057312
In grad_steps = 342, loss = 0.615777850151062
In grad_steps = 343, loss = 0.7421989440917969
In grad_steps = 344, loss = 0.6007543802261353
In grad_steps = 345, loss = 0.6360969543457031
In grad_steps = 346, loss = 0.650503933429718
In grad_steps = 347, loss = 0.6821202039718628
In grad_steps = 348, loss = 0.6754758954048157
In grad_steps = 349, loss = 0.6240805387496948
In grad_steps = 350, loss = 0.6221986413002014
In grad_steps = 351, loss = 0.6066538095474243
In grad_steps = 352, loss = 0.6212940812110901
In grad_steps = 353, loss = 0.6360636949539185
In grad_steps = 354, loss = 0.5942375063896179
In grad_steps = 355, loss = 0.48934924602508545
In grad_steps = 356, loss = 0.5346505045890808
In grad_steps = 357, loss = 0.4843313694000244
In grad_steps = 358, loss = 0.4770987033843994
In grad_steps = 359, loss = 0.48131492733955383
In grad_steps = 360, loss = 0.3891932964324951
In grad_steps = 361, loss = 0.3152643144130707
In grad_steps = 362, loss = 0.44411370158195496
In grad_steps = 363, loss = 0.43058204650878906
In grad_steps = 364, loss = 0.43961748480796814
In grad_steps = 365, loss = 0.29269811511039734
In grad_steps = 366, loss = 0.7355620861053467
In grad_steps = 367, loss = 0.3695682883262634
In grad_steps = 368, loss = 0.23762831091880798
In grad_steps = 369, loss = 0.564241886138916
In grad_steps = 370, loss = 0.3375222980976105
In grad_steps = 371, loss = 0.5144753456115723
In grad_steps = 372, loss = 0.8037225008010864
In grad_steps = 373, loss = 0.3843824863433838
In grad_steps = 374, loss = 0.5749619007110596
In grad_steps = 375, loss = 0.6212092638015747
In grad_steps = 376, loss = 0.5464728474617004
In grad_steps = 377, loss = 0.6972602605819702
In grad_steps = 378, loss = 0.5716055631637573
In grad_steps = 379, loss = 0.5731072425842285
In grad_steps = 380, loss = 0.4868825674057007
In grad_steps = 381, loss = 0.6775027513504028
In grad_steps = 382, loss = 0.5768541693687439
In grad_steps = 383, loss = 0.4570564031600952
In grad_steps = 384, loss = 0.6209831237792969
In grad_steps = 385, loss = 0.7825711965560913
In grad_steps = 386, loss = 0.5530837774276733
In grad_steps = 387, loss = 0.5075148344039917
In grad_steps = 388, loss = 0.4289647042751312
In grad_steps = 389, loss = 0.5734928846359253
In grad_steps = 390, loss = 0.336348295211792
In grad_steps = 391, loss = 0.39494436979293823
In grad_steps = 392, loss = 0.30350056290626526
In grad_steps = 393, loss = 0.2933560609817505
In grad_steps = 394, loss = 0.23960420489311218
In grad_steps = 395, loss = 0.1257186383008957
In grad_steps = 396, loss = 0.08612638711929321
In grad_steps = 397, loss = 0.6378023028373718
In grad_steps = 398, loss = 0.34058457612991333
In grad_steps = 399, loss = 0.1452970951795578
In grad_steps = 400, loss = 0.4585263431072235
In grad_steps = 401, loss = 0.08784335851669312
In grad_steps = 402, loss = 0.49445345997810364
In grad_steps = 403, loss = 0.9592437744140625
In grad_steps = 404, loss = 0.6132907867431641
In grad_steps = 405, loss = 0.7400873899459839
In grad_steps = 406, loss = 0.519224226474762
In grad_steps = 407, loss = 0.5839220285415649
In grad_steps = 408, loss = 0.6530007719993591
In grad_steps = 409, loss = 0.5983307361602783
In grad_steps = 410, loss = 0.7578763365745544
In grad_steps = 411, loss = 0.680884838104248
In grad_steps = 412, loss = 0.7883905172348022
In grad_steps = 413, loss = 0.6316776275634766
In grad_steps = 414, loss = 0.6834924221038818
In grad_steps = 415, loss = 0.6602498292922974
In grad_steps = 416, loss = 0.7388492822647095
In grad_steps = 417, loss = 0.7774863243103027
In grad_steps = 418, loss = 0.6451742649078369
In grad_steps = 419, loss = 0.6341091990470886
In grad_steps = 420, loss = 0.6787697672843933
In grad_steps = 421, loss = 0.7387993931770325
In grad_steps = 422, loss = 0.6585937142372131
In grad_steps = 423, loss = 0.6695988774299622
In grad_steps = 424, loss = 0.7129881978034973
In grad_steps = 425, loss = 0.6692641973495483
In grad_steps = 426, loss = 0.6438889503479004
In grad_steps = 427, loss = 0.6233833432197571
In grad_steps = 428, loss = 0.6924914717674255
In grad_steps = 429, loss = 0.6404085159301758
In grad_steps = 430, loss = 0.6709652543067932
In grad_steps = 431, loss = 0.6369648575782776
In grad_steps = 432, loss = 0.619195282459259
In grad_steps = 433, loss = 0.6759229302406311
In grad_steps = 434, loss = 0.6137319207191467
In grad_steps = 435, loss = 0.6935364603996277
In grad_steps = 436, loss = 0.5660958290100098
In grad_steps = 437, loss = 0.6073173880577087
In grad_steps = 438, loss = 0.6182481050491333
In grad_steps = 439, loss = 0.6604132056236267
In grad_steps = 440, loss = 0.6126129627227783
In grad_steps = 441, loss = 0.652248740196228
In grad_steps = 442, loss = 0.8937768340110779
In grad_steps = 443, loss = 0.658292829990387
In grad_steps = 444, loss = 0.6057073473930359
In grad_steps = 445, loss = 0.688054621219635
In grad_steps = 446, loss = 0.7758855223655701
In grad_steps = 447, loss = 0.028894685208797455
i = 0, Test ensemble probabilities = 
[array([[0.63234293, 0.3676571 ],
       [0.15927052, 0.8407295 ],
       [0.4707932 , 0.5292069 ],
       [0.26916105, 0.73083895],
       [0.34443957, 0.65556043],
       [0.63733286, 0.36266717],
       [0.3972575 , 0.6027425 ],
       [0.53274614, 0.4672539 ],
       [0.32415473, 0.67584527],
       [0.5880499 , 0.41195014],
       [0.53142023, 0.4685797 ],
       [0.4613917 , 0.5386083 ],
       [0.39642295, 0.6035771 ],
       [0.2630529 , 0.73694706],
       [0.25113425, 0.7488658 ],
       [0.31638494, 0.683615  ],
       [0.33599755, 0.6640024 ],
       [0.4374039 , 0.56259614],
       [0.29857844, 0.70142156],
       [0.34628177, 0.65371823],
       [0.48291063, 0.51708937],
       [0.7528585 , 0.24714147],
       [0.26550072, 0.7344993 ],
       [0.34369338, 0.6563066 ],
       [0.32360822, 0.6763918 ],
       [0.48903427, 0.5109657 ],
       [0.517874  , 0.48212597],
       [0.30616266, 0.6938373 ],
       [0.37618548, 0.62381446],
       [0.48018897, 0.51981103],
       [0.442965  , 0.557035  ],
       [0.32442498, 0.6755751 ],
       [0.3079656 , 0.69203436],
       [0.19288743, 0.8071125 ],
       [0.5714873 , 0.4285127 ],
       [0.6726957 , 0.3273043 ],
       [0.1593832 , 0.8406168 ],
       [0.673186  , 0.326814  ],
       [0.44626588, 0.5537341 ],
       [0.53769314, 0.46230686],
       [0.40586078, 0.5941392 ],
       [0.71461   , 0.28539005],
       [0.5180719 , 0.4819281 ],
       [0.2545866 , 0.74541336],
       [0.37031677, 0.6296832 ],
       [0.44500518, 0.5549948 ],
       [0.32468525, 0.6753148 ],
       [0.84190464, 0.15809539],
       [0.51651853, 0.48348144],
       [0.33493203, 0.66506803],
       [0.45689556, 0.5431044 ],
       [0.4609658 , 0.5390341 ],
       [0.54432595, 0.45567402],
       [0.26599905, 0.7340009 ],
       [0.04212758, 0.95787245],
       [0.2829374 , 0.71706253],
       [0.2043219 , 0.7956781 ],
       [0.5221622 , 0.4778378 ],
       [0.18717316, 0.8128269 ],
       [0.19525924, 0.8047408 ],
       [0.5766938 , 0.42330623],
       [0.4176103 , 0.58238965],
       [0.5324215 , 0.46757844],
       [0.4488813 , 0.5511187 ],
       [0.44950402, 0.55049604],
       [0.46307266, 0.53692734],
       [0.39134234, 0.6086577 ],
       [0.41254333, 0.58745664],
       [0.47793105, 0.5220689 ],
       [0.56690925, 0.4330908 ],
       [0.23565127, 0.76434875],
       [0.45820194, 0.5417981 ],
       [0.4637113 , 0.5362887 ],
       [0.49555954, 0.5044404 ],
       [0.22995174, 0.77004826],
       [0.6644476 , 0.3355524 ],
       [0.61461085, 0.38538912],
       [0.5472649 , 0.45273513],
       [0.21934402, 0.780656  ],
       [0.06799435, 0.93200564],
       [0.16100384, 0.8389961 ],
       [0.50630325, 0.49369678],
       [0.43458852, 0.56541145],
       [0.385639  , 0.614361  ],
       [0.61727566, 0.38272437],
       [0.25399345, 0.74600655],
       [0.2955888 , 0.70441115],
       [0.5325431 , 0.46745688],
       [0.678447  , 0.321553  ],
       [0.16606222, 0.8339377 ],
       [0.3081697 , 0.6918303 ],
       [0.4355301 , 0.5644699 ],
       [0.34492984, 0.6550701 ],
       [0.25015336, 0.74984664],
       [0.64061916, 0.3593808 ],
       [0.5967088 , 0.4032912 ],
       [0.18353361, 0.81646645],
       [0.52374434, 0.47625569],
       [0.34673527, 0.65326476],
       [0.38547316, 0.61452687],
       [0.5847594 , 0.41524062],
       [0.44700786, 0.5529921 ],
       [0.47356904, 0.52643096],
       [0.422917  , 0.577083  ],
       [0.19558223, 0.8044177 ],
       [0.42946094, 0.57053906],
       [0.37188283, 0.6281172 ],
       [0.17802247, 0.82197756],
       [0.27269948, 0.7273005 ],
       [0.5892211 , 0.4107789 ],
       [0.40044641, 0.5995536 ],
       [0.5170463 , 0.48295373],
       [0.36080304, 0.63919693],
       [0.43622753, 0.5637725 ],
       [0.23612504, 0.763875  ],
       [0.42220357, 0.5777964 ],
       [0.2890543 , 0.71094567],
       [0.40281117, 0.59718883],
       [0.4923154 , 0.5076846 ],
       [0.26968554, 0.73031443],
       [0.20687027, 0.79312974],
       [0.8389131 , 0.16108696],
       [0.63067454, 0.36932552],
       [0.18742153, 0.81257844],
       [0.30327702, 0.69672304],
       [0.3085698 , 0.6914302 ],
       [0.5595747 , 0.44042534],
       [0.24351013, 0.7564899 ],
       [0.21686015, 0.7831398 ],
       [0.41896248, 0.5810375 ],
       [0.56056327, 0.43943676],
       [0.17147662, 0.8285234 ],
       [0.5208721 , 0.47912785],
       [0.4153351 , 0.5846649 ],
       [0.19790003, 0.8021    ],
       [0.25866634, 0.7413336 ],
       [0.29833403, 0.70166594],
       [0.3234097 , 0.67659026],
       [0.7465652 , 0.25343475],
       [0.59630436, 0.4036956 ],
       [0.23582876, 0.76417124],
       [0.45957777, 0.54042226],
       [0.40649194, 0.593508  ],
       [0.17363618, 0.82636386],
       [0.63669163, 0.36330843],
       [0.30761877, 0.69238126],
       [0.51930255, 0.48069745],
       [0.4234089 , 0.5765911 ],
       [0.4946091 , 0.50539094],
       [0.4197197 , 0.58028024],
       [0.41942033, 0.58057964],
       [0.2945898 , 0.7054102 ],
       [0.55483747, 0.44516253],
       [0.22091393, 0.77908605],
       [0.35284546, 0.64715457],
       [0.46206364, 0.53793633],
       [0.16025013, 0.83974993],
       [0.5305458 , 0.4694542 ],
       [0.46310562, 0.5368943 ],
       [0.13942465, 0.8605754 ],
       [0.412065  , 0.58793503],
       [0.25527856, 0.7447215 ],
       [0.46506244, 0.53493756],
       [0.12068249, 0.8793175 ],
       [0.33459774, 0.6654023 ],
       [0.28955707, 0.7104429 ],
       [0.3447704 , 0.65522957],
       [0.09330033, 0.9066997 ],
       [0.42613664, 0.5738633 ],
       [0.52296096, 0.47703904],
       [0.6178167 , 0.3821833 ],
       [0.49262506, 0.50737494],
       [0.39261836, 0.6073817 ],
       [0.8172014 , 0.18279864],
       [0.63021123, 0.36978877],
       [0.33625674, 0.66374326],
       [0.5179674 , 0.48203266],
       [0.07003294, 0.92996705],
       [0.04434535, 0.9556546 ],
       [0.24915382, 0.75084615],
       [0.503802  , 0.496198  ],
       [0.31493956, 0.6850605 ],
       [0.290524  , 0.709476  ],
       [0.22521402, 0.774786  ],
       [0.40556347, 0.5944366 ],
       [0.19308199, 0.80691797],
       [0.38832644, 0.61167353],
       [0.3310196 , 0.66898036],
       [0.398134  , 0.60186595],
       [0.70039856, 0.29960147],
       [0.56304246, 0.43695754],
       [0.3614178 , 0.6385822 ],
       [0.43890896, 0.561091  ],
       [0.7291902 , 0.27080986],
       [0.1708664 , 0.82913357],
       [0.405132  , 0.594868  ],
       [0.779685  , 0.2203149 ],
       [0.2233463 , 0.7766537 ],
       [0.5981941 , 0.40180585],
       [0.4170927 , 0.58290726],
       [0.45992476, 0.54007524],
       [0.7984498 , 0.2015502 ],
       [0.44850412, 0.5514959 ],
       [0.2520903 , 0.74790967],
       [0.39397344, 0.60602653],
       [0.37396133, 0.6260387 ],
       [0.392793  , 0.60720694],
       [0.6720211 , 0.3279789 ],
       [0.5807893 , 0.41921067],
       [0.73216754, 0.2678325 ],
       [0.5034451 , 0.4965549 ],
       [0.41664135, 0.5833587 ],
       [0.5674081 , 0.43259186],
       [0.17715418, 0.8228458 ],
       [0.26898026, 0.73101974],
       [0.21245757, 0.7875424 ],
       [0.26290798, 0.73709196],
       [0.14729999, 0.85270005],
       [0.10998562, 0.89001435],
       [0.09227446, 0.9077256 ],
       [0.64987636, 0.35012358],
       [0.36819777, 0.6318022 ],
       [0.09723709, 0.9027629 ]], dtype=float32)]
i = 0, Test true class= 
[1 0 1 0 1 0 1 0 1 1 1 0 0 0 0 1 0 0 0 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1
 1 0 0 0 1 0 0 1 0 0 0 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 0 0 0 1 0 1 0 1 0 0
 1 1 1 1 1 1 0 0 0 0 0 0 1 0 1 1 0 1 1 1 0 1 0 1 0 0 0 1 0 0 1 0 1 0 0 1 0
 1 1 1 0 1 0 0 0 0 1 1 1 0 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 0
 0 1 0 1 0 1 1 0 0 0 1 1 0 0 1 1 0 1 1 1 1 0 1 1 1 0 1 1 1 0 0 1 0 1 1 0 0
 1 0 0 0 0 1 1 0 0 0 1 0 1 0 0 0 0 1 1 0 0 0 1 1 0 0 0 1 0 1 0 0 1 1 0 1 1
 0]
In grad_steps = 0, loss = 0.7334465384483337
In grad_steps = 1, loss = 0.814690351486206
In grad_steps = 2, loss = 0.7690542340278625
In grad_steps = 3, loss = 0.7813277244567871
In grad_steps = 4, loss = 0.7167102098464966
In grad_steps = 5, loss = 0.6902193427085876
In grad_steps = 6, loss = 0.6747526526451111
In grad_steps = 7, loss = 0.7728424668312073
In grad_steps = 8, loss = 0.6499598622322083
In grad_steps = 9, loss = 0.6728077530860901
In grad_steps = 10, loss = 0.7218049764633179
In grad_steps = 11, loss = 0.7480466365814209
In grad_steps = 12, loss = 0.7263231873512268
In grad_steps = 13, loss = 0.7462430000305176
In grad_steps = 14, loss = 0.7213929891586304
In grad_steps = 15, loss = 0.7054986357688904
In grad_steps = 16, loss = 0.6854953169822693
In grad_steps = 17, loss = 0.703865647315979
In grad_steps = 18, loss = 0.7431171536445618
In grad_steps = 19, loss = 0.7399609684944153
In grad_steps = 20, loss = 0.6971748471260071
In grad_steps = 21, loss = 0.709645688533783
In grad_steps = 22, loss = 0.7064162492752075
In grad_steps = 23, loss = 0.7495296001434326
In grad_steps = 24, loss = 0.6323515772819519
In grad_steps = 25, loss = 0.6469120979309082
In grad_steps = 26, loss = 0.6964060068130493
In grad_steps = 27, loss = 0.8558197021484375
In grad_steps = 28, loss = 0.6899962425231934
In grad_steps = 29, loss = 0.7589834332466125
In grad_steps = 30, loss = 0.7133461833000183
In grad_steps = 31, loss = 0.6853864192962646
In grad_steps = 32, loss = 0.7012597918510437
In grad_steps = 33, loss = 0.6969360709190369
In grad_steps = 34, loss = 0.7722470760345459
In grad_steps = 35, loss = 0.7162295579910278
In grad_steps = 36, loss = 0.6949169635772705
In grad_steps = 37, loss = 0.694246232509613
In grad_steps = 38, loss = 0.6966139674186707
In grad_steps = 39, loss = 0.6462385058403015
In grad_steps = 40, loss = 0.6727786064147949
In grad_steps = 41, loss = 0.7498311996459961
In grad_steps = 42, loss = 0.6935961842536926
In grad_steps = 43, loss = 0.728879451751709
In grad_steps = 44, loss = 0.6798022389411926
In grad_steps = 45, loss = 0.701571524143219
In grad_steps = 46, loss = 0.7325558662414551
In grad_steps = 47, loss = 0.6849008202552795
In grad_steps = 48, loss = 0.6853771805763245
In grad_steps = 49, loss = 0.7315493822097778
In grad_steps = 50, loss = 0.7659590840339661
In grad_steps = 51, loss = 0.6664406657218933
In grad_steps = 52, loss = 0.6510677933692932
In grad_steps = 53, loss = 0.7367042303085327
In grad_steps = 54, loss = 0.7011722922325134
In grad_steps = 55, loss = 0.646045446395874
In grad_steps = 56, loss = 0.6919379830360413
In grad_steps = 57, loss = 0.7286683917045593
In grad_steps = 58, loss = 0.6803220510482788
In grad_steps = 59, loss = 0.7163160443305969
In grad_steps = 60, loss = 0.7258124351501465
In grad_steps = 61, loss = 0.7050310969352722
In grad_steps = 62, loss = 0.7279877662658691
In grad_steps = 63, loss = 0.6972938776016235
In grad_steps = 64, loss = 0.7250635623931885
In grad_steps = 65, loss = 0.6977391242980957
In grad_steps = 66, loss = 0.6823940277099609
In grad_steps = 67, loss = 0.7120578289031982
In grad_steps = 68, loss = 0.6938956379890442
In grad_steps = 69, loss = 0.7130750417709351
In grad_steps = 70, loss = 0.7433058023452759
In grad_steps = 71, loss = 0.6752791404724121
In grad_steps = 72, loss = 0.6932223439216614
In grad_steps = 73, loss = 0.6979840397834778
In grad_steps = 74, loss = 0.7025790214538574
In grad_steps = 75, loss = 0.6787779927253723
In grad_steps = 76, loss = 0.6994237303733826
In grad_steps = 77, loss = 0.663181483745575
In grad_steps = 78, loss = 0.7330150604248047
In grad_steps = 79, loss = 0.7449648380279541
In grad_steps = 80, loss = 0.6943378448486328
In grad_steps = 81, loss = 0.695696234703064
In grad_steps = 82, loss = 0.6984357237815857
In grad_steps = 83, loss = 0.6902688145637512
In grad_steps = 84, loss = 0.6946098804473877
In grad_steps = 85, loss = 0.696121871471405
In grad_steps = 86, loss = 0.6621004939079285
In grad_steps = 87, loss = 0.6961976289749146
In grad_steps = 88, loss = 0.7291032671928406
In grad_steps = 89, loss = 0.6706187129020691
In grad_steps = 90, loss = 0.6694629192352295
In grad_steps = 91, loss = 0.7540106177330017
In grad_steps = 92, loss = 0.6573682427406311
In grad_steps = 93, loss = 0.6394511461257935
In grad_steps = 94, loss = 0.7614232301712036
In grad_steps = 95, loss = 0.7656673789024353
In grad_steps = 96, loss = 0.7362532615661621
In grad_steps = 97, loss = 0.70289146900177
In grad_steps = 98, loss = 0.6890775561332703
In grad_steps = 99, loss = 0.6961849331855774
In grad_steps = 100, loss = 0.6837482452392578
In grad_steps = 101, loss = 0.7008713483810425
In grad_steps = 102, loss = 0.6885365843772888
In grad_steps = 103, loss = 0.6964854001998901
In grad_steps = 104, loss = 0.6813772916793823
In grad_steps = 105, loss = 0.6664527058601379
In grad_steps = 106, loss = 0.768553614616394
In grad_steps = 107, loss = 0.700839638710022
In grad_steps = 108, loss = 0.6690593957901001
In grad_steps = 109, loss = 0.6759817004203796
In grad_steps = 110, loss = 0.7447900176048279
In grad_steps = 111, loss = 0.7703076601028442
In grad_steps = 112, loss = 0.690209150314331
In grad_steps = 113, loss = 0.6947224140167236
In grad_steps = 114, loss = 0.7082425355911255
In grad_steps = 115, loss = 0.6415663361549377
In grad_steps = 116, loss = 0.7389712333679199
In grad_steps = 117, loss = 0.6752171516418457
In grad_steps = 118, loss = 0.6290848255157471
In grad_steps = 119, loss = 0.8091766238212585
In grad_steps = 120, loss = 0.6429910063743591
In grad_steps = 121, loss = 0.6576995849609375
In grad_steps = 122, loss = 0.7075647711753845
In grad_steps = 123, loss = 0.713803231716156
In grad_steps = 124, loss = 0.6946660280227661
In grad_steps = 125, loss = 0.6781564950942993
In grad_steps = 126, loss = 0.731548011302948
In grad_steps = 127, loss = 0.6546341180801392
In grad_steps = 128, loss = 0.7016993761062622
In grad_steps = 129, loss = 0.6836655735969543
In grad_steps = 130, loss = 0.7335915565490723
In grad_steps = 131, loss = 0.7189466953277588
In grad_steps = 132, loss = 0.6751589775085449
In grad_steps = 133, loss = 0.6602206826210022
In grad_steps = 134, loss = 0.7021831274032593
In grad_steps = 135, loss = 0.7405864596366882
In grad_steps = 136, loss = 0.6137182712554932
In grad_steps = 137, loss = 0.6269063949584961
In grad_steps = 138, loss = 0.6622613668441772
In grad_steps = 139, loss = 0.8159494400024414
In grad_steps = 140, loss = 0.6961914300918579
In grad_steps = 141, loss = 0.7844412326812744
In grad_steps = 142, loss = 0.7347721457481384
In grad_steps = 143, loss = 0.6788267493247986
In grad_steps = 144, loss = 0.6799526810646057
In grad_steps = 145, loss = 0.6682389378547668
In grad_steps = 146, loss = 0.698006272315979
In grad_steps = 147, loss = 0.7000316381454468
In grad_steps = 148, loss = 0.6895471215248108
In grad_steps = 149, loss = 0.685856819152832
In grad_steps = 150, loss = 0.6750131845474243
In grad_steps = 151, loss = 0.6728681325912476
In grad_steps = 152, loss = 0.6584396362304688
In grad_steps = 153, loss = 0.6942635178565979
In grad_steps = 154, loss = 0.670249879360199
In grad_steps = 155, loss = 0.6824881434440613
In grad_steps = 156, loss = 0.6589088439941406
In grad_steps = 157, loss = 0.6942496299743652
In grad_steps = 158, loss = 0.7105295062065125
In grad_steps = 159, loss = 0.6790246367454529
In grad_steps = 160, loss = 0.6410576701164246
In grad_steps = 161, loss = 0.7036562561988831
In grad_steps = 162, loss = 0.7320989370346069
In grad_steps = 163, loss = 0.6431996822357178
In grad_steps = 164, loss = 0.6189463138580322
In grad_steps = 165, loss = 0.7343980669975281
In grad_steps = 166, loss = 0.6475095748901367
In grad_steps = 167, loss = 0.6038317680358887
In grad_steps = 168, loss = 0.700069785118103
In grad_steps = 169, loss = 0.7400867342948914
In grad_steps = 170, loss = 0.5839772820472717
In grad_steps = 171, loss = 0.7660685181617737
In grad_steps = 172, loss = 0.7663951516151428
In grad_steps = 173, loss = 0.8028347492218018
In grad_steps = 174, loss = 0.7027044892311096
In grad_steps = 175, loss = 0.6983315348625183
In grad_steps = 176, loss = 0.7372949123382568
In grad_steps = 177, loss = 0.7003148794174194
In grad_steps = 178, loss = 0.6534092426300049
In grad_steps = 179, loss = 0.7170857191085815
In grad_steps = 180, loss = 0.7025319337844849
In grad_steps = 181, loss = 0.6992455720901489
In grad_steps = 182, loss = 0.6865223050117493
In grad_steps = 183, loss = 0.6673936247825623
In grad_steps = 184, loss = 0.68853360414505
In grad_steps = 185, loss = 0.6836351156234741
In grad_steps = 186, loss = 0.7091152667999268
In grad_steps = 187, loss = 0.671548068523407
In grad_steps = 188, loss = 0.7001987099647522
In grad_steps = 189, loss = 0.66075599193573
In grad_steps = 190, loss = 0.7327996492385864
In grad_steps = 191, loss = 0.7329776883125305
In grad_steps = 192, loss = 0.6930620074272156
In grad_steps = 193, loss = 0.699012041091919
In grad_steps = 194, loss = 0.6788992881774902
In grad_steps = 195, loss = 0.6772168278694153
In grad_steps = 196, loss = 0.682411789894104
In grad_steps = 197, loss = 0.7212992310523987
In grad_steps = 198, loss = 0.6476326584815979
In grad_steps = 199, loss = 0.6924738883972168
In grad_steps = 200, loss = 0.7269411683082581
In grad_steps = 201, loss = 0.6423279047012329
In grad_steps = 202, loss = 0.6519440412521362
In grad_steps = 203, loss = 0.7300509214401245
In grad_steps = 204, loss = 0.6526942253112793
In grad_steps = 205, loss = 0.6370149850845337
In grad_steps = 206, loss = 0.7347635626792908
In grad_steps = 207, loss = 0.7366374135017395
In grad_steps = 208, loss = 0.7124364972114563
In grad_steps = 209, loss = 0.6939390301704407
In grad_steps = 210, loss = 0.6991996765136719
In grad_steps = 211, loss = 0.7275985479354858
In grad_steps = 212, loss = 0.674906849861145
In grad_steps = 213, loss = 0.7087544798851013
In grad_steps = 214, loss = 0.687012255191803
In grad_steps = 215, loss = 0.6806931495666504
In grad_steps = 216, loss = 0.6808091998100281
In grad_steps = 217, loss = 0.6697471737861633
In grad_steps = 218, loss = 0.7741331458091736
In grad_steps = 219, loss = 0.6981973648071289
In grad_steps = 220, loss = 0.6610153317451477
In grad_steps = 221, loss = 0.6636508107185364
In grad_steps = 222, loss = 0.7460910081863403
In grad_steps = 223, loss = 0.721393883228302
In grad_steps = 224, loss = 0.6951229572296143
In grad_steps = 225, loss = 0.685571014881134
In grad_steps = 226, loss = 0.6811937689781189
In grad_steps = 227, loss = 0.6383540630340576
In grad_steps = 228, loss = 0.730222225189209
In grad_steps = 229, loss = 0.6604120135307312
In grad_steps = 230, loss = 0.6334353685379028
In grad_steps = 231, loss = 0.782381534576416
In grad_steps = 232, loss = 0.6340985894203186
In grad_steps = 233, loss = 0.6375482678413391
In grad_steps = 234, loss = 0.7028790712356567
In grad_steps = 235, loss = 0.727495551109314
In grad_steps = 236, loss = 0.676727831363678
In grad_steps = 237, loss = 0.6704381704330444
In grad_steps = 238, loss = 0.6419486999511719
In grad_steps = 239, loss = 0.6501341462135315
In grad_steps = 240, loss = 0.6787123084068298
In grad_steps = 241, loss = 0.6511584520339966
In grad_steps = 242, loss = 0.7145375609397888
In grad_steps = 243, loss = 0.6466249227523804
In grad_steps = 244, loss = 0.6652700304985046
In grad_steps = 245, loss = 0.6264007091522217
In grad_steps = 246, loss = 0.6855046153068542
In grad_steps = 247, loss = 0.7267962098121643
In grad_steps = 248, loss = 0.5758616924285889
In grad_steps = 249, loss = 0.6027389764785767
In grad_steps = 250, loss = 0.5838823318481445
In grad_steps = 251, loss = 0.6844394207000732
In grad_steps = 252, loss = 0.7321295738220215
In grad_steps = 253, loss = 0.6446020603179932
In grad_steps = 254, loss = 0.836486279964447
In grad_steps = 255, loss = 0.819284975528717
In grad_steps = 256, loss = 0.730700671672821
In grad_steps = 257, loss = 0.6249682903289795
In grad_steps = 258, loss = 0.6615471839904785
In grad_steps = 259, loss = 0.6839045882225037
In grad_steps = 260, loss = 0.7469374537467957
In grad_steps = 261, loss = 0.6814361810684204
In grad_steps = 262, loss = 0.7680889964103699
In grad_steps = 263, loss = 0.5755792260169983
In grad_steps = 264, loss = 0.6568031907081604
In grad_steps = 265, loss = 0.7585057020187378
In grad_steps = 266, loss = 0.6994280219078064
In grad_steps = 267, loss = 0.682536780834198
In grad_steps = 268, loss = 0.6714433431625366
In grad_steps = 269, loss = 0.6711654663085938
In grad_steps = 270, loss = 0.7050527334213257
In grad_steps = 271, loss = 0.673330545425415
In grad_steps = 272, loss = 0.6665325164794922
In grad_steps = 273, loss = 0.7524169683456421
In grad_steps = 274, loss = 0.7773844003677368
In grad_steps = 275, loss = 0.6254413723945618
In grad_steps = 276, loss = 0.6306808590888977
In grad_steps = 277, loss = 0.6719950437545776
In grad_steps = 278, loss = 0.6171979904174805
In grad_steps = 279, loss = 0.6184182167053223
In grad_steps = 280, loss = 0.6404505968093872
In grad_steps = 281, loss = 0.654456377029419
In grad_steps = 282, loss = 0.5641717314720154
In grad_steps = 283, loss = 0.6418944001197815
In grad_steps = 284, loss = 0.5878342390060425
In grad_steps = 285, loss = 0.7868186831474304
In grad_steps = 286, loss = 0.600659191608429
In grad_steps = 287, loss = 0.5766215920448303
In grad_steps = 288, loss = 0.6444865465164185
In grad_steps = 289, loss = 0.555964469909668
In grad_steps = 290, loss = 0.7341456413269043
In grad_steps = 291, loss = 0.9000349044799805
In grad_steps = 292, loss = 0.6444931626319885
In grad_steps = 293, loss = 0.7386302351951599
In grad_steps = 294, loss = 0.6802608370780945
In grad_steps = 295, loss = 0.6662657260894775
In grad_steps = 296, loss = 0.7077578902244568
In grad_steps = 297, loss = 0.6567353010177612
In grad_steps = 298, loss = 0.7407903671264648
In grad_steps = 299, loss = 0.6845197677612305
In grad_steps = 300, loss = 0.6962394714355469
In grad_steps = 301, loss = 0.6514900326728821
In grad_steps = 302, loss = 0.7144860029220581
In grad_steps = 303, loss = 0.7149850130081177
In grad_steps = 304, loss = 0.703312337398529
In grad_steps = 305, loss = 0.7244200706481934
In grad_steps = 306, loss = 0.650453507900238
In grad_steps = 307, loss = 0.6525429487228394
In grad_steps = 308, loss = 0.6628757119178772
In grad_steps = 309, loss = 0.7327378392219543
In grad_steps = 310, loss = 0.6239001750946045
In grad_steps = 311, loss = 0.683961033821106
In grad_steps = 312, loss = 0.7047226428985596
In grad_steps = 313, loss = 0.6302783489227295
In grad_steps = 314, loss = 0.6627050042152405
In grad_steps = 315, loss = 0.6874807476997375
In grad_steps = 316, loss = 0.646795928478241
In grad_steps = 317, loss = 0.6331290006637573
In grad_steps = 318, loss = 0.7001461386680603
In grad_steps = 319, loss = 0.6879001259803772
In grad_steps = 320, loss = 0.6825252771377563
In grad_steps = 321, loss = 0.7154638767242432
In grad_steps = 322, loss = 0.6968000531196594
In grad_steps = 323, loss = 0.7759608030319214
In grad_steps = 324, loss = 0.6665008068084717
In grad_steps = 325, loss = 0.681259274482727
In grad_steps = 326, loss = 0.6364796757698059
In grad_steps = 327, loss = 0.6764771938323975
In grad_steps = 328, loss = 0.6711570024490356
In grad_steps = 329, loss = 0.6401032209396362
In grad_steps = 330, loss = 0.8945671916007996
In grad_steps = 331, loss = 0.7004101872444153
In grad_steps = 332, loss = 0.6428748965263367
In grad_steps = 333, loss = 0.6289349794387817
In grad_steps = 334, loss = 0.7024129033088684
In grad_steps = 335, loss = 0.3293907940387726
In grad_steps = 336, loss = 0.6377392411231995
In grad_steps = 337, loss = 0.6905267834663391
In grad_steps = 338, loss = 0.6466644406318665
In grad_steps = 339, loss = 0.5953434109687805
In grad_steps = 340, loss = 0.762653112411499
In grad_steps = 341, loss = 0.6245912313461304
In grad_steps = 342, loss = 0.6343455910682678
In grad_steps = 343, loss = 0.7154313921928406
In grad_steps = 344, loss = 0.6055747866630554
In grad_steps = 345, loss = 0.60292649269104
In grad_steps = 346, loss = 0.6315522789955139
In grad_steps = 347, loss = 0.6152427196502686
In grad_steps = 348, loss = 0.5888468027114868
In grad_steps = 349, loss = 0.6048461198806763
In grad_steps = 350, loss = 0.6143847703933716
In grad_steps = 351, loss = 0.6186637878417969
In grad_steps = 352, loss = 0.6198912858963013
In grad_steps = 353, loss = 0.6358724236488342
In grad_steps = 354, loss = 0.5318236351013184
In grad_steps = 355, loss = 0.5521648526191711
In grad_steps = 356, loss = 0.4904683232307434
In grad_steps = 357, loss = 0.5035332441329956
In grad_steps = 358, loss = 0.448477566242218
In grad_steps = 359, loss = 0.48934483528137207
In grad_steps = 360, loss = 0.35175538063049316
In grad_steps = 361, loss = 0.39263755083084106
In grad_steps = 362, loss = 0.3974689543247223
In grad_steps = 363, loss = 0.807102620601654
In grad_steps = 364, loss = 0.4927341043949127
In grad_steps = 365, loss = 0.4061565697193146
In grad_steps = 366, loss = 0.6992566585540771
In grad_steps = 367, loss = 0.5137456059455872
In grad_steps = 368, loss = 0.4139879047870636
In grad_steps = 369, loss = 0.455651193857193
In grad_steps = 370, loss = 0.3921496570110321
In grad_steps = 371, loss = 0.6085928082466125
In grad_steps = 372, loss = 0.8779329061508179
In grad_steps = 373, loss = 0.6077666282653809
In grad_steps = 374, loss = 0.7660089135169983
In grad_steps = 375, loss = 0.5551344156265259
In grad_steps = 376, loss = 0.6875882148742676
In grad_steps = 377, loss = 0.6184899806976318
In grad_steps = 378, loss = 0.7089307308197021
In grad_steps = 379, loss = 0.5566955804824829
In grad_steps = 380, loss = 0.5853564739227295
In grad_steps = 381, loss = 0.6871597170829773
In grad_steps = 382, loss = 0.6278662085533142
In grad_steps = 383, loss = 0.5978357195854187
In grad_steps = 384, loss = 0.5780335068702698
In grad_steps = 385, loss = 0.6475062370300293
In grad_steps = 386, loss = 0.6248091459274292
In grad_steps = 387, loss = 0.6308469772338867
In grad_steps = 388, loss = 0.5588013529777527
In grad_steps = 389, loss = 0.7589207887649536
In grad_steps = 390, loss = 0.48583436012268066
In grad_steps = 391, loss = 0.47017112374305725
In grad_steps = 392, loss = 0.5584253072738647
In grad_steps = 393, loss = 0.5929948091506958
In grad_steps = 394, loss = 0.392728716135025
In grad_steps = 395, loss = 0.4751443862915039
In grad_steps = 396, loss = 0.333327978849411
In grad_steps = 397, loss = 0.6870052218437195
In grad_steps = 398, loss = 0.4470127820968628
In grad_steps = 399, loss = 0.2099526971578598
In grad_steps = 400, loss = 0.19797421991825104
In grad_steps = 401, loss = 0.1714078187942505
In grad_steps = 402, loss = 0.14842064678668976
In grad_steps = 403, loss = 0.3122331500053406
In grad_steps = 404, loss = 0.38758161664009094
In grad_steps = 405, loss = 0.7802823781967163
In grad_steps = 406, loss = 0.4785522520542145
In grad_steps = 407, loss = 0.7435195446014404
In grad_steps = 408, loss = 0.7530146837234497
In grad_steps = 409, loss = 0.5470733046531677
In grad_steps = 410, loss = 0.7417256832122803
In grad_steps = 411, loss = 0.6678828001022339
In grad_steps = 412, loss = 0.8944103717803955
In grad_steps = 413, loss = 0.5200793743133545
In grad_steps = 414, loss = 0.6718840003013611
In grad_steps = 415, loss = 0.7311235666275024
In grad_steps = 416, loss = 0.6974383592605591
In grad_steps = 417, loss = 0.7429007291793823
In grad_steps = 418, loss = 0.6107032895088196
In grad_steps = 419, loss = 0.6481467485427856
In grad_steps = 420, loss = 0.659312903881073
In grad_steps = 421, loss = 0.7490084767341614
In grad_steps = 422, loss = 0.5949305891990662
In grad_steps = 423, loss = 0.6590362191200256
In grad_steps = 424, loss = 0.6950234174728394
In grad_steps = 425, loss = 0.6499847769737244
In grad_steps = 426, loss = 0.6274271011352539
In grad_steps = 427, loss = 0.70184725522995
In grad_steps = 428, loss = 0.6401346921920776
In grad_steps = 429, loss = 0.6524630784988403
In grad_steps = 430, loss = 0.7123464345932007
In grad_steps = 431, loss = 0.6333615183830261
In grad_steps = 432, loss = 0.7206339240074158
In grad_steps = 433, loss = 0.6721795797348022
In grad_steps = 434, loss = 0.6222081184387207
In grad_steps = 435, loss = 0.6746036410331726
In grad_steps = 436, loss = 0.6945734620094299
In grad_steps = 437, loss = 0.6244723796844482
In grad_steps = 438, loss = 0.5496913194656372
In grad_steps = 439, loss = 0.553889274597168
In grad_steps = 440, loss = 0.7141076326370239
In grad_steps = 441, loss = 0.6289173364639282
In grad_steps = 442, loss = 0.8101038932800293
In grad_steps = 443, loss = 0.6204218864440918
In grad_steps = 444, loss = 0.6645504832267761
In grad_steps = 445, loss = 0.5861304998397827
In grad_steps = 446, loss = 0.6493147015571594
In grad_steps = 447, loss = 0.02041880041360855
i = 1, Test ensemble probabilities = 
[array([[0.63234293, 0.3676571 ],
       [0.15927052, 0.8407295 ],
       [0.4707932 , 0.5292069 ],
       [0.26916105, 0.73083895],
       [0.34443957, 0.65556043],
       [0.63733286, 0.36266717],
       [0.3972575 , 0.6027425 ],
       [0.53274614, 0.4672539 ],
       [0.32415473, 0.67584527],
       [0.5880499 , 0.41195014],
       [0.53142023, 0.4685797 ],
       [0.4613917 , 0.5386083 ],
       [0.39642295, 0.6035771 ],
       [0.2630529 , 0.73694706],
       [0.25113425, 0.7488658 ],
       [0.31638494, 0.683615  ],
       [0.33599755, 0.6640024 ],
       [0.4374039 , 0.56259614],
       [0.29857844, 0.70142156],
       [0.34628177, 0.65371823],
       [0.48291063, 0.51708937],
       [0.7528585 , 0.24714147],
       [0.26550072, 0.7344993 ],
       [0.34369338, 0.6563066 ],
       [0.32360822, 0.6763918 ],
       [0.48903427, 0.5109657 ],
       [0.517874  , 0.48212597],
       [0.30616266, 0.6938373 ],
       [0.37618548, 0.62381446],
       [0.48018897, 0.51981103],
       [0.442965  , 0.557035  ],
       [0.32442498, 0.6755751 ],
       [0.3079656 , 0.69203436],
       [0.19288743, 0.8071125 ],
       [0.5714873 , 0.4285127 ],
       [0.6726957 , 0.3273043 ],
       [0.1593832 , 0.8406168 ],
       [0.673186  , 0.326814  ],
       [0.44626588, 0.5537341 ],
       [0.53769314, 0.46230686],
       [0.40586078, 0.5941392 ],
       [0.71461   , 0.28539005],
       [0.5180719 , 0.4819281 ],
       [0.2545866 , 0.74541336],
       [0.37031677, 0.6296832 ],
       [0.44500518, 0.5549948 ],
       [0.32468525, 0.6753148 ],
       [0.84190464, 0.15809539],
       [0.51651853, 0.48348144],
       [0.33493203, 0.66506803],
       [0.45689556, 0.5431044 ],
       [0.4609658 , 0.5390341 ],
       [0.54432595, 0.45567402],
       [0.26599905, 0.7340009 ],
       [0.04212758, 0.95787245],
       [0.2829374 , 0.71706253],
       [0.2043219 , 0.7956781 ],
       [0.5221622 , 0.4778378 ],
       [0.18717316, 0.8128269 ],
       [0.19525924, 0.8047408 ],
       [0.5766938 , 0.42330623],
       [0.4176103 , 0.58238965],
       [0.5324215 , 0.46757844],
       [0.4488813 , 0.5511187 ],
       [0.44950402, 0.55049604],
       [0.46307266, 0.53692734],
       [0.39134234, 0.6086577 ],
       [0.41254333, 0.58745664],
       [0.47793105, 0.5220689 ],
       [0.56690925, 0.4330908 ],
       [0.23565127, 0.76434875],
       [0.45820194, 0.5417981 ],
       [0.4637113 , 0.5362887 ],
       [0.49555954, 0.5044404 ],
       [0.22995174, 0.77004826],
       [0.6644476 , 0.3355524 ],
       [0.61461085, 0.38538912],
       [0.5472649 , 0.45273513],
       [0.21934402, 0.780656  ],
       [0.06799435, 0.93200564],
       [0.16100384, 0.8389961 ],
       [0.50630325, 0.49369678],
       [0.43458852, 0.56541145],
       [0.385639  , 0.614361  ],
       [0.61727566, 0.38272437],
       [0.25399345, 0.74600655],
       [0.2955888 , 0.70441115],
       [0.5325431 , 0.46745688],
       [0.678447  , 0.321553  ],
       [0.16606222, 0.8339377 ],
       [0.3081697 , 0.6918303 ],
       [0.4355301 , 0.5644699 ],
       [0.34492984, 0.6550701 ],
       [0.25015336, 0.74984664],
       [0.64061916, 0.3593808 ],
       [0.5967088 , 0.4032912 ],
       [0.18353361, 0.81646645],
       [0.52374434, 0.47625569],
       [0.34673527, 0.65326476],
       [0.38547316, 0.61452687],
       [0.5847594 , 0.41524062],
       [0.44700786, 0.5529921 ],
       [0.47356904, 0.52643096],
       [0.422917  , 0.577083  ],
       [0.19558223, 0.8044177 ],
       [0.42946094, 0.57053906],
       [0.37188283, 0.6281172 ],
       [0.17802247, 0.82197756],
       [0.27269948, 0.7273005 ],
       [0.5892211 , 0.4107789 ],
       [0.40044641, 0.5995536 ],
       [0.5170463 , 0.48295373],
       [0.36080304, 0.63919693],
       [0.43622753, 0.5637725 ],
       [0.23612504, 0.763875  ],
       [0.42220357, 0.5777964 ],
       [0.2890543 , 0.71094567],
       [0.40281117, 0.59718883],
       [0.4923154 , 0.5076846 ],
       [0.26968554, 0.73031443],
       [0.20687027, 0.79312974],
       [0.8389131 , 0.16108696],
       [0.63067454, 0.36932552],
       [0.18742153, 0.81257844],
       [0.30327702, 0.69672304],
       [0.3085698 , 0.6914302 ],
       [0.5595747 , 0.44042534],
       [0.24351013, 0.7564899 ],
       [0.21686015, 0.7831398 ],
       [0.41896248, 0.5810375 ],
       [0.56056327, 0.43943676],
       [0.17147662, 0.8285234 ],
       [0.5208721 , 0.47912785],
       [0.4153351 , 0.5846649 ],
       [0.19790003, 0.8021    ],
       [0.25866634, 0.7413336 ],
       [0.29833403, 0.70166594],
       [0.3234097 , 0.67659026],
       [0.7465652 , 0.25343475],
       [0.59630436, 0.4036956 ],
       [0.23582876, 0.76417124],
       [0.45957777, 0.54042226],
       [0.40649194, 0.593508  ],
       [0.17363618, 0.82636386],
       [0.63669163, 0.36330843],
       [0.30761877, 0.69238126],
       [0.51930255, 0.48069745],
       [0.4234089 , 0.5765911 ],
       [0.4946091 , 0.50539094],
       [0.4197197 , 0.58028024],
       [0.41942033, 0.58057964],
       [0.2945898 , 0.7054102 ],
       [0.55483747, 0.44516253],
       [0.22091393, 0.77908605],
       [0.35284546, 0.64715457],
       [0.46206364, 0.53793633],
       [0.16025013, 0.83974993],
       [0.5305458 , 0.4694542 ],
       [0.46310562, 0.5368943 ],
       [0.13942465, 0.8605754 ],
       [0.412065  , 0.58793503],
       [0.25527856, 0.7447215 ],
       [0.46506244, 0.53493756],
       [0.12068249, 0.8793175 ],
       [0.33459774, 0.6654023 ],
       [0.28955707, 0.7104429 ],
       [0.3447704 , 0.65522957],
       [0.09330033, 0.9066997 ],
       [0.42613664, 0.5738633 ],
       [0.52296096, 0.47703904],
       [0.6178167 , 0.3821833 ],
       [0.49262506, 0.50737494],
       [0.39261836, 0.6073817 ],
       [0.8172014 , 0.18279864],
       [0.63021123, 0.36978877],
       [0.33625674, 0.66374326],
       [0.5179674 , 0.48203266],
       [0.07003294, 0.92996705],
       [0.04434535, 0.9556546 ],
       [0.24915382, 0.75084615],
       [0.503802  , 0.496198  ],
       [0.31493956, 0.6850605 ],
       [0.290524  , 0.709476  ],
       [0.22521402, 0.774786  ],
       [0.40556347, 0.5944366 ],
       [0.19308199, 0.80691797],
       [0.38832644, 0.61167353],
       [0.3310196 , 0.66898036],
       [0.398134  , 0.60186595],
       [0.70039856, 0.29960147],
       [0.56304246, 0.43695754],
       [0.3614178 , 0.6385822 ],
       [0.43890896, 0.561091  ],
       [0.7291902 , 0.27080986],
       [0.1708664 , 0.82913357],
       [0.405132  , 0.594868  ],
       [0.779685  , 0.2203149 ],
       [0.2233463 , 0.7766537 ],
       [0.5981941 , 0.40180585],
       [0.4170927 , 0.58290726],
       [0.45992476, 0.54007524],
       [0.7984498 , 0.2015502 ],
       [0.44850412, 0.5514959 ],
       [0.2520903 , 0.74790967],
       [0.39397344, 0.60602653],
       [0.37396133, 0.6260387 ],
       [0.392793  , 0.60720694],
       [0.6720211 , 0.3279789 ],
       [0.5807893 , 0.41921067],
       [0.73216754, 0.2678325 ],
       [0.5034451 , 0.4965549 ],
       [0.41664135, 0.5833587 ],
       [0.5674081 , 0.43259186],
       [0.17715418, 0.8228458 ],
       [0.26898026, 0.73101974],
       [0.21245757, 0.7875424 ],
       [0.26290798, 0.73709196],
       [0.14729999, 0.85270005],
       [0.10998562, 0.89001435],
       [0.09227446, 0.9077256 ],
       [0.64987636, 0.35012358],
       [0.36819777, 0.6318022 ],
       [0.09723709, 0.9027629 ]], dtype=float32), array([[0.4571166 , 0.5428834 ],
       [0.5294107 , 0.4705893 ],
       [0.22323202, 0.77676797],
       [0.51862043, 0.4813795 ],
       [0.39006442, 0.6099356 ],
       [0.55189687, 0.44810316],
       [0.44895777, 0.5510422 ],
       [0.4808965 , 0.5191035 ],
       [0.36935973, 0.63064027],
       [0.33487913, 0.66512084],
       [0.48611328, 0.5138867 ],
       [0.5078283 , 0.4921717 ],
       [0.513526  , 0.486474  ],
       [0.43515244, 0.5648476 ],
       [0.27377552, 0.7262245 ],
       [0.2432999 , 0.7567001 ],
       [0.49993494, 0.5000651 ],
       [0.291736  , 0.708264  ],
       [0.41045925, 0.5895407 ],
       [0.32647273, 0.6735273 ],
       [0.42342755, 0.5765725 ],
       [0.4086655 , 0.5913345 ],
       [0.52046114, 0.47953892],
       [0.3826234 , 0.61737657],
       [0.25063252, 0.7493674 ],
       [0.50633806, 0.49366197],
       [0.36846852, 0.63153154],
       [0.51383686, 0.48616308],
       [0.3087037 , 0.6912963 ],
       [0.55610746, 0.44389254],
       [0.4161952 , 0.5838048 ],
       [0.18752672, 0.8124733 ],
       [0.42430365, 0.5756963 ],
       [0.15909182, 0.84090817],
       [0.46740913, 0.5325908 ],
       [0.65301734, 0.34698266],
       [0.19227678, 0.8077232 ],
       [0.5735935 , 0.4264065 ],
       [0.28589913, 0.7141009 ],
       [0.44559172, 0.55440825],
       [0.54125   , 0.45875004],
       [0.2907995 , 0.7092005 ],
       [0.37092564, 0.62907434],
       [0.4308764 , 0.5691236 ],
       [0.37090874, 0.62909126],
       [0.43027127, 0.56972873],
       [0.27650887, 0.7234912 ],
       [0.6256027 , 0.37439725],
       [0.57003075, 0.4299692 ],
       [0.32834244, 0.67165756],
       [0.4865562 , 0.5134438 ],
       [0.44996402, 0.550036  ],
       [0.47377753, 0.52622247],
       [0.16757809, 0.83242196],
       [0.268217  , 0.73178303],
       [0.33810017, 0.6618998 ],
       [0.26060697, 0.73939306],
       [0.3524904 , 0.64750963],
       [0.38166958, 0.6183304 ],
       [0.2701247 , 0.72987527],
       [0.5141597 , 0.4858403 ],
       [0.4992094 , 0.5007906 ],
       [0.5771801 , 0.42281997],
       [0.3514243 , 0.6485757 ],
       [0.6553351 , 0.34466484],
       [0.46719575, 0.53280425],
       [0.47575495, 0.5242451 ],
       [0.5901849 , 0.4098151 ],
       [0.4846395 , 0.51536053],
       [0.4624119 , 0.5375881 ],
       [0.33387172, 0.6661283 ],
       [0.47095305, 0.5290469 ],
       [0.41752917, 0.5824709 ],
       [0.40379873, 0.5962013 ],
       [0.25252804, 0.747472  ],
       [0.34796432, 0.6520357 ],
       [0.49279785, 0.50720215],
       [0.4465732 , 0.5534268 ],
       [0.2368767 , 0.76312333],
       [0.35196486, 0.6480351 ],
       [0.48743895, 0.512561  ],
       [0.3570835 , 0.64291644],
       [0.34352624, 0.65647376],
       [0.47918805, 0.5208119 ],
       [0.55404097, 0.44595906],
       [0.35746393, 0.6425361 ],
       [0.25150567, 0.7484943 ],
       [0.5243571 , 0.47564292],
       [0.17094587, 0.8290542 ],
       [0.36935994, 0.6306401 ],
       [0.43561053, 0.56438947],
       [0.42300782, 0.5769922 ],
       [0.33751628, 0.6624837 ],
       [0.27065283, 0.7293471 ],
       [0.4002573 , 0.5997427 ],
       [0.553859  , 0.44614094],
       [0.21557094, 0.784429  ],
       [0.35128933, 0.64871067],
       [0.31674996, 0.68325007],
       [0.2832636 , 0.71673644],
       [0.4784139 , 0.5215861 ],
       [0.38211825, 0.6178818 ],
       [0.4683589 , 0.5316411 ],
       [0.3947358 , 0.6052642 ],
       [0.1879898 , 0.8120102 ],
       [0.381092  , 0.618908  ],
       [0.28480652, 0.71519345],
       [0.09613001, 0.90387005],
       [0.36259332, 0.63740665],
       [0.5208426 , 0.4791574 ],
       [0.39169744, 0.6083026 ],
       [0.31034505, 0.689655  ],
       [0.40007278, 0.5999272 ],
       [0.48548177, 0.5145182 ],
       [0.32625067, 0.6737493 ],
       [0.28176755, 0.7182325 ],
       [0.19873911, 0.8012609 ],
       [0.3922998 , 0.6077002 ],
       [0.32452005, 0.6754799 ],
       [0.39452052, 0.6054795 ],
       [0.3498791 , 0.6501209 ],
       [0.5362299 , 0.46377012],
       [0.73773646, 0.2622635 ],
       [0.33494902, 0.665051  ],
       [0.20055218, 0.79944783],
       [0.19752063, 0.8024793 ],
       [0.4704736 , 0.5295264 ],
       [0.19533584, 0.8046642 ],
       [0.1866918 , 0.8133082 ],
       [0.5122611 , 0.48773894],
       [0.6391871 , 0.36081296],
       [0.3035828 , 0.6964172 ],
       [0.28577548, 0.71422446],
       [0.4295801 , 0.5704199 ],
       [0.36431298, 0.63568705],
       [0.39080298, 0.609197  ],
       [0.45286956, 0.5471304 ],
       [0.16296265, 0.8370374 ],
       [0.6365087 , 0.36349133],
       [0.5055456 , 0.49445438],
       [0.12050015, 0.87949985],
       [0.44568032, 0.5543197 ],
       [0.33894923, 0.66105074],
       [0.39209473, 0.60790527],
       [0.3043178 , 0.69568217],
       [0.22776023, 0.77223974],
       [0.5165726 , 0.4834274 ],
       [0.53855336, 0.4614467 ],
       [0.30221108, 0.69778895],
       [0.4841881 , 0.51581186],
       [0.35525265, 0.6447474 ],
       [0.13065955, 0.8693404 ],
       [0.5082202 , 0.49177983],
       [0.22805978, 0.77194023],
       [0.49857068, 0.5014293 ],
       [0.37822402, 0.621776  ],
       [0.18954322, 0.8104568 ],
       [0.45782456, 0.5421755 ],
       [0.343549  , 0.6564509 ],
       [0.23705377, 0.7629462 ],
       [0.53499   , 0.46501   ],
       [0.29976618, 0.7002338 ],
       [0.46745688, 0.5325431 ],
       [0.12526599, 0.874734  ],
       [0.48235503, 0.51764494],
       [0.25172153, 0.7482785 ],
       [0.39188084, 0.6081192 ],
       [0.4759117 , 0.5240883 ],
       [0.5841465 , 0.41585344],
       [0.4407678 , 0.55923223],
       [0.5083613 , 0.49163875],
       [0.34765303, 0.65234697],
       [0.24413344, 0.7558665 ],
       [0.6039984 , 0.39600158],
       [0.19725229, 0.80274767],
       [0.26565233, 0.7343477 ],
       [0.461022  , 0.53897804],
       [0.39643595, 0.603564  ],
       [0.0636213 , 0.93637866],
       [0.3433742 , 0.6566258 ],
       [0.48139504, 0.518605  ],
       [0.47634032, 0.52365965],
       [0.3635606 , 0.6364394 ],
       [0.35425   , 0.64575   ],
       [0.37250814, 0.6274919 ],
       [0.3279293 , 0.67207074],
       [0.46274236, 0.5372576 ],
       [0.08992676, 0.9100732 ],
       [0.24736886, 0.7526312 ],
       [0.4303706 , 0.5696294 ],
       [0.30280727, 0.6971927 ],
       [0.31135   , 0.6886501 ],
       [0.47995785, 0.5200422 ],
       [0.5823317 , 0.41766834],
       [0.1979877 , 0.8020123 ],
       [0.42627287, 0.57372713],
       [0.6199707 , 0.38002932],
       [0.24453898, 0.755461  ],
       [0.31230035, 0.6876996 ],
       [0.3407332 , 0.65926677],
       [0.33183002, 0.66817   ],
       [0.6402096 , 0.3597904 ],
       [0.27295148, 0.7270485 ],
       [0.36300975, 0.6369902 ],
       [0.33555153, 0.66444844],
       [0.5762244 , 0.42377564],
       [0.51039547, 0.48960456],
       [0.5192211 , 0.48077884],
       [0.72059435, 0.27940568],
       [0.6964563 , 0.30354366],
       [0.44113028, 0.5588697 ],
       [0.4867001 , 0.51329994],
       [0.31937218, 0.6806278 ],
       [0.37808475, 0.6219152 ],
       [0.40444702, 0.595553  ],
       [0.0488079 , 0.95119214],
       [0.40321523, 0.5967847 ],
       [0.272642  , 0.72735804],
       [0.32086807, 0.6791319 ],
       [0.17536823, 0.82463175],
       [0.58978885, 0.4102112 ],
       [0.6176899 , 0.38231012],
       [0.2914872 , 0.70851284]], dtype=float32)]
i = 1, Test true class= 
[1 0 1 0 1 0 1 0 1 1 1 0 0 0 0 1 0 0 0 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1
 1 0 0 0 1 0 0 1 0 0 0 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 0 0 0 1 0 1 0 1 0 0
 1 1 1 1 1 1 0 0 0 0 0 0 1 0 1 1 0 1 1 1 0 1 0 1 0 0 0 1 0 0 1 0 1 0 0 1 0
 1 1 1 0 1 0 0 0 0 1 1 1 0 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 0
 0 1 0 1 0 1 1 0 0 0 1 1 0 0 1 1 0 1 1 1 1 0 1 1 1 0 1 1 1 0 0 1 0 1 1 0 0
 1 0 0 0 0 1 1 0 0 0 1 0 1 0 0 0 0 1 1 0 0 0 1 1 0 0 0 1 0 1 0 0 1 1 0 1 1
 0]
In grad_steps = 0, loss = 0.7334465384483337
In grad_steps = 1, loss = 0.831328272819519
In grad_steps = 2, loss = 0.7763441205024719
In grad_steps = 3, loss = 0.7662916779518127
In grad_steps = 4, loss = 0.7179540991783142
In grad_steps = 5, loss = 0.6960977911949158
In grad_steps = 6, loss = 0.6789839267730713
In grad_steps = 7, loss = 0.7690185904502869
In grad_steps = 8, loss = 0.6518908739089966
In grad_steps = 9, loss = 0.6708518266677856
In grad_steps = 10, loss = 0.7241272330284119
In grad_steps = 11, loss = 0.7525054216384888
In grad_steps = 12, loss = 0.7241087555885315
In grad_steps = 13, loss = 0.7437741160392761
In grad_steps = 14, loss = 0.7221255302429199
In grad_steps = 15, loss = 0.700038492679596
In grad_steps = 16, loss = 0.6851128339767456
In grad_steps = 17, loss = 0.702141284942627
In grad_steps = 18, loss = 0.7428705096244812
In grad_steps = 19, loss = 0.7412734627723694
In grad_steps = 20, loss = 0.6936355829238892
In grad_steps = 21, loss = 0.7136448621749878
In grad_steps = 22, loss = 0.7050621509552002
In grad_steps = 23, loss = 0.7471789717674255
In grad_steps = 24, loss = 0.6320443749427795
In grad_steps = 25, loss = 0.6454946398735046
In grad_steps = 26, loss = 0.6979093551635742
In grad_steps = 27, loss = 0.856752336025238
In grad_steps = 28, loss = 0.6905996799468994
In grad_steps = 29, loss = 0.757073163986206
In grad_steps = 30, loss = 0.7109030485153198
In grad_steps = 31, loss = 0.6865797638893127
In grad_steps = 32, loss = 0.7015469670295715
In grad_steps = 33, loss = 0.696251392364502
In grad_steps = 34, loss = 0.7746366262435913
In grad_steps = 35, loss = 0.7201110124588013
In grad_steps = 36, loss = 0.6933938264846802
In grad_steps = 37, loss = 0.6964396238327026
In grad_steps = 38, loss = 0.6942178010940552
In grad_steps = 39, loss = 0.646880030632019
In grad_steps = 40, loss = 0.6736251711845398
In grad_steps = 41, loss = 0.7463908195495605
In grad_steps = 42, loss = 0.6919853687286377
In grad_steps = 43, loss = 0.7270012497901917
In grad_steps = 44, loss = 0.6806589365005493
In grad_steps = 45, loss = 0.7019066214561462
In grad_steps = 46, loss = 0.7342433929443359
In grad_steps = 47, loss = 0.6885083317756653
In grad_steps = 48, loss = 0.6864398717880249
In grad_steps = 49, loss = 0.7292203307151794
In grad_steps = 50, loss = 0.7638887763023376
In grad_steps = 51, loss = 0.6707190871238708
In grad_steps = 52, loss = 0.6514288783073425
In grad_steps = 53, loss = 0.7383849024772644
In grad_steps = 54, loss = 0.7029538154602051
In grad_steps = 55, loss = 0.6462621092796326
In grad_steps = 56, loss = 0.6944124698638916
In grad_steps = 57, loss = 0.7258583903312683
In grad_steps = 58, loss = 0.6817131042480469
In grad_steps = 59, loss = 0.7156223654747009
In grad_steps = 60, loss = 0.7244321703910828
In grad_steps = 61, loss = 0.7049761414527893
In grad_steps = 62, loss = 0.72575443983078
In grad_steps = 63, loss = 0.6977875828742981
In grad_steps = 64, loss = 0.7235471606254578
In grad_steps = 65, loss = 0.6988846063613892
In grad_steps = 66, loss = 0.6851488351821899
In grad_steps = 67, loss = 0.7106788754463196
In grad_steps = 68, loss = 0.6949920654296875
In grad_steps = 69, loss = 0.7131373882293701
In grad_steps = 70, loss = 0.7392608523368835
In grad_steps = 71, loss = 0.6778315305709839
In grad_steps = 72, loss = 0.691864013671875
In grad_steps = 73, loss = 0.6992791295051575
In grad_steps = 74, loss = 0.701473593711853
In grad_steps = 75, loss = 0.6810318231582642
In grad_steps = 76, loss = 0.6996338963508606
In grad_steps = 77, loss = 0.6669954061508179
In grad_steps = 78, loss = 0.7319017648696899
In grad_steps = 79, loss = 0.7413834929466248
In grad_steps = 80, loss = 0.6926109790802002
In grad_steps = 81, loss = 0.696042537689209
In grad_steps = 82, loss = 0.6988821029663086
In grad_steps = 83, loss = 0.6922157406806946
In grad_steps = 84, loss = 0.69556725025177
In grad_steps = 85, loss = 0.6971763968467712
In grad_steps = 86, loss = 0.6641910076141357
In grad_steps = 87, loss = 0.6950618028640747
In grad_steps = 88, loss = 0.7277379035949707
In grad_steps = 89, loss = 0.6698558926582336
In grad_steps = 90, loss = 0.6710416674613953
In grad_steps = 91, loss = 0.7502168416976929
In grad_steps = 92, loss = 0.6584454774856567
In grad_steps = 93, loss = 0.6390663385391235
In grad_steps = 94, loss = 0.760359525680542
In grad_steps = 95, loss = 0.7689865827560425
In grad_steps = 96, loss = 0.7385984659194946
In grad_steps = 97, loss = 0.7005637884140015
In grad_steps = 98, loss = 0.6867429614067078
In grad_steps = 99, loss = 0.6917679309844971
In grad_steps = 100, loss = 0.685376763343811
In grad_steps = 101, loss = 0.6999843120574951
In grad_steps = 102, loss = 0.6884777545928955
In grad_steps = 103, loss = 0.6979966759681702
In grad_steps = 104, loss = 0.6826656460762024
In grad_steps = 105, loss = 0.6689591407775879
In grad_steps = 106, loss = 0.7613611817359924
In grad_steps = 107, loss = 0.6987583637237549
In grad_steps = 108, loss = 0.6718300580978394
In grad_steps = 109, loss = 0.6770707368850708
In grad_steps = 110, loss = 0.7382794618606567
In grad_steps = 111, loss = 0.7569239735603333
In grad_steps = 112, loss = 0.6878522634506226
In grad_steps = 113, loss = 0.6940321922302246
In grad_steps = 114, loss = 0.7083549499511719
In grad_steps = 115, loss = 0.6420669555664062
In grad_steps = 116, loss = 0.7362924218177795
In grad_steps = 117, loss = 0.6774595379829407
In grad_steps = 118, loss = 0.6291747093200684
In grad_steps = 119, loss = 0.8073766827583313
In grad_steps = 120, loss = 0.6392917037010193
In grad_steps = 121, loss = 0.6533768773078918
In grad_steps = 122, loss = 0.709037184715271
In grad_steps = 123, loss = 0.7162423729896545
In grad_steps = 124, loss = 0.6956961750984192
In grad_steps = 125, loss = 0.6787577271461487
In grad_steps = 126, loss = 0.7304373383522034
In grad_steps = 127, loss = 0.6593388915061951
In grad_steps = 128, loss = 0.7012571692466736
In grad_steps = 129, loss = 0.6837445497512817
In grad_steps = 130, loss = 0.735473096370697
In grad_steps = 131, loss = 0.7160204648971558
In grad_steps = 132, loss = 0.6738333106040955
In grad_steps = 133, loss = 0.6633468866348267
In grad_steps = 134, loss = 0.7027133703231812
In grad_steps = 135, loss = 0.7399362921714783
In grad_steps = 136, loss = 0.6132776141166687
In grad_steps = 137, loss = 0.6270985007286072
In grad_steps = 138, loss = 0.6629438996315002
In grad_steps = 139, loss = 0.8105278015136719
In grad_steps = 140, loss = 0.69661945104599
In grad_steps = 141, loss = 0.7825204133987427
In grad_steps = 142, loss = 0.7354946136474609
In grad_steps = 143, loss = 0.6771759986877441
In grad_steps = 144, loss = 0.6794298887252808
In grad_steps = 145, loss = 0.6685548424720764
In grad_steps = 146, loss = 0.700684666633606
In grad_steps = 147, loss = 0.6986907124519348
In grad_steps = 148, loss = 0.6884220242500305
In grad_steps = 149, loss = 0.686460018157959
In grad_steps = 150, loss = 0.6779313087463379
In grad_steps = 151, loss = 0.6725651025772095
In grad_steps = 152, loss = 0.6595630049705505
In grad_steps = 153, loss = 0.6946908235549927
In grad_steps = 154, loss = 0.6728196740150452
In grad_steps = 155, loss = 0.6847817897796631
In grad_steps = 156, loss = 0.6630342602729797
In grad_steps = 157, loss = 0.6939664483070374
In grad_steps = 158, loss = 0.7093492746353149
In grad_steps = 159, loss = 0.678613543510437
In grad_steps = 160, loss = 0.6434702277183533
In grad_steps = 161, loss = 0.6983727812767029
In grad_steps = 162, loss = 0.7274033427238464
In grad_steps = 163, loss = 0.641942024230957
In grad_steps = 164, loss = 0.6195014715194702
In grad_steps = 165, loss = 0.7336136102676392
In grad_steps = 166, loss = 0.6520733833312988
In grad_steps = 167, loss = 0.601357638835907
In grad_steps = 168, loss = 0.7031846046447754
In grad_steps = 169, loss = 0.742673933506012
In grad_steps = 170, loss = 0.587002694606781
In grad_steps = 171, loss = 0.7728732824325562
In grad_steps = 172, loss = 0.7686693668365479
In grad_steps = 173, loss = 0.7999095916748047
In grad_steps = 174, loss = 0.7032772898674011
In grad_steps = 175, loss = 0.6908729672431946
In grad_steps = 176, loss = 0.737810492515564
In grad_steps = 177, loss = 0.7037262916564941
In grad_steps = 178, loss = 0.6519830226898193
In grad_steps = 179, loss = 0.7191169261932373
In grad_steps = 180, loss = 0.7006540298461914
In grad_steps = 181, loss = 0.7006826400756836
In grad_steps = 182, loss = 0.6906009316444397
In grad_steps = 183, loss = 0.6702938079833984
In grad_steps = 184, loss = 0.6868166923522949
In grad_steps = 185, loss = 0.6876475214958191
In grad_steps = 186, loss = 0.7112789750099182
In grad_steps = 187, loss = 0.6697162389755249
In grad_steps = 188, loss = 0.7006123661994934
In grad_steps = 189, loss = 0.6602303981781006
In grad_steps = 190, loss = 0.7392953634262085
In grad_steps = 191, loss = 0.735314130783081
In grad_steps = 192, loss = 0.6948419213294983
In grad_steps = 193, loss = 0.6962547302246094
In grad_steps = 194, loss = 0.6822579503059387
In grad_steps = 195, loss = 0.6803237199783325
In grad_steps = 196, loss = 0.6828534007072449
In grad_steps = 197, loss = 0.7217801809310913
In grad_steps = 198, loss = 0.6478366255760193
In grad_steps = 199, loss = 0.6969805955886841
In grad_steps = 200, loss = 0.7326220870018005
In grad_steps = 201, loss = 0.6429911851882935
In grad_steps = 202, loss = 0.6571001410484314
In grad_steps = 203, loss = 0.7343618869781494
In grad_steps = 204, loss = 0.6539463996887207
In grad_steps = 205, loss = 0.6346688866615295
In grad_steps = 206, loss = 0.7356468439102173
In grad_steps = 207, loss = 0.7434605360031128
In grad_steps = 208, loss = 0.7169123888015747
In grad_steps = 209, loss = 0.6937540173530579
In grad_steps = 210, loss = 0.6977556347846985
In grad_steps = 211, loss = 0.7213664054870605
In grad_steps = 212, loss = 0.6792047619819641
In grad_steps = 213, loss = 0.7045609354972839
In grad_steps = 214, loss = 0.6840917468070984
In grad_steps = 215, loss = 0.6811635494232178
In grad_steps = 216, loss = 0.6837109327316284
In grad_steps = 217, loss = 0.6731012463569641
In grad_steps = 218, loss = 0.7680511474609375
In grad_steps = 219, loss = 0.699931263923645
In grad_steps = 220, loss = 0.6588554978370667
In grad_steps = 221, loss = 0.66586834192276
In grad_steps = 222, loss = 0.7401619553565979
In grad_steps = 223, loss = 0.7256271839141846
In grad_steps = 224, loss = 0.6905083060264587
In grad_steps = 225, loss = 0.6852964162826538
In grad_steps = 226, loss = 0.6852362751960754
In grad_steps = 227, loss = 0.6398638486862183
In grad_steps = 228, loss = 0.7303817868232727
In grad_steps = 229, loss = 0.6632833480834961
In grad_steps = 230, loss = 0.6328539848327637
In grad_steps = 231, loss = 0.7825102806091309
In grad_steps = 232, loss = 0.6340391039848328
In grad_steps = 233, loss = 0.6389186382293701
In grad_steps = 234, loss = 0.7110930681228638
In grad_steps = 235, loss = 0.7311266660690308
In grad_steps = 236, loss = 0.6757060289382935
In grad_steps = 237, loss = 0.6782010793685913
In grad_steps = 238, loss = 0.6462044715881348
In grad_steps = 239, loss = 0.6603267788887024
In grad_steps = 240, loss = 0.6755293011665344
In grad_steps = 241, loss = 0.6510254144668579
In grad_steps = 242, loss = 0.7222347855567932
In grad_steps = 243, loss = 0.6660416722297668
In grad_steps = 244, loss = 0.6692944765090942
In grad_steps = 245, loss = 0.6751824617385864
In grad_steps = 246, loss = 0.6454674601554871
In grad_steps = 247, loss = 0.6729539036750793
In grad_steps = 248, loss = 0.554256796836853
In grad_steps = 249, loss = 0.5771914720535278
In grad_steps = 250, loss = 0.5750002264976501
In grad_steps = 251, loss = 0.7602447867393494
In grad_steps = 252, loss = 0.711280107498169
In grad_steps = 253, loss = 0.69380784034729
In grad_steps = 254, loss = 0.7391231656074524
In grad_steps = 255, loss = 0.7607443928718567
In grad_steps = 256, loss = 0.7089771032333374
In grad_steps = 257, loss = 0.6292791962623596
In grad_steps = 258, loss = 0.7059906721115112
In grad_steps = 259, loss = 0.6764691472053528
In grad_steps = 260, loss = 0.7230521440505981
In grad_steps = 261, loss = 0.6680259704589844
In grad_steps = 262, loss = 0.7661967277526855
In grad_steps = 263, loss = 0.5558410882949829
In grad_steps = 264, loss = 0.6377041339874268
In grad_steps = 265, loss = 0.7603337168693542
In grad_steps = 266, loss = 0.6698773503303528
In grad_steps = 267, loss = 0.683323323726654
In grad_steps = 268, loss = 0.6511695384979248
In grad_steps = 269, loss = 0.6912071704864502
In grad_steps = 270, loss = 0.6863182783126831
In grad_steps = 271, loss = 0.6521288156509399
In grad_steps = 272, loss = 0.6308248043060303
In grad_steps = 273, loss = 0.7380738854408264
In grad_steps = 274, loss = 0.7139444947242737
In grad_steps = 275, loss = 0.5827316045761108
In grad_steps = 276, loss = 0.5842130780220032
In grad_steps = 277, loss = 0.6519622206687927
In grad_steps = 278, loss = 0.5325599312782288
In grad_steps = 279, loss = 0.5336313843727112
In grad_steps = 280, loss = 0.584412157535553
In grad_steps = 281, loss = 0.5826488733291626
In grad_steps = 282, loss = 0.389535129070282
In grad_steps = 283, loss = 0.5395535230636597
In grad_steps = 284, loss = 0.36219078302383423
In grad_steps = 285, loss = 1.310397744178772
In grad_steps = 286, loss = 0.591708779335022
In grad_steps = 287, loss = 0.5882641077041626
In grad_steps = 288, loss = 0.7070834040641785
In grad_steps = 289, loss = 0.602465808391571
In grad_steps = 290, loss = 0.7158854603767395
In grad_steps = 291, loss = 0.7981435060501099
In grad_steps = 292, loss = 0.6284217238426208
In grad_steps = 293, loss = 0.704923689365387
In grad_steps = 294, loss = 0.7419542670249939
In grad_steps = 295, loss = 0.6486608386039734
In grad_steps = 296, loss = 0.6646482944488525
In grad_steps = 297, loss = 0.6852424740791321
In grad_steps = 298, loss = 0.7035173773765564
In grad_steps = 299, loss = 0.6890968680381775
In grad_steps = 300, loss = 0.6829159259796143
In grad_steps = 301, loss = 0.6469260454177856
In grad_steps = 302, loss = 0.7289121150970459
In grad_steps = 303, loss = 0.7395064830780029
In grad_steps = 304, loss = 0.6944292187690735
In grad_steps = 305, loss = 0.7049002647399902
In grad_steps = 306, loss = 0.6863890886306763
In grad_steps = 307, loss = 0.6529661417007446
In grad_steps = 308, loss = 0.6683676838874817
In grad_steps = 309, loss = 0.7028554677963257
In grad_steps = 310, loss = 0.6201263070106506
In grad_steps = 311, loss = 0.6794600486755371
In grad_steps = 312, loss = 0.7265284657478333
In grad_steps = 313, loss = 0.6039248704910278
In grad_steps = 314, loss = 0.6452571749687195
In grad_steps = 315, loss = 0.7024456858634949
In grad_steps = 316, loss = 0.6034230589866638
In grad_steps = 317, loss = 0.608234167098999
In grad_steps = 318, loss = 0.7033594846725464
In grad_steps = 319, loss = 0.6788457632064819
In grad_steps = 320, loss = 0.7070441246032715
In grad_steps = 321, loss = 0.7103083729743958
In grad_steps = 322, loss = 0.7375183701515198
In grad_steps = 323, loss = 0.8738979697227478
In grad_steps = 324, loss = 0.67988520860672
In grad_steps = 325, loss = 0.6879186630249023
In grad_steps = 326, loss = 0.6296539306640625
In grad_steps = 327, loss = 0.6658872961997986
In grad_steps = 328, loss = 0.6633989214897156
In grad_steps = 329, loss = 0.6636306047439575
In grad_steps = 330, loss = 0.8522714972496033
In grad_steps = 331, loss = 0.6926804780960083
In grad_steps = 332, loss = 0.6357754468917847
In grad_steps = 333, loss = 0.6363174915313721
In grad_steps = 334, loss = 0.7513224482536316
In grad_steps = 335, loss = 0.43504124879837036
In grad_steps = 336, loss = 0.6716528534889221
In grad_steps = 337, loss = 0.6876150369644165
In grad_steps = 338, loss = 0.6677352786064148
In grad_steps = 339, loss = 0.5973402857780457
In grad_steps = 340, loss = 0.7466273307800293
In grad_steps = 341, loss = 0.6375784873962402
In grad_steps = 342, loss = 0.6102778315544128
In grad_steps = 343, loss = 0.7702367305755615
In grad_steps = 344, loss = 0.5763609409332275
In grad_steps = 345, loss = 0.609102725982666
In grad_steps = 346, loss = 0.6661520600318909
In grad_steps = 347, loss = 0.6608653664588928
In grad_steps = 348, loss = 0.675534725189209
In grad_steps = 349, loss = 0.5750240087509155
In grad_steps = 350, loss = 0.576589822769165
In grad_steps = 351, loss = 0.6209309101104736
In grad_steps = 352, loss = 0.6010816097259521
In grad_steps = 353, loss = 0.6053368449211121
In grad_steps = 354, loss = 0.5743887424468994
In grad_steps = 355, loss = 0.49375662207603455
In grad_steps = 356, loss = 0.5282012224197388
In grad_steps = 357, loss = 0.38267213106155396
In grad_steps = 358, loss = 0.5978119969367981
In grad_steps = 359, loss = 0.5265021920204163
In grad_steps = 360, loss = 0.4109668731689453
In grad_steps = 361, loss = 0.6325955986976624
In grad_steps = 362, loss = 0.3261977732181549
In grad_steps = 363, loss = 0.46067506074905396
In grad_steps = 364, loss = 0.40521010756492615
In grad_steps = 365, loss = 0.6168310642242432
In grad_steps = 366, loss = 0.6979847550392151
In grad_steps = 367, loss = 0.5348151922225952
In grad_steps = 368, loss = 0.537958025932312
In grad_steps = 369, loss = 0.5614688992500305
In grad_steps = 370, loss = 0.8049739003181458
In grad_steps = 371, loss = 0.6011438965797424
In grad_steps = 372, loss = 0.6884299516677856
In grad_steps = 373, loss = 0.578608512878418
In grad_steps = 374, loss = 0.735187828540802
In grad_steps = 375, loss = 0.5290223956108093
In grad_steps = 376, loss = 0.6115385293960571
In grad_steps = 377, loss = 0.6904247999191284
In grad_steps = 378, loss = 0.6538679003715515
In grad_steps = 379, loss = 0.544060468673706
In grad_steps = 380, loss = 0.5465409159660339
In grad_steps = 381, loss = 0.6485742330551147
In grad_steps = 382, loss = 0.6543543338775635
In grad_steps = 383, loss = 0.5067818760871887
In grad_steps = 384, loss = 0.5295389890670776
In grad_steps = 385, loss = 0.8493791818618774
In grad_steps = 386, loss = 0.6765110492706299
In grad_steps = 387, loss = 0.45870891213417053
In grad_steps = 388, loss = 0.5702150464057922
In grad_steps = 389, loss = 0.5527152419090271
In grad_steps = 390, loss = 0.36482343077659607
In grad_steps = 391, loss = 0.3938809931278229
In grad_steps = 392, loss = 0.3954068422317505
In grad_steps = 393, loss = 0.3421887457370758
In grad_steps = 394, loss = 0.18556904792785645
In grad_steps = 395, loss = 0.09510082006454468
In grad_steps = 396, loss = 0.12403354048728943
In grad_steps = 397, loss = 0.4982154369354248
In grad_steps = 398, loss = 0.10042212903499603
In grad_steps = 399, loss = 0.3268516957759857
In grad_steps = 400, loss = 0.23732052743434906
In grad_steps = 401, loss = 0.8027443289756775
In grad_steps = 402, loss = 1.3996273279190063
In grad_steps = 403, loss = 0.4988698661327362
In grad_steps = 404, loss = 0.46943676471710205
In grad_steps = 405, loss = 0.7061706781387329
In grad_steps = 406, loss = 0.7585086822509766
In grad_steps = 407, loss = 0.665381669998169
In grad_steps = 408, loss = 0.7445417046546936
In grad_steps = 409, loss = 0.5985856056213379
In grad_steps = 410, loss = 0.748372495174408
In grad_steps = 411, loss = 0.7125298380851746
In grad_steps = 412, loss = 0.7741957902908325
In grad_steps = 413, loss = 0.6496660709381104
In grad_steps = 414, loss = 0.7811824679374695
In grad_steps = 415, loss = 0.7178773880004883
In grad_steps = 416, loss = 0.7469013929367065
In grad_steps = 417, loss = 0.7855746746063232
In grad_steps = 418, loss = 0.6516515016555786
In grad_steps = 419, loss = 0.6196034550666809
In grad_steps = 420, loss = 0.6965402960777283
In grad_steps = 421, loss = 0.775300920009613
In grad_steps = 422, loss = 0.6255949139595032
In grad_steps = 423, loss = 0.7022764086723328
In grad_steps = 424, loss = 0.686689019203186
In grad_steps = 425, loss = 0.6476128697395325
In grad_steps = 426, loss = 0.6669347286224365
In grad_steps = 427, loss = 0.6605386137962341
In grad_steps = 428, loss = 0.6762785911560059
In grad_steps = 429, loss = 0.664832592010498
In grad_steps = 430, loss = 0.6625635623931885
In grad_steps = 431, loss = 0.6623319387435913
In grad_steps = 432, loss = 0.6784623861312866
In grad_steps = 433, loss = 0.6889656186103821
In grad_steps = 434, loss = 0.6712197661399841
In grad_steps = 435, loss = 0.7323504090309143
In grad_steps = 436, loss = 0.6341302990913391
In grad_steps = 437, loss = 0.6739579439163208
In grad_steps = 438, loss = 0.6255703568458557
In grad_steps = 439, loss = 0.615182638168335
In grad_steps = 440, loss = 0.6414183378219604
In grad_steps = 441, loss = 0.6700385212898254
In grad_steps = 442, loss = 0.8987728357315063
In grad_steps = 443, loss = 0.7501933574676514
In grad_steps = 444, loss = 0.6124979853630066
In grad_steps = 445, loss = 0.6263404488563538
In grad_steps = 446, loss = 0.6757606267929077
In grad_steps = 447, loss = 0.15078236162662506
i = 2, Test ensemble probabilities = 
[array([[0.63234293, 0.3676571 ],
       [0.15927052, 0.8407295 ],
       [0.4707932 , 0.5292069 ],
       [0.26916105, 0.73083895],
       [0.34443957, 0.65556043],
       [0.63733286, 0.36266717],
       [0.3972575 , 0.6027425 ],
       [0.53274614, 0.4672539 ],
       [0.32415473, 0.67584527],
       [0.5880499 , 0.41195014],
       [0.53142023, 0.4685797 ],
       [0.4613917 , 0.5386083 ],
       [0.39642295, 0.6035771 ],
       [0.2630529 , 0.73694706],
       [0.25113425, 0.7488658 ],
       [0.31638494, 0.683615  ],
       [0.33599755, 0.6640024 ],
       [0.4374039 , 0.56259614],
       [0.29857844, 0.70142156],
       [0.34628177, 0.65371823],
       [0.48291063, 0.51708937],
       [0.7528585 , 0.24714147],
       [0.26550072, 0.7344993 ],
       [0.34369338, 0.6563066 ],
       [0.32360822, 0.6763918 ],
       [0.48903427, 0.5109657 ],
       [0.517874  , 0.48212597],
       [0.30616266, 0.6938373 ],
       [0.37618548, 0.62381446],
       [0.48018897, 0.51981103],
       [0.442965  , 0.557035  ],
       [0.32442498, 0.6755751 ],
       [0.3079656 , 0.69203436],
       [0.19288743, 0.8071125 ],
       [0.5714873 , 0.4285127 ],
       [0.6726957 , 0.3273043 ],
       [0.1593832 , 0.8406168 ],
       [0.673186  , 0.326814  ],
       [0.44626588, 0.5537341 ],
       [0.53769314, 0.46230686],
       [0.40586078, 0.5941392 ],
       [0.71461   , 0.28539005],
       [0.5180719 , 0.4819281 ],
       [0.2545866 , 0.74541336],
       [0.37031677, 0.6296832 ],
       [0.44500518, 0.5549948 ],
       [0.32468525, 0.6753148 ],
       [0.84190464, 0.15809539],
       [0.51651853, 0.48348144],
       [0.33493203, 0.66506803],
       [0.45689556, 0.5431044 ],
       [0.4609658 , 0.5390341 ],
       [0.54432595, 0.45567402],
       [0.26599905, 0.7340009 ],
       [0.04212758, 0.95787245],
       [0.2829374 , 0.71706253],
       [0.2043219 , 0.7956781 ],
       [0.5221622 , 0.4778378 ],
       [0.18717316, 0.8128269 ],
       [0.19525924, 0.8047408 ],
       [0.5766938 , 0.42330623],
       [0.4176103 , 0.58238965],
       [0.5324215 , 0.46757844],
       [0.4488813 , 0.5511187 ],
       [0.44950402, 0.55049604],
       [0.46307266, 0.53692734],
       [0.39134234, 0.6086577 ],
       [0.41254333, 0.58745664],
       [0.47793105, 0.5220689 ],
       [0.56690925, 0.4330908 ],
       [0.23565127, 0.76434875],
       [0.45820194, 0.5417981 ],
       [0.4637113 , 0.5362887 ],
       [0.49555954, 0.5044404 ],
       [0.22995174, 0.77004826],
       [0.6644476 , 0.3355524 ],
       [0.61461085, 0.38538912],
       [0.5472649 , 0.45273513],
       [0.21934402, 0.780656  ],
       [0.06799435, 0.93200564],
       [0.16100384, 0.8389961 ],
       [0.50630325, 0.49369678],
       [0.43458852, 0.56541145],
       [0.385639  , 0.614361  ],
       [0.61727566, 0.38272437],
       [0.25399345, 0.74600655],
       [0.2955888 , 0.70441115],
       [0.5325431 , 0.46745688],
       [0.678447  , 0.321553  ],
       [0.16606222, 0.8339377 ],
       [0.3081697 , 0.6918303 ],
       [0.4355301 , 0.5644699 ],
       [0.34492984, 0.6550701 ],
       [0.25015336, 0.74984664],
       [0.64061916, 0.3593808 ],
       [0.5967088 , 0.4032912 ],
       [0.18353361, 0.81646645],
       [0.52374434, 0.47625569],
       [0.34673527, 0.65326476],
       [0.38547316, 0.61452687],
       [0.5847594 , 0.41524062],
       [0.44700786, 0.5529921 ],
       [0.47356904, 0.52643096],
       [0.422917  , 0.577083  ],
       [0.19558223, 0.8044177 ],
       [0.42946094, 0.57053906],
       [0.37188283, 0.6281172 ],
       [0.17802247, 0.82197756],
       [0.27269948, 0.7273005 ],
       [0.5892211 , 0.4107789 ],
       [0.40044641, 0.5995536 ],
       [0.5170463 , 0.48295373],
       [0.36080304, 0.63919693],
       [0.43622753, 0.5637725 ],
       [0.23612504, 0.763875  ],
       [0.42220357, 0.5777964 ],
       [0.2890543 , 0.71094567],
       [0.40281117, 0.59718883],
       [0.4923154 , 0.5076846 ],
       [0.26968554, 0.73031443],
       [0.20687027, 0.79312974],
       [0.8389131 , 0.16108696],
       [0.63067454, 0.36932552],
       [0.18742153, 0.81257844],
       [0.30327702, 0.69672304],
       [0.3085698 , 0.6914302 ],
       [0.5595747 , 0.44042534],
       [0.24351013, 0.7564899 ],
       [0.21686015, 0.7831398 ],
       [0.41896248, 0.5810375 ],
       [0.56056327, 0.43943676],
       [0.17147662, 0.8285234 ],
       [0.5208721 , 0.47912785],
       [0.4153351 , 0.5846649 ],
       [0.19790003, 0.8021    ],
       [0.25866634, 0.7413336 ],
       [0.29833403, 0.70166594],
       [0.3234097 , 0.67659026],
       [0.7465652 , 0.25343475],
       [0.59630436, 0.4036956 ],
       [0.23582876, 0.76417124],
       [0.45957777, 0.54042226],
       [0.40649194, 0.593508  ],
       [0.17363618, 0.82636386],
       [0.63669163, 0.36330843],
       [0.30761877, 0.69238126],
       [0.51930255, 0.48069745],
       [0.4234089 , 0.5765911 ],
       [0.4946091 , 0.50539094],
       [0.4197197 , 0.58028024],
       [0.41942033, 0.58057964],
       [0.2945898 , 0.7054102 ],
       [0.55483747, 0.44516253],
       [0.22091393, 0.77908605],
       [0.35284546, 0.64715457],
       [0.46206364, 0.53793633],
       [0.16025013, 0.83974993],
       [0.5305458 , 0.4694542 ],
       [0.46310562, 0.5368943 ],
       [0.13942465, 0.8605754 ],
       [0.412065  , 0.58793503],
       [0.25527856, 0.7447215 ],
       [0.46506244, 0.53493756],
       [0.12068249, 0.8793175 ],
       [0.33459774, 0.6654023 ],
       [0.28955707, 0.7104429 ],
       [0.3447704 , 0.65522957],
       [0.09330033, 0.9066997 ],
       [0.42613664, 0.5738633 ],
       [0.52296096, 0.47703904],
       [0.6178167 , 0.3821833 ],
       [0.49262506, 0.50737494],
       [0.39261836, 0.6073817 ],
       [0.8172014 , 0.18279864],
       [0.63021123, 0.36978877],
       [0.33625674, 0.66374326],
       [0.5179674 , 0.48203266],
       [0.07003294, 0.92996705],
       [0.04434535, 0.9556546 ],
       [0.24915382, 0.75084615],
       [0.503802  , 0.496198  ],
       [0.31493956, 0.6850605 ],
       [0.290524  , 0.709476  ],
       [0.22521402, 0.774786  ],
       [0.40556347, 0.5944366 ],
       [0.19308199, 0.80691797],
       [0.38832644, 0.61167353],
       [0.3310196 , 0.66898036],
       [0.398134  , 0.60186595],
       [0.70039856, 0.29960147],
       [0.56304246, 0.43695754],
       [0.3614178 , 0.6385822 ],
       [0.43890896, 0.561091  ],
       [0.7291902 , 0.27080986],
       [0.1708664 , 0.82913357],
       [0.405132  , 0.594868  ],
       [0.779685  , 0.2203149 ],
       [0.2233463 , 0.7766537 ],
       [0.5981941 , 0.40180585],
       [0.4170927 , 0.58290726],
       [0.45992476, 0.54007524],
       [0.7984498 , 0.2015502 ],
       [0.44850412, 0.5514959 ],
       [0.2520903 , 0.74790967],
       [0.39397344, 0.60602653],
       [0.37396133, 0.6260387 ],
       [0.392793  , 0.60720694],
       [0.6720211 , 0.3279789 ],
       [0.5807893 , 0.41921067],
       [0.73216754, 0.2678325 ],
       [0.5034451 , 0.4965549 ],
       [0.41664135, 0.5833587 ],
       [0.5674081 , 0.43259186],
       [0.17715418, 0.8228458 ],
       [0.26898026, 0.73101974],
       [0.21245757, 0.7875424 ],
       [0.26290798, 0.73709196],
       [0.14729999, 0.85270005],
       [0.10998562, 0.89001435],
       [0.09227446, 0.9077256 ],
       [0.64987636, 0.35012358],
       [0.36819777, 0.6318022 ],
       [0.09723709, 0.9027629 ]], dtype=float32), array([[0.4571166 , 0.5428834 ],
       [0.5294107 , 0.4705893 ],
       [0.22323202, 0.77676797],
       [0.51862043, 0.4813795 ],
       [0.39006442, 0.6099356 ],
       [0.55189687, 0.44810316],
       [0.44895777, 0.5510422 ],
       [0.4808965 , 0.5191035 ],
       [0.36935973, 0.63064027],
       [0.33487913, 0.66512084],
       [0.48611328, 0.5138867 ],
       [0.5078283 , 0.4921717 ],
       [0.513526  , 0.486474  ],
       [0.43515244, 0.5648476 ],
       [0.27377552, 0.7262245 ],
       [0.2432999 , 0.7567001 ],
       [0.49993494, 0.5000651 ],
       [0.291736  , 0.708264  ],
       [0.41045925, 0.5895407 ],
       [0.32647273, 0.6735273 ],
       [0.42342755, 0.5765725 ],
       [0.4086655 , 0.5913345 ],
       [0.52046114, 0.47953892],
       [0.3826234 , 0.61737657],
       [0.25063252, 0.7493674 ],
       [0.50633806, 0.49366197],
       [0.36846852, 0.63153154],
       [0.51383686, 0.48616308],
       [0.3087037 , 0.6912963 ],
       [0.55610746, 0.44389254],
       [0.4161952 , 0.5838048 ],
       [0.18752672, 0.8124733 ],
       [0.42430365, 0.5756963 ],
       [0.15909182, 0.84090817],
       [0.46740913, 0.5325908 ],
       [0.65301734, 0.34698266],
       [0.19227678, 0.8077232 ],
       [0.5735935 , 0.4264065 ],
       [0.28589913, 0.7141009 ],
       [0.44559172, 0.55440825],
       [0.54125   , 0.45875004],
       [0.2907995 , 0.7092005 ],
       [0.37092564, 0.62907434],
       [0.4308764 , 0.5691236 ],
       [0.37090874, 0.62909126],
       [0.43027127, 0.56972873],
       [0.27650887, 0.7234912 ],
       [0.6256027 , 0.37439725],
       [0.57003075, 0.4299692 ],
       [0.32834244, 0.67165756],
       [0.4865562 , 0.5134438 ],
       [0.44996402, 0.550036  ],
       [0.47377753, 0.52622247],
       [0.16757809, 0.83242196],
       [0.268217  , 0.73178303],
       [0.33810017, 0.6618998 ],
       [0.26060697, 0.73939306],
       [0.3524904 , 0.64750963],
       [0.38166958, 0.6183304 ],
       [0.2701247 , 0.72987527],
       [0.5141597 , 0.4858403 ],
       [0.4992094 , 0.5007906 ],
       [0.5771801 , 0.42281997],
       [0.3514243 , 0.6485757 ],
       [0.6553351 , 0.34466484],
       [0.46719575, 0.53280425],
       [0.47575495, 0.5242451 ],
       [0.5901849 , 0.4098151 ],
       [0.4846395 , 0.51536053],
       [0.4624119 , 0.5375881 ],
       [0.33387172, 0.6661283 ],
       [0.47095305, 0.5290469 ],
       [0.41752917, 0.5824709 ],
       [0.40379873, 0.5962013 ],
       [0.25252804, 0.747472  ],
       [0.34796432, 0.6520357 ],
       [0.49279785, 0.50720215],
       [0.4465732 , 0.5534268 ],
       [0.2368767 , 0.76312333],
       [0.35196486, 0.6480351 ],
       [0.48743895, 0.512561  ],
       [0.3570835 , 0.64291644],
       [0.34352624, 0.65647376],
       [0.47918805, 0.5208119 ],
       [0.55404097, 0.44595906],
       [0.35746393, 0.6425361 ],
       [0.25150567, 0.7484943 ],
       [0.5243571 , 0.47564292],
       [0.17094587, 0.8290542 ],
       [0.36935994, 0.6306401 ],
       [0.43561053, 0.56438947],
       [0.42300782, 0.5769922 ],
       [0.33751628, 0.6624837 ],
       [0.27065283, 0.7293471 ],
       [0.4002573 , 0.5997427 ],
       [0.553859  , 0.44614094],
       [0.21557094, 0.784429  ],
       [0.35128933, 0.64871067],
       [0.31674996, 0.68325007],
       [0.2832636 , 0.71673644],
       [0.4784139 , 0.5215861 ],
       [0.38211825, 0.6178818 ],
       [0.4683589 , 0.5316411 ],
       [0.3947358 , 0.6052642 ],
       [0.1879898 , 0.8120102 ],
       [0.381092  , 0.618908  ],
       [0.28480652, 0.71519345],
       [0.09613001, 0.90387005],
       [0.36259332, 0.63740665],
       [0.5208426 , 0.4791574 ],
       [0.39169744, 0.6083026 ],
       [0.31034505, 0.689655  ],
       [0.40007278, 0.5999272 ],
       [0.48548177, 0.5145182 ],
       [0.32625067, 0.6737493 ],
       [0.28176755, 0.7182325 ],
       [0.19873911, 0.8012609 ],
       [0.3922998 , 0.6077002 ],
       [0.32452005, 0.6754799 ],
       [0.39452052, 0.6054795 ],
       [0.3498791 , 0.6501209 ],
       [0.5362299 , 0.46377012],
       [0.73773646, 0.2622635 ],
       [0.33494902, 0.665051  ],
       [0.20055218, 0.79944783],
       [0.19752063, 0.8024793 ],
       [0.4704736 , 0.5295264 ],
       [0.19533584, 0.8046642 ],
       [0.1866918 , 0.8133082 ],
       [0.5122611 , 0.48773894],
       [0.6391871 , 0.36081296],
       [0.3035828 , 0.6964172 ],
       [0.28577548, 0.71422446],
       [0.4295801 , 0.5704199 ],
       [0.36431298, 0.63568705],
       [0.39080298, 0.609197  ],
       [0.45286956, 0.5471304 ],
       [0.16296265, 0.8370374 ],
       [0.6365087 , 0.36349133],
       [0.5055456 , 0.49445438],
       [0.12050015, 0.87949985],
       [0.44568032, 0.5543197 ],
       [0.33894923, 0.66105074],
       [0.39209473, 0.60790527],
       [0.3043178 , 0.69568217],
       [0.22776023, 0.77223974],
       [0.5165726 , 0.4834274 ],
       [0.53855336, 0.4614467 ],
       [0.30221108, 0.69778895],
       [0.4841881 , 0.51581186],
       [0.35525265, 0.6447474 ],
       [0.13065955, 0.8693404 ],
       [0.5082202 , 0.49177983],
       [0.22805978, 0.77194023],
       [0.49857068, 0.5014293 ],
       [0.37822402, 0.621776  ],
       [0.18954322, 0.8104568 ],
       [0.45782456, 0.5421755 ],
       [0.343549  , 0.6564509 ],
       [0.23705377, 0.7629462 ],
       [0.53499   , 0.46501   ],
       [0.29976618, 0.7002338 ],
       [0.46745688, 0.5325431 ],
       [0.12526599, 0.874734  ],
       [0.48235503, 0.51764494],
       [0.25172153, 0.7482785 ],
       [0.39188084, 0.6081192 ],
       [0.4759117 , 0.5240883 ],
       [0.5841465 , 0.41585344],
       [0.4407678 , 0.55923223],
       [0.5083613 , 0.49163875],
       [0.34765303, 0.65234697],
       [0.24413344, 0.7558665 ],
       [0.6039984 , 0.39600158],
       [0.19725229, 0.80274767],
       [0.26565233, 0.7343477 ],
       [0.461022  , 0.53897804],
       [0.39643595, 0.603564  ],
       [0.0636213 , 0.93637866],
       [0.3433742 , 0.6566258 ],
       [0.48139504, 0.518605  ],
       [0.47634032, 0.52365965],
       [0.3635606 , 0.6364394 ],
       [0.35425   , 0.64575   ],
       [0.37250814, 0.6274919 ],
       [0.3279293 , 0.67207074],
       [0.46274236, 0.5372576 ],
       [0.08992676, 0.9100732 ],
       [0.24736886, 0.7526312 ],
       [0.4303706 , 0.5696294 ],
       [0.30280727, 0.6971927 ],
       [0.31135   , 0.6886501 ],
       [0.47995785, 0.5200422 ],
       [0.5823317 , 0.41766834],
       [0.1979877 , 0.8020123 ],
       [0.42627287, 0.57372713],
       [0.6199707 , 0.38002932],
       [0.24453898, 0.755461  ],
       [0.31230035, 0.6876996 ],
       [0.3407332 , 0.65926677],
       [0.33183002, 0.66817   ],
       [0.6402096 , 0.3597904 ],
       [0.27295148, 0.7270485 ],
       [0.36300975, 0.6369902 ],
       [0.33555153, 0.66444844],
       [0.5762244 , 0.42377564],
       [0.51039547, 0.48960456],
       [0.5192211 , 0.48077884],
       [0.72059435, 0.27940568],
       [0.6964563 , 0.30354366],
       [0.44113028, 0.5588697 ],
       [0.4867001 , 0.51329994],
       [0.31937218, 0.6806278 ],
       [0.37808475, 0.6219152 ],
       [0.40444702, 0.595553  ],
       [0.0488079 , 0.95119214],
       [0.40321523, 0.5967847 ],
       [0.272642  , 0.72735804],
       [0.32086807, 0.6791319 ],
       [0.17536823, 0.82463175],
       [0.58978885, 0.4102112 ],
       [0.6176899 , 0.38231012],
       [0.2914872 , 0.70851284]], dtype=float32), array([[0.52597165, 0.4740284 ],
       [0.4506134 , 0.54938656],
       [0.4263498 , 0.5736502 ],
       [0.47026357, 0.5297364 ],
       [0.37009287, 0.62990713],
       [0.5111935 , 0.48880646],
       [0.51298803, 0.48701194],
       [0.52645165, 0.47354832],
       [0.47769335, 0.5223066 ],
       [0.53333527, 0.46666473],
       [0.4864359 , 0.5135641 ],
       [0.53368205, 0.46631795],
       [0.43138957, 0.5686105 ],
       [0.5786386 , 0.4213614 ],
       [0.36942527, 0.63057476],
       [0.32840174, 0.67159826],
       [0.33742973, 0.6625703 ],
       [0.49621615, 0.5037839 ],
       [0.34859833, 0.65140164],
       [0.47249594, 0.5275041 ],
       [0.4168511 , 0.58314884],
       [0.6070685 , 0.39293158],
       [0.53147703, 0.46852297],
       [0.46537063, 0.53462934],
       [0.3369687 , 0.6630313 ],
       [0.4886492 , 0.5113508 ],
       [0.4758375 , 0.52416253],
       [0.4750207 , 0.5249793 ],
       [0.24396084, 0.75603914],
       [0.4952413 , 0.5047587 ],
       [0.46619245, 0.5338076 ],
       [0.3764747 , 0.6235253 ],
       [0.47742212, 0.5225779 ],
       [0.17826054, 0.82173944],
       [0.48025635, 0.5197437 ],
       [0.6525688 , 0.34743115],
       [0.2588509 , 0.7411491 ],
       [0.5926461 , 0.4073539 ],
       [0.44450098, 0.5554991 ],
       [0.41339898, 0.586601  ],
       [0.5589249 , 0.44107503],
       [0.31304538, 0.6869547 ],
       [0.45737883, 0.54262114],
       [0.41714117, 0.5828588 ],
       [0.40712023, 0.5928798 ],
       [0.33441728, 0.6655827 ],
       [0.43353665, 0.5664633 ],
       [0.7447269 , 0.25527313],
       [0.621356  , 0.37864396],
       [0.3783319 , 0.6216681 ],
       [0.44781   , 0.55219   ],
       [0.48301542, 0.51698464],
       [0.53647965, 0.4635204 ],
       [0.27235043, 0.7276495 ],
       [0.31286678, 0.68713325],
       [0.25652176, 0.74347824],
       [0.33214453, 0.6678555 ],
       [0.43208724, 0.5679127 ],
       [0.40833968, 0.5916603 ],
       [0.38505238, 0.6149477 ],
       [0.5692118 , 0.43078822],
       [0.46600804, 0.533992  ],
       [0.5497936 , 0.45020643],
       [0.45020807, 0.54979193],
       [0.42796022, 0.5720398 ],
       [0.3888206 , 0.61117935],
       [0.4846533 , 0.5153467 ],
       [0.5308398 , 0.46916017],
       [0.51557183, 0.4844282 ],
       [0.50774723, 0.49225274],
       [0.3074222 , 0.69257784],
       [0.5296552 , 0.47034484],
       [0.43636426, 0.56363577],
       [0.53991276, 0.4600873 ],
       [0.31721616, 0.68278384],
       [0.46047273, 0.5395273 ],
       [0.47490063, 0.5250994 ],
       [0.52824277, 0.47175723],
       [0.457207  , 0.542793  ],
       [0.4416229 , 0.5583771 ],
       [0.40086687, 0.59913313],
       [0.3833606 , 0.6166394 ],
       [0.43719593, 0.56280404],
       [0.45983097, 0.54016906],
       [0.55739146, 0.44260848],
       [0.40432483, 0.59567523],
       [0.48863846, 0.51136154],
       [0.53886354, 0.46113646],
       [0.3881491 , 0.6118509 ],
       [0.46499103, 0.53500897],
       [0.45677465, 0.5432254 ],
       [0.3161443 , 0.6838557 ],
       [0.30122072, 0.6987793 ],
       [0.23831676, 0.7616832 ],
       [0.48290133, 0.51709867],
       [0.48805034, 0.51194966],
       [0.42187557, 0.5781244 ],
       [0.34418422, 0.6558158 ],
       [0.50155234, 0.49844766],
       [0.4476942 , 0.55230576],
       [0.5141988 , 0.48580122],
       [0.5105721 , 0.48942798],
       [0.5035898 , 0.4964102 ],
       [0.44836116, 0.5516388 ],
       [0.34592777, 0.65407217],
       [0.46922907, 0.53077096],
       [0.46933192, 0.5306681 ],
       [0.324931  , 0.67506903],
       [0.46530113, 0.5346989 ],
       [0.47291952, 0.5270805 ],
       [0.46135852, 0.5386415 ],
       [0.4754733 , 0.5245267 ],
       [0.35891157, 0.6410885 ],
       [0.39086562, 0.6091344 ],
       [0.30816442, 0.6918356 ],
       [0.49687412, 0.50312585],
       [0.5200912 , 0.47990882],
       [0.4875519 , 0.51244813],
       [0.5540007 , 0.44599935],
       [0.54027206, 0.4597279 ],
       [0.5699338 , 0.43006626],
       [0.5900158 , 0.40998417],
       [0.5970119 , 0.40298808],
       [0.39661238, 0.60338765],
       [0.49449727, 0.5055027 ],
       [0.3779686 , 0.6220314 ],
       [0.50124097, 0.49875903],
       [0.22544992, 0.7745501 ],
       [0.47127435, 0.5287257 ],
       [0.28352153, 0.71647847],
       [0.64423805, 0.355762  ],
       [0.39413768, 0.6058624 ],
       [0.54282564, 0.45717433],
       [0.46165714, 0.53834283],
       [0.5180905 , 0.48190957],
       [0.39704147, 0.6029585 ],
       [0.43237814, 0.5676218 ],
       [0.31423116, 0.6857688 ],
       [0.5368261 , 0.46317393],
       [0.480811  , 0.519189  ],
       [0.23129821, 0.76870185],
       [0.48142907, 0.51857096],
       [0.46461168, 0.53538835],
       [0.34829763, 0.6517024 ],
       [0.3921143 , 0.6078857 ],
       [0.19087231, 0.8091277 ],
       [0.48862153, 0.51137847],
       [0.4947204 , 0.5052796 ],
       [0.420459  , 0.579541  ],
       [0.4614616 , 0.5385384 ],
       [0.47786373, 0.5221363 ],
       [0.41827264, 0.5817273 ],
       [0.5474173 , 0.4525827 ],
       [0.32548684, 0.6745131 ],
       [0.56679237, 0.43320763],
       [0.40780404, 0.5921959 ],
       [0.38193023, 0.6180697 ],
       [0.4480284 , 0.5519716 ],
       [0.5013325 , 0.49866748],
       [0.3713635 , 0.62863654],
       [0.39577278, 0.60422724],
       [0.33836445, 0.6616356 ],
       [0.45375347, 0.5462465 ],
       [0.38420883, 0.6157912 ],
       [0.45425016, 0.54574984],
       [0.4919748 , 0.50802517],
       [0.45068237, 0.54931766],
       [0.41700992, 0.58299005],
       [0.55554634, 0.44445363],
       [0.46723658, 0.5327634 ],
       [0.5878943 , 0.41210565],
       [0.568926  , 0.431074  ],
       [0.39234364, 0.6076563 ],
       [0.6473819 , 0.35261813],
       [0.38869643, 0.61130357],
       [0.26793516, 0.73206484],
       [0.45688847, 0.5431115 ],
       [0.38008037, 0.61991966],
       [0.16848788, 0.83151215],
       [0.4043579 , 0.59564215],
       [0.45271173, 0.54728824],
       [0.581272  , 0.41872802],
       [0.44324502, 0.55675495],
       [0.4609002 , 0.5390998 ],
       [0.52742726, 0.4725727 ],
       [0.4418219 , 0.5581781 ],
       [0.46416882, 0.53583115],
       [0.6136855 , 0.38631454],
       [0.32162246, 0.6783775 ],
       [0.47635484, 0.52364516],
       [0.32252866, 0.6774713 ],
       [0.36855057, 0.6314494 ],
       [0.44381416, 0.55618584],
       [0.50875145, 0.49124855],
       [0.34645924, 0.65354073],
       [0.46846077, 0.53153926],
       [0.48328805, 0.5167119 ],
       [0.41081214, 0.58918786],
       [0.44207048, 0.5579295 ],
       [0.46394467, 0.5360553 ],
       [0.50997555, 0.4900245 ],
       [0.67547196, 0.324528  ],
       [0.4019411 , 0.5980589 ],
       [0.35342014, 0.64657986],
       [0.36070785, 0.6392922 ],
       [0.53396183, 0.46603817],
       [0.4045713 , 0.5954287 ],
       [0.5447713 , 0.45522863],
       [0.57562697, 0.4243731 ],
       [0.44885394, 0.55114603],
       [0.55871075, 0.44128925],
       [0.52741945, 0.47258055],
       [0.51188314, 0.48811683],
       [0.40966976, 0.59033024],
       [0.49136174, 0.5086382 ],
       [0.400206  , 0.599794  ],
       [0.45711517, 0.5428848 ],
       [0.4116866 , 0.5883134 ],
       [0.35508472, 0.6449153 ],
       [0.28726262, 0.7127374 ],
       [0.6569646 , 0.34303543],
       [0.5542973 , 0.44570273],
       [0.30934086, 0.69065917]], dtype=float32)]
i = 2, Test true class= 
[1 0 1 0 1 0 1 0 1 1 1 0 0 0 0 1 0 0 0 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1
 1 0 0 0 1 0 0 1 0 0 0 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 0 0 0 1 0 1 0 1 0 0
 1 1 1 1 1 1 0 0 0 0 0 0 1 0 1 1 0 1 1 1 0 1 0 1 0 0 0 1 0 0 1 0 1 0 0 1 0
 1 1 1 0 1 0 0 0 0 1 1 1 0 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 0
 0 1 0 1 0 1 1 0 0 0 1 1 0 0 1 1 0 1 1 1 1 0 1 1 1 0 1 1 1 0 0 1 0 1 1 0 0
 1 0 0 0 0 1 1 0 0 0 1 0 1 0 0 0 0 1 1 0 0 0 1 1 0 0 0 1 0 1 0 0 1 1 0 1 1
 0]
In grad_steps = 0, loss = 0.7334465384483337
In grad_steps = 1, loss = 0.8167402148246765
In grad_steps = 2, loss = 0.7735605835914612
In grad_steps = 3, loss = 0.7710480093955994
In grad_steps = 4, loss = 0.7205700278282166
In grad_steps = 5, loss = 0.6913873553276062
In grad_steps = 6, loss = 0.6744750142097473
In grad_steps = 7, loss = 0.7702136039733887
In grad_steps = 8, loss = 0.6482344269752502
In grad_steps = 9, loss = 0.6768522262573242
In grad_steps = 10, loss = 0.7198688387870789
In grad_steps = 11, loss = 0.7473267316818237
In grad_steps = 12, loss = 0.728013277053833
In grad_steps = 13, loss = 0.7431149482727051
In grad_steps = 14, loss = 0.7244709134101868
In grad_steps = 15, loss = 0.7045044302940369
In grad_steps = 16, loss = 0.6863945126533508
In grad_steps = 17, loss = 0.7021472454071045
In grad_steps = 18, loss = 0.7429782748222351
In grad_steps = 19, loss = 0.7400995492935181
In grad_steps = 20, loss = 0.6985443830490112
In grad_steps = 21, loss = 0.7117323279380798
In grad_steps = 22, loss = 0.7077588438987732
In grad_steps = 23, loss = 0.7484424710273743
In grad_steps = 24, loss = 0.63241046667099
In grad_steps = 25, loss = 0.6481195688247681
In grad_steps = 26, loss = 0.6989924907684326
In grad_steps = 27, loss = 0.8552204370498657
In grad_steps = 28, loss = 0.6908009648323059
In grad_steps = 29, loss = 0.7597587704658508
In grad_steps = 30, loss = 0.7109095454216003
In grad_steps = 31, loss = 0.6878354549407959
In grad_steps = 32, loss = 0.7014252543449402
In grad_steps = 33, loss = 0.6940369009971619
In grad_steps = 34, loss = 0.7704764604568481
In grad_steps = 35, loss = 0.7202435731887817
In grad_steps = 36, loss = 0.6962742805480957
In grad_steps = 37, loss = 0.6961368918418884
In grad_steps = 38, loss = 0.689670979976654
In grad_steps = 39, loss = 0.644353449344635
In grad_steps = 40, loss = 0.6711612343788147
In grad_steps = 41, loss = 0.7495917081832886
In grad_steps = 42, loss = 0.6941903233528137
In grad_steps = 43, loss = 0.7282434105873108
In grad_steps = 44, loss = 0.6821383237838745
In grad_steps = 45, loss = 0.7016249299049377
In grad_steps = 46, loss = 0.7327237129211426
In grad_steps = 47, loss = 0.6869871020317078
In grad_steps = 48, loss = 0.6860596537590027
In grad_steps = 49, loss = 0.7253687381744385
In grad_steps = 50, loss = 0.757749080657959
In grad_steps = 51, loss = 0.6658582091331482
In grad_steps = 52, loss = 0.6524346470832825
In grad_steps = 53, loss = 0.7394354343414307
In grad_steps = 54, loss = 0.703565239906311
In grad_steps = 55, loss = 0.6469483971595764
In grad_steps = 56, loss = 0.6949186325073242
In grad_steps = 57, loss = 0.7321574687957764
In grad_steps = 58, loss = 0.6837966442108154
In grad_steps = 59, loss = 0.7132870554924011
In grad_steps = 60, loss = 0.7242144346237183
In grad_steps = 61, loss = 0.704759418964386
In grad_steps = 62, loss = 0.727321982383728
In grad_steps = 63, loss = 0.69830322265625
In grad_steps = 64, loss = 0.7261159420013428
In grad_steps = 65, loss = 0.6989838480949402
In grad_steps = 66, loss = 0.6849318146705627
In grad_steps = 67, loss = 0.711024284362793
In grad_steps = 68, loss = 0.6945339441299438
In grad_steps = 69, loss = 0.7123104333877563
In grad_steps = 70, loss = 0.7390173673629761
In grad_steps = 71, loss = 0.6766639351844788
In grad_steps = 72, loss = 0.6929800510406494
In grad_steps = 73, loss = 0.6975201368331909
In grad_steps = 74, loss = 0.7021886110305786
In grad_steps = 75, loss = 0.6780173182487488
In grad_steps = 76, loss = 0.6979866027832031
In grad_steps = 77, loss = 0.6654537916183472
In grad_steps = 78, loss = 0.7341403365135193
In grad_steps = 79, loss = 0.742652952671051
In grad_steps = 80, loss = 0.693855881690979
In grad_steps = 81, loss = 0.6989070177078247
In grad_steps = 82, loss = 0.6980915665626526
In grad_steps = 83, loss = 0.6877725124359131
In grad_steps = 84, loss = 0.6936109066009521
In grad_steps = 85, loss = 0.6979016661643982
In grad_steps = 86, loss = 0.6622697710990906
In grad_steps = 87, loss = 0.696712076663971
In grad_steps = 88, loss = 0.7295469045639038
In grad_steps = 89, loss = 0.6679514646530151
In grad_steps = 90, loss = 0.6699970364570618
In grad_steps = 91, loss = 0.7488227486610413
In grad_steps = 92, loss = 0.6588180065155029
In grad_steps = 93, loss = 0.6385700702667236
In grad_steps = 94, loss = 0.760293185710907
In grad_steps = 95, loss = 0.7652850151062012
In grad_steps = 96, loss = 0.7375245690345764
In grad_steps = 97, loss = 0.7027794122695923
In grad_steps = 98, loss = 0.6879298686981201
In grad_steps = 99, loss = 0.6967242956161499
In grad_steps = 100, loss = 0.6843401193618774
In grad_steps = 101, loss = 0.7017355561256409
In grad_steps = 102, loss = 0.6888898611068726
In grad_steps = 103, loss = 0.6978261470794678
In grad_steps = 104, loss = 0.6805458664894104
In grad_steps = 105, loss = 0.6663554310798645
In grad_steps = 106, loss = 0.7699444890022278
In grad_steps = 107, loss = 0.7019290924072266
In grad_steps = 108, loss = 0.6695727109909058
In grad_steps = 109, loss = 0.6781390309333801
In grad_steps = 110, loss = 0.7429921627044678
In grad_steps = 111, loss = 0.7662174105644226
In grad_steps = 112, loss = 0.6865647435188293
In grad_steps = 113, loss = 0.6949307322502136
In grad_steps = 114, loss = 0.7103422284126282
In grad_steps = 115, loss = 0.6386006474494934
In grad_steps = 116, loss = 0.740090012550354
In grad_steps = 117, loss = 0.6762267351150513
In grad_steps = 118, loss = 0.6282093524932861
In grad_steps = 119, loss = 0.8052622675895691
In grad_steps = 120, loss = 0.6416478157043457
In grad_steps = 121, loss = 0.6580159664154053
In grad_steps = 122, loss = 0.7021626830101013
In grad_steps = 123, loss = 0.706534206867218
In grad_steps = 124, loss = 0.6988142132759094
In grad_steps = 125, loss = 0.6805289387702942
In grad_steps = 126, loss = 0.7373635172843933
In grad_steps = 127, loss = 0.6541131734848022
In grad_steps = 128, loss = 0.6977714896202087
In grad_steps = 129, loss = 0.6880548000335693
In grad_steps = 130, loss = 0.7316650748252869
In grad_steps = 131, loss = 0.7123382091522217
In grad_steps = 132, loss = 0.6701130270957947
In grad_steps = 133, loss = 0.6577319502830505
In grad_steps = 134, loss = 0.7016422748565674
In grad_steps = 135, loss = 0.7434125542640686
In grad_steps = 136, loss = 0.6117503643035889
In grad_steps = 137, loss = 0.6270540952682495
In grad_steps = 138, loss = 0.6597634553909302
In grad_steps = 139, loss = 0.8141911625862122
In grad_steps = 140, loss = 0.6986808776855469
In grad_steps = 141, loss = 0.7785778641700745
In grad_steps = 142, loss = 0.7307484149932861
In grad_steps = 143, loss = 0.6794171333312988
In grad_steps = 144, loss = 0.6809212565422058
In grad_steps = 145, loss = 0.6672326922416687
In grad_steps = 146, loss = 0.7087128758430481
In grad_steps = 147, loss = 0.7033494114875793
In grad_steps = 148, loss = 0.6908038258552551
In grad_steps = 149, loss = 0.6858099699020386
In grad_steps = 150, loss = 0.6776715517044067
In grad_steps = 151, loss = 0.669433057308197
In grad_steps = 152, loss = 0.6604706645011902
In grad_steps = 153, loss = 0.7012704610824585
In grad_steps = 154, loss = 0.6714991927146912
In grad_steps = 155, loss = 0.6907410621643066
In grad_steps = 156, loss = 0.6630946397781372
In grad_steps = 157, loss = 0.696383535861969
In grad_steps = 158, loss = 0.7092316746711731
In grad_steps = 159, loss = 0.6821168661117554
In grad_steps = 160, loss = 0.6460258364677429
In grad_steps = 161, loss = 0.7034347057342529
In grad_steps = 162, loss = 0.7331361770629883
In grad_steps = 163, loss = 0.642798900604248
In grad_steps = 164, loss = 0.617365837097168
In grad_steps = 165, loss = 0.7345699667930603
In grad_steps = 166, loss = 0.6632410287857056
In grad_steps = 167, loss = 0.6096877455711365
In grad_steps = 168, loss = 0.6977525949478149
In grad_steps = 169, loss = 0.7366217970848083
In grad_steps = 170, loss = 0.6061229705810547
In grad_steps = 171, loss = 0.753520131111145
In grad_steps = 172, loss = 0.7718185186386108
In grad_steps = 173, loss = 0.761816143989563
In grad_steps = 174, loss = 0.7107806205749512
In grad_steps = 175, loss = 0.6790905594825745
In grad_steps = 176, loss = 0.7479899525642395
In grad_steps = 177, loss = 0.6947023868560791
In grad_steps = 178, loss = 0.6524375081062317
In grad_steps = 179, loss = 0.7220374941825867
In grad_steps = 180, loss = 0.7056506276130676
In grad_steps = 181, loss = 0.7141842246055603
In grad_steps = 182, loss = 0.6975355744361877
In grad_steps = 183, loss = 0.6611995100975037
In grad_steps = 184, loss = 0.6852782368659973
In grad_steps = 185, loss = 0.6859863996505737
In grad_steps = 186, loss = 0.7104734182357788
In grad_steps = 187, loss = 0.668373167514801
In grad_steps = 188, loss = 0.7003467082977295
In grad_steps = 189, loss = 0.6573787927627563
In grad_steps = 190, loss = 0.7356237769126892
In grad_steps = 191, loss = 0.7367639541625977
In grad_steps = 192, loss = 0.6914709806442261
In grad_steps = 193, loss = 0.6926873326301575
In grad_steps = 194, loss = 0.6890273094177246
In grad_steps = 195, loss = 0.6805786490440369
In grad_steps = 196, loss = 0.6825223565101624
In grad_steps = 197, loss = 0.7075490355491638
In grad_steps = 198, loss = 0.6554995775222778
In grad_steps = 199, loss = 0.6908900141716003
In grad_steps = 200, loss = 0.7270829677581787
In grad_steps = 201, loss = 0.6457304358482361
In grad_steps = 202, loss = 0.6583667993545532
In grad_steps = 203, loss = 0.7349683046340942
In grad_steps = 204, loss = 0.6508268713951111
In grad_steps = 205, loss = 0.631056547164917
In grad_steps = 206, loss = 0.7414957880973816
In grad_steps = 207, loss = 0.7423008680343628
In grad_steps = 208, loss = 0.7201961278915405
In grad_steps = 209, loss = 0.6928983926773071
In grad_steps = 210, loss = 0.6911879181861877
In grad_steps = 211, loss = 0.7201476693153381
In grad_steps = 212, loss = 0.6784369945526123
In grad_steps = 213, loss = 0.7083137631416321
In grad_steps = 214, loss = 0.6820064783096313
In grad_steps = 215, loss = 0.6826613545417786
In grad_steps = 216, loss = 0.6852009296417236
In grad_steps = 217, loss = 0.6720958948135376
In grad_steps = 218, loss = 0.7626476287841797
In grad_steps = 219, loss = 0.6948155164718628
In grad_steps = 220, loss = 0.661742627620697
In grad_steps = 221, loss = 0.6614388823509216
In grad_steps = 222, loss = 0.7365865707397461
In grad_steps = 223, loss = 0.7153894305229187
In grad_steps = 224, loss = 0.6848297715187073
In grad_steps = 225, loss = 0.6859768629074097
In grad_steps = 226, loss = 0.67905193567276
In grad_steps = 227, loss = 0.6417970061302185
In grad_steps = 228, loss = 0.7282391786575317
In grad_steps = 229, loss = 0.6630284190177917
In grad_steps = 230, loss = 0.6279245018959045
In grad_steps = 231, loss = 0.7716261744499207
In grad_steps = 232, loss = 0.6352139711380005
In grad_steps = 233, loss = 0.6290754675865173
In grad_steps = 234, loss = 0.6981096267700195
In grad_steps = 235, loss = 0.7209593057632446
In grad_steps = 236, loss = 0.6718096137046814
In grad_steps = 237, loss = 0.6581840515136719
In grad_steps = 238, loss = 0.6479547023773193
In grad_steps = 239, loss = 0.6324592232704163
In grad_steps = 240, loss = 0.6807542443275452
In grad_steps = 241, loss = 0.6477841734886169
In grad_steps = 242, loss = 0.7158609628677368
In grad_steps = 243, loss = 0.6309288740158081
In grad_steps = 244, loss = 0.6299228072166443
In grad_steps = 245, loss = 0.598946750164032
In grad_steps = 246, loss = 0.7006025314331055
In grad_steps = 247, loss = 0.7439272403717041
In grad_steps = 248, loss = 0.5291956663131714
In grad_steps = 249, loss = 0.5621960759162903
In grad_steps = 250, loss = 0.5143929123878479
In grad_steps = 251, loss = 0.629052996635437
In grad_steps = 252, loss = 0.7506105899810791
In grad_steps = 253, loss = 0.61933434009552
In grad_steps = 254, loss = 0.8414407968521118
In grad_steps = 255, loss = 0.8111788034439087
In grad_steps = 256, loss = 0.6731938719749451
In grad_steps = 257, loss = 0.5957669615745544
In grad_steps = 258, loss = 0.5919119119644165
In grad_steps = 259, loss = 0.6833581328392029
In grad_steps = 260, loss = 0.794751763343811
In grad_steps = 261, loss = 0.6559410691261292
In grad_steps = 262, loss = 0.7727957963943481
In grad_steps = 263, loss = 0.5599976181983948
In grad_steps = 264, loss = 0.6172143220901489
In grad_steps = 265, loss = 0.7375503778457642
In grad_steps = 266, loss = 0.661501407623291
In grad_steps = 267, loss = 0.65047687292099
In grad_steps = 268, loss = 0.638232409954071
In grad_steps = 269, loss = 0.674206554889679
In grad_steps = 270, loss = 0.7160652875900269
In grad_steps = 271, loss = 0.6624229550361633
In grad_steps = 272, loss = 0.6260565519332886
In grad_steps = 273, loss = 0.7380854487419128
In grad_steps = 274, loss = 0.7061177492141724
In grad_steps = 275, loss = 0.606810986995697
In grad_steps = 276, loss = 0.6660394668579102
In grad_steps = 277, loss = 0.6438063979148865
In grad_steps = 278, loss = 0.5805156826972961
In grad_steps = 279, loss = 0.5616411566734314
In grad_steps = 280, loss = 0.5716851353645325
In grad_steps = 281, loss = 0.6367981433868408
In grad_steps = 282, loss = 0.4737892150878906
In grad_steps = 283, loss = 0.5798704028129578
In grad_steps = 284, loss = 0.4716770052909851
In grad_steps = 285, loss = 1.1607260704040527
In grad_steps = 286, loss = 0.5816288590431213
In grad_steps = 287, loss = 0.6282803416252136
In grad_steps = 288, loss = 0.7533230781555176
In grad_steps = 289, loss = 0.595440149307251
In grad_steps = 290, loss = 0.7707967758178711
In grad_steps = 291, loss = 0.8523000478744507
In grad_steps = 292, loss = 0.6179596185684204
In grad_steps = 293, loss = 0.6873436570167542
In grad_steps = 294, loss = 0.7129321098327637
In grad_steps = 295, loss = 0.6541464328765869
In grad_steps = 296, loss = 0.6723880767822266
In grad_steps = 297, loss = 0.6817535161972046
In grad_steps = 298, loss = 0.6941098570823669
In grad_steps = 299, loss = 0.6765956282615662
In grad_steps = 300, loss = 0.702143669128418
In grad_steps = 301, loss = 0.6744035482406616
In grad_steps = 302, loss = 0.7312607169151306
In grad_steps = 303, loss = 0.7258040308952332
In grad_steps = 304, loss = 0.6822369694709778
In grad_steps = 305, loss = 0.6902923583984375
In grad_steps = 306, loss = 0.6761893630027771
In grad_steps = 307, loss = 0.6517075300216675
In grad_steps = 308, loss = 0.6750012040138245
In grad_steps = 309, loss = 0.7218705415725708
In grad_steps = 310, loss = 0.6380512714385986
In grad_steps = 311, loss = 0.6785875558853149
In grad_steps = 312, loss = 0.7190430164337158
In grad_steps = 313, loss = 0.6102018356323242
In grad_steps = 314, loss = 0.6610643267631531
In grad_steps = 315, loss = 0.7186191082000732
In grad_steps = 316, loss = 0.6186560988426208
In grad_steps = 317, loss = 0.6125434637069702
In grad_steps = 318, loss = 0.7013489603996277
In grad_steps = 319, loss = 0.7116097807884216
In grad_steps = 320, loss = 0.6762254238128662
In grad_steps = 321, loss = 0.6819460988044739
In grad_steps = 322, loss = 0.6934752464294434
In grad_steps = 323, loss = 0.7955999970436096
In grad_steps = 324, loss = 0.6587579846382141
In grad_steps = 325, loss = 0.6985412240028381
In grad_steps = 326, loss = 0.6456663012504578
In grad_steps = 327, loss = 0.6388766765594482
In grad_steps = 328, loss = 0.6641747951507568
In grad_steps = 329, loss = 0.6888409852981567
In grad_steps = 330, loss = 0.856351375579834
In grad_steps = 331, loss = 0.6761289238929749
In grad_steps = 332, loss = 0.6542364358901978
In grad_steps = 333, loss = 0.614467978477478
In grad_steps = 334, loss = 0.7271521091461182
In grad_steps = 335, loss = 0.2827022671699524
In grad_steps = 336, loss = 0.6477509140968323
In grad_steps = 337, loss = 0.6813328862190247
In grad_steps = 338, loss = 0.6509170532226562
In grad_steps = 339, loss = 0.5971815586090088
In grad_steps = 340, loss = 0.7504552602767944
In grad_steps = 341, loss = 0.6099185943603516
In grad_steps = 342, loss = 0.644528329372406
In grad_steps = 343, loss = 0.7144766449928284
In grad_steps = 344, loss = 0.5697241425514221
In grad_steps = 345, loss = 0.5792094469070435
In grad_steps = 346, loss = 0.6305620670318604
In grad_steps = 347, loss = 0.6218582391738892
In grad_steps = 348, loss = 0.6181043386459351
In grad_steps = 349, loss = 0.6443283557891846
In grad_steps = 350, loss = 0.5376946926116943
In grad_steps = 351, loss = 0.5565489530563354
In grad_steps = 352, loss = 0.5571480393409729
In grad_steps = 353, loss = 0.6922317147254944
In grad_steps = 354, loss = 0.5047213435173035
In grad_steps = 355, loss = 0.3762161433696747
In grad_steps = 356, loss = 0.4688936769962311
In grad_steps = 357, loss = 0.3780909478664398
In grad_steps = 358, loss = 0.3556223511695862
In grad_steps = 359, loss = 0.47631826996803284
In grad_steps = 360, loss = 0.25743311643600464
In grad_steps = 361, loss = 0.29086872935295105
In grad_steps = 362, loss = 0.16116856038570404
In grad_steps = 363, loss = 0.7305948138237
In grad_steps = 364, loss = 0.3642551004886627
In grad_steps = 365, loss = 0.2520262598991394
In grad_steps = 366, loss = 0.5820267796516418
In grad_steps = 367, loss = 0.46940848231315613
In grad_steps = 368, loss = 0.38477757573127747
In grad_steps = 369, loss = 0.43776631355285645
In grad_steps = 370, loss = 0.42337024211883545
In grad_steps = 371, loss = 0.526804506778717
In grad_steps = 372, loss = 0.8612520098686218
In grad_steps = 373, loss = 0.5693606734275818
In grad_steps = 374, loss = 0.6679696440696716
In grad_steps = 375, loss = 0.4516730308532715
In grad_steps = 376, loss = 0.6026129722595215
In grad_steps = 377, loss = 0.6271255612373352
In grad_steps = 378, loss = 0.6352453827857971
In grad_steps = 379, loss = 0.47782617807388306
In grad_steps = 380, loss = 0.5421800017356873
In grad_steps = 381, loss = 0.743203341960907
In grad_steps = 382, loss = 0.6873055696487427
In grad_steps = 383, loss = 0.5332408547401428
In grad_steps = 384, loss = 0.5364006757736206
In grad_steps = 385, loss = 0.7092111706733704
In grad_steps = 386, loss = 0.7456536293029785
In grad_steps = 387, loss = 0.6929206252098083
In grad_steps = 388, loss = 0.5687156915664673
In grad_steps = 389, loss = 0.5894032716751099
In grad_steps = 390, loss = 0.48779043555259705
In grad_steps = 391, loss = 0.5148648619651794
In grad_steps = 392, loss = 0.47449588775634766
In grad_steps = 393, loss = 0.532018780708313
In grad_steps = 394, loss = 0.3624815344810486
In grad_steps = 395, loss = 0.3544735312461853
In grad_steps = 396, loss = 0.3702163100242615
In grad_steps = 397, loss = 0.4388396739959717
In grad_steps = 398, loss = 0.3455908000469208
In grad_steps = 399, loss = 0.28447043895721436
In grad_steps = 400, loss = 0.42869195342063904
In grad_steps = 401, loss = 0.1795768290758133
In grad_steps = 402, loss = 0.24489761888980865
In grad_steps = 403, loss = 0.9460424184799194
In grad_steps = 404, loss = 0.4954775273799896
In grad_steps = 405, loss = 0.558533251285553
In grad_steps = 406, loss = 0.4352760314941406
In grad_steps = 407, loss = 0.5503928661346436
In grad_steps = 408, loss = 0.6086140275001526
In grad_steps = 409, loss = 0.8558478355407715
In grad_steps = 410, loss = 0.7114228010177612
In grad_steps = 411, loss = 0.7449256777763367
In grad_steps = 412, loss = 0.8221262693405151
In grad_steps = 413, loss = 0.6258995532989502
In grad_steps = 414, loss = 0.6637067198753357
In grad_steps = 415, loss = 0.6755730509757996
In grad_steps = 416, loss = 0.6625417470932007
In grad_steps = 417, loss = 0.6714541912078857
In grad_steps = 418, loss = 0.6635457873344421
In grad_steps = 419, loss = 0.6385322213172913
In grad_steps = 420, loss = 0.7087299823760986
In grad_steps = 421, loss = 0.7193858623504639
In grad_steps = 422, loss = 0.6415744423866272
In grad_steps = 423, loss = 0.6737635731697083
In grad_steps = 424, loss = 0.701211154460907
In grad_steps = 425, loss = 0.6333770751953125
In grad_steps = 426, loss = 0.6577706933021545
In grad_steps = 427, loss = 0.6571782827377319
In grad_steps = 428, loss = 0.6298326253890991
In grad_steps = 429, loss = 0.5949028730392456
In grad_steps = 430, loss = 0.7077493071556091
In grad_steps = 431, loss = 0.6888232231140137
In grad_steps = 432, loss = 0.6829125881195068
In grad_steps = 433, loss = 0.693966269493103
In grad_steps = 434, loss = 0.6249436736106873
In grad_steps = 435, loss = 0.7994261384010315
In grad_steps = 436, loss = 0.6551151871681213
In grad_steps = 437, loss = 0.6187759637832642
In grad_steps = 438, loss = 0.6005408763885498
In grad_steps = 439, loss = 0.6529699563980103
In grad_steps = 440, loss = 0.6346465945243835
In grad_steps = 441, loss = 0.6123555302619934
In grad_steps = 442, loss = 0.9247303605079651
In grad_steps = 443, loss = 0.7094835042953491
In grad_steps = 444, loss = 0.6975257396697998
In grad_steps = 445, loss = 0.5922509431838989
In grad_steps = 446, loss = 0.6636335253715515
In grad_steps = 447, loss = 0.05687335506081581
i = 3, Test ensemble probabilities = 
[array([[0.63234293, 0.3676571 ],
       [0.15927052, 0.8407295 ],
       [0.4707932 , 0.5292069 ],
       [0.26916105, 0.73083895],
       [0.34443957, 0.65556043],
       [0.63733286, 0.36266717],
       [0.3972575 , 0.6027425 ],
       [0.53274614, 0.4672539 ],
       [0.32415473, 0.67584527],
       [0.5880499 , 0.41195014],
       [0.53142023, 0.4685797 ],
       [0.4613917 , 0.5386083 ],
       [0.39642295, 0.6035771 ],
       [0.2630529 , 0.73694706],
       [0.25113425, 0.7488658 ],
       [0.31638494, 0.683615  ],
       [0.33599755, 0.6640024 ],
       [0.4374039 , 0.56259614],
       [0.29857844, 0.70142156],
       [0.34628177, 0.65371823],
       [0.48291063, 0.51708937],
       [0.7528585 , 0.24714147],
       [0.26550072, 0.7344993 ],
       [0.34369338, 0.6563066 ],
       [0.32360822, 0.6763918 ],
       [0.48903427, 0.5109657 ],
       [0.517874  , 0.48212597],
       [0.30616266, 0.6938373 ],
       [0.37618548, 0.62381446],
       [0.48018897, 0.51981103],
       [0.442965  , 0.557035  ],
       [0.32442498, 0.6755751 ],
       [0.3079656 , 0.69203436],
       [0.19288743, 0.8071125 ],
       [0.5714873 , 0.4285127 ],
       [0.6726957 , 0.3273043 ],
       [0.1593832 , 0.8406168 ],
       [0.673186  , 0.326814  ],
       [0.44626588, 0.5537341 ],
       [0.53769314, 0.46230686],
       [0.40586078, 0.5941392 ],
       [0.71461   , 0.28539005],
       [0.5180719 , 0.4819281 ],
       [0.2545866 , 0.74541336],
       [0.37031677, 0.6296832 ],
       [0.44500518, 0.5549948 ],
       [0.32468525, 0.6753148 ],
       [0.84190464, 0.15809539],
       [0.51651853, 0.48348144],
       [0.33493203, 0.66506803],
       [0.45689556, 0.5431044 ],
       [0.4609658 , 0.5390341 ],
       [0.54432595, 0.45567402],
       [0.26599905, 0.7340009 ],
       [0.04212758, 0.95787245],
       [0.2829374 , 0.71706253],
       [0.2043219 , 0.7956781 ],
       [0.5221622 , 0.4778378 ],
       [0.18717316, 0.8128269 ],
       [0.19525924, 0.8047408 ],
       [0.5766938 , 0.42330623],
       [0.4176103 , 0.58238965],
       [0.5324215 , 0.46757844],
       [0.4488813 , 0.5511187 ],
       [0.44950402, 0.55049604],
       [0.46307266, 0.53692734],
       [0.39134234, 0.6086577 ],
       [0.41254333, 0.58745664],
       [0.47793105, 0.5220689 ],
       [0.56690925, 0.4330908 ],
       [0.23565127, 0.76434875],
       [0.45820194, 0.5417981 ],
       [0.4637113 , 0.5362887 ],
       [0.49555954, 0.5044404 ],
       [0.22995174, 0.77004826],
       [0.6644476 , 0.3355524 ],
       [0.61461085, 0.38538912],
       [0.5472649 , 0.45273513],
       [0.21934402, 0.780656  ],
       [0.06799435, 0.93200564],
       [0.16100384, 0.8389961 ],
       [0.50630325, 0.49369678],
       [0.43458852, 0.56541145],
       [0.385639  , 0.614361  ],
       [0.61727566, 0.38272437],
       [0.25399345, 0.74600655],
       [0.2955888 , 0.70441115],
       [0.5325431 , 0.46745688],
       [0.678447  , 0.321553  ],
       [0.16606222, 0.8339377 ],
       [0.3081697 , 0.6918303 ],
       [0.4355301 , 0.5644699 ],
       [0.34492984, 0.6550701 ],
       [0.25015336, 0.74984664],
       [0.64061916, 0.3593808 ],
       [0.5967088 , 0.4032912 ],
       [0.18353361, 0.81646645],
       [0.52374434, 0.47625569],
       [0.34673527, 0.65326476],
       [0.38547316, 0.61452687],
       [0.5847594 , 0.41524062],
       [0.44700786, 0.5529921 ],
       [0.47356904, 0.52643096],
       [0.422917  , 0.577083  ],
       [0.19558223, 0.8044177 ],
       [0.42946094, 0.57053906],
       [0.37188283, 0.6281172 ],
       [0.17802247, 0.82197756],
       [0.27269948, 0.7273005 ],
       [0.5892211 , 0.4107789 ],
       [0.40044641, 0.5995536 ],
       [0.5170463 , 0.48295373],
       [0.36080304, 0.63919693],
       [0.43622753, 0.5637725 ],
       [0.23612504, 0.763875  ],
       [0.42220357, 0.5777964 ],
       [0.2890543 , 0.71094567],
       [0.40281117, 0.59718883],
       [0.4923154 , 0.5076846 ],
       [0.26968554, 0.73031443],
       [0.20687027, 0.79312974],
       [0.8389131 , 0.16108696],
       [0.63067454, 0.36932552],
       [0.18742153, 0.81257844],
       [0.30327702, 0.69672304],
       [0.3085698 , 0.6914302 ],
       [0.5595747 , 0.44042534],
       [0.24351013, 0.7564899 ],
       [0.21686015, 0.7831398 ],
       [0.41896248, 0.5810375 ],
       [0.56056327, 0.43943676],
       [0.17147662, 0.8285234 ],
       [0.5208721 , 0.47912785],
       [0.4153351 , 0.5846649 ],
       [0.19790003, 0.8021    ],
       [0.25866634, 0.7413336 ],
       [0.29833403, 0.70166594],
       [0.3234097 , 0.67659026],
       [0.7465652 , 0.25343475],
       [0.59630436, 0.4036956 ],
       [0.23582876, 0.76417124],
       [0.45957777, 0.54042226],
       [0.40649194, 0.593508  ],
       [0.17363618, 0.82636386],
       [0.63669163, 0.36330843],
       [0.30761877, 0.69238126],
       [0.51930255, 0.48069745],
       [0.4234089 , 0.5765911 ],
       [0.4946091 , 0.50539094],
       [0.4197197 , 0.58028024],
       [0.41942033, 0.58057964],
       [0.2945898 , 0.7054102 ],
       [0.55483747, 0.44516253],
       [0.22091393, 0.77908605],
       [0.35284546, 0.64715457],
       [0.46206364, 0.53793633],
       [0.16025013, 0.83974993],
       [0.5305458 , 0.4694542 ],
       [0.46310562, 0.5368943 ],
       [0.13942465, 0.8605754 ],
       [0.412065  , 0.58793503],
       [0.25527856, 0.7447215 ],
       [0.46506244, 0.53493756],
       [0.12068249, 0.8793175 ],
       [0.33459774, 0.6654023 ],
       [0.28955707, 0.7104429 ],
       [0.3447704 , 0.65522957],
       [0.09330033, 0.9066997 ],
       [0.42613664, 0.5738633 ],
       [0.52296096, 0.47703904],
       [0.6178167 , 0.3821833 ],
       [0.49262506, 0.50737494],
       [0.39261836, 0.6073817 ],
       [0.8172014 , 0.18279864],
       [0.63021123, 0.36978877],
       [0.33625674, 0.66374326],
       [0.5179674 , 0.48203266],
       [0.07003294, 0.92996705],
       [0.04434535, 0.9556546 ],
       [0.24915382, 0.75084615],
       [0.503802  , 0.496198  ],
       [0.31493956, 0.6850605 ],
       [0.290524  , 0.709476  ],
       [0.22521402, 0.774786  ],
       [0.40556347, 0.5944366 ],
       [0.19308199, 0.80691797],
       [0.38832644, 0.61167353],
       [0.3310196 , 0.66898036],
       [0.398134  , 0.60186595],
       [0.70039856, 0.29960147],
       [0.56304246, 0.43695754],
       [0.3614178 , 0.6385822 ],
       [0.43890896, 0.561091  ],
       [0.7291902 , 0.27080986],
       [0.1708664 , 0.82913357],
       [0.405132  , 0.594868  ],
       [0.779685  , 0.2203149 ],
       [0.2233463 , 0.7766537 ],
       [0.5981941 , 0.40180585],
       [0.4170927 , 0.58290726],
       [0.45992476, 0.54007524],
       [0.7984498 , 0.2015502 ],
       [0.44850412, 0.5514959 ],
       [0.2520903 , 0.74790967],
       [0.39397344, 0.60602653],
       [0.37396133, 0.6260387 ],
       [0.392793  , 0.60720694],
       [0.6720211 , 0.3279789 ],
       [0.5807893 , 0.41921067],
       [0.73216754, 0.2678325 ],
       [0.5034451 , 0.4965549 ],
       [0.41664135, 0.5833587 ],
       [0.5674081 , 0.43259186],
       [0.17715418, 0.8228458 ],
       [0.26898026, 0.73101974],
       [0.21245757, 0.7875424 ],
       [0.26290798, 0.73709196],
       [0.14729999, 0.85270005],
       [0.10998562, 0.89001435],
       [0.09227446, 0.9077256 ],
       [0.64987636, 0.35012358],
       [0.36819777, 0.6318022 ],
       [0.09723709, 0.9027629 ]], dtype=float32), array([[0.4571166 , 0.5428834 ],
       [0.5294107 , 0.4705893 ],
       [0.22323202, 0.77676797],
       [0.51862043, 0.4813795 ],
       [0.39006442, 0.6099356 ],
       [0.55189687, 0.44810316],
       [0.44895777, 0.5510422 ],
       [0.4808965 , 0.5191035 ],
       [0.36935973, 0.63064027],
       [0.33487913, 0.66512084],
       [0.48611328, 0.5138867 ],
       [0.5078283 , 0.4921717 ],
       [0.513526  , 0.486474  ],
       [0.43515244, 0.5648476 ],
       [0.27377552, 0.7262245 ],
       [0.2432999 , 0.7567001 ],
       [0.49993494, 0.5000651 ],
       [0.291736  , 0.708264  ],
       [0.41045925, 0.5895407 ],
       [0.32647273, 0.6735273 ],
       [0.42342755, 0.5765725 ],
       [0.4086655 , 0.5913345 ],
       [0.52046114, 0.47953892],
       [0.3826234 , 0.61737657],
       [0.25063252, 0.7493674 ],
       [0.50633806, 0.49366197],
       [0.36846852, 0.63153154],
       [0.51383686, 0.48616308],
       [0.3087037 , 0.6912963 ],
       [0.55610746, 0.44389254],
       [0.4161952 , 0.5838048 ],
       [0.18752672, 0.8124733 ],
       [0.42430365, 0.5756963 ],
       [0.15909182, 0.84090817],
       [0.46740913, 0.5325908 ],
       [0.65301734, 0.34698266],
       [0.19227678, 0.8077232 ],
       [0.5735935 , 0.4264065 ],
       [0.28589913, 0.7141009 ],
       [0.44559172, 0.55440825],
       [0.54125   , 0.45875004],
       [0.2907995 , 0.7092005 ],
       [0.37092564, 0.62907434],
       [0.4308764 , 0.5691236 ],
       [0.37090874, 0.62909126],
       [0.43027127, 0.56972873],
       [0.27650887, 0.7234912 ],
       [0.6256027 , 0.37439725],
       [0.57003075, 0.4299692 ],
       [0.32834244, 0.67165756],
       [0.4865562 , 0.5134438 ],
       [0.44996402, 0.550036  ],
       [0.47377753, 0.52622247],
       [0.16757809, 0.83242196],
       [0.268217  , 0.73178303],
       [0.33810017, 0.6618998 ],
       [0.26060697, 0.73939306],
       [0.3524904 , 0.64750963],
       [0.38166958, 0.6183304 ],
       [0.2701247 , 0.72987527],
       [0.5141597 , 0.4858403 ],
       [0.4992094 , 0.5007906 ],
       [0.5771801 , 0.42281997],
       [0.3514243 , 0.6485757 ],
       [0.6553351 , 0.34466484],
       [0.46719575, 0.53280425],
       [0.47575495, 0.5242451 ],
       [0.5901849 , 0.4098151 ],
       [0.4846395 , 0.51536053],
       [0.4624119 , 0.5375881 ],
       [0.33387172, 0.6661283 ],
       [0.47095305, 0.5290469 ],
       [0.41752917, 0.5824709 ],
       [0.40379873, 0.5962013 ],
       [0.25252804, 0.747472  ],
       [0.34796432, 0.6520357 ],
       [0.49279785, 0.50720215],
       [0.4465732 , 0.5534268 ],
       [0.2368767 , 0.76312333],
       [0.35196486, 0.6480351 ],
       [0.48743895, 0.512561  ],
       [0.3570835 , 0.64291644],
       [0.34352624, 0.65647376],
       [0.47918805, 0.5208119 ],
       [0.55404097, 0.44595906],
       [0.35746393, 0.6425361 ],
       [0.25150567, 0.7484943 ],
       [0.5243571 , 0.47564292],
       [0.17094587, 0.8290542 ],
       [0.36935994, 0.6306401 ],
       [0.43561053, 0.56438947],
       [0.42300782, 0.5769922 ],
       [0.33751628, 0.6624837 ],
       [0.27065283, 0.7293471 ],
       [0.4002573 , 0.5997427 ],
       [0.553859  , 0.44614094],
       [0.21557094, 0.784429  ],
       [0.35128933, 0.64871067],
       [0.31674996, 0.68325007],
       [0.2832636 , 0.71673644],
       [0.4784139 , 0.5215861 ],
       [0.38211825, 0.6178818 ],
       [0.4683589 , 0.5316411 ],
       [0.3947358 , 0.6052642 ],
       [0.1879898 , 0.8120102 ],
       [0.381092  , 0.618908  ],
       [0.28480652, 0.71519345],
       [0.09613001, 0.90387005],
       [0.36259332, 0.63740665],
       [0.5208426 , 0.4791574 ],
       [0.39169744, 0.6083026 ],
       [0.31034505, 0.689655  ],
       [0.40007278, 0.5999272 ],
       [0.48548177, 0.5145182 ],
       [0.32625067, 0.6737493 ],
       [0.28176755, 0.7182325 ],
       [0.19873911, 0.8012609 ],
       [0.3922998 , 0.6077002 ],
       [0.32452005, 0.6754799 ],
       [0.39452052, 0.6054795 ],
       [0.3498791 , 0.6501209 ],
       [0.5362299 , 0.46377012],
       [0.73773646, 0.2622635 ],
       [0.33494902, 0.665051  ],
       [0.20055218, 0.79944783],
       [0.19752063, 0.8024793 ],
       [0.4704736 , 0.5295264 ],
       [0.19533584, 0.8046642 ],
       [0.1866918 , 0.8133082 ],
       [0.5122611 , 0.48773894],
       [0.6391871 , 0.36081296],
       [0.3035828 , 0.6964172 ],
       [0.28577548, 0.71422446],
       [0.4295801 , 0.5704199 ],
       [0.36431298, 0.63568705],
       [0.39080298, 0.609197  ],
       [0.45286956, 0.5471304 ],
       [0.16296265, 0.8370374 ],
       [0.6365087 , 0.36349133],
       [0.5055456 , 0.49445438],
       [0.12050015, 0.87949985],
       [0.44568032, 0.5543197 ],
       [0.33894923, 0.66105074],
       [0.39209473, 0.60790527],
       [0.3043178 , 0.69568217],
       [0.22776023, 0.77223974],
       [0.5165726 , 0.4834274 ],
       [0.53855336, 0.4614467 ],
       [0.30221108, 0.69778895],
       [0.4841881 , 0.51581186],
       [0.35525265, 0.6447474 ],
       [0.13065955, 0.8693404 ],
       [0.5082202 , 0.49177983],
       [0.22805978, 0.77194023],
       [0.49857068, 0.5014293 ],
       [0.37822402, 0.621776  ],
       [0.18954322, 0.8104568 ],
       [0.45782456, 0.5421755 ],
       [0.343549  , 0.6564509 ],
       [0.23705377, 0.7629462 ],
       [0.53499   , 0.46501   ],
       [0.29976618, 0.7002338 ],
       [0.46745688, 0.5325431 ],
       [0.12526599, 0.874734  ],
       [0.48235503, 0.51764494],
       [0.25172153, 0.7482785 ],
       [0.39188084, 0.6081192 ],
       [0.4759117 , 0.5240883 ],
       [0.5841465 , 0.41585344],
       [0.4407678 , 0.55923223],
       [0.5083613 , 0.49163875],
       [0.34765303, 0.65234697],
       [0.24413344, 0.7558665 ],
       [0.6039984 , 0.39600158],
       [0.19725229, 0.80274767],
       [0.26565233, 0.7343477 ],
       [0.461022  , 0.53897804],
       [0.39643595, 0.603564  ],
       [0.0636213 , 0.93637866],
       [0.3433742 , 0.6566258 ],
       [0.48139504, 0.518605  ],
       [0.47634032, 0.52365965],
       [0.3635606 , 0.6364394 ],
       [0.35425   , 0.64575   ],
       [0.37250814, 0.6274919 ],
       [0.3279293 , 0.67207074],
       [0.46274236, 0.5372576 ],
       [0.08992676, 0.9100732 ],
       [0.24736886, 0.7526312 ],
       [0.4303706 , 0.5696294 ],
       [0.30280727, 0.6971927 ],
       [0.31135   , 0.6886501 ],
       [0.47995785, 0.5200422 ],
       [0.5823317 , 0.41766834],
       [0.1979877 , 0.8020123 ],
       [0.42627287, 0.57372713],
       [0.6199707 , 0.38002932],
       [0.24453898, 0.755461  ],
       [0.31230035, 0.6876996 ],
       [0.3407332 , 0.65926677],
       [0.33183002, 0.66817   ],
       [0.6402096 , 0.3597904 ],
       [0.27295148, 0.7270485 ],
       [0.36300975, 0.6369902 ],
       [0.33555153, 0.66444844],
       [0.5762244 , 0.42377564],
       [0.51039547, 0.48960456],
       [0.5192211 , 0.48077884],
       [0.72059435, 0.27940568],
       [0.6964563 , 0.30354366],
       [0.44113028, 0.5588697 ],
       [0.4867001 , 0.51329994],
       [0.31937218, 0.6806278 ],
       [0.37808475, 0.6219152 ],
       [0.40444702, 0.595553  ],
       [0.0488079 , 0.95119214],
       [0.40321523, 0.5967847 ],
       [0.272642  , 0.72735804],
       [0.32086807, 0.6791319 ],
       [0.17536823, 0.82463175],
       [0.58978885, 0.4102112 ],
       [0.6176899 , 0.38231012],
       [0.2914872 , 0.70851284]], dtype=float32), array([[0.52597165, 0.4740284 ],
       [0.4506134 , 0.54938656],
       [0.4263498 , 0.5736502 ],
       [0.47026357, 0.5297364 ],
       [0.37009287, 0.62990713],
       [0.5111935 , 0.48880646],
       [0.51298803, 0.48701194],
       [0.52645165, 0.47354832],
       [0.47769335, 0.5223066 ],
       [0.53333527, 0.46666473],
       [0.4864359 , 0.5135641 ],
       [0.53368205, 0.46631795],
       [0.43138957, 0.5686105 ],
       [0.5786386 , 0.4213614 ],
       [0.36942527, 0.63057476],
       [0.32840174, 0.67159826],
       [0.33742973, 0.6625703 ],
       [0.49621615, 0.5037839 ],
       [0.34859833, 0.65140164],
       [0.47249594, 0.5275041 ],
       [0.4168511 , 0.58314884],
       [0.6070685 , 0.39293158],
       [0.53147703, 0.46852297],
       [0.46537063, 0.53462934],
       [0.3369687 , 0.6630313 ],
       [0.4886492 , 0.5113508 ],
       [0.4758375 , 0.52416253],
       [0.4750207 , 0.5249793 ],
       [0.24396084, 0.75603914],
       [0.4952413 , 0.5047587 ],
       [0.46619245, 0.5338076 ],
       [0.3764747 , 0.6235253 ],
       [0.47742212, 0.5225779 ],
       [0.17826054, 0.82173944],
       [0.48025635, 0.5197437 ],
       [0.6525688 , 0.34743115],
       [0.2588509 , 0.7411491 ],
       [0.5926461 , 0.4073539 ],
       [0.44450098, 0.5554991 ],
       [0.41339898, 0.586601  ],
       [0.5589249 , 0.44107503],
       [0.31304538, 0.6869547 ],
       [0.45737883, 0.54262114],
       [0.41714117, 0.5828588 ],
       [0.40712023, 0.5928798 ],
       [0.33441728, 0.6655827 ],
       [0.43353665, 0.5664633 ],
       [0.7447269 , 0.25527313],
       [0.621356  , 0.37864396],
       [0.3783319 , 0.6216681 ],
       [0.44781   , 0.55219   ],
       [0.48301542, 0.51698464],
       [0.53647965, 0.4635204 ],
       [0.27235043, 0.7276495 ],
       [0.31286678, 0.68713325],
       [0.25652176, 0.74347824],
       [0.33214453, 0.6678555 ],
       [0.43208724, 0.5679127 ],
       [0.40833968, 0.5916603 ],
       [0.38505238, 0.6149477 ],
       [0.5692118 , 0.43078822],
       [0.46600804, 0.533992  ],
       [0.5497936 , 0.45020643],
       [0.45020807, 0.54979193],
       [0.42796022, 0.5720398 ],
       [0.3888206 , 0.61117935],
       [0.4846533 , 0.5153467 ],
       [0.5308398 , 0.46916017],
       [0.51557183, 0.4844282 ],
       [0.50774723, 0.49225274],
       [0.3074222 , 0.69257784],
       [0.5296552 , 0.47034484],
       [0.43636426, 0.56363577],
       [0.53991276, 0.4600873 ],
       [0.31721616, 0.68278384],
       [0.46047273, 0.5395273 ],
       [0.47490063, 0.5250994 ],
       [0.52824277, 0.47175723],
       [0.457207  , 0.542793  ],
       [0.4416229 , 0.5583771 ],
       [0.40086687, 0.59913313],
       [0.3833606 , 0.6166394 ],
       [0.43719593, 0.56280404],
       [0.45983097, 0.54016906],
       [0.55739146, 0.44260848],
       [0.40432483, 0.59567523],
       [0.48863846, 0.51136154],
       [0.53886354, 0.46113646],
       [0.3881491 , 0.6118509 ],
       [0.46499103, 0.53500897],
       [0.45677465, 0.5432254 ],
       [0.3161443 , 0.6838557 ],
       [0.30122072, 0.6987793 ],
       [0.23831676, 0.7616832 ],
       [0.48290133, 0.51709867],
       [0.48805034, 0.51194966],
       [0.42187557, 0.5781244 ],
       [0.34418422, 0.6558158 ],
       [0.50155234, 0.49844766],
       [0.4476942 , 0.55230576],
       [0.5141988 , 0.48580122],
       [0.5105721 , 0.48942798],
       [0.5035898 , 0.4964102 ],
       [0.44836116, 0.5516388 ],
       [0.34592777, 0.65407217],
       [0.46922907, 0.53077096],
       [0.46933192, 0.5306681 ],
       [0.324931  , 0.67506903],
       [0.46530113, 0.5346989 ],
       [0.47291952, 0.5270805 ],
       [0.46135852, 0.5386415 ],
       [0.4754733 , 0.5245267 ],
       [0.35891157, 0.6410885 ],
       [0.39086562, 0.6091344 ],
       [0.30816442, 0.6918356 ],
       [0.49687412, 0.50312585],
       [0.5200912 , 0.47990882],
       [0.4875519 , 0.51244813],
       [0.5540007 , 0.44599935],
       [0.54027206, 0.4597279 ],
       [0.5699338 , 0.43006626],
       [0.5900158 , 0.40998417],
       [0.5970119 , 0.40298808],
       [0.39661238, 0.60338765],
       [0.49449727, 0.5055027 ],
       [0.3779686 , 0.6220314 ],
       [0.50124097, 0.49875903],
       [0.22544992, 0.7745501 ],
       [0.47127435, 0.5287257 ],
       [0.28352153, 0.71647847],
       [0.64423805, 0.355762  ],
       [0.39413768, 0.6058624 ],
       [0.54282564, 0.45717433],
       [0.46165714, 0.53834283],
       [0.5180905 , 0.48190957],
       [0.39704147, 0.6029585 ],
       [0.43237814, 0.5676218 ],
       [0.31423116, 0.6857688 ],
       [0.5368261 , 0.46317393],
       [0.480811  , 0.519189  ],
       [0.23129821, 0.76870185],
       [0.48142907, 0.51857096],
       [0.46461168, 0.53538835],
       [0.34829763, 0.6517024 ],
       [0.3921143 , 0.6078857 ],
       [0.19087231, 0.8091277 ],
       [0.48862153, 0.51137847],
       [0.4947204 , 0.5052796 ],
       [0.420459  , 0.579541  ],
       [0.4614616 , 0.5385384 ],
       [0.47786373, 0.5221363 ],
       [0.41827264, 0.5817273 ],
       [0.5474173 , 0.4525827 ],
       [0.32548684, 0.6745131 ],
       [0.56679237, 0.43320763],
       [0.40780404, 0.5921959 ],
       [0.38193023, 0.6180697 ],
       [0.4480284 , 0.5519716 ],
       [0.5013325 , 0.49866748],
       [0.3713635 , 0.62863654],
       [0.39577278, 0.60422724],
       [0.33836445, 0.6616356 ],
       [0.45375347, 0.5462465 ],
       [0.38420883, 0.6157912 ],
       [0.45425016, 0.54574984],
       [0.4919748 , 0.50802517],
       [0.45068237, 0.54931766],
       [0.41700992, 0.58299005],
       [0.55554634, 0.44445363],
       [0.46723658, 0.5327634 ],
       [0.5878943 , 0.41210565],
       [0.568926  , 0.431074  ],
       [0.39234364, 0.6076563 ],
       [0.6473819 , 0.35261813],
       [0.38869643, 0.61130357],
       [0.26793516, 0.73206484],
       [0.45688847, 0.5431115 ],
       [0.38008037, 0.61991966],
       [0.16848788, 0.83151215],
       [0.4043579 , 0.59564215],
       [0.45271173, 0.54728824],
       [0.581272  , 0.41872802],
       [0.44324502, 0.55675495],
       [0.4609002 , 0.5390998 ],
       [0.52742726, 0.4725727 ],
       [0.4418219 , 0.5581781 ],
       [0.46416882, 0.53583115],
       [0.6136855 , 0.38631454],
       [0.32162246, 0.6783775 ],
       [0.47635484, 0.52364516],
       [0.32252866, 0.6774713 ],
       [0.36855057, 0.6314494 ],
       [0.44381416, 0.55618584],
       [0.50875145, 0.49124855],
       [0.34645924, 0.65354073],
       [0.46846077, 0.53153926],
       [0.48328805, 0.5167119 ],
       [0.41081214, 0.58918786],
       [0.44207048, 0.5579295 ],
       [0.46394467, 0.5360553 ],
       [0.50997555, 0.4900245 ],
       [0.67547196, 0.324528  ],
       [0.4019411 , 0.5980589 ],
       [0.35342014, 0.64657986],
       [0.36070785, 0.6392922 ],
       [0.53396183, 0.46603817],
       [0.4045713 , 0.5954287 ],
       [0.5447713 , 0.45522863],
       [0.57562697, 0.4243731 ],
       [0.44885394, 0.55114603],
       [0.55871075, 0.44128925],
       [0.52741945, 0.47258055],
       [0.51188314, 0.48811683],
       [0.40966976, 0.59033024],
       [0.49136174, 0.5086382 ],
       [0.400206  , 0.599794  ],
       [0.45711517, 0.5428848 ],
       [0.4116866 , 0.5883134 ],
       [0.35508472, 0.6449153 ],
       [0.28726262, 0.7127374 ],
       [0.6569646 , 0.34303543],
       [0.5542973 , 0.44570273],
       [0.30934086, 0.69065917]], dtype=float32), array([[0.4975281 , 0.5024719 ],
       [0.34214813, 0.65785193],
       [0.28249127, 0.7175087 ],
       [0.41587338, 0.58412665],
       [0.5120874 , 0.48791265],
       [0.53104264, 0.46895736],
       [0.4229184 , 0.57708156],
       [0.43053263, 0.5694673 ],
       [0.33686534, 0.66313463],
       [0.31016585, 0.6898341 ],
       [0.42058054, 0.5794195 ],
       [0.5048994 , 0.49510068],
       [0.458254  , 0.541746  ],
       [0.37955108, 0.6204489 ],
       [0.3592693 , 0.64073074],
       [0.5614177 , 0.4385823 ],
       [0.36434767, 0.63565236],
       [0.4204569 , 0.5795431 ],
       [0.5290315 , 0.4709685 ],
       [0.3206646 , 0.67933536],
       [0.44819656, 0.55180347],
       [0.5785793 , 0.4214207 ],
       [0.38130638, 0.6186936 ],
       [0.3441323 , 0.6558677 ],
       [0.35239023, 0.6476098 ],
       [0.44873622, 0.55126375],
       [0.41267183, 0.58732814],
       [0.39848185, 0.6015181 ],
       [0.34841406, 0.6515859 ],
       [0.39112338, 0.60887665],
       [0.43313992, 0.56686   ],
       [0.27085564, 0.7291444 ],
       [0.36473688, 0.6352631 ],
       [0.24493083, 0.7550692 ],
       [0.45285776, 0.5471422 ],
       [0.5904121 , 0.4095879 ],
       [0.26589707, 0.73410296],
       [0.56585264, 0.43414733],
       [0.41343364, 0.5865663 ],
       [0.4141425 , 0.5858575 ],
       [0.5256632 , 0.47433677],
       [0.48870754, 0.5112924 ],
       [0.46531394, 0.534686  ],
       [0.3570161 , 0.6429839 ],
       [0.39626345, 0.6037366 ],
       [0.58884317, 0.4111568 ],
       [0.3013811 , 0.6986189 ],
       [0.6789129 , 0.32108712],
       [0.43470687, 0.5652931 ],
       [0.40954593, 0.59045404],
       [0.44818994, 0.5518101 ],
       [0.33682126, 0.6631788 ],
       [0.50879055, 0.49120948],
       [0.24984632, 0.75015366],
       [0.4366075 , 0.5633925 ],
       [0.31752753, 0.6824725 ],
       [0.294134  , 0.70586604],
       [0.31967512, 0.68032485],
       [0.34528464, 0.6547154 ],
       [0.43219772, 0.5678023 ],
       [0.47712874, 0.52287126],
       [0.44606677, 0.55393326],
       [0.40324596, 0.596754  ],
       [0.39689013, 0.60310984],
       [0.40893677, 0.5910632 ],
       [0.50353855, 0.49646148],
       [0.52523136, 0.47476864],
       [0.39706182, 0.6029382 ],
       [0.51259685, 0.48740318],
       [0.41680107, 0.58319896],
       [0.2802717 , 0.7197283 ],
       [0.4062656 , 0.5937344 ],
       [0.40873578, 0.5912642 ],
       [0.46352583, 0.53647417],
       [0.347679  , 0.652321  ],
       [0.37072006, 0.6292799 ],
       [0.48274773, 0.51725227],
       [0.5499267 , 0.45007327],
       [0.3853394 , 0.61466056],
       [0.40091908, 0.5990809 ],
       [0.32207415, 0.6779258 ],
       [0.31473383, 0.68526614],
       [0.47890627, 0.52109367],
       [0.3614312 , 0.63856876],
       [0.5681323 , 0.43186778],
       [0.498384  , 0.501616  ],
       [0.3355692 , 0.66443086],
       [0.5033479 , 0.4966522 ],
       [0.37374055, 0.6262595 ],
       [0.32604465, 0.6739554 ],
       [0.36424738, 0.6357526 ],
       [0.35924536, 0.6407546 ],
       [0.44182354, 0.55817646],
       [0.35080224, 0.6491978 ],
       [0.34607497, 0.65392506],
       [0.49537146, 0.50462854],
       [0.39543146, 0.60456854],
       [0.4179339 , 0.58206606],
       [0.3296705 , 0.6703295 ],
       [0.4242175 , 0.57578254],
       [0.51610154, 0.4838985 ],
       [0.5582656 , 0.44173437],
       [0.54135954, 0.4586404 ],
       [0.4209352 , 0.5790648 ],
       [0.3343211 , 0.66567886],
       [0.382386  , 0.617614  ],
       [0.3637983 , 0.6362017 ],
       [0.28364903, 0.716351  ],
       [0.27990577, 0.7200942 ],
       [0.46484134, 0.5351587 ],
       [0.41827175, 0.5817282 ],
       [0.5004368 , 0.49956322],
       [0.3582526 , 0.64174736],
       [0.41380122, 0.5861988 ],
       [0.3524277 , 0.64757234],
       [0.35520962, 0.64479035],
       [0.40859103, 0.5914089 ],
       [0.46136323, 0.53863674],
       [0.37055278, 0.6294472 ],
       [0.3654578 , 0.6345422 ],
       [0.36021692, 0.6397831 ],
       [0.6028268 , 0.39717326],
       [0.57443887, 0.42556116],
       [0.24091408, 0.7590859 ],
       [0.28974935, 0.7102507 ],
       [0.36325702, 0.63674295],
       [0.47275195, 0.5272481 ],
       [0.24526396, 0.754736  ],
       [0.3955491 , 0.6044509 ],
       [0.42773935, 0.5722606 ],
       [0.540481  , 0.45951903],
       [0.30057293, 0.69942707],
       [0.34634456, 0.6536554 ],
       [0.34218547, 0.65781456],
       [0.42000172, 0.5799983 ],
       [0.40826982, 0.5917302 ],
       [0.36700016, 0.63299984],
       [0.40340757, 0.5965924 ],
       [0.5896227 , 0.41037732],
       [0.45167226, 0.54832774],
       [0.3121052 , 0.68789476],
       [0.4600169 , 0.5399831 ],
       [0.4062419 , 0.5937581 ],
       [0.26234806, 0.737652  ],
       [0.34580222, 0.65419775],
       [0.3493038 , 0.65069616],
       [0.47270155, 0.52729845],
       [0.5549685 , 0.44503158],
       [0.3765618 , 0.62343824],
       [0.40306148, 0.5969385 ],
       [0.3973105 , 0.60268956],
       [0.43234473, 0.5676553 ],
       [0.5381096 , 0.4618904 ],
       [0.24315053, 0.75684947],
       [0.45393524, 0.54606473],
       [0.32664132, 0.6733587 ],
       [0.32408518, 0.6759148 ],
       [0.36895686, 0.63104314],
       [0.34967673, 0.6503233 ],
       [0.31740478, 0.68259525],
       [0.48022828, 0.51977175],
       [0.30369127, 0.69630873],
       [0.42274854, 0.5772515 ],
       [0.340029  , 0.65997106],
       [0.3686598 , 0.63134015],
       [0.3866377 , 0.6133623 ],
       [0.48842686, 0.5115732 ],
       [0.4483272 , 0.5516728 ],
       [0.46348786, 0.5365121 ],
       [0.42265704, 0.5773429 ],
       [0.55735785, 0.44264215],
       [0.47513348, 0.5248665 ],
       [0.46431658, 0.5356834 ],
       [0.60109603, 0.3989039 ],
       [0.46764135, 0.53235865],
       [0.28522494, 0.7147751 ],
       [0.489993  , 0.51000696],
       [0.38008285, 0.6199172 ],
       [0.20709898, 0.792901  ],
       [0.33688408, 0.66311586],
       [0.45358992, 0.5464101 ],
       [0.4644347 , 0.53556526],
       [0.3584243 , 0.64157563],
       [0.34712592, 0.65287405],
       [0.45207855, 0.5479214 ],
       [0.3929659 , 0.6070341 ],
       [0.3601035 , 0.6398965 ],
       [0.48165905, 0.51834095],
       [0.38256985, 0.6174302 ],
       [0.59079623, 0.40920374],
       [0.4821034 , 0.51789665],
       [0.44372234, 0.55627763],
       [0.43080953, 0.5691905 ],
       [0.54561484, 0.45438513],
       [0.334602  , 0.66539806],
       [0.35077593, 0.64922404],
       [0.44644335, 0.5535567 ],
       [0.3281561 , 0.6718439 ],
       [0.53250396, 0.46749604],
       [0.4166163 , 0.5833837 ],
       [0.4601591 , 0.53984094],
       [0.649846  , 0.35015398],
       [0.36532223, 0.6346777 ],
       [0.33534148, 0.6646585 ],
       [0.38143194, 0.61856806],
       [0.44699773, 0.55300224],
       [0.36046067, 0.6395393 ],
       [0.6480288 , 0.35197118],
       [0.4563894 , 0.5436106 ],
       [0.5088077 , 0.4911923 ],
       [0.45247877, 0.54752123],
       [0.3474307 , 0.65256923],
       [0.47270963, 0.5272904 ],
       [0.47560006, 0.5243999 ],
       [0.43768498, 0.562315  ],
       [0.35918456, 0.64081544],
       [0.39405477, 0.6059452 ],
       [0.44974992, 0.55025005],
       [0.31695595, 0.683044  ],
       [0.3776153 , 0.6223847 ],
       [0.500257  , 0.499743  ],
       [0.6104312 , 0.38956875],
       [0.41657922, 0.58342075]], dtype=float32)]
i = 3, Test true class= 
[1 0 1 0 1 0 1 0 1 1 1 0 0 0 0 1 0 0 0 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1
 1 0 0 0 1 0 0 1 0 0 0 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 0 0 0 1 0 1 0 1 0 0
 1 1 1 1 1 1 0 0 0 0 0 0 1 0 1 1 0 1 1 1 0 1 0 1 0 0 0 1 0 0 1 0 1 0 0 1 0
 1 1 1 0 1 0 0 0 0 1 1 1 0 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 0
 0 1 0 1 0 1 1 0 0 0 1 1 0 0 1 1 0 1 1 1 1 0 1 1 1 0 1 1 1 0 0 1 0 1 1 0 0
 1 0 0 0 0 1 1 0 0 0 1 0 1 0 0 0 0 1 1 0 0 0 1 1 0 0 0 1 0 1 0 0 1 1 0 1 1
 0]
In grad_steps = 0, loss = 0.7334465384483337
In grad_steps = 1, loss = 0.8201635479927063
In grad_steps = 2, loss = 0.7736347913742065
In grad_steps = 3, loss = 0.7709853649139404
In grad_steps = 4, loss = 0.7171897292137146
In grad_steps = 5, loss = 0.6910615563392639
In grad_steps = 6, loss = 0.678469717502594
In grad_steps = 7, loss = 0.7704929709434509
In grad_steps = 8, loss = 0.6512031555175781
In grad_steps = 9, loss = 0.6731853485107422
In grad_steps = 10, loss = 0.7233402729034424
In grad_steps = 11, loss = 0.7497837543487549
In grad_steps = 12, loss = 0.7266000509262085
In grad_steps = 13, loss = 0.7454215288162231
In grad_steps = 14, loss = 0.7214365601539612
In grad_steps = 15, loss = 0.702437698841095
In grad_steps = 16, loss = 0.686454713344574
In grad_steps = 17, loss = 0.6999761462211609
In grad_steps = 18, loss = 0.7461642026901245
In grad_steps = 19, loss = 0.7417165040969849
In grad_steps = 20, loss = 0.696223795413971
In grad_steps = 21, loss = 0.7142642736434937
In grad_steps = 22, loss = 0.7059091329574585
In grad_steps = 23, loss = 0.746502697467804
In grad_steps = 24, loss = 0.6322128176689148
In grad_steps = 25, loss = 0.6465051174163818
In grad_steps = 26, loss = 0.7002791166305542
In grad_steps = 27, loss = 0.8584343791007996
In grad_steps = 28, loss = 0.6918361186981201
In grad_steps = 29, loss = 0.7602481245994568
In grad_steps = 30, loss = 0.7127359509468079
In grad_steps = 31, loss = 0.6889621615409851
In grad_steps = 32, loss = 0.6986287832260132
In grad_steps = 33, loss = 0.6956238150596619
In grad_steps = 34, loss = 0.7702105045318604
In grad_steps = 35, loss = 0.7209357023239136
In grad_steps = 36, loss = 0.6948632001876831
In grad_steps = 37, loss = 0.6991158723831177
In grad_steps = 38, loss = 0.6908749938011169
In grad_steps = 39, loss = 0.6484138369560242
In grad_steps = 40, loss = 0.6723084449768066
In grad_steps = 41, loss = 0.7496356964111328
In grad_steps = 42, loss = 0.6932699084281921
In grad_steps = 43, loss = 0.7298697829246521
In grad_steps = 44, loss = 0.6810144186019897
In grad_steps = 45, loss = 0.7001663446426392
In grad_steps = 46, loss = 0.7335436344146729
In grad_steps = 47, loss = 0.6846145391464233
In grad_steps = 48, loss = 0.6877835988998413
In grad_steps = 49, loss = 0.7273701429367065
In grad_steps = 50, loss = 0.7604513764381409
In grad_steps = 51, loss = 0.6701316237449646
In grad_steps = 52, loss = 0.6528775691986084
In grad_steps = 53, loss = 0.7394646406173706
In grad_steps = 54, loss = 0.7039928436279297
In grad_steps = 55, loss = 0.6454856991767883
In grad_steps = 56, loss = 0.6955617666244507
In grad_steps = 57, loss = 0.727418065071106
In grad_steps = 58, loss = 0.6836803555488586
In grad_steps = 59, loss = 0.7142904996871948
In grad_steps = 60, loss = 0.7238702774047852
In grad_steps = 61, loss = 0.7044693231582642
In grad_steps = 62, loss = 0.7253543138504028
In grad_steps = 63, loss = 0.7004666328430176
In grad_steps = 64, loss = 0.7246359586715698
In grad_steps = 65, loss = 0.6971902251243591
In grad_steps = 66, loss = 0.6847870945930481
In grad_steps = 67, loss = 0.7115659117698669
In grad_steps = 68, loss = 0.6933239698410034
In grad_steps = 69, loss = 0.7127261757850647
In grad_steps = 70, loss = 0.7406740784645081
In grad_steps = 71, loss = 0.6761715412139893
In grad_steps = 72, loss = 0.6938621997833252
In grad_steps = 73, loss = 0.6966078877449036
In grad_steps = 74, loss = 0.7028992772102356
In grad_steps = 75, loss = 0.6778035163879395
In grad_steps = 76, loss = 0.7006696462631226
In grad_steps = 77, loss = 0.6642245650291443
In grad_steps = 78, loss = 0.7354103326797485
In grad_steps = 79, loss = 0.7440147399902344
In grad_steps = 80, loss = 0.694316565990448
In grad_steps = 81, loss = 0.6963733434677124
In grad_steps = 82, loss = 0.6975532174110413
In grad_steps = 83, loss = 0.6894384026527405
In grad_steps = 84, loss = 0.6925522089004517
In grad_steps = 85, loss = 0.6993637681007385
In grad_steps = 86, loss = 0.6626205444335938
In grad_steps = 87, loss = 0.694850742816925
In grad_steps = 88, loss = 0.7271741032600403
In grad_steps = 89, loss = 0.6689421534538269
In grad_steps = 90, loss = 0.6697829365730286
In grad_steps = 91, loss = 0.7526113986968994
In grad_steps = 92, loss = 0.6588072776794434
In grad_steps = 93, loss = 0.639900803565979
In grad_steps = 94, loss = 0.7593164443969727
In grad_steps = 95, loss = 0.7637362480163574
In grad_steps = 96, loss = 0.737264096736908
In grad_steps = 97, loss = 0.7005234360694885
In grad_steps = 98, loss = 0.6891255378723145
In grad_steps = 99, loss = 0.6951538920402527
In grad_steps = 100, loss = 0.6845164895057678
In grad_steps = 101, loss = 0.7011768817901611
In grad_steps = 102, loss = 0.6873413324356079
In grad_steps = 103, loss = 0.6969848275184631
In grad_steps = 104, loss = 0.6815658211708069
In grad_steps = 105, loss = 0.6682606339454651
In grad_steps = 106, loss = 0.7673782706260681
In grad_steps = 107, loss = 0.6992635130882263
In grad_steps = 108, loss = 0.670318067073822
In grad_steps = 109, loss = 0.6766321063041687
In grad_steps = 110, loss = 0.7424011826515198
In grad_steps = 111, loss = 0.7583008408546448
In grad_steps = 112, loss = 0.6888718008995056
In grad_steps = 113, loss = 0.6941213011741638
In grad_steps = 114, loss = 0.7097141146659851
In grad_steps = 115, loss = 0.6399670243263245
In grad_steps = 116, loss = 0.7375588417053223
In grad_steps = 117, loss = 0.6768160462379456
In grad_steps = 118, loss = 0.6280472278594971
In grad_steps = 119, loss = 0.8060805797576904
In grad_steps = 120, loss = 0.6399115324020386
In grad_steps = 121, loss = 0.6579129695892334
In grad_steps = 122, loss = 0.7032947540283203
In grad_steps = 123, loss = 0.7093794345855713
In grad_steps = 124, loss = 0.6990785598754883
In grad_steps = 125, loss = 0.6753729581832886
In grad_steps = 126, loss = 0.7340049743652344
In grad_steps = 127, loss = 0.6532418131828308
In grad_steps = 128, loss = 0.702627420425415
In grad_steps = 129, loss = 0.6886086463928223
In grad_steps = 130, loss = 0.7309064269065857
In grad_steps = 131, loss = 0.716325581073761
In grad_steps = 132, loss = 0.6684747934341431
In grad_steps = 133, loss = 0.6538909673690796
In grad_steps = 134, loss = 0.7047683000564575
In grad_steps = 135, loss = 0.7441266179084778
In grad_steps = 136, loss = 0.608691930770874
In grad_steps = 137, loss = 0.6299444437026978
In grad_steps = 138, loss = 0.662102997303009
In grad_steps = 139, loss = 0.8198487162590027
In grad_steps = 140, loss = 0.6974450945854187
In grad_steps = 141, loss = 0.7763599157333374
In grad_steps = 142, loss = 0.7307868599891663
In grad_steps = 143, loss = 0.6823517084121704
In grad_steps = 144, loss = 0.6827812790870667
In grad_steps = 145, loss = 0.670635998249054
In grad_steps = 146, loss = 0.7117112874984741
In grad_steps = 147, loss = 0.7042368650436401
In grad_steps = 148, loss = 0.6921343803405762
In grad_steps = 149, loss = 0.6877462267875671
In grad_steps = 150, loss = 0.6770579814910889
In grad_steps = 151, loss = 0.6693105697631836
In grad_steps = 152, loss = 0.6584543585777283
In grad_steps = 153, loss = 0.7029111981391907
In grad_steps = 154, loss = 0.6734576225280762
In grad_steps = 155, loss = 0.6915180087089539
In grad_steps = 156, loss = 0.665765106678009
In grad_steps = 157, loss = 0.6950638890266418
In grad_steps = 158, loss = 0.7099647521972656
In grad_steps = 159, loss = 0.680316686630249
In grad_steps = 160, loss = 0.6512408256530762
In grad_steps = 161, loss = 0.7000742554664612
In grad_steps = 162, loss = 0.7360166311264038
In grad_steps = 163, loss = 0.6377014517784119
In grad_steps = 164, loss = 0.6156809329986572
In grad_steps = 165, loss = 0.7456793785095215
In grad_steps = 166, loss = 0.6676498055458069
In grad_steps = 167, loss = 0.608828067779541
In grad_steps = 168, loss = 0.7034980654716492
In grad_steps = 169, loss = 0.7301720380783081
In grad_steps = 170, loss = 0.6085710525512695
In grad_steps = 171, loss = 0.7457865476608276
In grad_steps = 172, loss = 0.7695031762123108
In grad_steps = 173, loss = 0.7528218626976013
In grad_steps = 174, loss = 0.7073811292648315
In grad_steps = 175, loss = 0.6846272349357605
In grad_steps = 176, loss = 0.7515902519226074
In grad_steps = 177, loss = 0.6987842321395874
In grad_steps = 178, loss = 0.650999128818512
In grad_steps = 179, loss = 0.7255955338478088
In grad_steps = 180, loss = 0.7022801637649536
In grad_steps = 181, loss = 0.7129247784614563
In grad_steps = 182, loss = 0.7051855325698853
In grad_steps = 183, loss = 0.662695050239563
In grad_steps = 184, loss = 0.6877752542495728
In grad_steps = 185, loss = 0.6859930157661438
In grad_steps = 186, loss = 0.7082086801528931
In grad_steps = 187, loss = 0.6680986285209656
In grad_steps = 188, loss = 0.7006263732910156
In grad_steps = 189, loss = 0.6575968265533447
In grad_steps = 190, loss = 0.7363713979721069
In grad_steps = 191, loss = 0.739382803440094
In grad_steps = 192, loss = 0.6937207579612732
In grad_steps = 193, loss = 0.6923534274101257
In grad_steps = 194, loss = 0.6930121779441833
In grad_steps = 195, loss = 0.6830417513847351
In grad_steps = 196, loss = 0.6850805878639221
In grad_steps = 197, loss = 0.7043234705924988
In grad_steps = 198, loss = 0.6544532179832458
In grad_steps = 199, loss = 0.6924980282783508
In grad_steps = 200, loss = 0.7232301235198975
In grad_steps = 201, loss = 0.6481468081474304
In grad_steps = 202, loss = 0.6570277214050293
In grad_steps = 203, loss = 0.7363206148147583
In grad_steps = 204, loss = 0.6545166373252869
In grad_steps = 205, loss = 0.6306555271148682
In grad_steps = 206, loss = 0.7448383569717407
In grad_steps = 207, loss = 0.7487142086029053
In grad_steps = 208, loss = 0.7177920937538147
In grad_steps = 209, loss = 0.6908873319625854
In grad_steps = 210, loss = 0.6890341639518738
In grad_steps = 211, loss = 0.7121465802192688
In grad_steps = 212, loss = 0.6783773303031921
In grad_steps = 213, loss = 0.7034388780593872
In grad_steps = 214, loss = 0.6845930218696594
In grad_steps = 215, loss = 0.6841689348220825
In grad_steps = 216, loss = 0.685539186000824
In grad_steps = 217, loss = 0.6749468445777893
In grad_steps = 218, loss = 0.7679004669189453
In grad_steps = 219, loss = 0.6953880786895752
In grad_steps = 220, loss = 0.6674851775169373
In grad_steps = 221, loss = 0.6648637056350708
In grad_steps = 222, loss = 0.7370291352272034
In grad_steps = 223, loss = 0.7061650156974792
In grad_steps = 224, loss = 0.6881741285324097
In grad_steps = 225, loss = 0.6856366991996765
In grad_steps = 226, loss = 0.6803540587425232
In grad_steps = 227, loss = 0.6410259008407593
In grad_steps = 228, loss = 0.7248438596725464
In grad_steps = 229, loss = 0.6601375937461853
In grad_steps = 230, loss = 0.6336675882339478
In grad_steps = 231, loss = 0.7743294835090637
In grad_steps = 232, loss = 0.637022852897644
In grad_steps = 233, loss = 0.6351608037948608
In grad_steps = 234, loss = 0.6982788443565369
In grad_steps = 235, loss = 0.7207954525947571
In grad_steps = 236, loss = 0.6740613579750061
In grad_steps = 237, loss = 0.6605035066604614
In grad_steps = 238, loss = 0.6495813131332397
In grad_steps = 239, loss = 0.6341099739074707
In grad_steps = 240, loss = 0.6819676756858826
In grad_steps = 241, loss = 0.6375629901885986
In grad_steps = 242, loss = 0.7299745082855225
In grad_steps = 243, loss = 0.6554587483406067
In grad_steps = 244, loss = 0.6358673572540283
In grad_steps = 245, loss = 0.6071780323982239
In grad_steps = 246, loss = 0.6972628235816956
In grad_steps = 247, loss = 0.7424160242080688
In grad_steps = 248, loss = 0.5249518156051636
In grad_steps = 249, loss = 0.5775927901268005
In grad_steps = 250, loss = 0.5387697815895081
In grad_steps = 251, loss = 0.669613778591156
In grad_steps = 252, loss = 0.6922504901885986
In grad_steps = 253, loss = 0.6520369648933411
In grad_steps = 254, loss = 0.7641656398773193
In grad_steps = 255, loss = 0.8065690398216248
In grad_steps = 256, loss = 0.6786601543426514
In grad_steps = 257, loss = 0.5995252132415771
In grad_steps = 258, loss = 0.6112152934074402
In grad_steps = 259, loss = 0.6791751384735107
In grad_steps = 260, loss = 0.7716631889343262
In grad_steps = 261, loss = 0.6534236073493958
In grad_steps = 262, loss = 0.7619514465332031
In grad_steps = 263, loss = 0.5554741024971008
In grad_steps = 264, loss = 0.6132760047912598
In grad_steps = 265, loss = 0.7335873246192932
In grad_steps = 266, loss = 0.6557254791259766
In grad_steps = 267, loss = 0.6450350284576416
In grad_steps = 268, loss = 0.6340355277061462
In grad_steps = 269, loss = 0.6714639663696289
In grad_steps = 270, loss = 0.6955257654190063
In grad_steps = 271, loss = 0.6540445685386658
In grad_steps = 272, loss = 0.6138830780982971
In grad_steps = 273, loss = 0.7168782949447632
In grad_steps = 274, loss = 0.6938953995704651
In grad_steps = 275, loss = 0.6220608353614807
In grad_steps = 276, loss = 0.6161923408508301
In grad_steps = 277, loss = 0.6446223855018616
In grad_steps = 278, loss = 0.5307298302650452
In grad_steps = 279, loss = 0.5265816450119019
In grad_steps = 280, loss = 0.586305558681488
In grad_steps = 281, loss = 0.6203517913818359
In grad_steps = 282, loss = 0.4703657329082489
In grad_steps = 283, loss = 0.5676157474517822
In grad_steps = 284, loss = 0.4743165969848633
In grad_steps = 285, loss = 1.222481369972229
In grad_steps = 286, loss = 0.5582365393638611
In grad_steps = 287, loss = 0.6012794375419617
In grad_steps = 288, loss = 0.6940016746520996
In grad_steps = 289, loss = 0.6178553104400635
In grad_steps = 290, loss = 0.7178086638450623
In grad_steps = 291, loss = 0.7977422475814819
In grad_steps = 292, loss = 0.6142694354057312
In grad_steps = 293, loss = 0.6891641020774841
In grad_steps = 294, loss = 0.7091207504272461
In grad_steps = 295, loss = 0.6412235498428345
In grad_steps = 296, loss = 0.6504516005516052
In grad_steps = 297, loss = 0.6674627065658569
In grad_steps = 298, loss = 0.701964259147644
In grad_steps = 299, loss = 0.6755393147468567
In grad_steps = 300, loss = 0.7212512493133545
In grad_steps = 301, loss = 0.6674450039863586
In grad_steps = 302, loss = 0.7066346406936646
In grad_steps = 303, loss = 0.7251076698303223
In grad_steps = 304, loss = 0.6897768974304199
In grad_steps = 305, loss = 0.705311119556427
In grad_steps = 306, loss = 0.6680862903594971
In grad_steps = 307, loss = 0.6322826743125916
In grad_steps = 308, loss = 0.68563312292099
In grad_steps = 309, loss = 0.7153565883636475
In grad_steps = 310, loss = 0.6291691660881042
In grad_steps = 311, loss = 0.6766996383666992
In grad_steps = 312, loss = 0.7046982645988464
In grad_steps = 313, loss = 0.5943465232849121
In grad_steps = 314, loss = 0.6369158625602722
In grad_steps = 315, loss = 0.6919602751731873
In grad_steps = 316, loss = 0.648448646068573
In grad_steps = 317, loss = 0.6082824468612671
In grad_steps = 318, loss = 0.69859379529953
In grad_steps = 319, loss = 0.6591325402259827
In grad_steps = 320, loss = 0.6758624315261841
In grad_steps = 321, loss = 0.6932295560836792
In grad_steps = 322, loss = 0.6705519556999207
In grad_steps = 323, loss = 0.833798885345459
In grad_steps = 324, loss = 0.637761116027832
In grad_steps = 325, loss = 0.6608954668045044
In grad_steps = 326, loss = 0.6101114153862
In grad_steps = 327, loss = 0.6372627019882202
In grad_steps = 328, loss = 0.6693388223648071
In grad_steps = 329, loss = 0.6805856823921204
In grad_steps = 330, loss = 0.9514330625534058
In grad_steps = 331, loss = 0.6922933459281921
In grad_steps = 332, loss = 0.6703544855117798
In grad_steps = 333, loss = 0.6170165538787842
In grad_steps = 334, loss = 0.6753570437431335
In grad_steps = 335, loss = 0.27437278628349304
In grad_steps = 336, loss = 0.6040216088294983
In grad_steps = 337, loss = 0.7003404498100281
In grad_steps = 338, loss = 0.6535322666168213
In grad_steps = 339, loss = 0.5997307300567627
In grad_steps = 340, loss = 0.7898268699645996
In grad_steps = 341, loss = 0.6338016986846924
In grad_steps = 342, loss = 0.6324066519737244
In grad_steps = 343, loss = 0.7346360683441162
In grad_steps = 344, loss = 0.6125999689102173
In grad_steps = 345, loss = 0.5775922536849976
In grad_steps = 346, loss = 0.6284003853797913
In grad_steps = 347, loss = 0.646974503993988
In grad_steps = 348, loss = 0.6686015725135803
In grad_steps = 349, loss = 0.6219476461410522
In grad_steps = 350, loss = 0.5998201370239258
In grad_steps = 351, loss = 0.6284670829772949
In grad_steps = 352, loss = 0.5851602554321289
In grad_steps = 353, loss = 0.601517915725708
In grad_steps = 354, loss = 0.5509755611419678
In grad_steps = 355, loss = 0.4678884446620941
In grad_steps = 356, loss = 0.4620476961135864
In grad_steps = 357, loss = 0.3842979371547699
In grad_steps = 358, loss = 0.4632473289966583
In grad_steps = 359, loss = 0.46583229303359985
In grad_steps = 360, loss = 0.25062352418899536
In grad_steps = 361, loss = 0.31108880043029785
In grad_steps = 362, loss = 0.5999738574028015
In grad_steps = 363, loss = 0.4573310315608978
In grad_steps = 364, loss = 0.4091978669166565
In grad_steps = 365, loss = 0.42237383127212524
In grad_steps = 366, loss = 0.5841562747955322
In grad_steps = 367, loss = 0.5721039175987244
In grad_steps = 368, loss = 0.4014911651611328
In grad_steps = 369, loss = 0.44647958874702454
In grad_steps = 370, loss = 0.295244038105011
In grad_steps = 371, loss = 0.6701605319976807
In grad_steps = 372, loss = 0.8469915390014648
In grad_steps = 373, loss = 0.5370109677314758
In grad_steps = 374, loss = 0.6128832697868347
In grad_steps = 375, loss = 0.4513917565345764
In grad_steps = 376, loss = 0.6185361742973328
In grad_steps = 377, loss = 0.5476882457733154
In grad_steps = 378, loss = 0.5360560417175293
In grad_steps = 379, loss = 0.4672756791114807
In grad_steps = 380, loss = 0.5124000310897827
In grad_steps = 381, loss = 0.6636294722557068
In grad_steps = 382, loss = 0.5375741720199585
In grad_steps = 383, loss = 0.4996117651462555
In grad_steps = 384, loss = 0.4773886501789093
In grad_steps = 385, loss = 0.6780048608779907
In grad_steps = 386, loss = 0.4934791624546051
In grad_steps = 387, loss = 0.4144200384616852
In grad_steps = 388, loss = 0.30066126585006714
In grad_steps = 389, loss = 0.5934081077575684
In grad_steps = 390, loss = 0.3578718304634094
In grad_steps = 391, loss = 0.2886222004890442
In grad_steps = 392, loss = 0.29933005571365356
In grad_steps = 393, loss = 0.27350470423698425
In grad_steps = 394, loss = 0.12934479117393494
In grad_steps = 395, loss = 0.03581327944993973
In grad_steps = 396, loss = 0.031128134578466415
In grad_steps = 397, loss = 0.4039543569087982
In grad_steps = 398, loss = 0.16497504711151123
In grad_steps = 399, loss = 0.18326422572135925
In grad_steps = 400, loss = 0.4688095152378082
In grad_steps = 401, loss = 0.2615370452404022
In grad_steps = 402, loss = 0.2378772646188736
In grad_steps = 403, loss = 1.3165315389633179
In grad_steps = 404, loss = 0.7227322459220886
In grad_steps = 405, loss = 0.5834968686103821
In grad_steps = 406, loss = 0.4755641222000122
In grad_steps = 407, loss = 0.7375325560569763
In grad_steps = 408, loss = 0.6869190335273743
In grad_steps = 409, loss = 0.543074369430542
In grad_steps = 410, loss = 0.6789421439170837
In grad_steps = 411, loss = 0.6893125772476196
In grad_steps = 412, loss = 0.6999600529670715
In grad_steps = 413, loss = 0.6585210561752319
In grad_steps = 414, loss = 0.6115970015525818
In grad_steps = 415, loss = 0.5694617629051208
In grad_steps = 416, loss = 0.7420065402984619
In grad_steps = 417, loss = 0.8343584537506104
In grad_steps = 418, loss = 0.680675745010376
In grad_steps = 419, loss = 0.5720727443695068
In grad_steps = 420, loss = 0.5469640493392944
In grad_steps = 421, loss = 0.5947214365005493
In grad_steps = 422, loss = 0.746218204498291
In grad_steps = 423, loss = 0.6215174794197083
In grad_steps = 424, loss = 0.6741365790367126
In grad_steps = 425, loss = 0.5766940116882324
In grad_steps = 426, loss = 0.5808919668197632
In grad_steps = 427, loss = 0.5827412605285645
In grad_steps = 428, loss = 0.6017367243766785
In grad_steps = 429, loss = 0.5481587052345276
In grad_steps = 430, loss = 0.9301420450210571
In grad_steps = 431, loss = 0.6223819851875305
In grad_steps = 432, loss = 0.6253111362457275
In grad_steps = 433, loss = 0.8024080991744995
In grad_steps = 434, loss = 0.7224347591400146
In grad_steps = 435, loss = 0.860084056854248
In grad_steps = 436, loss = 0.539993166923523
In grad_steps = 437, loss = 0.5681368112564087
In grad_steps = 438, loss = 0.531396746635437
In grad_steps = 439, loss = 0.6190438866615295
In grad_steps = 440, loss = 0.635029673576355
In grad_steps = 441, loss = 0.6351463794708252
In grad_steps = 442, loss = 0.8933366537094116
In grad_steps = 443, loss = 0.6769958734512329
In grad_steps = 444, loss = 0.6612094640731812
In grad_steps = 445, loss = 0.5771754384040833
In grad_steps = 446, loss = 0.5546898245811462
In grad_steps = 447, loss = 0.04876888915896416
i = 4, Test ensemble probabilities = 
[array([[0.63234293, 0.3676571 ],
       [0.15927052, 0.8407295 ],
       [0.4707932 , 0.5292069 ],
       [0.26916105, 0.73083895],
       [0.34443957, 0.65556043],
       [0.63733286, 0.36266717],
       [0.3972575 , 0.6027425 ],
       [0.53274614, 0.4672539 ],
       [0.32415473, 0.67584527],
       [0.5880499 , 0.41195014],
       [0.53142023, 0.4685797 ],
       [0.4613917 , 0.5386083 ],
       [0.39642295, 0.6035771 ],
       [0.2630529 , 0.73694706],
       [0.25113425, 0.7488658 ],
       [0.31638494, 0.683615  ],
       [0.33599755, 0.6640024 ],
       [0.4374039 , 0.56259614],
       [0.29857844, 0.70142156],
       [0.34628177, 0.65371823],
       [0.48291063, 0.51708937],
       [0.7528585 , 0.24714147],
       [0.26550072, 0.7344993 ],
       [0.34369338, 0.6563066 ],
       [0.32360822, 0.6763918 ],
       [0.48903427, 0.5109657 ],
       [0.517874  , 0.48212597],
       [0.30616266, 0.6938373 ],
       [0.37618548, 0.62381446],
       [0.48018897, 0.51981103],
       [0.442965  , 0.557035  ],
       [0.32442498, 0.6755751 ],
       [0.3079656 , 0.69203436],
       [0.19288743, 0.8071125 ],
       [0.5714873 , 0.4285127 ],
       [0.6726957 , 0.3273043 ],
       [0.1593832 , 0.8406168 ],
       [0.673186  , 0.326814  ],
       [0.44626588, 0.5537341 ],
       [0.53769314, 0.46230686],
       [0.40586078, 0.5941392 ],
       [0.71461   , 0.28539005],
       [0.5180719 , 0.4819281 ],
       [0.2545866 , 0.74541336],
       [0.37031677, 0.6296832 ],
       [0.44500518, 0.5549948 ],
       [0.32468525, 0.6753148 ],
       [0.84190464, 0.15809539],
       [0.51651853, 0.48348144],
       [0.33493203, 0.66506803],
       [0.45689556, 0.5431044 ],
       [0.4609658 , 0.5390341 ],
       [0.54432595, 0.45567402],
       [0.26599905, 0.7340009 ],
       [0.04212758, 0.95787245],
       [0.2829374 , 0.71706253],
       [0.2043219 , 0.7956781 ],
       [0.5221622 , 0.4778378 ],
       [0.18717316, 0.8128269 ],
       [0.19525924, 0.8047408 ],
       [0.5766938 , 0.42330623],
       [0.4176103 , 0.58238965],
       [0.5324215 , 0.46757844],
       [0.4488813 , 0.5511187 ],
       [0.44950402, 0.55049604],
       [0.46307266, 0.53692734],
       [0.39134234, 0.6086577 ],
       [0.41254333, 0.58745664],
       [0.47793105, 0.5220689 ],
       [0.56690925, 0.4330908 ],
       [0.23565127, 0.76434875],
       [0.45820194, 0.5417981 ],
       [0.4637113 , 0.5362887 ],
       [0.49555954, 0.5044404 ],
       [0.22995174, 0.77004826],
       [0.6644476 , 0.3355524 ],
       [0.61461085, 0.38538912],
       [0.5472649 , 0.45273513],
       [0.21934402, 0.780656  ],
       [0.06799435, 0.93200564],
       [0.16100384, 0.8389961 ],
       [0.50630325, 0.49369678],
       [0.43458852, 0.56541145],
       [0.385639  , 0.614361  ],
       [0.61727566, 0.38272437],
       [0.25399345, 0.74600655],
       [0.2955888 , 0.70441115],
       [0.5325431 , 0.46745688],
       [0.678447  , 0.321553  ],
       [0.16606222, 0.8339377 ],
       [0.3081697 , 0.6918303 ],
       [0.4355301 , 0.5644699 ],
       [0.34492984, 0.6550701 ],
       [0.25015336, 0.74984664],
       [0.64061916, 0.3593808 ],
       [0.5967088 , 0.4032912 ],
       [0.18353361, 0.81646645],
       [0.52374434, 0.47625569],
       [0.34673527, 0.65326476],
       [0.38547316, 0.61452687],
       [0.5847594 , 0.41524062],
       [0.44700786, 0.5529921 ],
       [0.47356904, 0.52643096],
       [0.422917  , 0.577083  ],
       [0.19558223, 0.8044177 ],
       [0.42946094, 0.57053906],
       [0.37188283, 0.6281172 ],
       [0.17802247, 0.82197756],
       [0.27269948, 0.7273005 ],
       [0.5892211 , 0.4107789 ],
       [0.40044641, 0.5995536 ],
       [0.5170463 , 0.48295373],
       [0.36080304, 0.63919693],
       [0.43622753, 0.5637725 ],
       [0.23612504, 0.763875  ],
       [0.42220357, 0.5777964 ],
       [0.2890543 , 0.71094567],
       [0.40281117, 0.59718883],
       [0.4923154 , 0.5076846 ],
       [0.26968554, 0.73031443],
       [0.20687027, 0.79312974],
       [0.8389131 , 0.16108696],
       [0.63067454, 0.36932552],
       [0.18742153, 0.81257844],
       [0.30327702, 0.69672304],
       [0.3085698 , 0.6914302 ],
       [0.5595747 , 0.44042534],
       [0.24351013, 0.7564899 ],
       [0.21686015, 0.7831398 ],
       [0.41896248, 0.5810375 ],
       [0.56056327, 0.43943676],
       [0.17147662, 0.8285234 ],
       [0.5208721 , 0.47912785],
       [0.4153351 , 0.5846649 ],
       [0.19790003, 0.8021    ],
       [0.25866634, 0.7413336 ],
       [0.29833403, 0.70166594],
       [0.3234097 , 0.67659026],
       [0.7465652 , 0.25343475],
       [0.59630436, 0.4036956 ],
       [0.23582876, 0.76417124],
       [0.45957777, 0.54042226],
       [0.40649194, 0.593508  ],
       [0.17363618, 0.82636386],
       [0.63669163, 0.36330843],
       [0.30761877, 0.69238126],
       [0.51930255, 0.48069745],
       [0.4234089 , 0.5765911 ],
       [0.4946091 , 0.50539094],
       [0.4197197 , 0.58028024],
       [0.41942033, 0.58057964],
       [0.2945898 , 0.7054102 ],
       [0.55483747, 0.44516253],
       [0.22091393, 0.77908605],
       [0.35284546, 0.64715457],
       [0.46206364, 0.53793633],
       [0.16025013, 0.83974993],
       [0.5305458 , 0.4694542 ],
       [0.46310562, 0.5368943 ],
       [0.13942465, 0.8605754 ],
       [0.412065  , 0.58793503],
       [0.25527856, 0.7447215 ],
       [0.46506244, 0.53493756],
       [0.12068249, 0.8793175 ],
       [0.33459774, 0.6654023 ],
       [0.28955707, 0.7104429 ],
       [0.3447704 , 0.65522957],
       [0.09330033, 0.9066997 ],
       [0.42613664, 0.5738633 ],
       [0.52296096, 0.47703904],
       [0.6178167 , 0.3821833 ],
       [0.49262506, 0.50737494],
       [0.39261836, 0.6073817 ],
       [0.8172014 , 0.18279864],
       [0.63021123, 0.36978877],
       [0.33625674, 0.66374326],
       [0.5179674 , 0.48203266],
       [0.07003294, 0.92996705],
       [0.04434535, 0.9556546 ],
       [0.24915382, 0.75084615],
       [0.503802  , 0.496198  ],
       [0.31493956, 0.6850605 ],
       [0.290524  , 0.709476  ],
       [0.22521402, 0.774786  ],
       [0.40556347, 0.5944366 ],
       [0.19308199, 0.80691797],
       [0.38832644, 0.61167353],
       [0.3310196 , 0.66898036],
       [0.398134  , 0.60186595],
       [0.70039856, 0.29960147],
       [0.56304246, 0.43695754],
       [0.3614178 , 0.6385822 ],
       [0.43890896, 0.561091  ],
       [0.7291902 , 0.27080986],
       [0.1708664 , 0.82913357],
       [0.405132  , 0.594868  ],
       [0.779685  , 0.2203149 ],
       [0.2233463 , 0.7766537 ],
       [0.5981941 , 0.40180585],
       [0.4170927 , 0.58290726],
       [0.45992476, 0.54007524],
       [0.7984498 , 0.2015502 ],
       [0.44850412, 0.5514959 ],
       [0.2520903 , 0.74790967],
       [0.39397344, 0.60602653],
       [0.37396133, 0.6260387 ],
       [0.392793  , 0.60720694],
       [0.6720211 , 0.3279789 ],
       [0.5807893 , 0.41921067],
       [0.73216754, 0.2678325 ],
       [0.5034451 , 0.4965549 ],
       [0.41664135, 0.5833587 ],
       [0.5674081 , 0.43259186],
       [0.17715418, 0.8228458 ],
       [0.26898026, 0.73101974],
       [0.21245757, 0.7875424 ],
       [0.26290798, 0.73709196],
       [0.14729999, 0.85270005],
       [0.10998562, 0.89001435],
       [0.09227446, 0.9077256 ],
       [0.64987636, 0.35012358],
       [0.36819777, 0.6318022 ],
       [0.09723709, 0.9027629 ]], dtype=float32), array([[0.4571166 , 0.5428834 ],
       [0.5294107 , 0.4705893 ],
       [0.22323202, 0.77676797],
       [0.51862043, 0.4813795 ],
       [0.39006442, 0.6099356 ],
       [0.55189687, 0.44810316],
       [0.44895777, 0.5510422 ],
       [0.4808965 , 0.5191035 ],
       [0.36935973, 0.63064027],
       [0.33487913, 0.66512084],
       [0.48611328, 0.5138867 ],
       [0.5078283 , 0.4921717 ],
       [0.513526  , 0.486474  ],
       [0.43515244, 0.5648476 ],
       [0.27377552, 0.7262245 ],
       [0.2432999 , 0.7567001 ],
       [0.49993494, 0.5000651 ],
       [0.291736  , 0.708264  ],
       [0.41045925, 0.5895407 ],
       [0.32647273, 0.6735273 ],
       [0.42342755, 0.5765725 ],
       [0.4086655 , 0.5913345 ],
       [0.52046114, 0.47953892],
       [0.3826234 , 0.61737657],
       [0.25063252, 0.7493674 ],
       [0.50633806, 0.49366197],
       [0.36846852, 0.63153154],
       [0.51383686, 0.48616308],
       [0.3087037 , 0.6912963 ],
       [0.55610746, 0.44389254],
       [0.4161952 , 0.5838048 ],
       [0.18752672, 0.8124733 ],
       [0.42430365, 0.5756963 ],
       [0.15909182, 0.84090817],
       [0.46740913, 0.5325908 ],
       [0.65301734, 0.34698266],
       [0.19227678, 0.8077232 ],
       [0.5735935 , 0.4264065 ],
       [0.28589913, 0.7141009 ],
       [0.44559172, 0.55440825],
       [0.54125   , 0.45875004],
       [0.2907995 , 0.7092005 ],
       [0.37092564, 0.62907434],
       [0.4308764 , 0.5691236 ],
       [0.37090874, 0.62909126],
       [0.43027127, 0.56972873],
       [0.27650887, 0.7234912 ],
       [0.6256027 , 0.37439725],
       [0.57003075, 0.4299692 ],
       [0.32834244, 0.67165756],
       [0.4865562 , 0.5134438 ],
       [0.44996402, 0.550036  ],
       [0.47377753, 0.52622247],
       [0.16757809, 0.83242196],
       [0.268217  , 0.73178303],
       [0.33810017, 0.6618998 ],
       [0.26060697, 0.73939306],
       [0.3524904 , 0.64750963],
       [0.38166958, 0.6183304 ],
       [0.2701247 , 0.72987527],
       [0.5141597 , 0.4858403 ],
       [0.4992094 , 0.5007906 ],
       [0.5771801 , 0.42281997],
       [0.3514243 , 0.6485757 ],
       [0.6553351 , 0.34466484],
       [0.46719575, 0.53280425],
       [0.47575495, 0.5242451 ],
       [0.5901849 , 0.4098151 ],
       [0.4846395 , 0.51536053],
       [0.4624119 , 0.5375881 ],
       [0.33387172, 0.6661283 ],
       [0.47095305, 0.5290469 ],
       [0.41752917, 0.5824709 ],
       [0.40379873, 0.5962013 ],
       [0.25252804, 0.747472  ],
       [0.34796432, 0.6520357 ],
       [0.49279785, 0.50720215],
       [0.4465732 , 0.5534268 ],
       [0.2368767 , 0.76312333],
       [0.35196486, 0.6480351 ],
       [0.48743895, 0.512561  ],
       [0.3570835 , 0.64291644],
       [0.34352624, 0.65647376],
       [0.47918805, 0.5208119 ],
       [0.55404097, 0.44595906],
       [0.35746393, 0.6425361 ],
       [0.25150567, 0.7484943 ],
       [0.5243571 , 0.47564292],
       [0.17094587, 0.8290542 ],
       [0.36935994, 0.6306401 ],
       [0.43561053, 0.56438947],
       [0.42300782, 0.5769922 ],
       [0.33751628, 0.6624837 ],
       [0.27065283, 0.7293471 ],
       [0.4002573 , 0.5997427 ],
       [0.553859  , 0.44614094],
       [0.21557094, 0.784429  ],
       [0.35128933, 0.64871067],
       [0.31674996, 0.68325007],
       [0.2832636 , 0.71673644],
       [0.4784139 , 0.5215861 ],
       [0.38211825, 0.6178818 ],
       [0.4683589 , 0.5316411 ],
       [0.3947358 , 0.6052642 ],
       [0.1879898 , 0.8120102 ],
       [0.381092  , 0.618908  ],
       [0.28480652, 0.71519345],
       [0.09613001, 0.90387005],
       [0.36259332, 0.63740665],
       [0.5208426 , 0.4791574 ],
       [0.39169744, 0.6083026 ],
       [0.31034505, 0.689655  ],
       [0.40007278, 0.5999272 ],
       [0.48548177, 0.5145182 ],
       [0.32625067, 0.6737493 ],
       [0.28176755, 0.7182325 ],
       [0.19873911, 0.8012609 ],
       [0.3922998 , 0.6077002 ],
       [0.32452005, 0.6754799 ],
       [0.39452052, 0.6054795 ],
       [0.3498791 , 0.6501209 ],
       [0.5362299 , 0.46377012],
       [0.73773646, 0.2622635 ],
       [0.33494902, 0.665051  ],
       [0.20055218, 0.79944783],
       [0.19752063, 0.8024793 ],
       [0.4704736 , 0.5295264 ],
       [0.19533584, 0.8046642 ],
       [0.1866918 , 0.8133082 ],
       [0.5122611 , 0.48773894],
       [0.6391871 , 0.36081296],
       [0.3035828 , 0.6964172 ],
       [0.28577548, 0.71422446],
       [0.4295801 , 0.5704199 ],
       [0.36431298, 0.63568705],
       [0.39080298, 0.609197  ],
       [0.45286956, 0.5471304 ],
       [0.16296265, 0.8370374 ],
       [0.6365087 , 0.36349133],
       [0.5055456 , 0.49445438],
       [0.12050015, 0.87949985],
       [0.44568032, 0.5543197 ],
       [0.33894923, 0.66105074],
       [0.39209473, 0.60790527],
       [0.3043178 , 0.69568217],
       [0.22776023, 0.77223974],
       [0.5165726 , 0.4834274 ],
       [0.53855336, 0.4614467 ],
       [0.30221108, 0.69778895],
       [0.4841881 , 0.51581186],
       [0.35525265, 0.6447474 ],
       [0.13065955, 0.8693404 ],
       [0.5082202 , 0.49177983],
       [0.22805978, 0.77194023],
       [0.49857068, 0.5014293 ],
       [0.37822402, 0.621776  ],
       [0.18954322, 0.8104568 ],
       [0.45782456, 0.5421755 ],
       [0.343549  , 0.6564509 ],
       [0.23705377, 0.7629462 ],
       [0.53499   , 0.46501   ],
       [0.29976618, 0.7002338 ],
       [0.46745688, 0.5325431 ],
       [0.12526599, 0.874734  ],
       [0.48235503, 0.51764494],
       [0.25172153, 0.7482785 ],
       [0.39188084, 0.6081192 ],
       [0.4759117 , 0.5240883 ],
       [0.5841465 , 0.41585344],
       [0.4407678 , 0.55923223],
       [0.5083613 , 0.49163875],
       [0.34765303, 0.65234697],
       [0.24413344, 0.7558665 ],
       [0.6039984 , 0.39600158],
       [0.19725229, 0.80274767],
       [0.26565233, 0.7343477 ],
       [0.461022  , 0.53897804],
       [0.39643595, 0.603564  ],
       [0.0636213 , 0.93637866],
       [0.3433742 , 0.6566258 ],
       [0.48139504, 0.518605  ],
       [0.47634032, 0.52365965],
       [0.3635606 , 0.6364394 ],
       [0.35425   , 0.64575   ],
       [0.37250814, 0.6274919 ],
       [0.3279293 , 0.67207074],
       [0.46274236, 0.5372576 ],
       [0.08992676, 0.9100732 ],
       [0.24736886, 0.7526312 ],
       [0.4303706 , 0.5696294 ],
       [0.30280727, 0.6971927 ],
       [0.31135   , 0.6886501 ],
       [0.47995785, 0.5200422 ],
       [0.5823317 , 0.41766834],
       [0.1979877 , 0.8020123 ],
       [0.42627287, 0.57372713],
       [0.6199707 , 0.38002932],
       [0.24453898, 0.755461  ],
       [0.31230035, 0.6876996 ],
       [0.3407332 , 0.65926677],
       [0.33183002, 0.66817   ],
       [0.6402096 , 0.3597904 ],
       [0.27295148, 0.7270485 ],
       [0.36300975, 0.6369902 ],
       [0.33555153, 0.66444844],
       [0.5762244 , 0.42377564],
       [0.51039547, 0.48960456],
       [0.5192211 , 0.48077884],
       [0.72059435, 0.27940568],
       [0.6964563 , 0.30354366],
       [0.44113028, 0.5588697 ],
       [0.4867001 , 0.51329994],
       [0.31937218, 0.6806278 ],
       [0.37808475, 0.6219152 ],
       [0.40444702, 0.595553  ],
       [0.0488079 , 0.95119214],
       [0.40321523, 0.5967847 ],
       [0.272642  , 0.72735804],
       [0.32086807, 0.6791319 ],
       [0.17536823, 0.82463175],
       [0.58978885, 0.4102112 ],
       [0.6176899 , 0.38231012],
       [0.2914872 , 0.70851284]], dtype=float32), array([[0.52597165, 0.4740284 ],
       [0.4506134 , 0.54938656],
       [0.4263498 , 0.5736502 ],
       [0.47026357, 0.5297364 ],
       [0.37009287, 0.62990713],
       [0.5111935 , 0.48880646],
       [0.51298803, 0.48701194],
       [0.52645165, 0.47354832],
       [0.47769335, 0.5223066 ],
       [0.53333527, 0.46666473],
       [0.4864359 , 0.5135641 ],
       [0.53368205, 0.46631795],
       [0.43138957, 0.5686105 ],
       [0.5786386 , 0.4213614 ],
       [0.36942527, 0.63057476],
       [0.32840174, 0.67159826],
       [0.33742973, 0.6625703 ],
       [0.49621615, 0.5037839 ],
       [0.34859833, 0.65140164],
       [0.47249594, 0.5275041 ],
       [0.4168511 , 0.58314884],
       [0.6070685 , 0.39293158],
       [0.53147703, 0.46852297],
       [0.46537063, 0.53462934],
       [0.3369687 , 0.6630313 ],
       [0.4886492 , 0.5113508 ],
       [0.4758375 , 0.52416253],
       [0.4750207 , 0.5249793 ],
       [0.24396084, 0.75603914],
       [0.4952413 , 0.5047587 ],
       [0.46619245, 0.5338076 ],
       [0.3764747 , 0.6235253 ],
       [0.47742212, 0.5225779 ],
       [0.17826054, 0.82173944],
       [0.48025635, 0.5197437 ],
       [0.6525688 , 0.34743115],
       [0.2588509 , 0.7411491 ],
       [0.5926461 , 0.4073539 ],
       [0.44450098, 0.5554991 ],
       [0.41339898, 0.586601  ],
       [0.5589249 , 0.44107503],
       [0.31304538, 0.6869547 ],
       [0.45737883, 0.54262114],
       [0.41714117, 0.5828588 ],
       [0.40712023, 0.5928798 ],
       [0.33441728, 0.6655827 ],
       [0.43353665, 0.5664633 ],
       [0.7447269 , 0.25527313],
       [0.621356  , 0.37864396],
       [0.3783319 , 0.6216681 ],
       [0.44781   , 0.55219   ],
       [0.48301542, 0.51698464],
       [0.53647965, 0.4635204 ],
       [0.27235043, 0.7276495 ],
       [0.31286678, 0.68713325],
       [0.25652176, 0.74347824],
       [0.33214453, 0.6678555 ],
       [0.43208724, 0.5679127 ],
       [0.40833968, 0.5916603 ],
       [0.38505238, 0.6149477 ],
       [0.5692118 , 0.43078822],
       [0.46600804, 0.533992  ],
       [0.5497936 , 0.45020643],
       [0.45020807, 0.54979193],
       [0.42796022, 0.5720398 ],
       [0.3888206 , 0.61117935],
       [0.4846533 , 0.5153467 ],
       [0.5308398 , 0.46916017],
       [0.51557183, 0.4844282 ],
       [0.50774723, 0.49225274],
       [0.3074222 , 0.69257784],
       [0.5296552 , 0.47034484],
       [0.43636426, 0.56363577],
       [0.53991276, 0.4600873 ],
       [0.31721616, 0.68278384],
       [0.46047273, 0.5395273 ],
       [0.47490063, 0.5250994 ],
       [0.52824277, 0.47175723],
       [0.457207  , 0.542793  ],
       [0.4416229 , 0.5583771 ],
       [0.40086687, 0.59913313],
       [0.3833606 , 0.6166394 ],
       [0.43719593, 0.56280404],
       [0.45983097, 0.54016906],
       [0.55739146, 0.44260848],
       [0.40432483, 0.59567523],
       [0.48863846, 0.51136154],
       [0.53886354, 0.46113646],
       [0.3881491 , 0.6118509 ],
       [0.46499103, 0.53500897],
       [0.45677465, 0.5432254 ],
       [0.3161443 , 0.6838557 ],
       [0.30122072, 0.6987793 ],
       [0.23831676, 0.7616832 ],
       [0.48290133, 0.51709867],
       [0.48805034, 0.51194966],
       [0.42187557, 0.5781244 ],
       [0.34418422, 0.6558158 ],
       [0.50155234, 0.49844766],
       [0.4476942 , 0.55230576],
       [0.5141988 , 0.48580122],
       [0.5105721 , 0.48942798],
       [0.5035898 , 0.4964102 ],
       [0.44836116, 0.5516388 ],
       [0.34592777, 0.65407217],
       [0.46922907, 0.53077096],
       [0.46933192, 0.5306681 ],
       [0.324931  , 0.67506903],
       [0.46530113, 0.5346989 ],
       [0.47291952, 0.5270805 ],
       [0.46135852, 0.5386415 ],
       [0.4754733 , 0.5245267 ],
       [0.35891157, 0.6410885 ],
       [0.39086562, 0.6091344 ],
       [0.30816442, 0.6918356 ],
       [0.49687412, 0.50312585],
       [0.5200912 , 0.47990882],
       [0.4875519 , 0.51244813],
       [0.5540007 , 0.44599935],
       [0.54027206, 0.4597279 ],
       [0.5699338 , 0.43006626],
       [0.5900158 , 0.40998417],
       [0.5970119 , 0.40298808],
       [0.39661238, 0.60338765],
       [0.49449727, 0.5055027 ],
       [0.3779686 , 0.6220314 ],
       [0.50124097, 0.49875903],
       [0.22544992, 0.7745501 ],
       [0.47127435, 0.5287257 ],
       [0.28352153, 0.71647847],
       [0.64423805, 0.355762  ],
       [0.39413768, 0.6058624 ],
       [0.54282564, 0.45717433],
       [0.46165714, 0.53834283],
       [0.5180905 , 0.48190957],
       [0.39704147, 0.6029585 ],
       [0.43237814, 0.5676218 ],
       [0.31423116, 0.6857688 ],
       [0.5368261 , 0.46317393],
       [0.480811  , 0.519189  ],
       [0.23129821, 0.76870185],
       [0.48142907, 0.51857096],
       [0.46461168, 0.53538835],
       [0.34829763, 0.6517024 ],
       [0.3921143 , 0.6078857 ],
       [0.19087231, 0.8091277 ],
       [0.48862153, 0.51137847],
       [0.4947204 , 0.5052796 ],
       [0.420459  , 0.579541  ],
       [0.4614616 , 0.5385384 ],
       [0.47786373, 0.5221363 ],
       [0.41827264, 0.5817273 ],
       [0.5474173 , 0.4525827 ],
       [0.32548684, 0.6745131 ],
       [0.56679237, 0.43320763],
       [0.40780404, 0.5921959 ],
       [0.38193023, 0.6180697 ],
       [0.4480284 , 0.5519716 ],
       [0.5013325 , 0.49866748],
       [0.3713635 , 0.62863654],
       [0.39577278, 0.60422724],
       [0.33836445, 0.6616356 ],
       [0.45375347, 0.5462465 ],
       [0.38420883, 0.6157912 ],
       [0.45425016, 0.54574984],
       [0.4919748 , 0.50802517],
       [0.45068237, 0.54931766],
       [0.41700992, 0.58299005],
       [0.55554634, 0.44445363],
       [0.46723658, 0.5327634 ],
       [0.5878943 , 0.41210565],
       [0.568926  , 0.431074  ],
       [0.39234364, 0.6076563 ],
       [0.6473819 , 0.35261813],
       [0.38869643, 0.61130357],
       [0.26793516, 0.73206484],
       [0.45688847, 0.5431115 ],
       [0.38008037, 0.61991966],
       [0.16848788, 0.83151215],
       [0.4043579 , 0.59564215],
       [0.45271173, 0.54728824],
       [0.581272  , 0.41872802],
       [0.44324502, 0.55675495],
       [0.4609002 , 0.5390998 ],
       [0.52742726, 0.4725727 ],
       [0.4418219 , 0.5581781 ],
       [0.46416882, 0.53583115],
       [0.6136855 , 0.38631454],
       [0.32162246, 0.6783775 ],
       [0.47635484, 0.52364516],
       [0.32252866, 0.6774713 ],
       [0.36855057, 0.6314494 ],
       [0.44381416, 0.55618584],
       [0.50875145, 0.49124855],
       [0.34645924, 0.65354073],
       [0.46846077, 0.53153926],
       [0.48328805, 0.5167119 ],
       [0.41081214, 0.58918786],
       [0.44207048, 0.5579295 ],
       [0.46394467, 0.5360553 ],
       [0.50997555, 0.4900245 ],
       [0.67547196, 0.324528  ],
       [0.4019411 , 0.5980589 ],
       [0.35342014, 0.64657986],
       [0.36070785, 0.6392922 ],
       [0.53396183, 0.46603817],
       [0.4045713 , 0.5954287 ],
       [0.5447713 , 0.45522863],
       [0.57562697, 0.4243731 ],
       [0.44885394, 0.55114603],
       [0.55871075, 0.44128925],
       [0.52741945, 0.47258055],
       [0.51188314, 0.48811683],
       [0.40966976, 0.59033024],
       [0.49136174, 0.5086382 ],
       [0.400206  , 0.599794  ],
       [0.45711517, 0.5428848 ],
       [0.4116866 , 0.5883134 ],
       [0.35508472, 0.6449153 ],
       [0.28726262, 0.7127374 ],
       [0.6569646 , 0.34303543],
       [0.5542973 , 0.44570273],
       [0.30934086, 0.69065917]], dtype=float32), array([[0.4975281 , 0.5024719 ],
       [0.34214813, 0.65785193],
       [0.28249127, 0.7175087 ],
       [0.41587338, 0.58412665],
       [0.5120874 , 0.48791265],
       [0.53104264, 0.46895736],
       [0.4229184 , 0.57708156],
       [0.43053263, 0.5694673 ],
       [0.33686534, 0.66313463],
       [0.31016585, 0.6898341 ],
       [0.42058054, 0.5794195 ],
       [0.5048994 , 0.49510068],
       [0.458254  , 0.541746  ],
       [0.37955108, 0.6204489 ],
       [0.3592693 , 0.64073074],
       [0.5614177 , 0.4385823 ],
       [0.36434767, 0.63565236],
       [0.4204569 , 0.5795431 ],
       [0.5290315 , 0.4709685 ],
       [0.3206646 , 0.67933536],
       [0.44819656, 0.55180347],
       [0.5785793 , 0.4214207 ],
       [0.38130638, 0.6186936 ],
       [0.3441323 , 0.6558677 ],
       [0.35239023, 0.6476098 ],
       [0.44873622, 0.55126375],
       [0.41267183, 0.58732814],
       [0.39848185, 0.6015181 ],
       [0.34841406, 0.6515859 ],
       [0.39112338, 0.60887665],
       [0.43313992, 0.56686   ],
       [0.27085564, 0.7291444 ],
       [0.36473688, 0.6352631 ],
       [0.24493083, 0.7550692 ],
       [0.45285776, 0.5471422 ],
       [0.5904121 , 0.4095879 ],
       [0.26589707, 0.73410296],
       [0.56585264, 0.43414733],
       [0.41343364, 0.5865663 ],
       [0.4141425 , 0.5858575 ],
       [0.5256632 , 0.47433677],
       [0.48870754, 0.5112924 ],
       [0.46531394, 0.534686  ],
       [0.3570161 , 0.6429839 ],
       [0.39626345, 0.6037366 ],
       [0.58884317, 0.4111568 ],
       [0.3013811 , 0.6986189 ],
       [0.6789129 , 0.32108712],
       [0.43470687, 0.5652931 ],
       [0.40954593, 0.59045404],
       [0.44818994, 0.5518101 ],
       [0.33682126, 0.6631788 ],
       [0.50879055, 0.49120948],
       [0.24984632, 0.75015366],
       [0.4366075 , 0.5633925 ],
       [0.31752753, 0.6824725 ],
       [0.294134  , 0.70586604],
       [0.31967512, 0.68032485],
       [0.34528464, 0.6547154 ],
       [0.43219772, 0.5678023 ],
       [0.47712874, 0.52287126],
       [0.44606677, 0.55393326],
       [0.40324596, 0.596754  ],
       [0.39689013, 0.60310984],
       [0.40893677, 0.5910632 ],
       [0.50353855, 0.49646148],
       [0.52523136, 0.47476864],
       [0.39706182, 0.6029382 ],
       [0.51259685, 0.48740318],
       [0.41680107, 0.58319896],
       [0.2802717 , 0.7197283 ],
       [0.4062656 , 0.5937344 ],
       [0.40873578, 0.5912642 ],
       [0.46352583, 0.53647417],
       [0.347679  , 0.652321  ],
       [0.37072006, 0.6292799 ],
       [0.48274773, 0.51725227],
       [0.5499267 , 0.45007327],
       [0.3853394 , 0.61466056],
       [0.40091908, 0.5990809 ],
       [0.32207415, 0.6779258 ],
       [0.31473383, 0.68526614],
       [0.47890627, 0.52109367],
       [0.3614312 , 0.63856876],
       [0.5681323 , 0.43186778],
       [0.498384  , 0.501616  ],
       [0.3355692 , 0.66443086],
       [0.5033479 , 0.4966522 ],
       [0.37374055, 0.6262595 ],
       [0.32604465, 0.6739554 ],
       [0.36424738, 0.6357526 ],
       [0.35924536, 0.6407546 ],
       [0.44182354, 0.55817646],
       [0.35080224, 0.6491978 ],
       [0.34607497, 0.65392506],
       [0.49537146, 0.50462854],
       [0.39543146, 0.60456854],
       [0.4179339 , 0.58206606],
       [0.3296705 , 0.6703295 ],
       [0.4242175 , 0.57578254],
       [0.51610154, 0.4838985 ],
       [0.5582656 , 0.44173437],
       [0.54135954, 0.4586404 ],
       [0.4209352 , 0.5790648 ],
       [0.3343211 , 0.66567886],
       [0.382386  , 0.617614  ],
       [0.3637983 , 0.6362017 ],
       [0.28364903, 0.716351  ],
       [0.27990577, 0.7200942 ],
       [0.46484134, 0.5351587 ],
       [0.41827175, 0.5817282 ],
       [0.5004368 , 0.49956322],
       [0.3582526 , 0.64174736],
       [0.41380122, 0.5861988 ],
       [0.3524277 , 0.64757234],
       [0.35520962, 0.64479035],
       [0.40859103, 0.5914089 ],
       [0.46136323, 0.53863674],
       [0.37055278, 0.6294472 ],
       [0.3654578 , 0.6345422 ],
       [0.36021692, 0.6397831 ],
       [0.6028268 , 0.39717326],
       [0.57443887, 0.42556116],
       [0.24091408, 0.7590859 ],
       [0.28974935, 0.7102507 ],
       [0.36325702, 0.63674295],
       [0.47275195, 0.5272481 ],
       [0.24526396, 0.754736  ],
       [0.3955491 , 0.6044509 ],
       [0.42773935, 0.5722606 ],
       [0.540481  , 0.45951903],
       [0.30057293, 0.69942707],
       [0.34634456, 0.6536554 ],
       [0.34218547, 0.65781456],
       [0.42000172, 0.5799983 ],
       [0.40826982, 0.5917302 ],
       [0.36700016, 0.63299984],
       [0.40340757, 0.5965924 ],
       [0.5896227 , 0.41037732],
       [0.45167226, 0.54832774],
       [0.3121052 , 0.68789476],
       [0.4600169 , 0.5399831 ],
       [0.4062419 , 0.5937581 ],
       [0.26234806, 0.737652  ],
       [0.34580222, 0.65419775],
       [0.3493038 , 0.65069616],
       [0.47270155, 0.52729845],
       [0.5549685 , 0.44503158],
       [0.3765618 , 0.62343824],
       [0.40306148, 0.5969385 ],
       [0.3973105 , 0.60268956],
       [0.43234473, 0.5676553 ],
       [0.5381096 , 0.4618904 ],
       [0.24315053, 0.75684947],
       [0.45393524, 0.54606473],
       [0.32664132, 0.6733587 ],
       [0.32408518, 0.6759148 ],
       [0.36895686, 0.63104314],
       [0.34967673, 0.6503233 ],
       [0.31740478, 0.68259525],
       [0.48022828, 0.51977175],
       [0.30369127, 0.69630873],
       [0.42274854, 0.5772515 ],
       [0.340029  , 0.65997106],
       [0.3686598 , 0.63134015],
       [0.3866377 , 0.6133623 ],
       [0.48842686, 0.5115732 ],
       [0.4483272 , 0.5516728 ],
       [0.46348786, 0.5365121 ],
       [0.42265704, 0.5773429 ],
       [0.55735785, 0.44264215],
       [0.47513348, 0.5248665 ],
       [0.46431658, 0.5356834 ],
       [0.60109603, 0.3989039 ],
       [0.46764135, 0.53235865],
       [0.28522494, 0.7147751 ],
       [0.489993  , 0.51000696],
       [0.38008285, 0.6199172 ],
       [0.20709898, 0.792901  ],
       [0.33688408, 0.66311586],
       [0.45358992, 0.5464101 ],
       [0.4644347 , 0.53556526],
       [0.3584243 , 0.64157563],
       [0.34712592, 0.65287405],
       [0.45207855, 0.5479214 ],
       [0.3929659 , 0.6070341 ],
       [0.3601035 , 0.6398965 ],
       [0.48165905, 0.51834095],
       [0.38256985, 0.6174302 ],
       [0.59079623, 0.40920374],
       [0.4821034 , 0.51789665],
       [0.44372234, 0.55627763],
       [0.43080953, 0.5691905 ],
       [0.54561484, 0.45438513],
       [0.334602  , 0.66539806],
       [0.35077593, 0.64922404],
       [0.44644335, 0.5535567 ],
       [0.3281561 , 0.6718439 ],
       [0.53250396, 0.46749604],
       [0.4166163 , 0.5833837 ],
       [0.4601591 , 0.53984094],
       [0.649846  , 0.35015398],
       [0.36532223, 0.6346777 ],
       [0.33534148, 0.6646585 ],
       [0.38143194, 0.61856806],
       [0.44699773, 0.55300224],
       [0.36046067, 0.6395393 ],
       [0.6480288 , 0.35197118],
       [0.4563894 , 0.5436106 ],
       [0.5088077 , 0.4911923 ],
       [0.45247877, 0.54752123],
       [0.3474307 , 0.65256923],
       [0.47270963, 0.5272904 ],
       [0.47560006, 0.5243999 ],
       [0.43768498, 0.562315  ],
       [0.35918456, 0.64081544],
       [0.39405477, 0.6059452 ],
       [0.44974992, 0.55025005],
       [0.31695595, 0.683044  ],
       [0.3776153 , 0.6223847 ],
       [0.500257  , 0.499743  ],
       [0.6104312 , 0.38956875],
       [0.41657922, 0.58342075]], dtype=float32), array([[0.4889797 , 0.5110203 ],
       [0.36245823, 0.63754183],
       [0.31847027, 0.68152976],
       [0.47339094, 0.526609  ],
       [0.41435814, 0.5856419 ],
       [0.56290615, 0.43709385],
       [0.43228337, 0.5677166 ],
       [0.5301206 , 0.46987945],
       [0.33414608, 0.665854  ],
       [0.3915286 , 0.6084714 ],
       [0.4432568 , 0.5567432 ],
       [0.5278808 , 0.4721192 ],
       [0.48416856, 0.5158314 ],
       [0.4154819 , 0.5845181 ],
       [0.2674857 , 0.7325143 ],
       [0.5551743 , 0.4448257 ],
       [0.27283397, 0.72716606],
       [0.37803656, 0.6219635 ],
       [0.39852485, 0.6014752 ],
       [0.1885372 , 0.8114628 ],
       [0.43530735, 0.5646926 ],
       [0.6341892 , 0.3658108 ],
       [0.41725385, 0.58274615],
       [0.44100145, 0.5589985 ],
       [0.31498978, 0.68501025],
       [0.4102527 , 0.5897473 ],
       [0.46508378, 0.5349162 ],
       [0.42898405, 0.57101595],
       [0.16462897, 0.835371  ],
       [0.41306278, 0.58693725],
       [0.4672919 , 0.5327081 ],
       [0.36903635, 0.6309637 ],
       [0.30663714, 0.69336283],
       [0.2083656 , 0.79163444],
       [0.4502503 , 0.5497497 ],
       [0.5680265 , 0.43197355],
       [0.20649546, 0.79350454],
       [0.58270115, 0.41729885],
       [0.47395587, 0.52604413],
       [0.5190485 , 0.4809515 ],
       [0.46258065, 0.5374193 ],
       [0.5553726 , 0.4446274 ],
       [0.46956444, 0.53043556],
       [0.29738384, 0.70261616],
       [0.44220316, 0.55779684],
       [0.5039934 , 0.49600658],
       [0.34085983, 0.65914017],
       [0.6401052 , 0.35989478],
       [0.5027222 , 0.49727774],
       [0.3131672 , 0.68683285],
       [0.46228862, 0.5377114 ],
       [0.4188996 , 0.58110046],
       [0.25953725, 0.7404628 ],
       [0.22519222, 0.77480775],
       [0.1721478 , 0.8278522 ],
       [0.29115012, 0.7088499 ],
       [0.3168744 , 0.6831256 ],
       [0.3893148 , 0.6106852 ],
       [0.2438826 , 0.7561174 ],
       [0.29333076, 0.7066693 ],
       [0.60695946, 0.3930405 ],
       [0.51968414, 0.48031586],
       [0.44851828, 0.5514817 ],
       [0.5263932 , 0.47360682],
       [0.38684693, 0.6131531 ],
       [0.49964955, 0.5003505 ],
       [0.43558356, 0.56441647],
       [0.46428812, 0.5357119 ],
       [0.54679865, 0.45320135],
       [0.3528903 , 0.6471096 ],
       [0.3345748 , 0.6654252 ],
       [0.43753576, 0.5624643 ],
       [0.36196208, 0.638038  ],
       [0.4955169 , 0.5044831 ],
       [0.19355717, 0.80644286],
       [0.48955593, 0.51044405],
       [0.49698788, 0.5030122 ],
       [0.683972  , 0.316028  ],
       [0.4169021 , 0.5830979 ],
       [0.29263636, 0.7073636 ],
       [0.4079692 , 0.59203076],
       [0.33841142, 0.66158855],
       [0.40953854, 0.59046143],
       [0.42521217, 0.5747878 ],
       [0.3945399 , 0.6054601 ],
       [0.39004695, 0.60995305],
       [0.36948502, 0.630515  ],
       [0.56688416, 0.43311584],
       [0.38702407, 0.61297596],
       [0.2849495 , 0.71505046],
       [0.41238362, 0.5876164 ],
       [0.4421472 , 0.5578528 ],
       [0.35284826, 0.6471517 ],
       [0.1605423 , 0.83945775],
       [0.53170997, 0.46829003],
       [0.4813991 , 0.51860094],
       [0.22307247, 0.77692753],
       [0.25035736, 0.74964267],
       [0.38490647, 0.61509347],
       [0.2810332 , 0.71896684],
       [0.5062067 , 0.49379334],
       [0.49536622, 0.5046338 ],
       [0.5741607 , 0.4258393 ],
       [0.47480574, 0.5251943 ],
       [0.31043914, 0.6895609 ],
       [0.4299711 , 0.5700289 ],
       [0.43235642, 0.5676436 ],
       [0.24836591, 0.7516341 ],
       [0.4619221 , 0.53807783],
       [0.5709501 , 0.42904994],
       [0.6299867 , 0.37001327],
       [0.46963334, 0.53036666],
       [0.38015246, 0.61984754],
       [0.4523299 , 0.54767007],
       [0.32777023, 0.6722298 ],
       [0.52326995, 0.47673   ],
       [0.49950412, 0.5004959 ],
       [0.4199044 , 0.5800956 ],
       [0.4018785 , 0.59812146],
       [0.3801444 , 0.61985564],
       [0.44696897, 0.553031  ],
       [0.6545964 , 0.34540364],
       [0.62174594, 0.37825406],
       [0.41056934, 0.5894307 ],
       [0.36301464, 0.63698536],
       [0.1041884 , 0.89581156],
       [0.48649544, 0.51350456],
       [0.17056964, 0.82943034],
       [0.36717963, 0.63282037],
       [0.28677765, 0.7132223 ],
       [0.5429286 , 0.4570714 ],
       [0.36603585, 0.6339641 ],
       [0.3226302 , 0.67736983],
       [0.45821375, 0.54178625],
       [0.28999728, 0.7100027 ],
       [0.35302335, 0.6469766 ],
       [0.39721778, 0.60278225],
       [0.18356818, 0.81643176],
       [0.69580495, 0.30419508],
       [0.5260204 , 0.47397965],
       [0.25228795, 0.747712  ],
       [0.45884973, 0.5411502 ],
       [0.4990778 , 0.5009222 ],
       [0.30435094, 0.6956491 ],
       [0.20250864, 0.7974913 ],
       [0.2144616 , 0.78553843],
       [0.50000095, 0.49999908],
       [0.5343624 , 0.46563756],
       [0.48063245, 0.5193675 ],
       [0.49598986, 0.5040101 ],
       [0.4527859 , 0.5472141 ],
       [0.35586888, 0.64413106],
       [0.45276463, 0.54723537],
       [0.2244498 , 0.7755502 ],
       [0.40200186, 0.59799814],
       [0.3834708 , 0.61652917],
       [0.3724241 , 0.62757593],
       [0.4477093 , 0.55229074],
       [0.42707747, 0.5729225 ],
       [0.24242012, 0.7575799 ],
       [0.5938066 , 0.40619335],
       [0.18428276, 0.8157173 ],
       [0.4770207 , 0.52297926],
       [0.1935597 , 0.8064403 ],
       [0.38597658, 0.6140234 ],
       [0.43538615, 0.5646139 ],
       [0.33808953, 0.6619105 ],
       [0.2371264 , 0.76287365],
       [0.5908163 , 0.40918368],
       [0.5094796 , 0.49052045],
       [0.61969626, 0.3803037 ],
       [0.45603642, 0.54396355],
       [0.34362257, 0.6563774 ],
       [0.6399928 , 0.36000726],
       [0.5078519 , 0.4921481 ],
       [0.18540704, 0.81459296],
       [0.48953116, 0.51046884],
       [0.29363847, 0.7063616 ],
       [0.14836662, 0.8516334 ],
       [0.4083846 , 0.5916154 ],
       [0.34194186, 0.65805817],
       [0.46591145, 0.5340886 ],
       [0.4219458 , 0.5780542 ],
       [0.35839033, 0.6416097 ],
       [0.41582566, 0.58417434],
       [0.3479509 , 0.6520491 ],
       [0.41828498, 0.58171505],
       [0.504157  , 0.49584302],
       [0.27776313, 0.7222369 ],
       [0.5386922 , 0.4613078 ],
       [0.32546568, 0.67453426],
       [0.43461338, 0.5653866 ],
       [0.4243626 , 0.5756374 ],
       [0.5981229 , 0.40187714],
       [0.26722625, 0.7327738 ],
       [0.38396248, 0.61603755],
       [0.5823291 , 0.41767085],
       [0.31304723, 0.68695277],
       [0.54460126, 0.45539868],
       [0.42045924, 0.5795408 ],
       [0.39375463, 0.60624534],
       [0.6302519 , 0.36974812],
       [0.30712357, 0.69287646],
       [0.30268368, 0.69731635],
       [0.5650353 , 0.43496466],
       [0.5244803 , 0.4755197 ],
       [0.4466484 , 0.55335164],
       [0.62527055, 0.37472945],
       [0.5496609 , 0.45033908],
       [0.556281  , 0.44371906],
       [0.4595465 , 0.5404535 ],
       [0.5048593 , 0.49514073],
       [0.480662  , 0.519338  ],
       [0.3138706 , 0.6861294 ],
       [0.48212147, 0.51787853],
       [0.2337993 , 0.7662008 ],
       [0.38681912, 0.61318094],
       [0.3691005 , 0.6308995 ],
       [0.38434693, 0.6156531 ],
       [0.28985652, 0.71014345],
       [0.44666156, 0.5533384 ],
       [0.57393354, 0.42606646],
       [0.21800043, 0.7819996 ]], dtype=float32)]
i = 4, Test true class= 
[1 0 1 0 1 0 1 0 1 1 1 0 0 0 0 1 0 0 0 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1
 1 0 0 0 1 0 0 1 0 0 0 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 0 0 0 1 0 1 0 1 0 0
 1 1 1 1 1 1 0 0 0 0 0 0 1 0 1 1 0 1 1 1 0 1 0 1 0 0 0 1 0 0 1 0 1 0 0 1 0
 1 1 1 0 1 0 0 0 0 1 1 1 0 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 0
 0 1 0 1 0 1 1 0 0 0 1 1 0 0 1 1 0 1 1 1 1 0 1 1 1 0 1 1 1 0 0 1 0 1 1 0 0
 1 0 0 0 0 1 1 0 0 0 1 0 1 0 0 0 0 1 1 0 0 0 1 1 0 0 0 1 0 1 0 0 1 1 0 1 1
 0]
Final, Test average ensemble probabilities = 
[[0.5203878  0.47961226]
 [0.3687802  0.6312198 ]
 [0.3442673  0.6557327 ]
 [0.42946187 0.5705381 ]
 [0.40620846 0.59379154]
 [0.5588744  0.44112557]
 [0.44288102 0.557119  ]
 [0.50014955 0.4998505 ]
 [0.36844382 0.63155615]
 [0.43159175 0.56840825]
 [0.47356135 0.5264386 ]
 [0.50713646 0.49286357]
 [0.4567522  0.5432478 ]
 [0.4143754  0.5856246 ]
 [0.304218   0.695782  ]
 [0.40093574 0.59906423]
 [0.36210877 0.6378912 ]
 [0.4047699  0.59523016]
 [0.3970385  0.60296154]
 [0.33089048 0.6691095 ]
 [0.44133863 0.55866134]
 [0.5962722  0.40372783]
 [0.42319983 0.57680017]
 [0.39536422 0.6046358 ]
 [0.31571788 0.68428206]
 [0.4686021  0.53139794]
 [0.44798714 0.55201286]
 [0.42449722 0.57550275]
 [0.28837863 0.71162134]
 [0.4671448  0.5328552 ]
 [0.4451569  0.55484307]
 [0.30566368 0.6943363 ]
 [0.3762131  0.6237869 ]
 [0.19670726 0.80329275]
 [0.48445216 0.5155478 ]
 [0.6273441  0.37265593]
 [0.21658067 0.7834193 ]
 [0.5975959  0.40240413]
 [0.4128111  0.5871889 ]
 [0.46597496 0.5340251 ]
 [0.49885592 0.50114405]
 [0.472507   0.527493  ]
 [0.45625097 0.54374903]
 [0.3514008  0.6485992 ]
 [0.39736247 0.6026376 ]
 [0.46050605 0.5394939 ]
 [0.33539432 0.6646057 ]
 [0.7062505  0.2937495 ]
 [0.5290669  0.47093305]
 [0.3528639  0.6471361 ]
 [0.46034807 0.53965193]
 [0.42993322 0.5700668 ]
 [0.46458215 0.53541785]
 [0.23619322 0.76380676]
 [0.24639332 0.7536067 ]
 [0.2972474  0.7027526 ]
 [0.28161636 0.71838367]
 [0.40314594 0.59685403]
 [0.31326994 0.6867301 ]
 [0.31519297 0.6848071 ]
 [0.5488307  0.4511693 ]
 [0.46971574 0.5302843 ]
 [0.5022319  0.4977681 ]
 [0.43475944 0.5652406 ]
 [0.4657166  0.53428346]
 [0.46445543 0.5355446 ]
 [0.46251312 0.5374869 ]
 [0.47898358 0.5210164 ]
 [0.50750756 0.49249244]
 [0.46135196 0.538648  ]
 [0.29835832 0.7016417 ]
 [0.46052232 0.5394777 ]
 [0.41766053 0.5823395 ]
 [0.47966272 0.5203373 ]
 [0.26818642 0.73181355]
 [0.46663213 0.5333678 ]
 [0.512409   0.48759103]
 [0.5511959  0.44880408]
 [0.34313384 0.6568662 ]
 [0.31102753 0.6889725 ]
 [0.35587063 0.6441294 ]
 [0.37997854 0.62002146]
 [0.4207511  0.57924885]
 [0.42226028 0.57773966]
 [0.538276   0.46172398]
 [0.38084263 0.61915743]
 [0.34815744 0.6518426 ]
 [0.53319913 0.46680087]
 [0.39966133 0.60033876]
 [0.32228148 0.6777185 ]
 [0.39543715 0.6045629 ]
 [0.39521495 0.6047851 ]
 [0.35566774 0.6443323 ]
 [0.2540935  0.7459065 ]
 [0.48031253 0.5196875 ]
 [0.5230777  0.4769222 ]
 [0.2878968  0.7121032 ]
 [0.37750185 0.62249815]
 [0.37592292 0.6240771 ]
 [0.3643363  0.6356637 ]
 [0.5199361  0.48006397]
 [0.478666   0.521334  ]
 [0.5122076  0.4877924 ]
 [0.43235102 0.567649  ]
 [0.274852   0.725148  ]
 [0.41842785 0.5815722 ]
 [0.38443518 0.61556476]
 [0.22621968 0.77378035]
 [0.36848435 0.6315156 ]
 [0.52375495 0.47624508]
 [0.46035218 0.5396478 ]
 [0.45458698 0.5454131 ]
 [0.3716385  0.62836146]
 [0.4357412  0.5642588 ]
 [0.3101476  0.68985236]
 [0.41586494 0.58413506]
 [0.38319594 0.616804  ]
 [0.43278608 0.5672139 ]
 [0.42865348 0.5713465 ]
 [0.39001602 0.609984  ]
 [0.38677382 0.6132262 ]
 [0.6445164  0.35548365]
 [0.63232154 0.36767846]
 [0.3140933  0.6859067 ]
 [0.33021808 0.6697819 ]
 [0.2703009  0.72969913]
 [0.49810734 0.5018927 ]
 [0.2160259  0.7839741 ]
 [0.327511   0.672489  ]
 [0.3858524  0.61414754]
 [0.5854796  0.4145204 ]
 [0.30716115 0.69283885]
 [0.40368962 0.5963104 ]
 [0.4213943  0.5786057 ]
 [0.3580605  0.6419395 ]
 [0.36156076 0.6384392 ]
 [0.38955992 0.6104401 ]
 [0.27751583 0.7224841 ]
 [0.6410655  0.35893446]
 [0.5120708  0.48792928]
 [0.23040406 0.7695959 ]
 [0.46111074 0.5388892 ]
 [0.42307454 0.57692546]
 [0.2961455  0.70385456]
 [0.37628692 0.6237131 ]
 [0.25800332 0.7419967 ]
 [0.4994398  0.50056016]
 [0.50920266 0.49079734]
 [0.41489467 0.5851053 ]
 [0.45288414 0.5471158 ]
 [0.42052665 0.5794734 ]
 [0.3263471  0.6736528 ]
 [0.5202699  0.47973013]
 [0.24841216 0.7515878 ]
 [0.45482913 0.5451709 ]
 [0.39164075 0.6083592 ]
 [0.2856466  0.71435344]
 [0.45061296 0.54938704]
 [0.41694826 0.58305174]
 [0.26153335 0.7384666 ]
 [0.48337254 0.51662743]
 [0.27627665 0.7237234 ]
 [0.4572084  0.5427916 ]
 [0.2327492  0.7672508 ]
 [0.40516788 0.5948322 ]
 [0.37105545 0.6289445 ]
 [0.40276998 0.5972301 ]
 [0.3343351  0.665665  ]
 [0.52402675 0.47597322]
 [0.4726204  0.52737963]
 [0.5782253  0.42177472]
 [0.46807474 0.5319252 ]
 [0.3674069  0.63259304]
 [0.6619341  0.33806592]
 [0.43833065 0.56166935]
 [0.26809523 0.7319048 ]
 [0.4830804  0.5169196 ]
 [0.3040541  0.69594586]
 [0.12638402 0.87361604]
 [0.34843093 0.651569  ]
 [0.44668812 0.5533119 ]
 [0.46057963 0.5394204 ]
 [0.37553996 0.62446004]
 [0.3491761  0.65082395]
 [0.4346806  0.5653194 ]
 [0.34075    0.65924996]
 [0.4187252  0.5812748 ]
 [0.4040896  0.59591043]
 [0.32549167 0.67450833]
 [0.5473225  0.45267755]
 [0.39918947 0.6008105 ]
 [0.3839308  0.6160692 ]
 [0.4435706  0.5564294 ]
 [0.59280217 0.4071978 ]
 [0.2634283  0.73657167]
 [0.40692082 0.5930792 ]
 [0.5823432  0.41765672]
 [0.30398017 0.6960199 ]
 [0.48593402 0.5140659 ]
 [0.4117692  0.5882307 ]
 [0.43112883 0.5688712 ]
 [0.6788459  0.32115418]
 [0.35916847 0.6408315 ]
 [0.32130906 0.67869097]
 [0.40734    0.59265995]
 [0.4911251  0.5088749 ]
 [0.42297378 0.57702625]
 [0.60186255 0.3981374 ]
 [0.5766122  0.42338783]
 [0.58851326 0.41148672]
 [0.48306227 0.51693773]
 [0.4566102  0.54338986]
 [0.470407   0.529593  ]
 [0.3508759  0.6491241 ]
 [0.41691908 0.5830809 ]
 [0.2508911  0.7491089 ]
 [0.38082245 0.61917746]
 [0.33009583 0.6699042 ]
 [0.29744825 0.7025517 ]
 [0.24447544 0.7555245 ]
 [0.5687097  0.43129033]
 [0.54490995 0.45509005]
 [0.26652896 0.73347104]]
Accuracy: 0.5381
MCC: 0.0429
AUC: 0.5563
Confusion Matrix:
tensor([[ 20,  84],
        [ 19, 100]])
Specificity: 0.1923
Precision (Macro): 0.5281
F1 Score (Macro): 0.4699
Expected Calibration Error (ECE): 0.0851
NLL loss: 0.7008
Main task is done! Can finish
