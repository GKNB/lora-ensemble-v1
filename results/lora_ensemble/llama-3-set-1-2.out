Job is running on partition: gpu_ss11
Job is running under account: m2616_g
/pscratch/sd/t/tianle/conda/envs/lucid_surp_2024/lib/python3.11/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:09<00:29,  9.99s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:20<00:20, 10.03s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:30<00:10, 10.05s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:32<00:00,  7.01s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:32<00:00,  8.12s/it]
Llama3 has been loaded successfully.
Model:  LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(128256, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)
          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)
          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)
          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)
)
Size of dataset:  1332
In grad_steps = 0, loss = 1.0984740257263184
In grad_steps = 1, loss = 0.7592287659645081
In grad_steps = 2, loss = 0.7095652222633362
In grad_steps = 3, loss = 0.6924886703491211
In grad_steps = 4, loss = 0.9845622777938843
In grad_steps = 5, loss = 0.7296270728111267
In grad_steps = 6, loss = 0.7091129422187805
In grad_steps = 7, loss = 0.7123016715049744
In grad_steps = 8, loss = 0.6977198719978333
In grad_steps = 9, loss = 0.776858925819397
In grad_steps = 10, loss = 0.697952926158905
In grad_steps = 11, loss = 0.7493088841438293
In grad_steps = 12, loss = 0.7777855396270752
In grad_steps = 13, loss = 0.6974202990531921
In grad_steps = 14, loss = 0.667948842048645
In grad_steps = 15, loss = 0.6532963514328003
In grad_steps = 16, loss = 0.885023832321167
In grad_steps = 17, loss = 0.7841035723686218
In grad_steps = 18, loss = 0.7254934310913086
In grad_steps = 19, loss = 0.6536522507667542
In grad_steps = 20, loss = 0.8377748131752014
In grad_steps = 21, loss = 0.715324878692627
In grad_steps = 22, loss = 0.7258952260017395
In grad_steps = 23, loss = 0.6659665107727051
In grad_steps = 24, loss = 0.7479876279830933
In grad_steps = 25, loss = 0.7241640090942383
In grad_steps = 26, loss = 0.6206314563751221
In grad_steps = 27, loss = 0.6401798129081726
In grad_steps = 28, loss = 0.5988637208938599
In grad_steps = 29, loss = 0.7971249222755432
In grad_steps = 30, loss = 0.7841924428939819
In grad_steps = 31, loss = 0.762964129447937
In grad_steps = 32, loss = 0.6641064882278442
In grad_steps = 33, loss = 0.6697784066200256
In grad_steps = 34, loss = 0.7284198999404907
In grad_steps = 35, loss = 0.6798660755157471
In grad_steps = 36, loss = 0.6976125240325928
In grad_steps = 37, loss = 0.6861007213592529
In grad_steps = 38, loss = 0.6820439100265503
In grad_steps = 39, loss = 0.6719828844070435
In grad_steps = 40, loss = 0.7406363487243652
In grad_steps = 41, loss = 0.6804864406585693
In grad_steps = 42, loss = 0.8356232047080994
In grad_steps = 43, loss = 0.6816458106040955
In grad_steps = 44, loss = 0.6954590082168579
In grad_steps = 45, loss = 0.7041046023368835
In grad_steps = 46, loss = 0.6876593828201294
In grad_steps = 47, loss = 0.7060314416885376
In grad_steps = 48, loss = 0.6762629151344299
In grad_steps = 49, loss = 0.7036311030387878
In grad_steps = 50, loss = 0.7162333130836487
In grad_steps = 51, loss = 0.6770913004875183
In grad_steps = 52, loss = 0.6625911593437195
In grad_steps = 53, loss = 0.7412611246109009
In grad_steps = 54, loss = 0.7286055684089661
In grad_steps = 55, loss = 0.6757721900939941
In grad_steps = 56, loss = 0.7004512548446655
In grad_steps = 57, loss = 0.7001165151596069
In grad_steps = 58, loss = 0.7092839479446411
In grad_steps = 59, loss = 0.7109143137931824
In grad_steps = 60, loss = 0.7100321650505066
In grad_steps = 61, loss = 0.6946007013320923
In grad_steps = 62, loss = 0.6927860975265503
In grad_steps = 63, loss = 0.7000016570091248
In grad_steps = 64, loss = 0.6896237134933472
In grad_steps = 65, loss = 0.7227299809455872
In grad_steps = 66, loss = 0.7212862968444824
In grad_steps = 67, loss = 0.6762130260467529
In grad_steps = 68, loss = 0.6805655360221863
In grad_steps = 69, loss = 0.6804208755493164
In grad_steps = 70, loss = 0.7389801144599915
In grad_steps = 71, loss = 0.6580409407615662
In grad_steps = 72, loss = 0.7156954407691956
In grad_steps = 73, loss = 0.6976338028907776
In grad_steps = 74, loss = 0.6373708248138428
In grad_steps = 75, loss = 0.716727614402771
In grad_steps = 76, loss = 0.7442061305046082
In grad_steps = 77, loss = 0.6794849038124084
In grad_steps = 78, loss = 0.7187184691429138
In grad_steps = 79, loss = 0.7343820333480835
In grad_steps = 80, loss = 0.6859567165374756
In grad_steps = 81, loss = 0.695828914642334
In grad_steps = 82, loss = 0.6781834363937378
In grad_steps = 83, loss = 0.730154812335968
In grad_steps = 84, loss = 0.7081975936889648
In grad_steps = 85, loss = 0.6915944218635559
In grad_steps = 86, loss = 0.6579049229621887
In grad_steps = 87, loss = 0.8271135091781616
In grad_steps = 88, loss = 0.7219382524490356
In grad_steps = 89, loss = 0.6633897423744202
In grad_steps = 90, loss = 0.6929336190223694
In grad_steps = 91, loss = 0.7040191292762756
In grad_steps = 92, loss = 0.6846060156822205
In grad_steps = 93, loss = 0.7041183710098267
In grad_steps = 94, loss = 0.6882447004318237
In grad_steps = 95, loss = 0.664318323135376
In grad_steps = 96, loss = 0.7317281365394592
In grad_steps = 97, loss = 0.7328397035598755
In grad_steps = 98, loss = 0.7340295314788818
In grad_steps = 99, loss = 0.6597738265991211
In grad_steps = 100, loss = 0.6594520211219788
In grad_steps = 101, loss = 0.7412043213844299
In grad_steps = 102, loss = 0.6868553161621094
In grad_steps = 103, loss = 0.7158493399620056
In grad_steps = 104, loss = 0.6679050326347351
In grad_steps = 105, loss = 0.7195244431495667
In grad_steps = 106, loss = 0.6943259835243225
In grad_steps = 107, loss = 0.6818853616714478
In grad_steps = 108, loss = 0.6799460053443909
In grad_steps = 109, loss = 0.7352498173713684
In grad_steps = 110, loss = 0.6943737268447876
In grad_steps = 111, loss = 0.6889846324920654
In grad_steps = 112, loss = 0.7034404873847961
In grad_steps = 113, loss = 0.6902701258659363
In grad_steps = 114, loss = 0.6881829500198364
In grad_steps = 115, loss = 0.6887054443359375
In grad_steps = 116, loss = 0.6944841742515564
In grad_steps = 117, loss = 0.693202555179596
In grad_steps = 118, loss = 0.6729838252067566
In grad_steps = 119, loss = 0.693696916103363
In grad_steps = 120, loss = 0.700592041015625
In grad_steps = 121, loss = 0.7078126668930054
In grad_steps = 122, loss = 0.6855740547180176
In grad_steps = 123, loss = 0.6961326003074646
In grad_steps = 124, loss = 0.6965512037277222
In grad_steps = 125, loss = 0.7071877121925354
In grad_steps = 126, loss = 0.7080069780349731
In grad_steps = 127, loss = 0.7078925371170044
In grad_steps = 128, loss = 0.6884445548057556
In grad_steps = 129, loss = 0.6956825852394104
In grad_steps = 130, loss = 0.6949654221534729
In grad_steps = 131, loss = 0.6880156993865967
In grad_steps = 132, loss = 0.6959277391433716
In grad_steps = 133, loss = 0.6819414496421814
In grad_steps = 134, loss = 0.6643098592758179
In grad_steps = 135, loss = 0.6741567850112915
In grad_steps = 136, loss = 0.678001344203949
In grad_steps = 137, loss = 0.7590653300285339
In grad_steps = 138, loss = 0.6524872779846191
In grad_steps = 139, loss = 0.7131586074829102
In grad_steps = 140, loss = 0.6959055066108704
In grad_steps = 141, loss = 0.6312198638916016
In grad_steps = 142, loss = 0.7057871222496033
In grad_steps = 143, loss = 0.7375612258911133
In grad_steps = 144, loss = 0.670833170413971
In grad_steps = 145, loss = 0.7016891837120056
In grad_steps = 146, loss = 0.702565610408783
In grad_steps = 147, loss = 0.6687988042831421
In grad_steps = 148, loss = 0.6633487343788147
In grad_steps = 149, loss = 0.6503870487213135
In grad_steps = 150, loss = 0.7642905712127686
In grad_steps = 151, loss = 0.7157080173492432
In grad_steps = 152, loss = 0.7030613422393799
In grad_steps = 153, loss = 0.6519818902015686
In grad_steps = 154, loss = 0.8441265225410461
In grad_steps = 155, loss = 0.7059839367866516
In grad_steps = 156, loss = 0.6689261794090271
In grad_steps = 157, loss = 0.6735008955001831
In grad_steps = 158, loss = 0.7089995741844177
In grad_steps = 159, loss = 0.6930709481239319
In grad_steps = 160, loss = 0.6593056321144104
In grad_steps = 161, loss = 0.6473843455314636
In grad_steps = 162, loss = 0.6143425703048706
In grad_steps = 163, loss = 0.7769169807434082
In grad_steps = 164, loss = 0.778390109539032
In grad_steps = 165, loss = 0.7687992453575134
In grad_steps = 166, loss = 0.6440938711166382
In grad_steps = 167, loss = 0.648823618888855
In grad_steps = 168, loss = 0.7261584401130676
In grad_steps = 169, loss = 0.6723272800445557
In grad_steps = 170, loss = 0.6961808204650879
In grad_steps = 171, loss = 0.6670641899108887
In grad_steps = 172, loss = 0.6784122586250305
In grad_steps = 173, loss = 0.6638380289077759
In grad_steps = 174, loss = 0.7237095832824707
In grad_steps = 175, loss = 0.6704416275024414
In grad_steps = 176, loss = 0.7992820143699646
In grad_steps = 177, loss = 0.6802735924720764
In grad_steps = 178, loss = 0.6920425891876221
In grad_steps = 179, loss = 0.7022592425346375
In grad_steps = 180, loss = 0.6794658899307251
In grad_steps = 181, loss = 0.6930634379386902
In grad_steps = 182, loss = 0.6607198119163513
In grad_steps = 183, loss = 0.6851385831832886
In grad_steps = 184, loss = 0.710534930229187
In grad_steps = 185, loss = 0.6539331078529358
In grad_steps = 186, loss = 0.6621925830841064
In grad_steps = 187, loss = 0.7337062358856201
In grad_steps = 188, loss = 0.7277437448501587
In grad_steps = 189, loss = 0.6806785464286804
In grad_steps = 190, loss = 0.683900773525238
In grad_steps = 191, loss = 0.6921067833900452
In grad_steps = 192, loss = 0.6974475383758545
In grad_steps = 193, loss = 0.7033490538597107
In grad_steps = 194, loss = 0.698196530342102
In grad_steps = 195, loss = 0.6953437328338623
In grad_steps = 196, loss = 0.6712954044342041
In grad_steps = 197, loss = 0.6856047511100769
In grad_steps = 198, loss = 0.6811613440513611
In grad_steps = 199, loss = 0.7179636359214783
In grad_steps = 200, loss = 0.6960707902908325
In grad_steps = 201, loss = 0.6541801691055298
In grad_steps = 202, loss = 0.6610103845596313
In grad_steps = 203, loss = 0.6671918630599976
In grad_steps = 204, loss = 0.7490230798721313
In grad_steps = 205, loss = 0.6382591724395752
In grad_steps = 206, loss = 0.7081587314605713
In grad_steps = 207, loss = 0.693300187587738
In grad_steps = 208, loss = 0.6000521779060364
In grad_steps = 209, loss = 0.6946396827697754
In grad_steps = 210, loss = 0.7195500135421753
In grad_steps = 211, loss = 0.6427118182182312
In grad_steps = 212, loss = 0.6810030937194824
In grad_steps = 213, loss = 0.6549301147460938
In grad_steps = 214, loss = 0.6233999729156494
In grad_steps = 215, loss = 0.5853689312934875
In grad_steps = 216, loss = 0.5723079442977905
In grad_steps = 217, loss = 0.9530289173126221
In grad_steps = 218, loss = 0.7222310900688171
In grad_steps = 219, loss = 0.6971148252487183
In grad_steps = 220, loss = 0.6576429009437561
In grad_steps = 221, loss = 0.664972722530365
In grad_steps = 222, loss = 0.6322895884513855
In grad_steps = 223, loss = 0.7766150832176208
In grad_steps = 224, loss = 0.6493781208992004
In grad_steps = 225, loss = 0.7455235719680786
In grad_steps = 226, loss = 0.7091720700263977
In grad_steps = 227, loss = 0.6551299095153809
In grad_steps = 228, loss = 0.6665523052215576
In grad_steps = 229, loss = 0.6674244999885559
In grad_steps = 230, loss = 0.6903857588768005
In grad_steps = 231, loss = 0.6944025754928589
In grad_steps = 232, loss = 0.6830548644065857
In grad_steps = 233, loss = 0.6557183861732483
In grad_steps = 234, loss = 0.6269782185554504
In grad_steps = 235, loss = 0.6946386694908142
In grad_steps = 236, loss = 0.6493398547172546
In grad_steps = 237, loss = 0.683189868927002
In grad_steps = 238, loss = 0.6333153247833252
In grad_steps = 239, loss = 0.6590157747268677
In grad_steps = 240, loss = 0.6389939785003662
In grad_steps = 241, loss = 0.7086684703826904
In grad_steps = 242, loss = 0.6529613137245178
In grad_steps = 243, loss = 0.734519362449646
In grad_steps = 244, loss = 0.7259016036987305
In grad_steps = 245, loss = 0.7142563462257385
In grad_steps = 246, loss = 0.7112233638763428
In grad_steps = 247, loss = 0.6836506128311157
In grad_steps = 248, loss = 0.7194443941116333
In grad_steps = 249, loss = 0.6097702980041504
In grad_steps = 250, loss = 0.628604531288147
In grad_steps = 251, loss = 0.6718516945838928
In grad_steps = 252, loss = 0.6086385250091553
In grad_steps = 253, loss = 0.693500280380249
In grad_steps = 254, loss = 0.6599668860435486
In grad_steps = 255, loss = 0.7148544788360596
In grad_steps = 256, loss = 0.6875263452529907
In grad_steps = 257, loss = 0.6412622928619385
In grad_steps = 258, loss = 0.6756641864776611
In grad_steps = 259, loss = 0.6620151400566101
In grad_steps = 260, loss = 0.6897827386856079
In grad_steps = 261, loss = 0.6779077649116516
In grad_steps = 262, loss = 0.6616820693016052
In grad_steps = 263, loss = 0.6470518112182617
In grad_steps = 264, loss = 0.6283096671104431
In grad_steps = 265, loss = 0.6734619140625
In grad_steps = 266, loss = 0.6809394955635071
In grad_steps = 267, loss = 0.6080033183097839
i = 0, Test ensemble probabilities = 
[array([[0.34539714, 0.6546028 ],
       [0.49670178, 0.5032982 ],
       [0.2995092 , 0.70049083],
       [0.5216206 , 0.4783794 ],
       [0.4783175 , 0.5216825 ],
       [0.37964696, 0.620353  ],
       [0.5360174 , 0.4639826 ],
       [0.41234112, 0.5876589 ],
       [0.2805267 , 0.71947336],
       [0.3660757 , 0.6339243 ],
       [0.31246424, 0.68753576],
       [0.5163459 , 0.4836541 ],
       [0.37822625, 0.6217737 ],
       [0.43147096, 0.56852907],
       [0.54082584, 0.4591742 ],
       [0.5715256 , 0.4284744 ],
       [0.4142887 , 0.5857113 ],
       [0.37641716, 0.6235828 ],
       [0.46707872, 0.5329213 ],
       [0.4730467 , 0.5269533 ],
       [0.4055867 , 0.59441334],
       [0.4146544 , 0.5853456 ],
       [0.36647767, 0.6335223 ],
       [0.45998093, 0.5400191 ],
       [0.31840318, 0.6815968 ],
       [0.24469352, 0.75530654],
       [0.44522896, 0.55477107],
       [0.3880463 , 0.61195374],
       [0.37943542, 0.6205646 ],
       [0.2172548 , 0.7827452 ],
       [0.4649846 , 0.5350154 ],
       [0.4034645 , 0.59653556],
       [0.45316827, 0.5468317 ],
       [0.41054946, 0.58945054],
       [0.38889134, 0.61110866],
       [0.30868906, 0.691311  ],
       [0.48083103, 0.519169  ],
       [0.55902576, 0.44097418],
       [0.4085316 , 0.5914684 ],
       [0.41088137, 0.5891186 ],
       [0.4298178 , 0.5701822 ],
       [0.29877663, 0.70122343],
       [0.3293201 , 0.6706799 ],
       [0.42829737, 0.5717026 ],
       [0.49717334, 0.5028267 ],
       [0.52718437, 0.4728156 ],
       [0.42836556, 0.5716345 ],
       [0.4556835 , 0.54431653],
       [0.3669375 , 0.63306254],
       [0.4527557 , 0.5472443 ],
       [0.2731709 , 0.7268291 ],
       [0.42771834, 0.5722816 ],
       [0.38610366, 0.61389637],
       [0.3611515 , 0.6388485 ],
       [0.39574474, 0.60425526],
       [0.48206717, 0.51793283],
       [0.4352225 , 0.56477743],
       [0.5293537 , 0.47064632],
       [0.4351939 , 0.56480604],
       [0.39494088, 0.6050591 ],
       [0.29629105, 0.703709  ],
       [0.39273342, 0.60726655],
       [0.37942597, 0.620574  ],
       [0.40392777, 0.59607226],
       [0.42278576, 0.57721424],
       [0.42321032, 0.5767897 ],
       [0.3290817 , 0.6709183 ],
       [0.38383117, 0.6161688 ],
       [0.4492992 , 0.55070084],
       [0.41927034, 0.58072966],
       [0.38925087, 0.6107492 ],
       [0.4150021 , 0.5849979 ],
       [0.5914758 , 0.40852422],
       [0.4447122 , 0.5552878 ],
       [0.5072293 , 0.4927707 ],
       [0.5590465 , 0.44095352],
       [0.4567155 , 0.54328454],
       [0.26308432, 0.73691565],
       [0.47329345, 0.5267065 ],
       [0.5217634 , 0.47823662],
       [0.2883945 , 0.71160555],
       [0.48484313, 0.51515687],
       [0.785682  , 0.21431798],
       [0.44177085, 0.55822915],
       [0.4268143 , 0.57318574],
       [0.3282022 , 0.6717979 ],
       [0.27888694, 0.721113  ],
       [0.43272623, 0.56727374],
       [0.48700717, 0.5129928 ],
       [0.24735552, 0.7526445 ],
       [0.3437499 , 0.65625006],
       [0.47496793, 0.52503204],
       [0.35789973, 0.64210033],
       [0.43090022, 0.5690998 ],
       [0.40508464, 0.5949154 ],
       [0.4418508 , 0.5581492 ],
       [0.50487167, 0.49512833],
       [0.5045017 , 0.4954983 ],
       [0.33627528, 0.6637247 ],
       [0.43236625, 0.5676338 ],
       [0.4723027 , 0.5276973 ],
       [0.55153644, 0.44846356],
       [0.410058  , 0.58994204],
       [0.43052799, 0.5694721 ],
       [0.45123821, 0.54876184],
       [0.35328087, 0.6467191 ],
       [0.45968103, 0.54031897],
       [0.33205697, 0.66794306],
       [0.5815101 , 0.41848987],
       [0.55743647, 0.44256356],
       [0.4615889 , 0.53841114],
       [0.5988489 , 0.40115115],
       [0.26768813, 0.73231184],
       [0.4092803 , 0.5907197 ],
       [0.44905618, 0.55094385],
       [0.40612987, 0.5938701 ],
       [0.4797793 , 0.5202207 ],
       [0.490779  , 0.50922096],
       [0.50890255, 0.49109742],
       [0.46478626, 0.5352137 ],
       [0.39073965, 0.6092604 ],
       [0.6140519 , 0.38594812],
       [0.21095832, 0.78904164],
       [0.50982255, 0.49017748],
       [0.33999068, 0.6600093 ],
       [0.4652964 , 0.5347036 ],
       [0.4872016 , 0.51279837],
       [0.3138431 , 0.6861569 ],
       [0.5067988 , 0.49320123],
       [0.41474557, 0.5852544 ],
       [0.4137968 , 0.58620316],
       [0.6387266 , 0.3612734 ],
       [0.29403043, 0.7059696 ],
       [0.46303543, 0.5369646 ]], dtype=float32)]
i = 0, Test true class= 
[1 0 0 0 0 1 1 0 1 0 0 0 1 1 1 1 0 1 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 0 1 1 1
 1 1 0 0 0 0 0 0 0 1 0 1 0 1 0 1 0 0 0 0 1 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 1
 1 0 1 1 1 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 1 1 0 0 0 0 1 1 0
 0 0 1 1 0 0 0 0 1 0 0 1 0 0 0 0 1 1 0 1 1 0 0]
In grad_steps = 0, loss = 1.0984740257263184
In grad_steps = 1, loss = 0.7664747834205627
In grad_steps = 2, loss = 0.7049790024757385
In grad_steps = 3, loss = 0.6902543902397156
In grad_steps = 4, loss = 0.9943068623542786
In grad_steps = 5, loss = 0.7364214062690735
In grad_steps = 6, loss = 0.7133119106292725
In grad_steps = 7, loss = 0.7124478816986084
In grad_steps = 8, loss = 0.6954291462898254
In grad_steps = 9, loss = 0.7772217392921448
In grad_steps = 10, loss = 0.6979620456695557
In grad_steps = 11, loss = 0.7499814033508301
In grad_steps = 12, loss = 0.7696343660354614
In grad_steps = 13, loss = 0.6845820546150208
In grad_steps = 14, loss = 0.6730040311813354
In grad_steps = 15, loss = 0.6560288071632385
In grad_steps = 16, loss = 0.8765748739242554
In grad_steps = 17, loss = 0.7778900265693665
In grad_steps = 18, loss = 0.7292841672897339
In grad_steps = 19, loss = 0.6495487093925476
In grad_steps = 20, loss = 0.8472261428833008
In grad_steps = 21, loss = 0.719637930393219
In grad_steps = 22, loss = 0.713346004486084
In grad_steps = 23, loss = 0.6688369512557983
In grad_steps = 24, loss = 0.7423847913742065
In grad_steps = 25, loss = 0.716562032699585
In grad_steps = 26, loss = 0.6226153373718262
In grad_steps = 27, loss = 0.6405465006828308
In grad_steps = 28, loss = 0.6011167168617249
In grad_steps = 29, loss = 0.7949509620666504
In grad_steps = 30, loss = 0.7878979444503784
In grad_steps = 31, loss = 0.7657532691955566
In grad_steps = 32, loss = 0.6692814230918884
In grad_steps = 33, loss = 0.6712695360183716
In grad_steps = 34, loss = 0.7289281487464905
In grad_steps = 35, loss = 0.6819969415664673
In grad_steps = 36, loss = 0.6975507736206055
In grad_steps = 37, loss = 0.6831241250038147
In grad_steps = 38, loss = 0.683272123336792
In grad_steps = 39, loss = 0.6703354716300964
In grad_steps = 40, loss = 0.7411984801292419
In grad_steps = 41, loss = 0.6783519387245178
In grad_steps = 42, loss = 0.8357030749320984
In grad_steps = 43, loss = 0.6811509132385254
In grad_steps = 44, loss = 0.6939243078231812
In grad_steps = 45, loss = 0.7059435844421387
In grad_steps = 46, loss = 0.688184916973114
In grad_steps = 47, loss = 0.7057693600654602
In grad_steps = 48, loss = 0.6761347651481628
In grad_steps = 49, loss = 0.7014864087104797
In grad_steps = 50, loss = 0.7102381587028503
In grad_steps = 51, loss = 0.674892246723175
In grad_steps = 52, loss = 0.6670195460319519
In grad_steps = 53, loss = 0.7426992058753967
In grad_steps = 54, loss = 0.7311448454856873
In grad_steps = 55, loss = 0.6756108403205872
In grad_steps = 56, loss = 0.7000599503517151
In grad_steps = 57, loss = 0.6994423866271973
In grad_steps = 58, loss = 0.7121918201446533
In grad_steps = 59, loss = 0.7097682952880859
In grad_steps = 60, loss = 0.7106364369392395
In grad_steps = 61, loss = 0.6947289705276489
In grad_steps = 62, loss = 0.6928255558013916
In grad_steps = 63, loss = 0.6965305209159851
In grad_steps = 64, loss = 0.6925996541976929
In grad_steps = 65, loss = 0.7267327904701233
In grad_steps = 66, loss = 0.7222591042518616
In grad_steps = 67, loss = 0.6751267910003662
In grad_steps = 68, loss = 0.676936149597168
In grad_steps = 69, loss = 0.6824799180030823
In grad_steps = 70, loss = 0.7415202856063843
In grad_steps = 71, loss = 0.6556289196014404
In grad_steps = 72, loss = 0.7141674757003784
In grad_steps = 73, loss = 0.6987065076828003
In grad_steps = 74, loss = 0.6393305063247681
In grad_steps = 75, loss = 0.7140057682991028
In grad_steps = 76, loss = 0.7438777685165405
In grad_steps = 77, loss = 0.6788152456283569
In grad_steps = 78, loss = 0.7196559906005859
In grad_steps = 79, loss = 0.7315903902053833
In grad_steps = 80, loss = 0.6875396370887756
In grad_steps = 81, loss = 0.6947981119155884
In grad_steps = 82, loss = 0.6752868294715881
In grad_steps = 83, loss = 0.7337234616279602
In grad_steps = 84, loss = 0.7063632607460022
In grad_steps = 85, loss = 0.6907214522361755
In grad_steps = 86, loss = 0.6584104895591736
In grad_steps = 87, loss = 0.8267303705215454
In grad_steps = 88, loss = 0.7192950248718262
In grad_steps = 89, loss = 0.6630308628082275
In grad_steps = 90, loss = 0.6891323328018188
In grad_steps = 91, loss = 0.7059506773948669
In grad_steps = 92, loss = 0.6827714443206787
In grad_steps = 93, loss = 0.6986215710639954
In grad_steps = 94, loss = 0.6826192736625671
In grad_steps = 95, loss = 0.6567689776420593
In grad_steps = 96, loss = 0.7377795577049255
In grad_steps = 97, loss = 0.7390258312225342
In grad_steps = 98, loss = 0.7443662285804749
In grad_steps = 99, loss = 0.6570595502853394
In grad_steps = 100, loss = 0.6567848324775696
In grad_steps = 101, loss = 0.7461643218994141
In grad_steps = 102, loss = 0.683479905128479
In grad_steps = 103, loss = 0.7103269100189209
In grad_steps = 104, loss = 0.6693474650382996
In grad_steps = 105, loss = 0.7141497731208801
In grad_steps = 106, loss = 0.689340353012085
In grad_steps = 107, loss = 0.6896080374717712
In grad_steps = 108, loss = 0.6774241924285889
In grad_steps = 109, loss = 0.7421251535415649
In grad_steps = 110, loss = 0.6911964416503906
In grad_steps = 111, loss = 0.692176103591919
In grad_steps = 112, loss = 0.7026753425598145
In grad_steps = 113, loss = 0.6900017261505127
In grad_steps = 114, loss = 0.6864948272705078
In grad_steps = 115, loss = 0.6836134195327759
In grad_steps = 116, loss = 0.6967834830284119
In grad_steps = 117, loss = 0.6948137283325195
In grad_steps = 118, loss = 0.669719934463501
In grad_steps = 119, loss = 0.6933519840240479
In grad_steps = 120, loss = 0.7045584917068481
In grad_steps = 121, loss = 0.71407550573349
In grad_steps = 122, loss = 0.6848408579826355
In grad_steps = 123, loss = 0.6983113288879395
In grad_steps = 124, loss = 0.6963608860969543
In grad_steps = 125, loss = 0.7076680660247803
In grad_steps = 126, loss = 0.7088839411735535
In grad_steps = 127, loss = 0.7083266377449036
In grad_steps = 128, loss = 0.6864631175994873
In grad_steps = 129, loss = 0.6946683526039124
In grad_steps = 130, loss = 0.692419707775116
In grad_steps = 131, loss = 0.6928077936172485
In grad_steps = 132, loss = 0.701630175113678
In grad_steps = 133, loss = 0.6800916194915771
In grad_steps = 134, loss = 0.6649701595306396
In grad_steps = 135, loss = 0.6726467609405518
In grad_steps = 136, loss = 0.6790103912353516
In grad_steps = 137, loss = 0.7532387971878052
In grad_steps = 138, loss = 0.6506574153900146
In grad_steps = 139, loss = 0.7113694548606873
In grad_steps = 140, loss = 0.696990430355072
In grad_steps = 141, loss = 0.6350581645965576
In grad_steps = 142, loss = 0.7084053754806519
In grad_steps = 143, loss = 0.7377080917358398
In grad_steps = 144, loss = 0.6695523858070374
In grad_steps = 145, loss = 0.7017971277236938
In grad_steps = 146, loss = 0.6983303427696228
In grad_steps = 147, loss = 0.6669047474861145
In grad_steps = 148, loss = 0.6580110192298889
In grad_steps = 149, loss = 0.6491656303405762
In grad_steps = 150, loss = 0.7655841112136841
In grad_steps = 151, loss = 0.7094954252243042
In grad_steps = 152, loss = 0.6953160762786865
In grad_steps = 153, loss = 0.6503146886825562
In grad_steps = 154, loss = 0.8312529921531677
In grad_steps = 155, loss = 0.7021920680999756
In grad_steps = 156, loss = 0.6768918633460999
In grad_steps = 157, loss = 0.6548234224319458
In grad_steps = 158, loss = 0.7237585783004761
In grad_steps = 159, loss = 0.6936948299407959
In grad_steps = 160, loss = 0.6366459727287292
In grad_steps = 161, loss = 0.6374302506446838
In grad_steps = 162, loss = 0.588443398475647
In grad_steps = 163, loss = 0.8213353157043457
In grad_steps = 164, loss = 0.8007650971412659
In grad_steps = 165, loss = 0.7599369287490845
In grad_steps = 166, loss = 0.6476299166679382
In grad_steps = 167, loss = 0.6391516327857971
In grad_steps = 168, loss = 0.704977810382843
In grad_steps = 169, loss = 0.6701909303665161
In grad_steps = 170, loss = 0.6855261325836182
In grad_steps = 171, loss = 0.6820725798606873
In grad_steps = 172, loss = 0.6738084554672241
In grad_steps = 173, loss = 0.6652462482452393
In grad_steps = 174, loss = 0.7277812957763672
In grad_steps = 175, loss = 0.6706224083900452
In grad_steps = 176, loss = 0.7786677479743958
In grad_steps = 177, loss = 0.6758233904838562
In grad_steps = 178, loss = 0.6906461715698242
In grad_steps = 179, loss = 0.6982874870300293
In grad_steps = 180, loss = 0.6813687682151794
In grad_steps = 181, loss = 0.7151127457618713
In grad_steps = 182, loss = 0.6501739025115967
In grad_steps = 183, loss = 0.688117265701294
In grad_steps = 184, loss = 0.7164394855499268
In grad_steps = 185, loss = 0.6670798063278198
In grad_steps = 186, loss = 0.6652528047561646
In grad_steps = 187, loss = 0.7299763560295105
In grad_steps = 188, loss = 0.7205184698104858
In grad_steps = 189, loss = 0.6780471205711365
In grad_steps = 190, loss = 0.6851896047592163
In grad_steps = 191, loss = 0.6944307088851929
In grad_steps = 192, loss = 0.6973430514335632
In grad_steps = 193, loss = 0.7000424861907959
In grad_steps = 194, loss = 0.6955017447471619
In grad_steps = 195, loss = 0.7010464072227478
In grad_steps = 196, loss = 0.6701189279556274
In grad_steps = 197, loss = 0.6862180233001709
In grad_steps = 198, loss = 0.6807024478912354
In grad_steps = 199, loss = 0.729459285736084
In grad_steps = 200, loss = 0.6954993009567261
In grad_steps = 201, loss = 0.6574458479881287
In grad_steps = 202, loss = 0.6677272915840149
In grad_steps = 203, loss = 0.6726659536361694
In grad_steps = 204, loss = 0.7690921425819397
In grad_steps = 205, loss = 0.63713538646698
In grad_steps = 206, loss = 0.7122883200645447
In grad_steps = 207, loss = 0.6939352750778198
In grad_steps = 208, loss = 0.6120858192443848
In grad_steps = 209, loss = 0.7112571597099304
In grad_steps = 210, loss = 0.731003999710083
In grad_steps = 211, loss = 0.6490303874015808
In grad_steps = 212, loss = 0.6730241775512695
In grad_steps = 213, loss = 0.663112998008728
In grad_steps = 214, loss = 0.634584367275238
In grad_steps = 215, loss = 0.60331791639328
In grad_steps = 216, loss = 0.5885995030403137
In grad_steps = 217, loss = 0.8299727439880371
In grad_steps = 218, loss = 0.6998305916786194
In grad_steps = 219, loss = 0.6854247450828552
In grad_steps = 220, loss = 0.6534470319747925
In grad_steps = 221, loss = 0.691246509552002
In grad_steps = 222, loss = 0.6157382130622864
In grad_steps = 223, loss = 0.7944621443748474
In grad_steps = 224, loss = 0.6422522068023682
In grad_steps = 225, loss = 0.734248697757721
In grad_steps = 226, loss = 0.6766079068183899
In grad_steps = 227, loss = 0.6785173416137695
In grad_steps = 228, loss = 0.6791684627532959
In grad_steps = 229, loss = 0.6759569644927979
In grad_steps = 230, loss = 0.6804285645484924
In grad_steps = 231, loss = 0.6947928667068481
In grad_steps = 232, loss = 0.689052164554596
In grad_steps = 233, loss = 0.636077344417572
In grad_steps = 234, loss = 0.5840958952903748
In grad_steps = 235, loss = 0.6964840888977051
In grad_steps = 236, loss = 0.6283844113349915
In grad_steps = 237, loss = 0.6883057355880737
In grad_steps = 238, loss = 0.6148890256881714
In grad_steps = 239, loss = 0.63787841796875
In grad_steps = 240, loss = 0.6157687902450562
In grad_steps = 241, loss = 0.7576203346252441
In grad_steps = 242, loss = 0.5991162061691284
In grad_steps = 243, loss = 0.7615218162536621
In grad_steps = 244, loss = 0.7232747673988342
In grad_steps = 245, loss = 0.7017556428909302
In grad_steps = 246, loss = 0.728931188583374
In grad_steps = 247, loss = 0.6619665622711182
In grad_steps = 248, loss = 0.6942643523216248
In grad_steps = 249, loss = 0.6013868451118469
In grad_steps = 250, loss = 0.6116238832473755
In grad_steps = 251, loss = 0.6790009140968323
In grad_steps = 252, loss = 0.5846942663192749
In grad_steps = 253, loss = 0.6710382699966431
In grad_steps = 254, loss = 0.6725450754165649
In grad_steps = 255, loss = 0.7650034427642822
In grad_steps = 256, loss = 0.684851348400116
In grad_steps = 257, loss = 0.6323121786117554
In grad_steps = 258, loss = 0.698152482509613
In grad_steps = 259, loss = 0.6582321524620056
In grad_steps = 260, loss = 0.6859185099601746
In grad_steps = 261, loss = 0.6782528162002563
In grad_steps = 262, loss = 0.6703301668167114
In grad_steps = 263, loss = 0.6850643754005432
In grad_steps = 264, loss = 0.6736496686935425
In grad_steps = 265, loss = 0.6831085681915283
In grad_steps = 266, loss = 0.658901572227478
In grad_steps = 267, loss = 0.6527432203292847
i = 1, Test ensemble probabilities = 
[array([[0.34539714, 0.6546028 ],
       [0.49670178, 0.5032982 ],
       [0.2995092 , 0.70049083],
       [0.5216206 , 0.4783794 ],
       [0.4783175 , 0.5216825 ],
       [0.37964696, 0.620353  ],
       [0.5360174 , 0.4639826 ],
       [0.41234112, 0.5876589 ],
       [0.2805267 , 0.71947336],
       [0.3660757 , 0.6339243 ],
       [0.31246424, 0.68753576],
       [0.5163459 , 0.4836541 ],
       [0.37822625, 0.6217737 ],
       [0.43147096, 0.56852907],
       [0.54082584, 0.4591742 ],
       [0.5715256 , 0.4284744 ],
       [0.4142887 , 0.5857113 ],
       [0.37641716, 0.6235828 ],
       [0.46707872, 0.5329213 ],
       [0.4730467 , 0.5269533 ],
       [0.4055867 , 0.59441334],
       [0.4146544 , 0.5853456 ],
       [0.36647767, 0.6335223 ],
       [0.45998093, 0.5400191 ],
       [0.31840318, 0.6815968 ],
       [0.24469352, 0.75530654],
       [0.44522896, 0.55477107],
       [0.3880463 , 0.61195374],
       [0.37943542, 0.6205646 ],
       [0.2172548 , 0.7827452 ],
       [0.4649846 , 0.5350154 ],
       [0.4034645 , 0.59653556],
       [0.45316827, 0.5468317 ],
       [0.41054946, 0.58945054],
       [0.38889134, 0.61110866],
       [0.30868906, 0.691311  ],
       [0.48083103, 0.519169  ],
       [0.55902576, 0.44097418],
       [0.4085316 , 0.5914684 ],
       [0.41088137, 0.5891186 ],
       [0.4298178 , 0.5701822 ],
       [0.29877663, 0.70122343],
       [0.3293201 , 0.6706799 ],
       [0.42829737, 0.5717026 ],
       [0.49717334, 0.5028267 ],
       [0.52718437, 0.4728156 ],
       [0.42836556, 0.5716345 ],
       [0.4556835 , 0.54431653],
       [0.3669375 , 0.63306254],
       [0.4527557 , 0.5472443 ],
       [0.2731709 , 0.7268291 ],
       [0.42771834, 0.5722816 ],
       [0.38610366, 0.61389637],
       [0.3611515 , 0.6388485 ],
       [0.39574474, 0.60425526],
       [0.48206717, 0.51793283],
       [0.4352225 , 0.56477743],
       [0.5293537 , 0.47064632],
       [0.4351939 , 0.56480604],
       [0.39494088, 0.6050591 ],
       [0.29629105, 0.703709  ],
       [0.39273342, 0.60726655],
       [0.37942597, 0.620574  ],
       [0.40392777, 0.59607226],
       [0.42278576, 0.57721424],
       [0.42321032, 0.5767897 ],
       [0.3290817 , 0.6709183 ],
       [0.38383117, 0.6161688 ],
       [0.4492992 , 0.55070084],
       [0.41927034, 0.58072966],
       [0.38925087, 0.6107492 ],
       [0.4150021 , 0.5849979 ],
       [0.5914758 , 0.40852422],
       [0.4447122 , 0.5552878 ],
       [0.5072293 , 0.4927707 ],
       [0.5590465 , 0.44095352],
       [0.4567155 , 0.54328454],
       [0.26308432, 0.73691565],
       [0.47329345, 0.5267065 ],
       [0.5217634 , 0.47823662],
       [0.2883945 , 0.71160555],
       [0.48484313, 0.51515687],
       [0.785682  , 0.21431798],
       [0.44177085, 0.55822915],
       [0.4268143 , 0.57318574],
       [0.3282022 , 0.6717979 ],
       [0.27888694, 0.721113  ],
       [0.43272623, 0.56727374],
       [0.48700717, 0.5129928 ],
       [0.24735552, 0.7526445 ],
       [0.3437499 , 0.65625006],
       [0.47496793, 0.52503204],
       [0.35789973, 0.64210033],
       [0.43090022, 0.5690998 ],
       [0.40508464, 0.5949154 ],
       [0.4418508 , 0.5581492 ],
       [0.50487167, 0.49512833],
       [0.5045017 , 0.4954983 ],
       [0.33627528, 0.6637247 ],
       [0.43236625, 0.5676338 ],
       [0.4723027 , 0.5276973 ],
       [0.55153644, 0.44846356],
       [0.410058  , 0.58994204],
       [0.43052799, 0.5694721 ],
       [0.45123821, 0.54876184],
       [0.35328087, 0.6467191 ],
       [0.45968103, 0.54031897],
       [0.33205697, 0.66794306],
       [0.5815101 , 0.41848987],
       [0.55743647, 0.44256356],
       [0.4615889 , 0.53841114],
       [0.5988489 , 0.40115115],
       [0.26768813, 0.73231184],
       [0.4092803 , 0.5907197 ],
       [0.44905618, 0.55094385],
       [0.40612987, 0.5938701 ],
       [0.4797793 , 0.5202207 ],
       [0.490779  , 0.50922096],
       [0.50890255, 0.49109742],
       [0.46478626, 0.5352137 ],
       [0.39073965, 0.6092604 ],
       [0.6140519 , 0.38594812],
       [0.21095832, 0.78904164],
       [0.50982255, 0.49017748],
       [0.33999068, 0.6600093 ],
       [0.4652964 , 0.5347036 ],
       [0.4872016 , 0.51279837],
       [0.3138431 , 0.6861569 ],
       [0.5067988 , 0.49320123],
       [0.41474557, 0.5852544 ],
       [0.4137968 , 0.58620316],
       [0.6387266 , 0.3612734 ],
       [0.29403043, 0.7059696 ],
       [0.46303543, 0.5369646 ]], dtype=float32), array([[0.3880703 , 0.6119297 ],
       [0.51199114, 0.48800892],
       [0.3143604 , 0.68563956],
       [0.46775058, 0.53224945],
       [0.4228081 , 0.5771919 ],
       [0.46305248, 0.5369475 ],
       [0.5530051 , 0.4469949 ],
       [0.40783724, 0.5921628 ],
       [0.34845847, 0.65154153],
       [0.42968783, 0.5703122 ],
       [0.36605135, 0.6339487 ],
       [0.52814126, 0.4718587 ],
       [0.4303292 , 0.5696708 ],
       [0.5311472 , 0.46885288],
       [0.54523605, 0.45476392],
       [0.54236245, 0.45763755],
       [0.49351203, 0.50648797],
       [0.38893303, 0.61106694],
       [0.4891141 , 0.5108859 ],
       [0.56814957, 0.43185046],
       [0.48065817, 0.5193418 ],
       [0.4705589 , 0.5294411 ],
       [0.48349407, 0.51650596],
       [0.4876348 , 0.5123652 ],
       [0.3791615 , 0.62083846],
       [0.37122786, 0.62877214],
       [0.43571815, 0.5642818 ],
       [0.41554213, 0.5844579 ],
       [0.4376437 , 0.5623563 ],
       [0.2725442 , 0.72745574],
       [0.48336023, 0.51663977],
       [0.41728634, 0.58271366],
       [0.44753623, 0.55246377],
       [0.45130196, 0.54869807],
       [0.47825754, 0.52174246],
       [0.3706702 , 0.6293298 ],
       [0.5130376 , 0.48696244],
       [0.5888656 , 0.41113442],
       [0.49567425, 0.50432575],
       [0.4533961 , 0.5466039 ],
       [0.46075943, 0.53924054],
       [0.299956  , 0.70004404],
       [0.40043336, 0.59956664],
       [0.46529213, 0.5347079 ],
       [0.50553155, 0.49446842],
       [0.47945037, 0.5205496 ],
       [0.47140384, 0.5285961 ],
       [0.520463  , 0.479537  ],
       [0.4369172 , 0.5630828 ],
       [0.47227037, 0.52772963],
       [0.39890897, 0.6010911 ],
       [0.5099803 , 0.4900197 ],
       [0.44546404, 0.554536  ],
       [0.42215705, 0.577843  ],
       [0.3576284 , 0.64237154],
       [0.4861414 , 0.5138586 ],
       [0.4707984 , 0.5292016 ],
       [0.49237502, 0.50762504],
       [0.50471   , 0.49528992],
       [0.44178778, 0.5582122 ],
       [0.37154227, 0.6284577 ],
       [0.40878373, 0.59121627],
       [0.41240627, 0.58759373],
       [0.4968262 , 0.50317377],
       [0.47246245, 0.5275376 ],
       [0.43943956, 0.5605604 ],
       [0.3889983 , 0.6110017 ],
       [0.41751316, 0.58248687],
       [0.47119093, 0.5288091 ],
       [0.3927289 , 0.60727113],
       [0.49005207, 0.5099479 ],
       [0.41542542, 0.5845746 ],
       [0.62426174, 0.37573823],
       [0.4453439 , 0.5546561 ],
       [0.5046319 , 0.49536812],
       [0.57116544, 0.42883456],
       [0.46410406, 0.53589594],
       [0.27322844, 0.72677153],
       [0.48610952, 0.5138905 ],
       [0.5164864 , 0.48351362],
       [0.37881854, 0.6211814 ],
       [0.5108697 , 0.48913032],
       [0.6448277 , 0.3551723 ],
       [0.429549  , 0.57045096],
       [0.43403113, 0.5659689 ],
       [0.3908836 , 0.60911644],
       [0.33232817, 0.6676718 ],
       [0.57920706, 0.42079297],
       [0.47585174, 0.5241482 ],
       [0.30917242, 0.6908276 ],
       [0.3856376 , 0.61436236],
       [0.40961602, 0.590384  ],
       [0.39530924, 0.6046908 ],
       [0.47819042, 0.5218095 ],
       [0.49915648, 0.5008435 ],
       [0.50102425, 0.49897575],
       [0.4867456 , 0.5132544 ],
       [0.46412638, 0.53587365],
       [0.39740893, 0.6025911 ],
       [0.5355354 , 0.46446458],
       [0.46589816, 0.53410184],
       [0.6296572 , 0.3703428 ],
       [0.41445395, 0.5855461 ],
       [0.4707105 , 0.52928954],
       [0.479547  , 0.52045304],
       [0.3625186 , 0.63748145],
       [0.44902927, 0.55097073],
       [0.3706048 , 0.6293952 ],
       [0.5403005 , 0.45969948],
       [0.58481735, 0.4151827 ],
       [0.4457463 , 0.55425376],
       [0.55166525, 0.44833475],
       [0.35043052, 0.6495695 ],
       [0.38592914, 0.61407083],
       [0.49783325, 0.50216675],
       [0.41118985, 0.5888102 ],
       [0.5627369 , 0.43726304],
       [0.4429057 , 0.55709434],
       [0.42174813, 0.5782519 ],
       [0.52862275, 0.47137725],
       [0.39108476, 0.6089152 ],
       [0.5507947 , 0.44920528],
       [0.25938937, 0.74061066],
       [0.5540879 , 0.44591215],
       [0.3860679 , 0.6139321 ],
       [0.5129485 , 0.48705152],
       [0.4602273 , 0.53977275],
       [0.35588855, 0.64411145],
       [0.5478818 , 0.45211825],
       [0.4290892 , 0.5709108 ],
       [0.38805577, 0.6119442 ],
       [0.6379908 , 0.3620092 ],
       [0.37617096, 0.62382907],
       [0.46227485, 0.53772515]], dtype=float32)]
i = 1, Test true class= 
[1 0 0 0 0 1 1 0 1 0 0 0 1 1 1 1 0 1 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 0 1 1 1
 1 1 0 0 0 0 0 0 0 1 0 1 0 1 0 1 0 0 0 0 1 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 1
 1 0 1 1 1 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 1 1 0 0 0 0 1 1 0
 0 0 1 1 0 0 0 0 1 0 0 1 0 0 0 0 1 1 0 1 1 0 0]
In grad_steps = 0, loss = 1.0984740257263184
In grad_steps = 1, loss = 0.7536655068397522
In grad_steps = 2, loss = 0.7097871899604797
In grad_steps = 3, loss = 0.6895343065261841
In grad_steps = 4, loss = 0.96734219789505
In grad_steps = 5, loss = 0.7311791181564331
In grad_steps = 6, loss = 0.7080577611923218
In grad_steps = 7, loss = 0.7018312215805054
In grad_steps = 8, loss = 0.7075697779655457
In grad_steps = 9, loss = 0.7807775735855103
In grad_steps = 10, loss = 0.6966695189476013
In grad_steps = 11, loss = 0.7411905527114868
In grad_steps = 12, loss = 0.7597598433494568
In grad_steps = 13, loss = 0.6885761022567749
In grad_steps = 14, loss = 0.6645247936248779
In grad_steps = 15, loss = 0.6520309448242188
In grad_steps = 16, loss = 0.8917779922485352
In grad_steps = 17, loss = 0.7888872027397156
In grad_steps = 18, loss = 0.727520227432251
In grad_steps = 19, loss = 0.6520497798919678
In grad_steps = 20, loss = 0.8402814865112305
In grad_steps = 21, loss = 0.7193410396575928
In grad_steps = 22, loss = 0.7222328782081604
In grad_steps = 23, loss = 0.6672329902648926
In grad_steps = 24, loss = 0.747209906578064
In grad_steps = 25, loss = 0.7202641367912292
In grad_steps = 26, loss = 0.6184583306312561
In grad_steps = 27, loss = 0.6398696303367615
In grad_steps = 28, loss = 0.5998815894126892
In grad_steps = 29, loss = 0.7981279492378235
In grad_steps = 30, loss = 0.7888866066932678
In grad_steps = 31, loss = 0.7681913375854492
In grad_steps = 32, loss = 0.6647467613220215
In grad_steps = 33, loss = 0.6717265844345093
In grad_steps = 34, loss = 0.7282145023345947
In grad_steps = 35, loss = 0.6794800758361816
In grad_steps = 36, loss = 0.6956429481506348
In grad_steps = 37, loss = 0.6858524680137634
In grad_steps = 38, loss = 0.6826626062393188
In grad_steps = 39, loss = 0.6697537899017334
In grad_steps = 40, loss = 0.7458122968673706
In grad_steps = 41, loss = 0.6816704869270325
In grad_steps = 42, loss = 0.8406667113304138
In grad_steps = 43, loss = 0.6796578168869019
In grad_steps = 44, loss = 0.6956460475921631
In grad_steps = 45, loss = 0.7041513323783875
In grad_steps = 46, loss = 0.6870683431625366
In grad_steps = 47, loss = 0.7096362113952637
In grad_steps = 48, loss = 0.673103392124176
In grad_steps = 49, loss = 0.7043824195861816
In grad_steps = 50, loss = 0.7142369747161865
In grad_steps = 51, loss = 0.6759647727012634
In grad_steps = 52, loss = 0.6662759780883789
In grad_steps = 53, loss = 0.7444260120391846
In grad_steps = 54, loss = 0.7337893843650818
In grad_steps = 55, loss = 0.674976646900177
In grad_steps = 56, loss = 0.698560357093811
In grad_steps = 57, loss = 0.7014384865760803
In grad_steps = 58, loss = 0.7094176411628723
In grad_steps = 59, loss = 0.7122442722320557
In grad_steps = 60, loss = 0.7089365720748901
In grad_steps = 61, loss = 0.6988998651504517
In grad_steps = 62, loss = 0.6942952275276184
In grad_steps = 63, loss = 0.6988952159881592
In grad_steps = 64, loss = 0.689371645450592
In grad_steps = 65, loss = 0.7251190543174744
In grad_steps = 66, loss = 0.7214373350143433
In grad_steps = 67, loss = 0.6766600608825684
In grad_steps = 68, loss = 0.6790449619293213
In grad_steps = 69, loss = 0.6804254651069641
In grad_steps = 70, loss = 0.7412782311439514
In grad_steps = 71, loss = 0.657866895198822
In grad_steps = 72, loss = 0.7146416902542114
In grad_steps = 73, loss = 0.6984066963195801
In grad_steps = 74, loss = 0.6383746266365051
In grad_steps = 75, loss = 0.7171608209609985
In grad_steps = 76, loss = 0.7426797747612
In grad_steps = 77, loss = 0.6805797815322876
In grad_steps = 78, loss = 0.7221618890762329
In grad_steps = 79, loss = 0.7344751954078674
In grad_steps = 80, loss = 0.6866982579231262
In grad_steps = 81, loss = 0.6980330944061279
In grad_steps = 82, loss = 0.6784054636955261
In grad_steps = 83, loss = 0.729047417640686
In grad_steps = 84, loss = 0.7074926495552063
In grad_steps = 85, loss = 0.6913983821868896
In grad_steps = 86, loss = 0.6589987874031067
In grad_steps = 87, loss = 0.8252895474433899
In grad_steps = 88, loss = 0.7182044982910156
In grad_steps = 89, loss = 0.6648359298706055
In grad_steps = 90, loss = 0.6912626624107361
In grad_steps = 91, loss = 0.7078127861022949
In grad_steps = 92, loss = 0.6852378249168396
In grad_steps = 93, loss = 0.6975386738777161
In grad_steps = 94, loss = 0.6823768615722656
In grad_steps = 95, loss = 0.6557105779647827
In grad_steps = 96, loss = 0.7406200766563416
In grad_steps = 97, loss = 0.7409679889678955
In grad_steps = 98, loss = 0.7429504990577698
In grad_steps = 99, loss = 0.6549161076545715
In grad_steps = 100, loss = 0.6546434164047241
In grad_steps = 101, loss = 0.7456549406051636
In grad_steps = 102, loss = 0.6805475354194641
In grad_steps = 103, loss = 0.7065103650093079
In grad_steps = 104, loss = 0.6689759492874146
In grad_steps = 105, loss = 0.7124602198600769
In grad_steps = 106, loss = 0.6878069043159485
In grad_steps = 107, loss = 0.6901997923851013
In grad_steps = 108, loss = 0.6784746646881104
In grad_steps = 109, loss = 0.7400308847427368
In grad_steps = 110, loss = 0.6916171908378601
In grad_steps = 111, loss = 0.6881954073905945
In grad_steps = 112, loss = 0.703205406665802
In grad_steps = 113, loss = 0.6877504587173462
In grad_steps = 114, loss = 0.6863131523132324
In grad_steps = 115, loss = 0.691135823726654
In grad_steps = 116, loss = 0.6939315795898438
In grad_steps = 117, loss = 0.6931520104408264
In grad_steps = 118, loss = 0.6750449538230896
In grad_steps = 119, loss = 0.6939911842346191
In grad_steps = 120, loss = 0.7006797194480896
In grad_steps = 121, loss = 0.7105903029441833
In grad_steps = 122, loss = 0.6839287281036377
In grad_steps = 123, loss = 0.696848452091217
In grad_steps = 124, loss = 0.6978970170021057
In grad_steps = 125, loss = 0.7068384289741516
In grad_steps = 126, loss = 0.7096690535545349
In grad_steps = 127, loss = 0.7097409963607788
In grad_steps = 128, loss = 0.6875213980674744
In grad_steps = 129, loss = 0.6965903043746948
In grad_steps = 130, loss = 0.694709300994873
In grad_steps = 131, loss = 0.6915774345397949
In grad_steps = 132, loss = 0.7000569105148315
In grad_steps = 133, loss = 0.6783068180084229
In grad_steps = 134, loss = 0.6685766577720642
In grad_steps = 135, loss = 0.6722821593284607
In grad_steps = 136, loss = 0.6794687509536743
In grad_steps = 137, loss = 0.7517439126968384
In grad_steps = 138, loss = 0.6539912223815918
In grad_steps = 139, loss = 0.712914764881134
In grad_steps = 140, loss = 0.6963692307472229
In grad_steps = 141, loss = 0.6366596221923828
In grad_steps = 142, loss = 0.7114760875701904
In grad_steps = 143, loss = 0.7328637838363647
In grad_steps = 144, loss = 0.6716840267181396
In grad_steps = 145, loss = 0.7025458216667175
In grad_steps = 146, loss = 0.7032421231269836
In grad_steps = 147, loss = 0.6692027449607849
In grad_steps = 148, loss = 0.6671950817108154
In grad_steps = 149, loss = 0.6546918153762817
In grad_steps = 150, loss = 0.7532976269721985
In grad_steps = 151, loss = 0.7071272134780884
In grad_steps = 152, loss = 0.6946868896484375
In grad_steps = 153, loss = 0.6514344215393066
In grad_steps = 154, loss = 0.8298736214637756
In grad_steps = 155, loss = 0.7000011801719666
In grad_steps = 156, loss = 0.6784297823905945
In grad_steps = 157, loss = 0.6599758863449097
In grad_steps = 158, loss = 0.7299742698669434
In grad_steps = 159, loss = 0.6986734867095947
In grad_steps = 160, loss = 0.6457144618034363
In grad_steps = 161, loss = 0.6390477418899536
In grad_steps = 162, loss = 0.5922296643257141
In grad_steps = 163, loss = 0.8139516711235046
In grad_steps = 164, loss = 0.7939172983169556
In grad_steps = 165, loss = 0.753366231918335
In grad_steps = 166, loss = 0.6405888795852661
In grad_steps = 167, loss = 0.6392199397087097
In grad_steps = 168, loss = 0.7067580223083496
In grad_steps = 169, loss = 0.6674737334251404
In grad_steps = 170, loss = 0.6845788955688477
In grad_steps = 171, loss = 0.6795960068702698
In grad_steps = 172, loss = 0.6667739152908325
In grad_steps = 173, loss = 0.6577795147895813
In grad_steps = 174, loss = 0.7274067401885986
In grad_steps = 175, loss = 0.6697303056716919
In grad_steps = 176, loss = 0.7880167365074158
In grad_steps = 177, loss = 0.6819362640380859
In grad_steps = 178, loss = 0.6896340847015381
In grad_steps = 179, loss = 0.7019867300987244
In grad_steps = 180, loss = 0.6788122057914734
In grad_steps = 181, loss = 0.7053897976875305
In grad_steps = 182, loss = 0.6590045094490051
In grad_steps = 183, loss = 0.6849889755249023
In grad_steps = 184, loss = 0.7151862382888794
In grad_steps = 185, loss = 0.664018452167511
In grad_steps = 186, loss = 0.6631349921226501
In grad_steps = 187, loss = 0.7318202257156372
In grad_steps = 188, loss = 0.7185449600219727
In grad_steps = 189, loss = 0.6774740219116211
In grad_steps = 190, loss = 0.6824819445610046
In grad_steps = 191, loss = 0.6927013397216797
In grad_steps = 192, loss = 0.6995028257369995
In grad_steps = 193, loss = 0.7069900035858154
In grad_steps = 194, loss = 0.6975548267364502
In grad_steps = 195, loss = 0.6996142864227295
In grad_steps = 196, loss = 0.6697208881378174
In grad_steps = 197, loss = 0.6856720447540283
In grad_steps = 198, loss = 0.6847675442695618
In grad_steps = 199, loss = 0.7200236320495605
In grad_steps = 200, loss = 0.7022504806518555
In grad_steps = 201, loss = 0.6593071222305298
In grad_steps = 202, loss = 0.6645635366439819
In grad_steps = 203, loss = 0.6666882634162903
In grad_steps = 204, loss = 0.7521546483039856
In grad_steps = 205, loss = 0.642898440361023
In grad_steps = 206, loss = 0.7081210613250732
In grad_steps = 207, loss = 0.694007158279419
In grad_steps = 208, loss = 0.6126322746276855
In grad_steps = 209, loss = 0.7052750587463379
In grad_steps = 210, loss = 0.7283754348754883
In grad_steps = 211, loss = 0.6428276300430298
In grad_steps = 212, loss = 0.6818986535072327
In grad_steps = 213, loss = 0.6668142080307007
In grad_steps = 214, loss = 0.6278194189071655
In grad_steps = 215, loss = 0.604986846446991
In grad_steps = 216, loss = 0.5847418904304504
In grad_steps = 217, loss = 0.8452701568603516
In grad_steps = 218, loss = 0.7013081908226013
In grad_steps = 219, loss = 0.6919935941696167
In grad_steps = 220, loss = 0.6475080251693726
In grad_steps = 221, loss = 0.6894033551216125
In grad_steps = 222, loss = 0.6144883036613464
In grad_steps = 223, loss = 0.7820684909820557
In grad_steps = 224, loss = 0.6381053924560547
In grad_steps = 225, loss = 0.7495389580726624
In grad_steps = 226, loss = 0.6839292645454407
In grad_steps = 227, loss = 0.6542435884475708
In grad_steps = 228, loss = 0.6661475300788879
In grad_steps = 229, loss = 0.6722310781478882
In grad_steps = 230, loss = 0.671782374382019
In grad_steps = 231, loss = 0.6853246688842773
In grad_steps = 232, loss = 0.666832685470581
In grad_steps = 233, loss = 0.638388991355896
In grad_steps = 234, loss = 0.5919157266616821
In grad_steps = 235, loss = 0.6903408169746399
In grad_steps = 236, loss = 0.6204785108566284
In grad_steps = 237, loss = 0.6810478568077087
In grad_steps = 238, loss = 0.5994618535041809
In grad_steps = 239, loss = 0.6331250667572021
In grad_steps = 240, loss = 0.605994462966919
In grad_steps = 241, loss = 0.7164251804351807
In grad_steps = 242, loss = 0.6149460077285767
In grad_steps = 243, loss = 0.7212561368942261
In grad_steps = 244, loss = 0.7631604671478271
In grad_steps = 245, loss = 0.7074369788169861
In grad_steps = 246, loss = 0.73590087890625
In grad_steps = 247, loss = 0.6345217823982239
In grad_steps = 248, loss = 0.6555437445640564
In grad_steps = 249, loss = 0.6139428019523621
In grad_steps = 250, loss = 0.6352361440658569
In grad_steps = 251, loss = 0.6951671242713928
In grad_steps = 252, loss = 0.5733729004859924
In grad_steps = 253, loss = 0.6620346903800964
In grad_steps = 254, loss = 0.6748011708259583
In grad_steps = 255, loss = 0.769683301448822
In grad_steps = 256, loss = 0.677517294883728
In grad_steps = 257, loss = 0.6244731545448303
In grad_steps = 258, loss = 0.6847338676452637
In grad_steps = 259, loss = 0.6809592247009277
In grad_steps = 260, loss = 0.6814775466918945
In grad_steps = 261, loss = 0.6815483570098877
In grad_steps = 262, loss = 0.6943686008453369
In grad_steps = 263, loss = 0.6513043642044067
In grad_steps = 264, loss = 0.6295872926712036
In grad_steps = 265, loss = 0.6637954711914062
In grad_steps = 266, loss = 0.6891807913780212
In grad_steps = 267, loss = 0.6797506809234619
i = 2, Test ensemble probabilities = 
[array([[0.34539714, 0.6546028 ],
       [0.49670178, 0.5032982 ],
       [0.2995092 , 0.70049083],
       [0.5216206 , 0.4783794 ],
       [0.4783175 , 0.5216825 ],
       [0.37964696, 0.620353  ],
       [0.5360174 , 0.4639826 ],
       [0.41234112, 0.5876589 ],
       [0.2805267 , 0.71947336],
       [0.3660757 , 0.6339243 ],
       [0.31246424, 0.68753576],
       [0.5163459 , 0.4836541 ],
       [0.37822625, 0.6217737 ],
       [0.43147096, 0.56852907],
       [0.54082584, 0.4591742 ],
       [0.5715256 , 0.4284744 ],
       [0.4142887 , 0.5857113 ],
       [0.37641716, 0.6235828 ],
       [0.46707872, 0.5329213 ],
       [0.4730467 , 0.5269533 ],
       [0.4055867 , 0.59441334],
       [0.4146544 , 0.5853456 ],
       [0.36647767, 0.6335223 ],
       [0.45998093, 0.5400191 ],
       [0.31840318, 0.6815968 ],
       [0.24469352, 0.75530654],
       [0.44522896, 0.55477107],
       [0.3880463 , 0.61195374],
       [0.37943542, 0.6205646 ],
       [0.2172548 , 0.7827452 ],
       [0.4649846 , 0.5350154 ],
       [0.4034645 , 0.59653556],
       [0.45316827, 0.5468317 ],
       [0.41054946, 0.58945054],
       [0.38889134, 0.61110866],
       [0.30868906, 0.691311  ],
       [0.48083103, 0.519169  ],
       [0.55902576, 0.44097418],
       [0.4085316 , 0.5914684 ],
       [0.41088137, 0.5891186 ],
       [0.4298178 , 0.5701822 ],
       [0.29877663, 0.70122343],
       [0.3293201 , 0.6706799 ],
       [0.42829737, 0.5717026 ],
       [0.49717334, 0.5028267 ],
       [0.52718437, 0.4728156 ],
       [0.42836556, 0.5716345 ],
       [0.4556835 , 0.54431653],
       [0.3669375 , 0.63306254],
       [0.4527557 , 0.5472443 ],
       [0.2731709 , 0.7268291 ],
       [0.42771834, 0.5722816 ],
       [0.38610366, 0.61389637],
       [0.3611515 , 0.6388485 ],
       [0.39574474, 0.60425526],
       [0.48206717, 0.51793283],
       [0.4352225 , 0.56477743],
       [0.5293537 , 0.47064632],
       [0.4351939 , 0.56480604],
       [0.39494088, 0.6050591 ],
       [0.29629105, 0.703709  ],
       [0.39273342, 0.60726655],
       [0.37942597, 0.620574  ],
       [0.40392777, 0.59607226],
       [0.42278576, 0.57721424],
       [0.42321032, 0.5767897 ],
       [0.3290817 , 0.6709183 ],
       [0.38383117, 0.6161688 ],
       [0.4492992 , 0.55070084],
       [0.41927034, 0.58072966],
       [0.38925087, 0.6107492 ],
       [0.4150021 , 0.5849979 ],
       [0.5914758 , 0.40852422],
       [0.4447122 , 0.5552878 ],
       [0.5072293 , 0.4927707 ],
       [0.5590465 , 0.44095352],
       [0.4567155 , 0.54328454],
       [0.26308432, 0.73691565],
       [0.47329345, 0.5267065 ],
       [0.5217634 , 0.47823662],
       [0.2883945 , 0.71160555],
       [0.48484313, 0.51515687],
       [0.785682  , 0.21431798],
       [0.44177085, 0.55822915],
       [0.4268143 , 0.57318574],
       [0.3282022 , 0.6717979 ],
       [0.27888694, 0.721113  ],
       [0.43272623, 0.56727374],
       [0.48700717, 0.5129928 ],
       [0.24735552, 0.7526445 ],
       [0.3437499 , 0.65625006],
       [0.47496793, 0.52503204],
       [0.35789973, 0.64210033],
       [0.43090022, 0.5690998 ],
       [0.40508464, 0.5949154 ],
       [0.4418508 , 0.5581492 ],
       [0.50487167, 0.49512833],
       [0.5045017 , 0.4954983 ],
       [0.33627528, 0.6637247 ],
       [0.43236625, 0.5676338 ],
       [0.4723027 , 0.5276973 ],
       [0.55153644, 0.44846356],
       [0.410058  , 0.58994204],
       [0.43052799, 0.5694721 ],
       [0.45123821, 0.54876184],
       [0.35328087, 0.6467191 ],
       [0.45968103, 0.54031897],
       [0.33205697, 0.66794306],
       [0.5815101 , 0.41848987],
       [0.55743647, 0.44256356],
       [0.4615889 , 0.53841114],
       [0.5988489 , 0.40115115],
       [0.26768813, 0.73231184],
       [0.4092803 , 0.5907197 ],
       [0.44905618, 0.55094385],
       [0.40612987, 0.5938701 ],
       [0.4797793 , 0.5202207 ],
       [0.490779  , 0.50922096],
       [0.50890255, 0.49109742],
       [0.46478626, 0.5352137 ],
       [0.39073965, 0.6092604 ],
       [0.6140519 , 0.38594812],
       [0.21095832, 0.78904164],
       [0.50982255, 0.49017748],
       [0.33999068, 0.6600093 ],
       [0.4652964 , 0.5347036 ],
       [0.4872016 , 0.51279837],
       [0.3138431 , 0.6861569 ],
       [0.5067988 , 0.49320123],
       [0.41474557, 0.5852544 ],
       [0.4137968 , 0.58620316],
       [0.6387266 , 0.3612734 ],
       [0.29403043, 0.7059696 ],
       [0.46303543, 0.5369646 ]], dtype=float32), array([[0.3880703 , 0.6119297 ],
       [0.51199114, 0.48800892],
       [0.3143604 , 0.68563956],
       [0.46775058, 0.53224945],
       [0.4228081 , 0.5771919 ],
       [0.46305248, 0.5369475 ],
       [0.5530051 , 0.4469949 ],
       [0.40783724, 0.5921628 ],
       [0.34845847, 0.65154153],
       [0.42968783, 0.5703122 ],
       [0.36605135, 0.6339487 ],
       [0.52814126, 0.4718587 ],
       [0.4303292 , 0.5696708 ],
       [0.5311472 , 0.46885288],
       [0.54523605, 0.45476392],
       [0.54236245, 0.45763755],
       [0.49351203, 0.50648797],
       [0.38893303, 0.61106694],
       [0.4891141 , 0.5108859 ],
       [0.56814957, 0.43185046],
       [0.48065817, 0.5193418 ],
       [0.4705589 , 0.5294411 ],
       [0.48349407, 0.51650596],
       [0.4876348 , 0.5123652 ],
       [0.3791615 , 0.62083846],
       [0.37122786, 0.62877214],
       [0.43571815, 0.5642818 ],
       [0.41554213, 0.5844579 ],
       [0.4376437 , 0.5623563 ],
       [0.2725442 , 0.72745574],
       [0.48336023, 0.51663977],
       [0.41728634, 0.58271366],
       [0.44753623, 0.55246377],
       [0.45130196, 0.54869807],
       [0.47825754, 0.52174246],
       [0.3706702 , 0.6293298 ],
       [0.5130376 , 0.48696244],
       [0.5888656 , 0.41113442],
       [0.49567425, 0.50432575],
       [0.4533961 , 0.5466039 ],
       [0.46075943, 0.53924054],
       [0.299956  , 0.70004404],
       [0.40043336, 0.59956664],
       [0.46529213, 0.5347079 ],
       [0.50553155, 0.49446842],
       [0.47945037, 0.5205496 ],
       [0.47140384, 0.5285961 ],
       [0.520463  , 0.479537  ],
       [0.4369172 , 0.5630828 ],
       [0.47227037, 0.52772963],
       [0.39890897, 0.6010911 ],
       [0.5099803 , 0.4900197 ],
       [0.44546404, 0.554536  ],
       [0.42215705, 0.577843  ],
       [0.3576284 , 0.64237154],
       [0.4861414 , 0.5138586 ],
       [0.4707984 , 0.5292016 ],
       [0.49237502, 0.50762504],
       [0.50471   , 0.49528992],
       [0.44178778, 0.5582122 ],
       [0.37154227, 0.6284577 ],
       [0.40878373, 0.59121627],
       [0.41240627, 0.58759373],
       [0.4968262 , 0.50317377],
       [0.47246245, 0.5275376 ],
       [0.43943956, 0.5605604 ],
       [0.3889983 , 0.6110017 ],
       [0.41751316, 0.58248687],
       [0.47119093, 0.5288091 ],
       [0.3927289 , 0.60727113],
       [0.49005207, 0.5099479 ],
       [0.41542542, 0.5845746 ],
       [0.62426174, 0.37573823],
       [0.4453439 , 0.5546561 ],
       [0.5046319 , 0.49536812],
       [0.57116544, 0.42883456],
       [0.46410406, 0.53589594],
       [0.27322844, 0.72677153],
       [0.48610952, 0.5138905 ],
       [0.5164864 , 0.48351362],
       [0.37881854, 0.6211814 ],
       [0.5108697 , 0.48913032],
       [0.6448277 , 0.3551723 ],
       [0.429549  , 0.57045096],
       [0.43403113, 0.5659689 ],
       [0.3908836 , 0.60911644],
       [0.33232817, 0.6676718 ],
       [0.57920706, 0.42079297],
       [0.47585174, 0.5241482 ],
       [0.30917242, 0.6908276 ],
       [0.3856376 , 0.61436236],
       [0.40961602, 0.590384  ],
       [0.39530924, 0.6046908 ],
       [0.47819042, 0.5218095 ],
       [0.49915648, 0.5008435 ],
       [0.50102425, 0.49897575],
       [0.4867456 , 0.5132544 ],
       [0.46412638, 0.53587365],
       [0.39740893, 0.6025911 ],
       [0.5355354 , 0.46446458],
       [0.46589816, 0.53410184],
       [0.6296572 , 0.3703428 ],
       [0.41445395, 0.5855461 ],
       [0.4707105 , 0.52928954],
       [0.479547  , 0.52045304],
       [0.3625186 , 0.63748145],
       [0.44902927, 0.55097073],
       [0.3706048 , 0.6293952 ],
       [0.5403005 , 0.45969948],
       [0.58481735, 0.4151827 ],
       [0.4457463 , 0.55425376],
       [0.55166525, 0.44833475],
       [0.35043052, 0.6495695 ],
       [0.38592914, 0.61407083],
       [0.49783325, 0.50216675],
       [0.41118985, 0.5888102 ],
       [0.5627369 , 0.43726304],
       [0.4429057 , 0.55709434],
       [0.42174813, 0.5782519 ],
       [0.52862275, 0.47137725],
       [0.39108476, 0.6089152 ],
       [0.5507947 , 0.44920528],
       [0.25938937, 0.74061066],
       [0.5540879 , 0.44591215],
       [0.3860679 , 0.6139321 ],
       [0.5129485 , 0.48705152],
       [0.4602273 , 0.53977275],
       [0.35588855, 0.64411145],
       [0.5478818 , 0.45211825],
       [0.4290892 , 0.5709108 ],
       [0.38805577, 0.6119442 ],
       [0.6379908 , 0.3620092 ],
       [0.37617096, 0.62382907],
       [0.46227485, 0.53772515]], dtype=float32), array([[0.44110677, 0.5588932 ],
       [0.5099021 , 0.49009785],
       [0.39384803, 0.606152  ],
       [0.5550005 , 0.44499952],
       [0.49347296, 0.50652707],
       [0.47046053, 0.52953947],
       [0.53958225, 0.46041778],
       [0.42666543, 0.5733346 ],
       [0.39377427, 0.6062258 ],
       [0.45215416, 0.54784584],
       [0.3697928 , 0.63020724],
       [0.54529893, 0.45470107],
       [0.41305026, 0.5869497 ],
       [0.57336444, 0.42663556],
       [0.5874033 , 0.41259673],
       [0.6317128 , 0.3682872 ],
       [0.54657364, 0.45342636],
       [0.48408425, 0.5159157 ],
       [0.53951687, 0.46048316],
       [0.62078726, 0.37921268],
       [0.46029982, 0.5397002 ],
       [0.5045198 , 0.49548018],
       [0.41166788, 0.5883321 ],
       [0.53231734, 0.46768266],
       [0.4298416 , 0.57015836],
       [0.34908837, 0.6509116 ],
       [0.4417619 , 0.5582381 ],
       [0.48157573, 0.5184243 ],
       [0.42407236, 0.5759276 ],
       [0.27093172, 0.7290683 ],
       [0.54841894, 0.45158112],
       [0.53199255, 0.46800745],
       [0.49500385, 0.5049961 ],
       [0.45061103, 0.54938895],
       [0.45088303, 0.54911697],
       [0.3670385 , 0.63296145],
       [0.52968395, 0.47031608],
       [0.560933  , 0.439067  ],
       [0.51174086, 0.48825908],
       [0.45324862, 0.5467514 ],
       [0.48598278, 0.5140172 ],
       [0.36735138, 0.6326486 ],
       [0.43714756, 0.5628524 ],
       [0.4742983 , 0.52570176],
       [0.49965146, 0.50034857],
       [0.5289379 , 0.47106215],
       [0.48558396, 0.51441604],
       [0.50400245, 0.4959975 ],
       [0.61590755, 0.3840924 ],
       [0.5272604 , 0.47273958],
       [0.3950243 , 0.6049757 ],
       [0.53334624, 0.46665382],
       [0.48781684, 0.5121831 ],
       [0.42478013, 0.5752199 ],
       [0.45202702, 0.547973  ],
       [0.5503648 , 0.4496352 ],
       [0.5060665 , 0.4939335 ],
       [0.5314929 , 0.46850708],
       [0.52381426, 0.47618568],
       [0.4952375 , 0.5047625 ],
       [0.34677762, 0.6532224 ],
       [0.43402877, 0.5659712 ],
       [0.472528  , 0.52747196],
       [0.5363826 , 0.46361735],
       [0.48564258, 0.51435745],
       [0.45882842, 0.54117155],
       [0.36617044, 0.6338296 ],
       [0.4553812 , 0.54461884],
       [0.52849823, 0.47150177],
       [0.46100917, 0.5389908 ],
       [0.53343207, 0.46656787],
       [0.4725518 , 0.5274482 ],
       [0.5915467 , 0.40845326],
       [0.5089526 , 0.4910474 ],
       [0.5787737 , 0.4212263 ],
       [0.6289706 , 0.37102935],
       [0.4610983 , 0.5389017 ],
       [0.36730307, 0.63269687],
       [0.49290228, 0.5070977 ],
       [0.5743463 , 0.42565373],
       [0.41176444, 0.58823556],
       [0.57724077, 0.4227592 ],
       [0.667577  , 0.33242297],
       [0.48716778, 0.5128322 ],
       [0.48380274, 0.51619726],
       [0.3990585 , 0.6009415 ],
       [0.41994804, 0.58005196],
       [0.5866533 , 0.41334674],
       [0.52895737, 0.47104266],
       [0.3075473 , 0.6924527 ],
       [0.4288635 , 0.5711365 ],
       [0.44757962, 0.5524204 ],
       [0.42163134, 0.57836866],
       [0.5359737 , 0.46402627],
       [0.46092388, 0.53907615],
       [0.47902292, 0.5209771 ],
       [0.5037398 , 0.49626023],
       [0.52997375, 0.47002625],
       [0.38771752, 0.61228245],
       [0.53718835, 0.4628116 ],
       [0.48906595, 0.51093405],
       [0.622202  , 0.377798  ],
       [0.46290788, 0.53709215],
       [0.46353817, 0.53646183],
       [0.50774276, 0.49225727],
       [0.419484  , 0.58051604],
       [0.47130263, 0.52869743],
       [0.4380897 , 0.5619103 ],
       [0.6185946 , 0.38140538],
       [0.5557781 , 0.4442219 ],
       [0.46582744, 0.53417253],
       [0.5828968 , 0.41710314],
       [0.34994653, 0.65005344],
       [0.4323789 , 0.5676211 ],
       [0.48609424, 0.51390576],
       [0.45360595, 0.546394  ],
       [0.5449939 , 0.4550061 ],
       [0.51030445, 0.48969558],
       [0.52679735, 0.47320265],
       [0.55073667, 0.4492633 ],
       [0.47507444, 0.52492553],
       [0.6020182 , 0.39798182],
       [0.28503522, 0.71496475],
       [0.59075934, 0.40924063],
       [0.47842413, 0.52157587],
       [0.5427274 , 0.45727256],
       [0.5574557 , 0.44254425],
       [0.40166306, 0.59833694],
       [0.54595387, 0.45404613],
       [0.4896713 , 0.5103287 ],
       [0.45220754, 0.5477925 ],
       [0.68647027, 0.3135298 ],
       [0.4484612 , 0.5515388 ],
       [0.49787235, 0.50212765]], dtype=float32)]
i = 2, Test true class= 
[1 0 0 0 0 1 1 0 1 0 0 0 1 1 1 1 0 1 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 0 1 1 1
 1 1 0 0 0 0 0 0 0 1 0 1 0 1 0 1 0 0 0 0 1 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 1
 1 0 1 1 1 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 1 1 0 0 0 0 1 1 0
 0 0 1 1 0 0 0 0 1 0 0 1 0 0 0 0 1 1 0 1 1 0 0]
In grad_steps = 0, loss = 1.0984740257263184
In grad_steps = 1, loss = 0.7675802111625671
In grad_steps = 2, loss = 0.7049485445022583
In grad_steps = 3, loss = 0.6932734847068787
In grad_steps = 4, loss = 1.0026894807815552
In grad_steps = 5, loss = 0.7405692338943481
In grad_steps = 6, loss = 0.7139723300933838
In grad_steps = 7, loss = 0.7165236473083496
In grad_steps = 8, loss = 0.6944718360900879
In grad_steps = 9, loss = 0.7755879163742065
In grad_steps = 10, loss = 0.698726236820221
In grad_steps = 11, loss = 0.7443526387214661
In grad_steps = 12, loss = 0.7703661322593689
In grad_steps = 13, loss = 0.6924628019332886
In grad_steps = 14, loss = 0.6672053933143616
In grad_steps = 15, loss = 0.6731950044631958
In grad_steps = 16, loss = 0.8747222423553467
In grad_steps = 17, loss = 0.7770135998725891
In grad_steps = 18, loss = 0.7177914381027222
In grad_steps = 19, loss = 0.6511161923408508
In grad_steps = 20, loss = 0.7807417511940002
In grad_steps = 21, loss = 0.7044361233711243
In grad_steps = 22, loss = 0.7211729884147644
In grad_steps = 23, loss = 0.671140193939209
In grad_steps = 24, loss = 0.7347800135612488
In grad_steps = 25, loss = 0.7143563032150269
In grad_steps = 26, loss = 0.6242538690567017
In grad_steps = 27, loss = 0.6488443613052368
In grad_steps = 28, loss = 0.6015213131904602
In grad_steps = 29, loss = 0.7788979411125183
In grad_steps = 30, loss = 0.7906956672668457
In grad_steps = 31, loss = 0.7563419342041016
In grad_steps = 32, loss = 0.6668667793273926
In grad_steps = 33, loss = 0.6689741611480713
In grad_steps = 34, loss = 0.7273121476173401
In grad_steps = 35, loss = 0.6822370886802673
In grad_steps = 36, loss = 0.6928701996803284
In grad_steps = 37, loss = 0.6820685863494873
In grad_steps = 38, loss = 0.6841220259666443
In grad_steps = 39, loss = 0.6748635172843933
In grad_steps = 40, loss = 0.7406649589538574
In grad_steps = 41, loss = 0.676608145236969
In grad_steps = 42, loss = 0.8280740976333618
In grad_steps = 43, loss = 0.6653454303741455
In grad_steps = 44, loss = 0.6931300163269043
In grad_steps = 45, loss = 0.7089244723320007
In grad_steps = 46, loss = 0.6901988983154297
In grad_steps = 47, loss = 0.7043938040733337
In grad_steps = 48, loss = 0.6772710084915161
In grad_steps = 49, loss = 0.692601203918457
In grad_steps = 50, loss = 0.707095742225647
In grad_steps = 51, loss = 0.6825551986694336
In grad_steps = 52, loss = 0.671474039554596
In grad_steps = 53, loss = 0.7408117055892944
In grad_steps = 54, loss = 0.7250826358795166
In grad_steps = 55, loss = 0.682846188545227
In grad_steps = 56, loss = 0.6918433308601379
In grad_steps = 57, loss = 0.6948958039283752
In grad_steps = 58, loss = 0.7105014324188232
In grad_steps = 59, loss = 0.7114546298980713
In grad_steps = 60, loss = 0.714850127696991
In grad_steps = 61, loss = 0.6984339356422424
In grad_steps = 62, loss = 0.6951812505722046
In grad_steps = 63, loss = 0.6941382884979248
In grad_steps = 64, loss = 0.6924048066139221
In grad_steps = 65, loss = 0.7222252488136292
In grad_steps = 66, loss = 0.717377245426178
In grad_steps = 67, loss = 0.6789389848709106
In grad_steps = 68, loss = 0.6842083930969238
In grad_steps = 69, loss = 0.683916449546814
In grad_steps = 70, loss = 0.74211186170578
In grad_steps = 71, loss = 0.6573785543441772
In grad_steps = 72, loss = 0.7186473608016968
In grad_steps = 73, loss = 0.695149302482605
In grad_steps = 74, loss = 0.6468666195869446
In grad_steps = 75, loss = 0.7199645638465881
In grad_steps = 76, loss = 0.743330180644989
In grad_steps = 77, loss = 0.6810839772224426
In grad_steps = 78, loss = 0.7148836851119995
In grad_steps = 79, loss = 0.7199217081069946
In grad_steps = 80, loss = 0.6879608035087585
In grad_steps = 81, loss = 0.6868036985397339
In grad_steps = 82, loss = 0.6770455837249756
In grad_steps = 83, loss = 0.7412688732147217
In grad_steps = 84, loss = 0.7171315550804138
In grad_steps = 85, loss = 0.6967992782592773
In grad_steps = 86, loss = 0.6554227471351624
In grad_steps = 87, loss = 0.8393855690956116
In grad_steps = 88, loss = 0.7180463671684265
In grad_steps = 89, loss = 0.6580090522766113
In grad_steps = 90, loss = 0.6912949085235596
In grad_steps = 91, loss = 0.7025704979896545
In grad_steps = 92, loss = 0.6785432696342468
In grad_steps = 93, loss = 0.6904506683349609
In grad_steps = 94, loss = 0.6779197454452515
In grad_steps = 95, loss = 0.6372476816177368
In grad_steps = 96, loss = 0.7508471608161926
In grad_steps = 97, loss = 0.7645050883293152
In grad_steps = 98, loss = 0.7560042142868042
In grad_steps = 99, loss = 0.6473855972290039
In grad_steps = 100, loss = 0.6570674777030945
In grad_steps = 101, loss = 0.7460087537765503
In grad_steps = 102, loss = 0.6853038668632507
In grad_steps = 103, loss = 0.7035544514656067
In grad_steps = 104, loss = 0.6695518493652344
In grad_steps = 105, loss = 0.7022135257720947
In grad_steps = 106, loss = 0.6881154775619507
In grad_steps = 107, loss = 0.6988273859024048
In grad_steps = 108, loss = 0.674017071723938
In grad_steps = 109, loss = 0.7633904814720154
In grad_steps = 110, loss = 0.6772638559341431
In grad_steps = 111, loss = 0.6882583498954773
In grad_steps = 112, loss = 0.7084255814552307
In grad_steps = 113, loss = 0.6900038123130798
In grad_steps = 114, loss = 0.6887860298156738
In grad_steps = 115, loss = 0.6834717392921448
In grad_steps = 116, loss = 0.6906974911689758
In grad_steps = 117, loss = 0.6949629187583923
In grad_steps = 118, loss = 0.677541196346283
In grad_steps = 119, loss = 0.6877766847610474
In grad_steps = 120, loss = 0.7138305306434631
In grad_steps = 121, loss = 0.7097837328910828
In grad_steps = 122, loss = 0.6815529465675354
In grad_steps = 123, loss = 0.6900061964988708
In grad_steps = 124, loss = 0.6968991756439209
In grad_steps = 125, loss = 0.7043212652206421
In grad_steps = 126, loss = 0.7126732468605042
In grad_steps = 127, loss = 0.7063886523246765
In grad_steps = 128, loss = 0.691264808177948
In grad_steps = 129, loss = 0.6906567811965942
In grad_steps = 130, loss = 0.6884642243385315
In grad_steps = 131, loss = 0.6890009045600891
In grad_steps = 132, loss = 0.7113414406776428
In grad_steps = 133, loss = 0.692401647567749
In grad_steps = 134, loss = 0.6751453280448914
In grad_steps = 135, loss = 0.6789371967315674
In grad_steps = 136, loss = 0.6824461221694946
In grad_steps = 137, loss = 0.7502117156982422
In grad_steps = 138, loss = 0.6525682806968689
In grad_steps = 139, loss = 0.717334508895874
In grad_steps = 140, loss = 0.6982766389846802
In grad_steps = 141, loss = 0.6414979696273804
In grad_steps = 142, loss = 0.7159586548805237
In grad_steps = 143, loss = 0.7349168062210083
In grad_steps = 144, loss = 0.6705929636955261
In grad_steps = 145, loss = 0.7026488184928894
In grad_steps = 146, loss = 0.6975517272949219
In grad_steps = 147, loss = 0.6736562848091125
In grad_steps = 148, loss = 0.6653285622596741
In grad_steps = 149, loss = 0.6578038930892944
In grad_steps = 150, loss = 0.7514424324035645
In grad_steps = 151, loss = 0.7057336568832397
In grad_steps = 152, loss = 0.7054659724235535
In grad_steps = 153, loss = 0.6506693959236145
In grad_steps = 154, loss = 0.8310993909835815
In grad_steps = 155, loss = 0.6950018405914307
In grad_steps = 156, loss = 0.6686071753501892
In grad_steps = 157, loss = 0.6629319787025452
In grad_steps = 158, loss = 0.7221434712409973
In grad_steps = 159, loss = 0.690962016582489
In grad_steps = 160, loss = 0.6408112645149231
In grad_steps = 161, loss = 0.6392615437507629
In grad_steps = 162, loss = 0.574506938457489
In grad_steps = 163, loss = 0.8256950974464417
In grad_steps = 164, loss = 0.8092756867408752
In grad_steps = 165, loss = 0.7533363103866577
In grad_steps = 166, loss = 0.6374513506889343
In grad_steps = 167, loss = 0.6371985077857971
In grad_steps = 168, loss = 0.7064740061759949
In grad_steps = 169, loss = 0.6708957552909851
In grad_steps = 170, loss = 0.6797740459442139
In grad_steps = 171, loss = 0.6781275868415833
In grad_steps = 172, loss = 0.6601736545562744
In grad_steps = 173, loss = 0.6564773917198181
In grad_steps = 174, loss = 0.7363116145133972
In grad_steps = 175, loss = 0.666687548160553
In grad_steps = 176, loss = 0.7821676731109619
In grad_steps = 177, loss = 0.678713321685791
In grad_steps = 178, loss = 0.691371738910675
In grad_steps = 179, loss = 0.7049020528793335
In grad_steps = 180, loss = 0.6819173097610474
In grad_steps = 181, loss = 0.7256266474723816
In grad_steps = 182, loss = 0.6468197107315063
In grad_steps = 183, loss = 0.6861981153488159
In grad_steps = 184, loss = 0.7203962206840515
In grad_steps = 185, loss = 0.6691094636917114
In grad_steps = 186, loss = 0.6671035289764404
In grad_steps = 187, loss = 0.7286720871925354
In grad_steps = 188, loss = 0.7139014005661011
In grad_steps = 189, loss = 0.6886468529701233
In grad_steps = 190, loss = 0.6811323165893555
In grad_steps = 191, loss = 0.6883174180984497
In grad_steps = 192, loss = 0.6959434151649475
In grad_steps = 193, loss = 0.7000714540481567
In grad_steps = 194, loss = 0.6884480118751526
In grad_steps = 195, loss = 0.7044790983200073
In grad_steps = 196, loss = 0.6651917695999146
In grad_steps = 197, loss = 0.6806579232215881
In grad_steps = 198, loss = 0.6852551102638245
In grad_steps = 199, loss = 0.7307747006416321
In grad_steps = 200, loss = 0.691849410533905
In grad_steps = 201, loss = 0.6641296148300171
In grad_steps = 202, loss = 0.6723417043685913
In grad_steps = 203, loss = 0.6823241710662842
In grad_steps = 204, loss = 0.7680323123931885
In grad_steps = 205, loss = 0.6434144973754883
In grad_steps = 206, loss = 0.7200676798820496
In grad_steps = 207, loss = 0.7022700905799866
In grad_steps = 208, loss = 0.6266768574714661
In grad_steps = 209, loss = 0.717525064945221
In grad_steps = 210, loss = 0.7379908561706543
In grad_steps = 211, loss = 0.6550403833389282
In grad_steps = 212, loss = 0.6747188568115234
In grad_steps = 213, loss = 0.6482905745506287
In grad_steps = 214, loss = 0.649882435798645
In grad_steps = 215, loss = 0.6013333797454834
In grad_steps = 216, loss = 0.6022725701332092
In grad_steps = 217, loss = 0.8364469408988953
In grad_steps = 218, loss = 0.701004683971405
In grad_steps = 219, loss = 0.6948245763778687
In grad_steps = 220, loss = 0.6667526364326477
In grad_steps = 221, loss = 0.6727316975593567
In grad_steps = 222, loss = 0.6244945526123047
In grad_steps = 223, loss = 0.7681680917739868
In grad_steps = 224, loss = 0.6509721875190735
In grad_steps = 225, loss = 0.7245369553565979
In grad_steps = 226, loss = 0.6795483827590942
In grad_steps = 227, loss = 0.6430464386940002
In grad_steps = 228, loss = 0.6721435189247131
In grad_steps = 229, loss = 0.6393103003501892
In grad_steps = 230, loss = 0.6737887263298035
In grad_steps = 231, loss = 0.6986797451972961
In grad_steps = 232, loss = 0.6765281558036804
In grad_steps = 233, loss = 0.633995771408081
In grad_steps = 234, loss = 0.5780115127563477
In grad_steps = 235, loss = 0.6890854239463806
In grad_steps = 236, loss = 0.6098027229309082
In grad_steps = 237, loss = 0.6761960983276367
In grad_steps = 238, loss = 0.5792621970176697
In grad_steps = 239, loss = 0.5763460397720337
In grad_steps = 240, loss = 0.6024170517921448
In grad_steps = 241, loss = 0.837173342704773
In grad_steps = 242, loss = 0.6010688543319702
In grad_steps = 243, loss = 0.5333008766174316
In grad_steps = 244, loss = 1.0120514631271362
In grad_steps = 245, loss = 0.7902212738990784
In grad_steps = 246, loss = 0.7044734954833984
In grad_steps = 247, loss = 0.6548151969909668
In grad_steps = 248, loss = 0.6321843266487122
In grad_steps = 249, loss = 0.666296660900116
In grad_steps = 250, loss = 0.6663950085639954
In grad_steps = 251, loss = 0.6856522560119629
In grad_steps = 252, loss = 0.6299993991851807
In grad_steps = 253, loss = 0.7275435328483582
In grad_steps = 254, loss = 0.6488624215126038
In grad_steps = 255, loss = 0.7225961089134216
In grad_steps = 256, loss = 0.6717621684074402
In grad_steps = 257, loss = 0.6476626992225647
In grad_steps = 258, loss = 0.6921368837356567
In grad_steps = 259, loss = 0.6853217482566833
In grad_steps = 260, loss = 0.7158886790275574
In grad_steps = 261, loss = 0.6837705373764038
In grad_steps = 262, loss = 0.6732988953590393
In grad_steps = 263, loss = 0.6779298186302185
In grad_steps = 264, loss = 0.6521232724189758
In grad_steps = 265, loss = 0.6789429187774658
In grad_steps = 266, loss = 0.6703939437866211
In grad_steps = 267, loss = 0.6800650954246521
i = 3, Test ensemble probabilities = 
[array([[0.34539714, 0.6546028 ],
       [0.49670178, 0.5032982 ],
       [0.2995092 , 0.70049083],
       [0.5216206 , 0.4783794 ],
       [0.4783175 , 0.5216825 ],
       [0.37964696, 0.620353  ],
       [0.5360174 , 0.4639826 ],
       [0.41234112, 0.5876589 ],
       [0.2805267 , 0.71947336],
       [0.3660757 , 0.6339243 ],
       [0.31246424, 0.68753576],
       [0.5163459 , 0.4836541 ],
       [0.37822625, 0.6217737 ],
       [0.43147096, 0.56852907],
       [0.54082584, 0.4591742 ],
       [0.5715256 , 0.4284744 ],
       [0.4142887 , 0.5857113 ],
       [0.37641716, 0.6235828 ],
       [0.46707872, 0.5329213 ],
       [0.4730467 , 0.5269533 ],
       [0.4055867 , 0.59441334],
       [0.4146544 , 0.5853456 ],
       [0.36647767, 0.6335223 ],
       [0.45998093, 0.5400191 ],
       [0.31840318, 0.6815968 ],
       [0.24469352, 0.75530654],
       [0.44522896, 0.55477107],
       [0.3880463 , 0.61195374],
       [0.37943542, 0.6205646 ],
       [0.2172548 , 0.7827452 ],
       [0.4649846 , 0.5350154 ],
       [0.4034645 , 0.59653556],
       [0.45316827, 0.5468317 ],
       [0.41054946, 0.58945054],
       [0.38889134, 0.61110866],
       [0.30868906, 0.691311  ],
       [0.48083103, 0.519169  ],
       [0.55902576, 0.44097418],
       [0.4085316 , 0.5914684 ],
       [0.41088137, 0.5891186 ],
       [0.4298178 , 0.5701822 ],
       [0.29877663, 0.70122343],
       [0.3293201 , 0.6706799 ],
       [0.42829737, 0.5717026 ],
       [0.49717334, 0.5028267 ],
       [0.52718437, 0.4728156 ],
       [0.42836556, 0.5716345 ],
       [0.4556835 , 0.54431653],
       [0.3669375 , 0.63306254],
       [0.4527557 , 0.5472443 ],
       [0.2731709 , 0.7268291 ],
       [0.42771834, 0.5722816 ],
       [0.38610366, 0.61389637],
       [0.3611515 , 0.6388485 ],
       [0.39574474, 0.60425526],
       [0.48206717, 0.51793283],
       [0.4352225 , 0.56477743],
       [0.5293537 , 0.47064632],
       [0.4351939 , 0.56480604],
       [0.39494088, 0.6050591 ],
       [0.29629105, 0.703709  ],
       [0.39273342, 0.60726655],
       [0.37942597, 0.620574  ],
       [0.40392777, 0.59607226],
       [0.42278576, 0.57721424],
       [0.42321032, 0.5767897 ],
       [0.3290817 , 0.6709183 ],
       [0.38383117, 0.6161688 ],
       [0.4492992 , 0.55070084],
       [0.41927034, 0.58072966],
       [0.38925087, 0.6107492 ],
       [0.4150021 , 0.5849979 ],
       [0.5914758 , 0.40852422],
       [0.4447122 , 0.5552878 ],
       [0.5072293 , 0.4927707 ],
       [0.5590465 , 0.44095352],
       [0.4567155 , 0.54328454],
       [0.26308432, 0.73691565],
       [0.47329345, 0.5267065 ],
       [0.5217634 , 0.47823662],
       [0.2883945 , 0.71160555],
       [0.48484313, 0.51515687],
       [0.785682  , 0.21431798],
       [0.44177085, 0.55822915],
       [0.4268143 , 0.57318574],
       [0.3282022 , 0.6717979 ],
       [0.27888694, 0.721113  ],
       [0.43272623, 0.56727374],
       [0.48700717, 0.5129928 ],
       [0.24735552, 0.7526445 ],
       [0.3437499 , 0.65625006],
       [0.47496793, 0.52503204],
       [0.35789973, 0.64210033],
       [0.43090022, 0.5690998 ],
       [0.40508464, 0.5949154 ],
       [0.4418508 , 0.5581492 ],
       [0.50487167, 0.49512833],
       [0.5045017 , 0.4954983 ],
       [0.33627528, 0.6637247 ],
       [0.43236625, 0.5676338 ],
       [0.4723027 , 0.5276973 ],
       [0.55153644, 0.44846356],
       [0.410058  , 0.58994204],
       [0.43052799, 0.5694721 ],
       [0.45123821, 0.54876184],
       [0.35328087, 0.6467191 ],
       [0.45968103, 0.54031897],
       [0.33205697, 0.66794306],
       [0.5815101 , 0.41848987],
       [0.55743647, 0.44256356],
       [0.4615889 , 0.53841114],
       [0.5988489 , 0.40115115],
       [0.26768813, 0.73231184],
       [0.4092803 , 0.5907197 ],
       [0.44905618, 0.55094385],
       [0.40612987, 0.5938701 ],
       [0.4797793 , 0.5202207 ],
       [0.490779  , 0.50922096],
       [0.50890255, 0.49109742],
       [0.46478626, 0.5352137 ],
       [0.39073965, 0.6092604 ],
       [0.6140519 , 0.38594812],
       [0.21095832, 0.78904164],
       [0.50982255, 0.49017748],
       [0.33999068, 0.6600093 ],
       [0.4652964 , 0.5347036 ],
       [0.4872016 , 0.51279837],
       [0.3138431 , 0.6861569 ],
       [0.5067988 , 0.49320123],
       [0.41474557, 0.5852544 ],
       [0.4137968 , 0.58620316],
       [0.6387266 , 0.3612734 ],
       [0.29403043, 0.7059696 ],
       [0.46303543, 0.5369646 ]], dtype=float32), array([[0.3880703 , 0.6119297 ],
       [0.51199114, 0.48800892],
       [0.3143604 , 0.68563956],
       [0.46775058, 0.53224945],
       [0.4228081 , 0.5771919 ],
       [0.46305248, 0.5369475 ],
       [0.5530051 , 0.4469949 ],
       [0.40783724, 0.5921628 ],
       [0.34845847, 0.65154153],
       [0.42968783, 0.5703122 ],
       [0.36605135, 0.6339487 ],
       [0.52814126, 0.4718587 ],
       [0.4303292 , 0.5696708 ],
       [0.5311472 , 0.46885288],
       [0.54523605, 0.45476392],
       [0.54236245, 0.45763755],
       [0.49351203, 0.50648797],
       [0.38893303, 0.61106694],
       [0.4891141 , 0.5108859 ],
       [0.56814957, 0.43185046],
       [0.48065817, 0.5193418 ],
       [0.4705589 , 0.5294411 ],
       [0.48349407, 0.51650596],
       [0.4876348 , 0.5123652 ],
       [0.3791615 , 0.62083846],
       [0.37122786, 0.62877214],
       [0.43571815, 0.5642818 ],
       [0.41554213, 0.5844579 ],
       [0.4376437 , 0.5623563 ],
       [0.2725442 , 0.72745574],
       [0.48336023, 0.51663977],
       [0.41728634, 0.58271366],
       [0.44753623, 0.55246377],
       [0.45130196, 0.54869807],
       [0.47825754, 0.52174246],
       [0.3706702 , 0.6293298 ],
       [0.5130376 , 0.48696244],
       [0.5888656 , 0.41113442],
       [0.49567425, 0.50432575],
       [0.4533961 , 0.5466039 ],
       [0.46075943, 0.53924054],
       [0.299956  , 0.70004404],
       [0.40043336, 0.59956664],
       [0.46529213, 0.5347079 ],
       [0.50553155, 0.49446842],
       [0.47945037, 0.5205496 ],
       [0.47140384, 0.5285961 ],
       [0.520463  , 0.479537  ],
       [0.4369172 , 0.5630828 ],
       [0.47227037, 0.52772963],
       [0.39890897, 0.6010911 ],
       [0.5099803 , 0.4900197 ],
       [0.44546404, 0.554536  ],
       [0.42215705, 0.577843  ],
       [0.3576284 , 0.64237154],
       [0.4861414 , 0.5138586 ],
       [0.4707984 , 0.5292016 ],
       [0.49237502, 0.50762504],
       [0.50471   , 0.49528992],
       [0.44178778, 0.5582122 ],
       [0.37154227, 0.6284577 ],
       [0.40878373, 0.59121627],
       [0.41240627, 0.58759373],
       [0.4968262 , 0.50317377],
       [0.47246245, 0.5275376 ],
       [0.43943956, 0.5605604 ],
       [0.3889983 , 0.6110017 ],
       [0.41751316, 0.58248687],
       [0.47119093, 0.5288091 ],
       [0.3927289 , 0.60727113],
       [0.49005207, 0.5099479 ],
       [0.41542542, 0.5845746 ],
       [0.62426174, 0.37573823],
       [0.4453439 , 0.5546561 ],
       [0.5046319 , 0.49536812],
       [0.57116544, 0.42883456],
       [0.46410406, 0.53589594],
       [0.27322844, 0.72677153],
       [0.48610952, 0.5138905 ],
       [0.5164864 , 0.48351362],
       [0.37881854, 0.6211814 ],
       [0.5108697 , 0.48913032],
       [0.6448277 , 0.3551723 ],
       [0.429549  , 0.57045096],
       [0.43403113, 0.5659689 ],
       [0.3908836 , 0.60911644],
       [0.33232817, 0.6676718 ],
       [0.57920706, 0.42079297],
       [0.47585174, 0.5241482 ],
       [0.30917242, 0.6908276 ],
       [0.3856376 , 0.61436236],
       [0.40961602, 0.590384  ],
       [0.39530924, 0.6046908 ],
       [0.47819042, 0.5218095 ],
       [0.49915648, 0.5008435 ],
       [0.50102425, 0.49897575],
       [0.4867456 , 0.5132544 ],
       [0.46412638, 0.53587365],
       [0.39740893, 0.6025911 ],
       [0.5355354 , 0.46446458],
       [0.46589816, 0.53410184],
       [0.6296572 , 0.3703428 ],
       [0.41445395, 0.5855461 ],
       [0.4707105 , 0.52928954],
       [0.479547  , 0.52045304],
       [0.3625186 , 0.63748145],
       [0.44902927, 0.55097073],
       [0.3706048 , 0.6293952 ],
       [0.5403005 , 0.45969948],
       [0.58481735, 0.4151827 ],
       [0.4457463 , 0.55425376],
       [0.55166525, 0.44833475],
       [0.35043052, 0.6495695 ],
       [0.38592914, 0.61407083],
       [0.49783325, 0.50216675],
       [0.41118985, 0.5888102 ],
       [0.5627369 , 0.43726304],
       [0.4429057 , 0.55709434],
       [0.42174813, 0.5782519 ],
       [0.52862275, 0.47137725],
       [0.39108476, 0.6089152 ],
       [0.5507947 , 0.44920528],
       [0.25938937, 0.74061066],
       [0.5540879 , 0.44591215],
       [0.3860679 , 0.6139321 ],
       [0.5129485 , 0.48705152],
       [0.4602273 , 0.53977275],
       [0.35588855, 0.64411145],
       [0.5478818 , 0.45211825],
       [0.4290892 , 0.5709108 ],
       [0.38805577, 0.6119442 ],
       [0.6379908 , 0.3620092 ],
       [0.37617096, 0.62382907],
       [0.46227485, 0.53772515]], dtype=float32), array([[0.44110677, 0.5588932 ],
       [0.5099021 , 0.49009785],
       [0.39384803, 0.606152  ],
       [0.5550005 , 0.44499952],
       [0.49347296, 0.50652707],
       [0.47046053, 0.52953947],
       [0.53958225, 0.46041778],
       [0.42666543, 0.5733346 ],
       [0.39377427, 0.6062258 ],
       [0.45215416, 0.54784584],
       [0.3697928 , 0.63020724],
       [0.54529893, 0.45470107],
       [0.41305026, 0.5869497 ],
       [0.57336444, 0.42663556],
       [0.5874033 , 0.41259673],
       [0.6317128 , 0.3682872 ],
       [0.54657364, 0.45342636],
       [0.48408425, 0.5159157 ],
       [0.53951687, 0.46048316],
       [0.62078726, 0.37921268],
       [0.46029982, 0.5397002 ],
       [0.5045198 , 0.49548018],
       [0.41166788, 0.5883321 ],
       [0.53231734, 0.46768266],
       [0.4298416 , 0.57015836],
       [0.34908837, 0.6509116 ],
       [0.4417619 , 0.5582381 ],
       [0.48157573, 0.5184243 ],
       [0.42407236, 0.5759276 ],
       [0.27093172, 0.7290683 ],
       [0.54841894, 0.45158112],
       [0.53199255, 0.46800745],
       [0.49500385, 0.5049961 ],
       [0.45061103, 0.54938895],
       [0.45088303, 0.54911697],
       [0.3670385 , 0.63296145],
       [0.52968395, 0.47031608],
       [0.560933  , 0.439067  ],
       [0.51174086, 0.48825908],
       [0.45324862, 0.5467514 ],
       [0.48598278, 0.5140172 ],
       [0.36735138, 0.6326486 ],
       [0.43714756, 0.5628524 ],
       [0.4742983 , 0.52570176],
       [0.49965146, 0.50034857],
       [0.5289379 , 0.47106215],
       [0.48558396, 0.51441604],
       [0.50400245, 0.4959975 ],
       [0.61590755, 0.3840924 ],
       [0.5272604 , 0.47273958],
       [0.3950243 , 0.6049757 ],
       [0.53334624, 0.46665382],
       [0.48781684, 0.5121831 ],
       [0.42478013, 0.5752199 ],
       [0.45202702, 0.547973  ],
       [0.5503648 , 0.4496352 ],
       [0.5060665 , 0.4939335 ],
       [0.5314929 , 0.46850708],
       [0.52381426, 0.47618568],
       [0.4952375 , 0.5047625 ],
       [0.34677762, 0.6532224 ],
       [0.43402877, 0.5659712 ],
       [0.472528  , 0.52747196],
       [0.5363826 , 0.46361735],
       [0.48564258, 0.51435745],
       [0.45882842, 0.54117155],
       [0.36617044, 0.6338296 ],
       [0.4553812 , 0.54461884],
       [0.52849823, 0.47150177],
       [0.46100917, 0.5389908 ],
       [0.53343207, 0.46656787],
       [0.4725518 , 0.5274482 ],
       [0.5915467 , 0.40845326],
       [0.5089526 , 0.4910474 ],
       [0.5787737 , 0.4212263 ],
       [0.6289706 , 0.37102935],
       [0.4610983 , 0.5389017 ],
       [0.36730307, 0.63269687],
       [0.49290228, 0.5070977 ],
       [0.5743463 , 0.42565373],
       [0.41176444, 0.58823556],
       [0.57724077, 0.4227592 ],
       [0.667577  , 0.33242297],
       [0.48716778, 0.5128322 ],
       [0.48380274, 0.51619726],
       [0.3990585 , 0.6009415 ],
       [0.41994804, 0.58005196],
       [0.5866533 , 0.41334674],
       [0.52895737, 0.47104266],
       [0.3075473 , 0.6924527 ],
       [0.4288635 , 0.5711365 ],
       [0.44757962, 0.5524204 ],
       [0.42163134, 0.57836866],
       [0.5359737 , 0.46402627],
       [0.46092388, 0.53907615],
       [0.47902292, 0.5209771 ],
       [0.5037398 , 0.49626023],
       [0.52997375, 0.47002625],
       [0.38771752, 0.61228245],
       [0.53718835, 0.4628116 ],
       [0.48906595, 0.51093405],
       [0.622202  , 0.377798  ],
       [0.46290788, 0.53709215],
       [0.46353817, 0.53646183],
       [0.50774276, 0.49225727],
       [0.419484  , 0.58051604],
       [0.47130263, 0.52869743],
       [0.4380897 , 0.5619103 ],
       [0.6185946 , 0.38140538],
       [0.5557781 , 0.4442219 ],
       [0.46582744, 0.53417253],
       [0.5828968 , 0.41710314],
       [0.34994653, 0.65005344],
       [0.4323789 , 0.5676211 ],
       [0.48609424, 0.51390576],
       [0.45360595, 0.546394  ],
       [0.5449939 , 0.4550061 ],
       [0.51030445, 0.48969558],
       [0.52679735, 0.47320265],
       [0.55073667, 0.4492633 ],
       [0.47507444, 0.52492553],
       [0.6020182 , 0.39798182],
       [0.28503522, 0.71496475],
       [0.59075934, 0.40924063],
       [0.47842413, 0.52157587],
       [0.5427274 , 0.45727256],
       [0.5574557 , 0.44254425],
       [0.40166306, 0.59833694],
       [0.54595387, 0.45404613],
       [0.4896713 , 0.5103287 ],
       [0.45220754, 0.5477925 ],
       [0.68647027, 0.3135298 ],
       [0.4484612 , 0.5515388 ],
       [0.49787235, 0.50212765]], dtype=float32), array([[0.43671542, 0.5632846 ],
       [0.5093111 , 0.4906889 ],
       [0.42088732, 0.57911265],
       [0.50538373, 0.49461624],
       [0.5882425 , 0.4117575 ],
       [0.43135256, 0.5686474 ],
       [0.4381418 , 0.5618582 ],
       [0.3954127 , 0.60458726],
       [0.40681872, 0.59318125],
       [0.43817   , 0.56183   ],
       [0.38717157, 0.61282843],
       [0.52158344, 0.4784165 ],
       [0.51398957, 0.4860104 ],
       [0.5696497 , 0.43035027],
       [0.5272609 , 0.4727391 ],
       [0.6357447 , 0.3642553 ],
       [0.5520544 , 0.4479456 ],
       [0.48447585, 0.51552415],
       [0.49648294, 0.5035171 ],
       [0.54752076, 0.4524792 ],
       [0.5183395 , 0.4816605 ],
       [0.49926475, 0.5007353 ],
       [0.44698644, 0.5530136 ],
       [0.5456371 , 0.45436293],
       [0.44092247, 0.55907756],
       [0.42247203, 0.57752794],
       [0.4747463 , 0.5252537 ],
       [0.53254145, 0.46745852],
       [0.42020965, 0.5797903 ],
       [0.29123276, 0.70876724],
       [0.4787949 , 0.5212051 ],
       [0.516856  , 0.483144  ],
       [0.48889866, 0.5111013 ],
       [0.43215653, 0.5678435 ],
       [0.43104523, 0.56895477],
       [0.4007338 , 0.59926623],
       [0.4891589 , 0.5108411 ],
       [0.45863473, 0.54136527],
       [0.44973433, 0.5502656 ],
       [0.46241805, 0.537582  ],
       [0.45896107, 0.541039  ],
       [0.37531465, 0.6246853 ],
       [0.46547195, 0.5345281 ],
       [0.43697584, 0.56302416],
       [0.5241335 , 0.47586653],
       [0.5294074 , 0.47059262],
       [0.5238709 , 0.4761291 ],
       [0.52410734, 0.47589266],
       [0.5341194 , 0.46588057],
       [0.50167894, 0.49832106],
       [0.36778504, 0.632215  ],
       [0.4517072 , 0.5482928 ],
       [0.5310759 , 0.46892408],
       [0.4266953 , 0.5733047 ],
       [0.42593956, 0.5740604 ],
       [0.47998306, 0.5200169 ],
       [0.50930387, 0.49069607],
       [0.50381607, 0.49618396],
       [0.5439593 , 0.45604068],
       [0.43747985, 0.5625201 ],
       [0.43533456, 0.56466544],
       [0.48361173, 0.5163883 ],
       [0.4615007 , 0.5384993 ],
       [0.48602468, 0.5139753 ],
       [0.526698  , 0.47330204],
       [0.37234518, 0.62765485],
       [0.44436696, 0.555633  ],
       [0.46949035, 0.5305097 ],
       [0.50633925, 0.4936608 ],
       [0.564129  , 0.43587103],
       [0.58388335, 0.41611668],
       [0.5034618 , 0.49653822],
       [0.53555584, 0.4644442 ],
       [0.46539414, 0.53460586],
       [0.5422957 , 0.4577043 ],
       [0.5783114 , 0.4216886 ],
       [0.41256666, 0.58743334],
       [0.33143878, 0.6685613 ],
       [0.47399628, 0.5260037 ],
       [0.51917994, 0.48082006],
       [0.3937583 , 0.6062417 ],
       [0.47512013, 0.5248799 ],
       [0.7006732 , 0.29932678],
       [0.49780226, 0.5021977 ],
       [0.42937934, 0.5706206 ],
       [0.46787924, 0.53212076],
       [0.42558563, 0.5744143 ],
       [0.5435798 , 0.45642018],
       [0.54188   , 0.45811996],
       [0.46426347, 0.53573656],
       [0.46248776, 0.5375123 ],
       [0.44834372, 0.5516563 ],
       [0.5023575 , 0.49764255],
       [0.4990497 , 0.50095034],
       [0.5072717 , 0.49272826],
       [0.5086318 , 0.4913682 ],
       [0.46707729, 0.53292274],
       [0.49241173, 0.50758827],
       [0.36267993, 0.63732004],
       [0.53108776, 0.46891224],
       [0.535291  , 0.46470892],
       [0.5604402 , 0.43955985],
       [0.3733165 , 0.6266835 ],
       [0.49815416, 0.50184584],
       [0.5157128 , 0.48428717],
       [0.40596244, 0.5940376 ],
       [0.43896085, 0.56103915],
       [0.44719672, 0.5528033 ],
       [0.5582875 , 0.44171253],
       [0.5588034 , 0.4411966 ],
       [0.47023004, 0.5297699 ],
       [0.47750467, 0.5224953 ],
       [0.37319073, 0.6268093 ],
       [0.43431652, 0.56568354],
       [0.49656063, 0.50343937],
       [0.45866695, 0.5413331 ],
       [0.5994503 , 0.40054965],
       [0.4955641 , 0.5044359 ],
       [0.4977174 , 0.5022826 ],
       [0.5035867 , 0.4964133 ],
       [0.45735967, 0.5426403 ],
       [0.54821247, 0.45178753],
       [0.29917556, 0.70082444],
       [0.57587874, 0.4241213 ],
       [0.49783564, 0.50216436],
       [0.5540685 , 0.44593146],
       [0.4977632 , 0.50223684],
       [0.39913398, 0.600866  ],
       [0.46271914, 0.53728086],
       [0.51685077, 0.48314923],
       [0.5075397 , 0.49246034],
       [0.6433092 , 0.35669085],
       [0.51885235, 0.48114765],
       [0.5023899 , 0.49761012]], dtype=float32)]
i = 3, Test true class= 
[1 0 0 0 0 1 1 0 1 0 0 0 1 1 1 1 0 1 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 0 1 1 1
 1 1 0 0 0 0 0 0 0 1 0 1 0 1 0 1 0 0 0 0 1 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 1
 1 0 1 1 1 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 1 1 0 0 0 0 1 1 0
 0 0 1 1 0 0 0 0 1 0 0 1 0 0 0 0 1 1 0 1 1 0 0]
In grad_steps = 0, loss = 1.0984740257263184
In grad_steps = 1, loss = 0.7668248414993286
In grad_steps = 2, loss = 0.7006935477256775
In grad_steps = 3, loss = 0.6965586543083191
In grad_steps = 4, loss = 1.0089219808578491
In grad_steps = 5, loss = 0.7415937185287476
In grad_steps = 6, loss = 0.7149452567100525
In grad_steps = 7, loss = 0.7152484655380249
In grad_steps = 8, loss = 0.699260950088501
In grad_steps = 9, loss = 0.775812566280365
In grad_steps = 10, loss = 0.6938817501068115
In grad_steps = 11, loss = 0.754122793674469
In grad_steps = 12, loss = 0.764795184135437
In grad_steps = 13, loss = 0.6902106404304504
In grad_steps = 14, loss = 0.6773438453674316
In grad_steps = 15, loss = 0.6627306342124939
In grad_steps = 16, loss = 0.8549500703811646
In grad_steps = 17, loss = 0.7695615887641907
In grad_steps = 18, loss = 0.7215092182159424
In grad_steps = 19, loss = 0.6489392518997192
In grad_steps = 20, loss = 0.8520436882972717
In grad_steps = 21, loss = 0.725988507270813
In grad_steps = 22, loss = 0.7110267877578735
In grad_steps = 23, loss = 0.6674636602401733
In grad_steps = 24, loss = 0.7391120195388794
In grad_steps = 25, loss = 0.7157577872276306
In grad_steps = 26, loss = 0.6259884238243103
In grad_steps = 27, loss = 0.6401512026786804
In grad_steps = 28, loss = 0.6026355624198914
In grad_steps = 29, loss = 0.7980740070343018
In grad_steps = 30, loss = 0.7916744947433472
In grad_steps = 31, loss = 0.7685218453407288
In grad_steps = 32, loss = 0.6642219424247742
In grad_steps = 33, loss = 0.6720571517944336
In grad_steps = 34, loss = 0.732456624507904
In grad_steps = 35, loss = 0.6805587410926819
In grad_steps = 36, loss = 0.7005337476730347
In grad_steps = 37, loss = 0.6819812655448914
In grad_steps = 38, loss = 0.6867151856422424
In grad_steps = 39, loss = 0.6736962199211121
In grad_steps = 40, loss = 0.7346751093864441
In grad_steps = 41, loss = 0.6791222095489502
In grad_steps = 42, loss = 0.8318390250205994
In grad_steps = 43, loss = 0.679865837097168
In grad_steps = 44, loss = 0.6958391666412354
In grad_steps = 45, loss = 0.7047971487045288
In grad_steps = 46, loss = 0.6899535059928894
In grad_steps = 47, loss = 0.7000755667686462
In grad_steps = 48, loss = 0.6820566058158875
In grad_steps = 49, loss = 0.7014504075050354
In grad_steps = 50, loss = 0.7084249258041382
In grad_steps = 51, loss = 0.6786184310913086
In grad_steps = 52, loss = 0.6716384291648865
In grad_steps = 53, loss = 0.7357625365257263
In grad_steps = 54, loss = 0.727509081363678
In grad_steps = 55, loss = 0.6751651763916016
In grad_steps = 56, loss = 0.7003108859062195
In grad_steps = 57, loss = 0.7025097608566284
In grad_steps = 58, loss = 0.7099305987358093
In grad_steps = 59, loss = 0.7108637094497681
In grad_steps = 60, loss = 0.715447187423706
In grad_steps = 61, loss = 0.6908618807792664
In grad_steps = 62, loss = 0.6971994638442993
In grad_steps = 63, loss = 0.698675811290741
In grad_steps = 64, loss = 0.6934446096420288
In grad_steps = 65, loss = 0.7169649004936218
In grad_steps = 66, loss = 0.7134283185005188
In grad_steps = 67, loss = 0.6724380254745483
In grad_steps = 68, loss = 0.6776993274688721
In grad_steps = 69, loss = 0.6808862686157227
In grad_steps = 70, loss = 0.7399554252624512
In grad_steps = 71, loss = 0.6590837836265564
In grad_steps = 72, loss = 0.7125149369239807
In grad_steps = 73, loss = 0.695979118347168
In grad_steps = 74, loss = 0.6409481167793274
In grad_steps = 75, loss = 0.711496889591217
In grad_steps = 76, loss = 0.7414262890815735
In grad_steps = 77, loss = 0.6817812323570251
In grad_steps = 78, loss = 0.7154505252838135
In grad_steps = 79, loss = 0.7288458347320557
In grad_steps = 80, loss = 0.6836507320404053
In grad_steps = 81, loss = 0.6940367221832275
In grad_steps = 82, loss = 0.6745303869247437
In grad_steps = 83, loss = 0.7337607741355896
In grad_steps = 84, loss = 0.70661860704422
In grad_steps = 85, loss = 0.690750777721405
In grad_steps = 86, loss = 0.6590115427970886
In grad_steps = 87, loss = 0.8259494304656982
In grad_steps = 88, loss = 0.7192367911338806
In grad_steps = 89, loss = 0.6667046546936035
In grad_steps = 90, loss = 0.6888118982315063
In grad_steps = 91, loss = 0.7058295011520386
In grad_steps = 92, loss = 0.6845477819442749
In grad_steps = 93, loss = 0.6958760619163513
In grad_steps = 94, loss = 0.6826627254486084
In grad_steps = 95, loss = 0.6543959379196167
In grad_steps = 96, loss = 0.7406903505325317
In grad_steps = 97, loss = 0.7413594126701355
In grad_steps = 98, loss = 0.7428480982780457
In grad_steps = 99, loss = 0.6554136276245117
In grad_steps = 100, loss = 0.6570874452590942
In grad_steps = 101, loss = 0.7456915378570557
In grad_steps = 102, loss = 0.6809173226356506
In grad_steps = 103, loss = 0.7143961787223816
In grad_steps = 104, loss = 0.6690731644630432
In grad_steps = 105, loss = 0.7140830755233765
In grad_steps = 106, loss = 0.6913593411445618
In grad_steps = 107, loss = 0.6928070783615112
In grad_steps = 108, loss = 0.6760650277137756
In grad_steps = 109, loss = 0.7488384246826172
In grad_steps = 110, loss = 0.6898430585861206
In grad_steps = 111, loss = 0.6903718113899231
In grad_steps = 112, loss = 0.706641435623169
In grad_steps = 113, loss = 0.6881187558174133
In grad_steps = 114, loss = 0.6889275908470154
In grad_steps = 115, loss = 0.6867609024047852
In grad_steps = 116, loss = 0.6946746706962585
In grad_steps = 117, loss = 0.6930052638053894
In grad_steps = 118, loss = 0.6733924746513367
In grad_steps = 119, loss = 0.6918542981147766
In grad_steps = 120, loss = 0.7080063819885254
In grad_steps = 121, loss = 0.7107595205307007
In grad_steps = 122, loss = 0.6850067973136902
In grad_steps = 123, loss = 0.6967852115631104
In grad_steps = 124, loss = 0.6976727843284607
In grad_steps = 125, loss = 0.706047534942627
In grad_steps = 126, loss = 0.7090420722961426
In grad_steps = 127, loss = 0.7113817930221558
In grad_steps = 128, loss = 0.688267707824707
In grad_steps = 129, loss = 0.6945347189903259
In grad_steps = 130, loss = 0.6927223205566406
In grad_steps = 131, loss = 0.6906353235244751
In grad_steps = 132, loss = 0.6992770433425903
In grad_steps = 133, loss = 0.6840195059776306
In grad_steps = 134, loss = 0.6706317663192749
In grad_steps = 135, loss = 0.6736055612564087
In grad_steps = 136, loss = 0.6804126501083374
In grad_steps = 137, loss = 0.7509602904319763
In grad_steps = 138, loss = 0.6537728905677795
In grad_steps = 139, loss = 0.7118623852729797
In grad_steps = 140, loss = 0.696448028087616
In grad_steps = 141, loss = 0.6370326280593872
In grad_steps = 142, loss = 0.7095990180969238
In grad_steps = 143, loss = 0.7345696091651917
In grad_steps = 144, loss = 0.6720324158668518
In grad_steps = 145, loss = 0.7049402594566345
In grad_steps = 146, loss = 0.7030254602432251
In grad_steps = 147, loss = 0.6718875169754028
In grad_steps = 148, loss = 0.6660937070846558
In grad_steps = 149, loss = 0.6533077955245972
In grad_steps = 150, loss = 0.7557061314582825
In grad_steps = 151, loss = 0.7070478796958923
In grad_steps = 152, loss = 0.6938812732696533
In grad_steps = 153, loss = 0.6468755602836609
In grad_steps = 154, loss = 0.8257231712341309
In grad_steps = 155, loss = 0.6978989839553833
In grad_steps = 156, loss = 0.6827525496482849
In grad_steps = 157, loss = 0.6574240922927856
In grad_steps = 158, loss = 0.726995050907135
In grad_steps = 159, loss = 0.6981638669967651
In grad_steps = 160, loss = 0.640038013458252
In grad_steps = 161, loss = 0.6336487531661987
In grad_steps = 162, loss = 0.5910237431526184
In grad_steps = 163, loss = 0.8237685561180115
In grad_steps = 164, loss = 0.7990738749504089
In grad_steps = 165, loss = 0.7539265155792236
In grad_steps = 166, loss = 0.6480584740638733
In grad_steps = 167, loss = 0.6448346376419067
In grad_steps = 168, loss = 0.7062203288078308
In grad_steps = 169, loss = 0.6703594923019409
In grad_steps = 170, loss = 0.6847160458564758
In grad_steps = 171, loss = 0.6812465786933899
In grad_steps = 172, loss = 0.6664085984230042
In grad_steps = 173, loss = 0.6585729122161865
In grad_steps = 174, loss = 0.7294800281524658
In grad_steps = 175, loss = 0.6689682006835938
In grad_steps = 176, loss = 0.7846634387969971
In grad_steps = 177, loss = 0.6840243935585022
In grad_steps = 178, loss = 0.6906059980392456
In grad_steps = 179, loss = 0.6984761953353882
In grad_steps = 180, loss = 0.6795265078544617
In grad_steps = 181, loss = 0.7112647294998169
In grad_steps = 182, loss = 0.6582809686660767
In grad_steps = 183, loss = 0.6868953108787537
In grad_steps = 184, loss = 0.7150892019271851
In grad_steps = 185, loss = 0.6688709855079651
In grad_steps = 186, loss = 0.6617249250411987
In grad_steps = 187, loss = 0.7361137270927429
In grad_steps = 188, loss = 0.7182218432426453
In grad_steps = 189, loss = 0.6781787872314453
In grad_steps = 190, loss = 0.6816588044166565
In grad_steps = 191, loss = 0.6924843788146973
In grad_steps = 192, loss = 0.698095440864563
In grad_steps = 193, loss = 0.7024742960929871
In grad_steps = 194, loss = 0.6968687176704407
In grad_steps = 195, loss = 0.6988173723220825
In grad_steps = 196, loss = 0.6726672649383545
In grad_steps = 197, loss = 0.6874983310699463
In grad_steps = 198, loss = 0.6823806166648865
In grad_steps = 199, loss = 0.7290842533111572
In grad_steps = 200, loss = 0.7105209827423096
In grad_steps = 201, loss = 0.6640114784240723
In grad_steps = 202, loss = 0.6668566465377808
In grad_steps = 203, loss = 0.6716024875640869
In grad_steps = 204, loss = 0.7501500844955444
In grad_steps = 205, loss = 0.6447227001190186
In grad_steps = 206, loss = 0.7128124833106995
In grad_steps = 207, loss = 0.6976185441017151
In grad_steps = 208, loss = 0.6186703443527222
In grad_steps = 209, loss = 0.7165543437004089
In grad_steps = 210, loss = 0.7398242950439453
In grad_steps = 211, loss = 0.6533069610595703
In grad_steps = 212, loss = 0.6834007501602173
In grad_steps = 213, loss = 0.6739884614944458
In grad_steps = 214, loss = 0.6451069116592407
In grad_steps = 215, loss = 0.6193270087242126
In grad_steps = 216, loss = 0.6070024967193604
In grad_steps = 217, loss = 0.8057824373245239
In grad_steps = 218, loss = 0.7025790810585022
In grad_steps = 219, loss = 0.6899659633636475
In grad_steps = 220, loss = 0.6404811143875122
In grad_steps = 221, loss = 0.7531634569168091
In grad_steps = 222, loss = 0.6316299438476562
In grad_steps = 223, loss = 0.7199623584747314
In grad_steps = 224, loss = 0.6249131560325623
In grad_steps = 225, loss = 0.7491861581802368
In grad_steps = 226, loss = 0.696116030216217
In grad_steps = 227, loss = 0.6307700872421265
In grad_steps = 228, loss = 0.6320810317993164
In grad_steps = 229, loss = 0.6262524127960205
In grad_steps = 230, loss = 0.6987330913543701
In grad_steps = 231, loss = 0.7106643915176392
In grad_steps = 232, loss = 0.6841957569122314
In grad_steps = 233, loss = 0.6180858612060547
In grad_steps = 234, loss = 0.5698918700218201
In grad_steps = 235, loss = 0.7001426815986633
In grad_steps = 236, loss = 0.5967501401901245
In grad_steps = 237, loss = 0.681740939617157
In grad_steps = 238, loss = 0.5922149419784546
In grad_steps = 239, loss = 0.5893301963806152
In grad_steps = 240, loss = 0.5940345525741577
In grad_steps = 241, loss = 0.801520586013794
In grad_steps = 242, loss = 0.6376438140869141
In grad_steps = 243, loss = 0.5518768429756165
In grad_steps = 244, loss = 1.0499285459518433
In grad_steps = 245, loss = 0.7737800478935242
In grad_steps = 246, loss = 0.7105448246002197
In grad_steps = 247, loss = 0.6751299500465393
In grad_steps = 248, loss = 0.620361328125
In grad_steps = 249, loss = 0.6907851696014404
In grad_steps = 250, loss = 0.6586024165153503
In grad_steps = 251, loss = 0.6787497401237488
In grad_steps = 252, loss = 0.6437832117080688
In grad_steps = 253, loss = 0.7133943438529968
In grad_steps = 254, loss = 0.6726552248001099
In grad_steps = 255, loss = 0.7091230154037476
In grad_steps = 256, loss = 0.6695259213447571
In grad_steps = 257, loss = 0.6486311554908752
In grad_steps = 258, loss = 0.6996278762817383
In grad_steps = 259, loss = 0.6988978385925293
In grad_steps = 260, loss = 0.702659010887146
In grad_steps = 261, loss = 0.698203444480896
In grad_steps = 262, loss = 0.6693977117538452
In grad_steps = 263, loss = 0.6774213314056396
In grad_steps = 264, loss = 0.6690149307250977
In grad_steps = 265, loss = 0.6828594207763672
In grad_steps = 266, loss = 0.7240821123123169
In grad_steps = 267, loss = 0.6794164180755615
i = 4, Test ensemble probabilities = 
[array([[0.34539714, 0.6546028 ],
       [0.49670178, 0.5032982 ],
       [0.2995092 , 0.70049083],
       [0.5216206 , 0.4783794 ],
       [0.4783175 , 0.5216825 ],
       [0.37964696, 0.620353  ],
       [0.5360174 , 0.4639826 ],
       [0.41234112, 0.5876589 ],
       [0.2805267 , 0.71947336],
       [0.3660757 , 0.6339243 ],
       [0.31246424, 0.68753576],
       [0.5163459 , 0.4836541 ],
       [0.37822625, 0.6217737 ],
       [0.43147096, 0.56852907],
       [0.54082584, 0.4591742 ],
       [0.5715256 , 0.4284744 ],
       [0.4142887 , 0.5857113 ],
       [0.37641716, 0.6235828 ],
       [0.46707872, 0.5329213 ],
       [0.4730467 , 0.5269533 ],
       [0.4055867 , 0.59441334],
       [0.4146544 , 0.5853456 ],
       [0.36647767, 0.6335223 ],
       [0.45998093, 0.5400191 ],
       [0.31840318, 0.6815968 ],
       [0.24469352, 0.75530654],
       [0.44522896, 0.55477107],
       [0.3880463 , 0.61195374],
       [0.37943542, 0.6205646 ],
       [0.2172548 , 0.7827452 ],
       [0.4649846 , 0.5350154 ],
       [0.4034645 , 0.59653556],
       [0.45316827, 0.5468317 ],
       [0.41054946, 0.58945054],
       [0.38889134, 0.61110866],
       [0.30868906, 0.691311  ],
       [0.48083103, 0.519169  ],
       [0.55902576, 0.44097418],
       [0.4085316 , 0.5914684 ],
       [0.41088137, 0.5891186 ],
       [0.4298178 , 0.5701822 ],
       [0.29877663, 0.70122343],
       [0.3293201 , 0.6706799 ],
       [0.42829737, 0.5717026 ],
       [0.49717334, 0.5028267 ],
       [0.52718437, 0.4728156 ],
       [0.42836556, 0.5716345 ],
       [0.4556835 , 0.54431653],
       [0.3669375 , 0.63306254],
       [0.4527557 , 0.5472443 ],
       [0.2731709 , 0.7268291 ],
       [0.42771834, 0.5722816 ],
       [0.38610366, 0.61389637],
       [0.3611515 , 0.6388485 ],
       [0.39574474, 0.60425526],
       [0.48206717, 0.51793283],
       [0.4352225 , 0.56477743],
       [0.5293537 , 0.47064632],
       [0.4351939 , 0.56480604],
       [0.39494088, 0.6050591 ],
       [0.29629105, 0.703709  ],
       [0.39273342, 0.60726655],
       [0.37942597, 0.620574  ],
       [0.40392777, 0.59607226],
       [0.42278576, 0.57721424],
       [0.42321032, 0.5767897 ],
       [0.3290817 , 0.6709183 ],
       [0.38383117, 0.6161688 ],
       [0.4492992 , 0.55070084],
       [0.41927034, 0.58072966],
       [0.38925087, 0.6107492 ],
       [0.4150021 , 0.5849979 ],
       [0.5914758 , 0.40852422],
       [0.4447122 , 0.5552878 ],
       [0.5072293 , 0.4927707 ],
       [0.5590465 , 0.44095352],
       [0.4567155 , 0.54328454],
       [0.26308432, 0.73691565],
       [0.47329345, 0.5267065 ],
       [0.5217634 , 0.47823662],
       [0.2883945 , 0.71160555],
       [0.48484313, 0.51515687],
       [0.785682  , 0.21431798],
       [0.44177085, 0.55822915],
       [0.4268143 , 0.57318574],
       [0.3282022 , 0.6717979 ],
       [0.27888694, 0.721113  ],
       [0.43272623, 0.56727374],
       [0.48700717, 0.5129928 ],
       [0.24735552, 0.7526445 ],
       [0.3437499 , 0.65625006],
       [0.47496793, 0.52503204],
       [0.35789973, 0.64210033],
       [0.43090022, 0.5690998 ],
       [0.40508464, 0.5949154 ],
       [0.4418508 , 0.5581492 ],
       [0.50487167, 0.49512833],
       [0.5045017 , 0.4954983 ],
       [0.33627528, 0.6637247 ],
       [0.43236625, 0.5676338 ],
       [0.4723027 , 0.5276973 ],
       [0.55153644, 0.44846356],
       [0.410058  , 0.58994204],
       [0.43052799, 0.5694721 ],
       [0.45123821, 0.54876184],
       [0.35328087, 0.6467191 ],
       [0.45968103, 0.54031897],
       [0.33205697, 0.66794306],
       [0.5815101 , 0.41848987],
       [0.55743647, 0.44256356],
       [0.4615889 , 0.53841114],
       [0.5988489 , 0.40115115],
       [0.26768813, 0.73231184],
       [0.4092803 , 0.5907197 ],
       [0.44905618, 0.55094385],
       [0.40612987, 0.5938701 ],
       [0.4797793 , 0.5202207 ],
       [0.490779  , 0.50922096],
       [0.50890255, 0.49109742],
       [0.46478626, 0.5352137 ],
       [0.39073965, 0.6092604 ],
       [0.6140519 , 0.38594812],
       [0.21095832, 0.78904164],
       [0.50982255, 0.49017748],
       [0.33999068, 0.6600093 ],
       [0.4652964 , 0.5347036 ],
       [0.4872016 , 0.51279837],
       [0.3138431 , 0.6861569 ],
       [0.5067988 , 0.49320123],
       [0.41474557, 0.5852544 ],
       [0.4137968 , 0.58620316],
       [0.6387266 , 0.3612734 ],
       [0.29403043, 0.7059696 ],
       [0.46303543, 0.5369646 ]], dtype=float32), array([[0.3880703 , 0.6119297 ],
       [0.51199114, 0.48800892],
       [0.3143604 , 0.68563956],
       [0.46775058, 0.53224945],
       [0.4228081 , 0.5771919 ],
       [0.46305248, 0.5369475 ],
       [0.5530051 , 0.4469949 ],
       [0.40783724, 0.5921628 ],
       [0.34845847, 0.65154153],
       [0.42968783, 0.5703122 ],
       [0.36605135, 0.6339487 ],
       [0.52814126, 0.4718587 ],
       [0.4303292 , 0.5696708 ],
       [0.5311472 , 0.46885288],
       [0.54523605, 0.45476392],
       [0.54236245, 0.45763755],
       [0.49351203, 0.50648797],
       [0.38893303, 0.61106694],
       [0.4891141 , 0.5108859 ],
       [0.56814957, 0.43185046],
       [0.48065817, 0.5193418 ],
       [0.4705589 , 0.5294411 ],
       [0.48349407, 0.51650596],
       [0.4876348 , 0.5123652 ],
       [0.3791615 , 0.62083846],
       [0.37122786, 0.62877214],
       [0.43571815, 0.5642818 ],
       [0.41554213, 0.5844579 ],
       [0.4376437 , 0.5623563 ],
       [0.2725442 , 0.72745574],
       [0.48336023, 0.51663977],
       [0.41728634, 0.58271366],
       [0.44753623, 0.55246377],
       [0.45130196, 0.54869807],
       [0.47825754, 0.52174246],
       [0.3706702 , 0.6293298 ],
       [0.5130376 , 0.48696244],
       [0.5888656 , 0.41113442],
       [0.49567425, 0.50432575],
       [0.4533961 , 0.5466039 ],
       [0.46075943, 0.53924054],
       [0.299956  , 0.70004404],
       [0.40043336, 0.59956664],
       [0.46529213, 0.5347079 ],
       [0.50553155, 0.49446842],
       [0.47945037, 0.5205496 ],
       [0.47140384, 0.5285961 ],
       [0.520463  , 0.479537  ],
       [0.4369172 , 0.5630828 ],
       [0.47227037, 0.52772963],
       [0.39890897, 0.6010911 ],
       [0.5099803 , 0.4900197 ],
       [0.44546404, 0.554536  ],
       [0.42215705, 0.577843  ],
       [0.3576284 , 0.64237154],
       [0.4861414 , 0.5138586 ],
       [0.4707984 , 0.5292016 ],
       [0.49237502, 0.50762504],
       [0.50471   , 0.49528992],
       [0.44178778, 0.5582122 ],
       [0.37154227, 0.6284577 ],
       [0.40878373, 0.59121627],
       [0.41240627, 0.58759373],
       [0.4968262 , 0.50317377],
       [0.47246245, 0.5275376 ],
       [0.43943956, 0.5605604 ],
       [0.3889983 , 0.6110017 ],
       [0.41751316, 0.58248687],
       [0.47119093, 0.5288091 ],
       [0.3927289 , 0.60727113],
       [0.49005207, 0.5099479 ],
       [0.41542542, 0.5845746 ],
       [0.62426174, 0.37573823],
       [0.4453439 , 0.5546561 ],
       [0.5046319 , 0.49536812],
       [0.57116544, 0.42883456],
       [0.46410406, 0.53589594],
       [0.27322844, 0.72677153],
       [0.48610952, 0.5138905 ],
       [0.5164864 , 0.48351362],
       [0.37881854, 0.6211814 ],
       [0.5108697 , 0.48913032],
       [0.6448277 , 0.3551723 ],
       [0.429549  , 0.57045096],
       [0.43403113, 0.5659689 ],
       [0.3908836 , 0.60911644],
       [0.33232817, 0.6676718 ],
       [0.57920706, 0.42079297],
       [0.47585174, 0.5241482 ],
       [0.30917242, 0.6908276 ],
       [0.3856376 , 0.61436236],
       [0.40961602, 0.590384  ],
       [0.39530924, 0.6046908 ],
       [0.47819042, 0.5218095 ],
       [0.49915648, 0.5008435 ],
       [0.50102425, 0.49897575],
       [0.4867456 , 0.5132544 ],
       [0.46412638, 0.53587365],
       [0.39740893, 0.6025911 ],
       [0.5355354 , 0.46446458],
       [0.46589816, 0.53410184],
       [0.6296572 , 0.3703428 ],
       [0.41445395, 0.5855461 ],
       [0.4707105 , 0.52928954],
       [0.479547  , 0.52045304],
       [0.3625186 , 0.63748145],
       [0.44902927, 0.55097073],
       [0.3706048 , 0.6293952 ],
       [0.5403005 , 0.45969948],
       [0.58481735, 0.4151827 ],
       [0.4457463 , 0.55425376],
       [0.55166525, 0.44833475],
       [0.35043052, 0.6495695 ],
       [0.38592914, 0.61407083],
       [0.49783325, 0.50216675],
       [0.41118985, 0.5888102 ],
       [0.5627369 , 0.43726304],
       [0.4429057 , 0.55709434],
       [0.42174813, 0.5782519 ],
       [0.52862275, 0.47137725],
       [0.39108476, 0.6089152 ],
       [0.5507947 , 0.44920528],
       [0.25938937, 0.74061066],
       [0.5540879 , 0.44591215],
       [0.3860679 , 0.6139321 ],
       [0.5129485 , 0.48705152],
       [0.4602273 , 0.53977275],
       [0.35588855, 0.64411145],
       [0.5478818 , 0.45211825],
       [0.4290892 , 0.5709108 ],
       [0.38805577, 0.6119442 ],
       [0.6379908 , 0.3620092 ],
       [0.37617096, 0.62382907],
       [0.46227485, 0.53772515]], dtype=float32), array([[0.44110677, 0.5588932 ],
       [0.5099021 , 0.49009785],
       [0.39384803, 0.606152  ],
       [0.5550005 , 0.44499952],
       [0.49347296, 0.50652707],
       [0.47046053, 0.52953947],
       [0.53958225, 0.46041778],
       [0.42666543, 0.5733346 ],
       [0.39377427, 0.6062258 ],
       [0.45215416, 0.54784584],
       [0.3697928 , 0.63020724],
       [0.54529893, 0.45470107],
       [0.41305026, 0.5869497 ],
       [0.57336444, 0.42663556],
       [0.5874033 , 0.41259673],
       [0.6317128 , 0.3682872 ],
       [0.54657364, 0.45342636],
       [0.48408425, 0.5159157 ],
       [0.53951687, 0.46048316],
       [0.62078726, 0.37921268],
       [0.46029982, 0.5397002 ],
       [0.5045198 , 0.49548018],
       [0.41166788, 0.5883321 ],
       [0.53231734, 0.46768266],
       [0.4298416 , 0.57015836],
       [0.34908837, 0.6509116 ],
       [0.4417619 , 0.5582381 ],
       [0.48157573, 0.5184243 ],
       [0.42407236, 0.5759276 ],
       [0.27093172, 0.7290683 ],
       [0.54841894, 0.45158112],
       [0.53199255, 0.46800745],
       [0.49500385, 0.5049961 ],
       [0.45061103, 0.54938895],
       [0.45088303, 0.54911697],
       [0.3670385 , 0.63296145],
       [0.52968395, 0.47031608],
       [0.560933  , 0.439067  ],
       [0.51174086, 0.48825908],
       [0.45324862, 0.5467514 ],
       [0.48598278, 0.5140172 ],
       [0.36735138, 0.6326486 ],
       [0.43714756, 0.5628524 ],
       [0.4742983 , 0.52570176],
       [0.49965146, 0.50034857],
       [0.5289379 , 0.47106215],
       [0.48558396, 0.51441604],
       [0.50400245, 0.4959975 ],
       [0.61590755, 0.3840924 ],
       [0.5272604 , 0.47273958],
       [0.3950243 , 0.6049757 ],
       [0.53334624, 0.46665382],
       [0.48781684, 0.5121831 ],
       [0.42478013, 0.5752199 ],
       [0.45202702, 0.547973  ],
       [0.5503648 , 0.4496352 ],
       [0.5060665 , 0.4939335 ],
       [0.5314929 , 0.46850708],
       [0.52381426, 0.47618568],
       [0.4952375 , 0.5047625 ],
       [0.34677762, 0.6532224 ],
       [0.43402877, 0.5659712 ],
       [0.472528  , 0.52747196],
       [0.5363826 , 0.46361735],
       [0.48564258, 0.51435745],
       [0.45882842, 0.54117155],
       [0.36617044, 0.6338296 ],
       [0.4553812 , 0.54461884],
       [0.52849823, 0.47150177],
       [0.46100917, 0.5389908 ],
       [0.53343207, 0.46656787],
       [0.4725518 , 0.5274482 ],
       [0.5915467 , 0.40845326],
       [0.5089526 , 0.4910474 ],
       [0.5787737 , 0.4212263 ],
       [0.6289706 , 0.37102935],
       [0.4610983 , 0.5389017 ],
       [0.36730307, 0.63269687],
       [0.49290228, 0.5070977 ],
       [0.5743463 , 0.42565373],
       [0.41176444, 0.58823556],
       [0.57724077, 0.4227592 ],
       [0.667577  , 0.33242297],
       [0.48716778, 0.5128322 ],
       [0.48380274, 0.51619726],
       [0.3990585 , 0.6009415 ],
       [0.41994804, 0.58005196],
       [0.5866533 , 0.41334674],
       [0.52895737, 0.47104266],
       [0.3075473 , 0.6924527 ],
       [0.4288635 , 0.5711365 ],
       [0.44757962, 0.5524204 ],
       [0.42163134, 0.57836866],
       [0.5359737 , 0.46402627],
       [0.46092388, 0.53907615],
       [0.47902292, 0.5209771 ],
       [0.5037398 , 0.49626023],
       [0.52997375, 0.47002625],
       [0.38771752, 0.61228245],
       [0.53718835, 0.4628116 ],
       [0.48906595, 0.51093405],
       [0.622202  , 0.377798  ],
       [0.46290788, 0.53709215],
       [0.46353817, 0.53646183],
       [0.50774276, 0.49225727],
       [0.419484  , 0.58051604],
       [0.47130263, 0.52869743],
       [0.4380897 , 0.5619103 ],
       [0.6185946 , 0.38140538],
       [0.5557781 , 0.4442219 ],
       [0.46582744, 0.53417253],
       [0.5828968 , 0.41710314],
       [0.34994653, 0.65005344],
       [0.4323789 , 0.5676211 ],
       [0.48609424, 0.51390576],
       [0.45360595, 0.546394  ],
       [0.5449939 , 0.4550061 ],
       [0.51030445, 0.48969558],
       [0.52679735, 0.47320265],
       [0.55073667, 0.4492633 ],
       [0.47507444, 0.52492553],
       [0.6020182 , 0.39798182],
       [0.28503522, 0.71496475],
       [0.59075934, 0.40924063],
       [0.47842413, 0.52157587],
       [0.5427274 , 0.45727256],
       [0.5574557 , 0.44254425],
       [0.40166306, 0.59833694],
       [0.54595387, 0.45404613],
       [0.4896713 , 0.5103287 ],
       [0.45220754, 0.5477925 ],
       [0.68647027, 0.3135298 ],
       [0.4484612 , 0.5515388 ],
       [0.49787235, 0.50212765]], dtype=float32), array([[0.43671542, 0.5632846 ],
       [0.5093111 , 0.4906889 ],
       [0.42088732, 0.57911265],
       [0.50538373, 0.49461624],
       [0.5882425 , 0.4117575 ],
       [0.43135256, 0.5686474 ],
       [0.4381418 , 0.5618582 ],
       [0.3954127 , 0.60458726],
       [0.40681872, 0.59318125],
       [0.43817   , 0.56183   ],
       [0.38717157, 0.61282843],
       [0.52158344, 0.4784165 ],
       [0.51398957, 0.4860104 ],
       [0.5696497 , 0.43035027],
       [0.5272609 , 0.4727391 ],
       [0.6357447 , 0.3642553 ],
       [0.5520544 , 0.4479456 ],
       [0.48447585, 0.51552415],
       [0.49648294, 0.5035171 ],
       [0.54752076, 0.4524792 ],
       [0.5183395 , 0.4816605 ],
       [0.49926475, 0.5007353 ],
       [0.44698644, 0.5530136 ],
       [0.5456371 , 0.45436293],
       [0.44092247, 0.55907756],
       [0.42247203, 0.57752794],
       [0.4747463 , 0.5252537 ],
       [0.53254145, 0.46745852],
       [0.42020965, 0.5797903 ],
       [0.29123276, 0.70876724],
       [0.4787949 , 0.5212051 ],
       [0.516856  , 0.483144  ],
       [0.48889866, 0.5111013 ],
       [0.43215653, 0.5678435 ],
       [0.43104523, 0.56895477],
       [0.4007338 , 0.59926623],
       [0.4891589 , 0.5108411 ],
       [0.45863473, 0.54136527],
       [0.44973433, 0.5502656 ],
       [0.46241805, 0.537582  ],
       [0.45896107, 0.541039  ],
       [0.37531465, 0.6246853 ],
       [0.46547195, 0.5345281 ],
       [0.43697584, 0.56302416],
       [0.5241335 , 0.47586653],
       [0.5294074 , 0.47059262],
       [0.5238709 , 0.4761291 ],
       [0.52410734, 0.47589266],
       [0.5341194 , 0.46588057],
       [0.50167894, 0.49832106],
       [0.36778504, 0.632215  ],
       [0.4517072 , 0.5482928 ],
       [0.5310759 , 0.46892408],
       [0.4266953 , 0.5733047 ],
       [0.42593956, 0.5740604 ],
       [0.47998306, 0.5200169 ],
       [0.50930387, 0.49069607],
       [0.50381607, 0.49618396],
       [0.5439593 , 0.45604068],
       [0.43747985, 0.5625201 ],
       [0.43533456, 0.56466544],
       [0.48361173, 0.5163883 ],
       [0.4615007 , 0.5384993 ],
       [0.48602468, 0.5139753 ],
       [0.526698  , 0.47330204],
       [0.37234518, 0.62765485],
       [0.44436696, 0.555633  ],
       [0.46949035, 0.5305097 ],
       [0.50633925, 0.4936608 ],
       [0.564129  , 0.43587103],
       [0.58388335, 0.41611668],
       [0.5034618 , 0.49653822],
       [0.53555584, 0.4644442 ],
       [0.46539414, 0.53460586],
       [0.5422957 , 0.4577043 ],
       [0.5783114 , 0.4216886 ],
       [0.41256666, 0.58743334],
       [0.33143878, 0.6685613 ],
       [0.47399628, 0.5260037 ],
       [0.51917994, 0.48082006],
       [0.3937583 , 0.6062417 ],
       [0.47512013, 0.5248799 ],
       [0.7006732 , 0.29932678],
       [0.49780226, 0.5021977 ],
       [0.42937934, 0.5706206 ],
       [0.46787924, 0.53212076],
       [0.42558563, 0.5744143 ],
       [0.5435798 , 0.45642018],
       [0.54188   , 0.45811996],
       [0.46426347, 0.53573656],
       [0.46248776, 0.5375123 ],
       [0.44834372, 0.5516563 ],
       [0.5023575 , 0.49764255],
       [0.4990497 , 0.50095034],
       [0.5072717 , 0.49272826],
       [0.5086318 , 0.4913682 ],
       [0.46707729, 0.53292274],
       [0.49241173, 0.50758827],
       [0.36267993, 0.63732004],
       [0.53108776, 0.46891224],
       [0.535291  , 0.46470892],
       [0.5604402 , 0.43955985],
       [0.3733165 , 0.6266835 ],
       [0.49815416, 0.50184584],
       [0.5157128 , 0.48428717],
       [0.40596244, 0.5940376 ],
       [0.43896085, 0.56103915],
       [0.44719672, 0.5528033 ],
       [0.5582875 , 0.44171253],
       [0.5588034 , 0.4411966 ],
       [0.47023004, 0.5297699 ],
       [0.47750467, 0.5224953 ],
       [0.37319073, 0.6268093 ],
       [0.43431652, 0.56568354],
       [0.49656063, 0.50343937],
       [0.45866695, 0.5413331 ],
       [0.5994503 , 0.40054965],
       [0.4955641 , 0.5044359 ],
       [0.4977174 , 0.5022826 ],
       [0.5035867 , 0.4964133 ],
       [0.45735967, 0.5426403 ],
       [0.54821247, 0.45178753],
       [0.29917556, 0.70082444],
       [0.57587874, 0.4241213 ],
       [0.49783564, 0.50216436],
       [0.5540685 , 0.44593146],
       [0.4977632 , 0.50223684],
       [0.39913398, 0.600866  ],
       [0.46271914, 0.53728086],
       [0.51685077, 0.48314923],
       [0.5075397 , 0.49246034],
       [0.6433092 , 0.35669085],
       [0.51885235, 0.48114765],
       [0.5023899 , 0.49761012]], dtype=float32), array([[0.4144993 , 0.5855007 ],
       [0.5199027 , 0.48009735],
       [0.4142466 , 0.58575344],
       [0.5571132 , 0.44288686],
       [0.54075384, 0.4592462 ],
       [0.50333923, 0.49666077],
       [0.5805485 , 0.41945145],
       [0.477804  , 0.522196  ],
       [0.43255585, 0.5674442 ],
       [0.4624811 , 0.5375189 ],
       [0.42684975, 0.5731503 ],
       [0.4823305 , 0.51766944],
       [0.484503  , 0.51549697],
       [0.55689573, 0.4431043 ],
       [0.55612695, 0.443873  ],
       [0.5467859 , 0.4532141 ],
       [0.52049303, 0.479507  ],
       [0.43660375, 0.5633963 ],
       [0.50367725, 0.4963227 ],
       [0.54109484, 0.45890516],
       [0.52047825, 0.47952178],
       [0.5002179 , 0.4997821 ],
       [0.4727334 , 0.5272666 ],
       [0.51864094, 0.48135906],
       [0.43163612, 0.56836385],
       [0.41115057, 0.5888494 ],
       [0.47127885, 0.52872115],
       [0.52322286, 0.4767771 ],
       [0.41716483, 0.5828352 ],
       [0.3482199 , 0.65178007],
       [0.5294368 , 0.47056317],
       [0.48861223, 0.51138777],
       [0.5051897 , 0.49481028],
       [0.43763432, 0.56236565],
       [0.43125525, 0.5687447 ],
       [0.4086542 , 0.5913458 ],
       [0.54560494, 0.4543951 ],
       [0.5417441 , 0.4582559 ],
       [0.51985747, 0.48014256],
       [0.51416785, 0.4858322 ],
       [0.5080941 , 0.49190593],
       [0.46863297, 0.53136706],
       [0.4718202 , 0.52817976],
       [0.44524452, 0.5547555 ],
       [0.50973725, 0.4902628 ],
       [0.48320687, 0.51679313],
       [0.53371406, 0.4662859 ],
       [0.48546007, 0.5145399 ],
       [0.565877  , 0.43412298],
       [0.515638  , 0.48436198],
       [0.39421687, 0.6057831 ],
       [0.43995598, 0.560044  ],
       [0.49259526, 0.50740474],
       [0.46818745, 0.53181255],
       [0.46235594, 0.5376441 ],
       [0.5304859 , 0.4695141 ],
       [0.5338076 , 0.46619245],
       [0.52465916, 0.47534084],
       [0.50269127, 0.4973088 ],
       [0.4799926 , 0.52000743],
       [0.4114706 , 0.58852935],
       [0.5141759 , 0.4858241 ],
       [0.48793596, 0.512064  ],
       [0.49044725, 0.5095528 ],
       [0.5302802 , 0.4697198 ],
       [0.44063002, 0.5593699 ],
       [0.4437732 , 0.5562268 ],
       [0.4474504 , 0.5525496 ],
       [0.53816795, 0.46183208],
       [0.49553928, 0.5044607 ],
       [0.52805096, 0.471949  ],
       [0.44623974, 0.5537603 ],
       [0.56177694, 0.43822303],
       [0.47467163, 0.5253284 ],
       [0.51837474, 0.48162526],
       [0.55727786, 0.44272217],
       [0.48007733, 0.5199227 ],
       [0.3959322 , 0.60406774],
       [0.4907967 , 0.5092034 ],
       [0.52102613, 0.47897387],
       [0.47689986, 0.52310014],
       [0.51897854, 0.48102146],
       [0.7017346 , 0.29826537],
       [0.48518142, 0.5148186 ],
       [0.47607723, 0.52392274],
       [0.4353069 , 0.56469303],
       [0.4226922 , 0.5773078 ],
       [0.48826623, 0.5117338 ],
       [0.53689533, 0.4631047 ],
       [0.41604808, 0.5839519 ],
       [0.4558642 , 0.5441358 ],
       [0.48199576, 0.51800424],
       [0.48737365, 0.51262635],
       [0.5053642 , 0.4946358 ],
       [0.48574358, 0.5142564 ],
       [0.49467298, 0.50532705],
       [0.51627296, 0.48372698],
       [0.5230673 , 0.4769327 ],
       [0.45366225, 0.5463378 ],
       [0.5219409 , 0.47805908],
       [0.4850375 , 0.5149625 ],
       [0.5201845 , 0.47981548],
       [0.48545197, 0.514548  ],
       [0.45414022, 0.54585975],
       [0.54502034, 0.4549796 ],
       [0.47657204, 0.52342796],
       [0.49952126, 0.50047874],
       [0.4142758 , 0.58572423],
       [0.563667  , 0.43633306],
       [0.59131676, 0.40868324],
       [0.4716282 , 0.5283718 ],
       [0.6205695 , 0.37943047],
       [0.39382887, 0.60617113],
       [0.44315088, 0.5568491 ],
       [0.4888739 , 0.5111261 ],
       [0.45780182, 0.5421982 ],
       [0.51164603, 0.4883539 ],
       [0.5445171 , 0.4554829 ],
       [0.48898208, 0.5110179 ],
       [0.5050066 , 0.4949934 ],
       [0.46831807, 0.53168195],
       [0.57932794, 0.42067212],
       [0.37057725, 0.6294228 ],
       [0.56177413, 0.43822587],
       [0.4613803 , 0.5386197 ],
       [0.53383416, 0.46616584],
       [0.5351264 , 0.46487358],
       [0.4190312 , 0.5809688 ],
       [0.47846884, 0.5215311 ],
       [0.48827532, 0.5117247 ],
       [0.4572432 , 0.5427568 ],
       [0.5983209 , 0.4016791 ],
       [0.47842318, 0.5215768 ],
       [0.4984665 , 0.5015335 ]], dtype=float32)]
i = 4, Test true class= 
[1 0 0 0 0 1 1 0 1 0 0 0 1 1 1 1 0 1 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 0 1 1 1
 1 1 0 0 0 0 0 0 0 1 0 1 0 1 0 1 0 0 0 0 1 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 1
 1 0 1 1 1 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 1 1 0 0 0 0 1 1 0
 0 0 1 1 0 0 0 0 1 0 0 1 0 0 0 0 1 1 0 1 1 0 0]
Final, Test average ensemble probabilities = 
[[0.4051578  0.5948422 ]
 [0.5095618  0.49043828]
 [0.3685703  0.6314297 ]
 [0.5213737  0.4786263 ]
 [0.50471896 0.49528104]
 [0.44957036 0.55042964]
 [0.529459   0.47054094]
 [0.4240121  0.57598794]
 [0.3724268  0.6275732 ]
 [0.42971373 0.5702863 ]
 [0.3724659  0.6275341 ]
 [0.51874006 0.48125997]
 [0.44401965 0.5559803 ]
 [0.53250563 0.4674944 ]
 [0.5513706  0.44862938]
 [0.58562624 0.41437373]
 [0.5053843  0.49461564]
 [0.43410283 0.5658971 ]
 [0.49917397 0.500826  ]
 [0.55011976 0.44988018]
 [0.47707254 0.5229276 ]
 [0.47784314 0.52215683]
 [0.4362719  0.56372815]
 [0.50884223 0.49115777]
 [0.399993   0.600007  ]
 [0.3597265  0.6402736 ]
 [0.45374686 0.54625314]
 [0.46818572 0.5318143 ]
 [0.4157052  0.5842948 ]
 [0.2800367  0.7199633 ]
 [0.5009991  0.49900094]
 [0.47164232 0.5283577 ]
 [0.47795933 0.52204067]
 [0.43645066 0.56354934]
 [0.43606648 0.5639335 ]
 [0.37115714 0.6288429 ]
 [0.5116633  0.4883367 ]
 [0.5418407  0.45815936]
 [0.4771077  0.5228923 ]
 [0.4588224  0.54117763]
 [0.468723   0.53127694]
 [0.36200634 0.6379937 ]
 [0.42083865 0.57916135]
 [0.4500216  0.5499784 ]
 [0.5072454  0.4927546 ]
 [0.50963736 0.49036264]
 [0.48858768 0.5114123 ]
 [0.49794325 0.5020567 ]
 [0.5039517  0.49604827]
 [0.4939207  0.5060793 ]
 [0.36582118 0.6341788 ]
 [0.47254163 0.52745837]
 [0.46861115 0.5313889 ]
 [0.4205943  0.57940567]
 [0.41873914 0.58126086]
 [0.5058085  0.49419156]
 [0.49103975 0.5089602 ]
 [0.5163394  0.48366064]
 [0.50207376 0.49792624]
 [0.4498877  0.55011225]
 [0.37228322 0.6277168 ]
 [0.44666672 0.5533333 ]
 [0.44275936 0.5572406 ]
 [0.48272172 0.5172783 ]
 [0.48757377 0.51242626]
 [0.4268907  0.57310927]
 [0.39447814 0.60552186]
 [0.43473324 0.5652667 ]
 [0.49869913 0.50130093]
 [0.46653533 0.5334647 ]
 [0.50493383 0.4950661 ]
 [0.45053616 0.54946387]
 [0.58092344 0.4190766 ]
 [0.46781492 0.53218514]
 [0.53026104 0.4697389 ]
 [0.57895434 0.42104563]
 [0.45491236 0.54508764]
 [0.32619736 0.6738026 ]
 [0.48341966 0.5165804 ]
 [0.5305604  0.4694396 ]
 [0.38992715 0.6100729 ]
 [0.51341045 0.48658952]
 [0.7000989  0.29990107]
 [0.46829423 0.53170574]
 [0.45002094 0.54997903]
 [0.40426606 0.5957339 ]
 [0.3758882  0.6241118 ]
 [0.5260865  0.4739135 ]
 [0.5141183  0.48588166]
 [0.34887737 0.6511227 ]
 [0.41532058 0.5846794 ]
 [0.45250064 0.54749936]
 [0.43291432 0.56708574]
 [0.48989564 0.5101043 ]
 [0.47163606 0.52836394]
 [0.48504052 0.51495945]
 [0.49574146 0.5042585 ]
 [0.50281614 0.49718386]
 [0.38754877 0.6124512 ]
 [0.51162374 0.4883763 ]
 [0.48951906 0.51048094]
 [0.57680404 0.42319593]
 [0.42923766 0.57076234]
 [0.4634142  0.5365858 ]
 [0.49985224 0.50014776]
 [0.4035636  0.5964364 ]
 [0.463699   0.536301  ]
 [0.4004448  0.5995552 ]
 [0.572472   0.42752805]
 [0.5696304  0.43036956]
 [0.46300417 0.5369958 ]
 [0.566297   0.433703  ]
 [0.34701696 0.65298307]
 [0.42101115 0.57898885]
 [0.48368365 0.51631635]
 [0.43747893 0.5625211 ]
 [0.5397213  0.4602787 ]
 [0.49681407 0.5031859 ]
 [0.48882952 0.5111705 ]
 [0.51054776 0.4894522 ]
 [0.43651533 0.56348467]
 [0.5788811  0.42111897]
 [0.28502715 0.71497285]
 [0.5584645  0.44153547]
 [0.43273973 0.56726027]
 [0.521775   0.478225  ]
 [0.5075549  0.49244514]
 [0.37791196 0.6220881 ]
 [0.5083645  0.4916355 ]
 [0.4677264  0.5322736 ]
 [0.4437686  0.5562314 ]
 [0.64096355 0.35903645]
 [0.42318764 0.5768124 ]
 [0.4848078  0.5151922 ]]
Accuracy: 0.4627
MCC: -0.0458
AUC: 0.4855
Confusion Matrix:
tensor([[22, 51],
        [21, 40]])
Specificity: 0.3014
Precision (Macro): 0.4756
F1 Score (Macro): 0.4528
Expected Calibration Error (ECE): 0.1194
NLL loss: 0.7096
Main task is done! Can finish
