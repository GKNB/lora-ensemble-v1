Job is running on partition: gpu_ss11
Job is running under account: m2616_g
/pscratch/sd/t/tianle/conda/envs/lucid_surp_2024/lib/python3.11/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:09<00:29,  9.87s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:20<00:20, 10.05s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:30<00:10, 10.15s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:32<00:00,  7.09s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:32<00:00,  8.18s/it]
Llama3 has been loaded successfully.
Model:  LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(128256, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)
          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)
          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)
          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)
)
Size of dataset:  98
In grad_steps = 0, loss = 0.311132550239563
In grad_steps = 1, loss = 1.8503429889678955
In grad_steps = 2, loss = 0.7704724073410034
In grad_steps = 3, loss = 0.8450108766555786
In grad_steps = 4, loss = 0.816707968711853
In grad_steps = 5, loss = 0.6224702596664429
In grad_steps = 6, loss = 0.33529114723205566
In grad_steps = 7, loss = 1.3698105812072754
In grad_steps = 8, loss = 0.7917951345443726
In grad_steps = 9, loss = 0.47051697969436646
In grad_steps = 10, loss = 1.4566447734832764
In grad_steps = 11, loss = 0.9405338764190674
In grad_steps = 12, loss = 0.651216447353363
In grad_steps = 13, loss = 1.1836445331573486
In grad_steps = 14, loss = 0.38644808530807495
In grad_steps = 15, loss = 0.21854716539382935
In grad_steps = 16, loss = 0.8914787173271179
In grad_steps = 17, loss = 0.9955219626426697
In grad_steps = 18, loss = 1.056585669517517
In grad_steps = 19, loss = 0.9960461854934692
In grad_steps = 20, loss = 0.3532053232192993
In grad_steps = 21, loss = 0.7997174263000488
In grad_steps = 22, loss = 0.3774365782737732
In grad_steps = 23, loss = 0.442889541387558
In grad_steps = 24, loss = 0.5067412257194519
In grad_steps = 25, loss = 0.7759097814559937
In grad_steps = 26, loss = 0.7370818257331848
In grad_steps = 27, loss = 1.3676927089691162
In grad_steps = 28, loss = 0.7960914373397827
In grad_steps = 29, loss = 1.0309245586395264
In grad_steps = 30, loss = 0.7065779566764832
In grad_steps = 31, loss = 0.7094327807426453
In grad_steps = 32, loss = 0.8706367015838623
In grad_steps = 33, loss = 0.6581581234931946
In grad_steps = 34, loss = 0.5252199172973633
In grad_steps = 35, loss = 0.6800829172134399
In grad_steps = 36, loss = 0.5208219885826111
In grad_steps = 37, loss = 0.7828278541564941
In grad_steps = 38, loss = 0.7478902339935303
In grad_steps = 39, loss = 1.0263879299163818
In grad_steps = 40, loss = 0.40038609504699707
In grad_steps = 41, loss = 0.7288659811019897
In grad_steps = 42, loss = 0.734541118144989
In grad_steps = 43, loss = 0.7175202369689941
In grad_steps = 44, loss = 0.5146227478981018
In grad_steps = 45, loss = 0.4862844944000244
In grad_steps = 46, loss = 0.8727055788040161
In grad_steps = 47, loss = 0.698864221572876
In grad_steps = 48, loss = 0.5643097162246704
In grad_steps = 49, loss = 0.9340505599975586
In grad_steps = 50, loss = 0.8258808255195618
In grad_steps = 51, loss = 0.7481812834739685
In grad_steps = 52, loss = 0.7169313430786133
In grad_steps = 53, loss = 0.6299868822097778
In grad_steps = 54, loss = 0.5037624835968018
In grad_steps = 55, loss = 0.6877157688140869
In grad_steps = 56, loss = 0.7335491180419922
In grad_steps = 57, loss = 0.7864950299263
In grad_steps = 58, loss = 0.7914038896560669
In grad_steps = 59, loss = 0.40205150842666626
In grad_steps = 60, loss = 0.7501927018165588
In grad_steps = 61, loss = 0.3228614330291748
In grad_steps = 62, loss = 0.3297361135482788
In grad_steps = 63, loss = 0.3364379405975342
In grad_steps = 64, loss = 0.8666337728500366
In grad_steps = 65, loss = 0.8935020565986633
In grad_steps = 66, loss = 1.7472469806671143
In grad_steps = 67, loss = 0.897070586681366
In grad_steps = 68, loss = 1.3180408477783203
In grad_steps = 69, loss = 0.7753709554672241
In grad_steps = 70, loss = 0.7183319330215454
In grad_steps = 71, loss = 0.6062071323394775
In grad_steps = 72, loss = 0.6371095180511475
In grad_steps = 73, loss = 0.6916020512580872
In grad_steps = 74, loss = 0.6625174283981323
In grad_steps = 75, loss = 0.6263224482536316
In grad_steps = 76, loss = 0.7345295548439026
In grad_steps = 77, loss = 0.6704767346382141
In grad_steps = 78, loss = 0.851190984249115
In grad_steps = 79, loss = 0.4555845260620117
In grad_steps = 80, loss = 0.7154958248138428
In grad_steps = 81, loss = 0.6921285390853882
In grad_steps = 82, loss = 0.7131885290145874
In grad_steps = 83, loss = 0.49320632219314575
In grad_steps = 84, loss = 0.457538366317749
In grad_steps = 85, loss = 0.9089913368225098
In grad_steps = 86, loss = 0.7039851546287537
In grad_steps = 87, loss = 0.4997180104255676
In grad_steps = 88, loss = 0.9831402897834778
In grad_steps = 89, loss = 0.8292520046234131
In grad_steps = 90, loss = 0.7344179749488831
In grad_steps = 91, loss = 0.7785710096359253
In grad_steps = 92, loss = 0.5487536191940308
In grad_steps = 93, loss = 0.3745085597038269
In grad_steps = 94, loss = 0.6927191019058228
In grad_steps = 95, loss = 0.8032971024513245
In grad_steps = 96, loss = 0.9032193422317505
In grad_steps = 97, loss = 0.88496994972229
In grad_steps = 98, loss = 0.28301748633384705
In grad_steps = 99, loss = 0.7453053593635559
In grad_steps = 100, loss = 0.26513317227363586
In grad_steps = 101, loss = 0.2963879108428955
In grad_steps = 102, loss = 0.3342542052268982
In grad_steps = 103, loss = 0.8370136618614197
In grad_steps = 104, loss = 0.8374437093734741
In grad_steps = 105, loss = 1.6856722831726074
In grad_steps = 106, loss = 0.8645451068878174
In grad_steps = 107, loss = 1.1205906867980957
In grad_steps = 108, loss = 0.7209874391555786
In grad_steps = 109, loss = 0.6445379257202148
In grad_steps = 110, loss = 0.794945240020752
In grad_steps = 111, loss = 0.6058158278465271
In grad_steps = 112, loss = 0.48613131046295166
In grad_steps = 113, loss = 0.6986328363418579
In grad_steps = 114, loss = 0.43019717931747437
In grad_steps = 115, loss = 0.7943927049636841
In grad_steps = 116, loss = 0.6557504534721375
In grad_steps = 117, loss = 0.9629136323928833
In grad_steps = 118, loss = 0.3312987983226776
In grad_steps = 119, loss = 0.7211562991142273
In grad_steps = 120, loss = 0.6462022662162781
In grad_steps = 121, loss = 0.7703191041946411
In grad_steps = 122, loss = 0.4016159772872925
In grad_steps = 123, loss = 0.4084954857826233
In grad_steps = 124, loss = 0.8015761375427246
In grad_steps = 125, loss = 0.6717451214790344
In grad_steps = 126, loss = 0.4736718237400055
In grad_steps = 127, loss = 0.9139858484268188
In grad_steps = 128, loss = 0.7408279180526733
In grad_steps = 129, loss = 0.61888587474823
In grad_steps = 130, loss = 0.9143388867378235
In grad_steps = 131, loss = 0.5342873334884644
In grad_steps = 132, loss = 0.30596286058425903
In grad_steps = 133, loss = 0.5366581678390503
In grad_steps = 134, loss = 0.6707068085670471
In grad_steps = 135, loss = 0.942892849445343
In grad_steps = 136, loss = 0.8727321028709412
In grad_steps = 137, loss = 0.27116090059280396
In grad_steps = 138, loss = 0.5166367292404175
In grad_steps = 139, loss = 0.2586268186569214
In grad_steps = 140, loss = 0.2442552149295807
In grad_steps = 141, loss = 0.31207722425460815
In grad_steps = 142, loss = 0.5706043243408203
In grad_steps = 143, loss = 0.3882085084915161
In grad_steps = 144, loss = 1.5102452039718628
In grad_steps = 145, loss = 0.9676689505577087
In grad_steps = 146, loss = 1.0640339851379395
In grad_steps = 147, loss = 0.545700192451477
In grad_steps = 148, loss = 0.4072495102882385
In grad_steps = 149, loss = 1.0236942768096924
In grad_steps = 150, loss = 0.49612048268318176
In grad_steps = 151, loss = 0.37156084179878235
In grad_steps = 152, loss = 0.6316781640052795
In grad_steps = 153, loss = 0.3092905580997467
In grad_steps = 154, loss = 0.7714158296585083
In grad_steps = 155, loss = 0.5597930550575256
i = 0, Test ensemble probabilities = 
[array([[0.6541136 , 0.34588635],
       [0.66178155, 0.3382184 ],
       [0.74429244, 0.2557076 ],
       [0.7156705 , 0.2843295 ],
       [0.7945011 , 0.20549884],
       [0.8219187 , 0.17808123],
       [0.5996179 , 0.40038204],
       [0.63564837, 0.36435163],
       [0.6979162 , 0.30208376],
       [0.45069554, 0.5493044 ],
       [0.7504167 , 0.2495833 ]], dtype=float32)]
i = 0, Test true class= 
[0 0 0 1 0 1 0 1 1 0 0]
In grad_steps = 0, loss = 0.311132550239563
In grad_steps = 1, loss = 1.796303391456604
In grad_steps = 2, loss = 0.7804747819900513
In grad_steps = 3, loss = 0.8475536108016968
In grad_steps = 4, loss = 0.8236082792282104
In grad_steps = 5, loss = 0.615710437297821
In grad_steps = 6, loss = 0.3112797141075134
In grad_steps = 7, loss = 1.4133610725402832
In grad_steps = 8, loss = 0.8010104298591614
In grad_steps = 9, loss = 0.4744836091995239
In grad_steps = 10, loss = 1.4663329124450684
In grad_steps = 11, loss = 0.9351848363876343
In grad_steps = 12, loss = 0.6684256196022034
In grad_steps = 13, loss = 1.158810019493103
In grad_steps = 14, loss = 0.3816385269165039
In grad_steps = 15, loss = 0.21745708584785461
In grad_steps = 16, loss = 0.894476592540741
In grad_steps = 17, loss = 0.9998300075531006
In grad_steps = 18, loss = 1.0555000305175781
In grad_steps = 19, loss = 1.0003924369812012
In grad_steps = 20, loss = 0.35357528924942017
In grad_steps = 21, loss = 0.7808480262756348
In grad_steps = 22, loss = 0.36090099811553955
In grad_steps = 23, loss = 0.44872909784317017
In grad_steps = 24, loss = 0.5058092474937439
In grad_steps = 25, loss = 0.7845374345779419
In grad_steps = 26, loss = 0.7288877964019775
In grad_steps = 27, loss = 1.3662841320037842
In grad_steps = 28, loss = 0.7929330468177795
In grad_steps = 29, loss = 1.0262811183929443
In grad_steps = 30, loss = 0.7017866373062134
In grad_steps = 31, loss = 0.7381551265716553
In grad_steps = 32, loss = 0.9038479328155518
In grad_steps = 33, loss = 0.6498620510101318
In grad_steps = 34, loss = 0.5197200179100037
In grad_steps = 35, loss = 0.6842533349990845
In grad_steps = 36, loss = 0.5143097639083862
In grad_steps = 37, loss = 0.7931377291679382
In grad_steps = 38, loss = 0.7267473936080933
In grad_steps = 39, loss = 1.018847942352295
In grad_steps = 40, loss = 0.40919023752212524
In grad_steps = 41, loss = 0.7294163703918457
In grad_steps = 42, loss = 0.7356582880020142
In grad_steps = 43, loss = 0.7112175226211548
In grad_steps = 44, loss = 0.5196843147277832
In grad_steps = 45, loss = 0.49085474014282227
In grad_steps = 46, loss = 0.8726159334182739
In grad_steps = 47, loss = 0.6909340620040894
In grad_steps = 48, loss = 0.5731232762336731
In grad_steps = 49, loss = 0.9495434761047363
In grad_steps = 50, loss = 0.825135350227356
In grad_steps = 51, loss = 0.7428418397903442
In grad_steps = 52, loss = 0.7065926790237427
In grad_steps = 53, loss = 0.6297296285629272
In grad_steps = 54, loss = 0.4930803179740906
In grad_steps = 55, loss = 0.6831002235412598
In grad_steps = 56, loss = 0.7366689443588257
In grad_steps = 57, loss = 0.7929930090904236
In grad_steps = 58, loss = 0.7831421494483948
In grad_steps = 59, loss = 0.4107690751552582
In grad_steps = 60, loss = 0.7535754442214966
In grad_steps = 61, loss = 0.33225369453430176
In grad_steps = 62, loss = 0.3366209864616394
In grad_steps = 63, loss = 0.33407706022262573
In grad_steps = 64, loss = 0.8716269731521606
In grad_steps = 65, loss = 0.893487274646759
In grad_steps = 66, loss = 1.7369637489318848
In grad_steps = 67, loss = 0.8905124664306641
In grad_steps = 68, loss = 1.325132131576538
In grad_steps = 69, loss = 0.7752489447593689
In grad_steps = 70, loss = 0.708122968673706
In grad_steps = 71, loss = 0.6130391359329224
In grad_steps = 72, loss = 0.6362541913986206
In grad_steps = 73, loss = 0.6803569793701172
In grad_steps = 74, loss = 0.6648439168930054
In grad_steps = 75, loss = 0.6041960716247559
In grad_steps = 76, loss = 0.748306393623352
In grad_steps = 77, loss = 0.6690764427185059
In grad_steps = 78, loss = 0.876983642578125
In grad_steps = 79, loss = 0.44119009375572205
In grad_steps = 80, loss = 0.7153433561325073
In grad_steps = 81, loss = 0.6819774508476257
In grad_steps = 82, loss = 0.7134921550750732
In grad_steps = 83, loss = 0.4786450266838074
In grad_steps = 84, loss = 0.45041340589523315
In grad_steps = 85, loss = 0.9154605269432068
In grad_steps = 86, loss = 0.7120029926300049
In grad_steps = 87, loss = 0.5032758712768555
In grad_steps = 88, loss = 0.9841333627700806
In grad_steps = 89, loss = 0.8335950374603271
In grad_steps = 90, loss = 0.725516676902771
In grad_steps = 91, loss = 0.773977518081665
In grad_steps = 92, loss = 0.5552038550376892
In grad_steps = 93, loss = 0.38389116525650024
In grad_steps = 94, loss = 0.6868869066238403
In grad_steps = 95, loss = 0.7888439893722534
In grad_steps = 96, loss = 0.9070578813552856
In grad_steps = 97, loss = 0.910176157951355
In grad_steps = 98, loss = 0.27432531118392944
In grad_steps = 99, loss = 0.7417379021644592
In grad_steps = 100, loss = 0.2593000531196594
In grad_steps = 101, loss = 0.29545632004737854
In grad_steps = 102, loss = 0.3385155200958252
In grad_steps = 103, loss = 0.8342084884643555
In grad_steps = 104, loss = 0.8087493777275085
In grad_steps = 105, loss = 1.6688311100006104
In grad_steps = 106, loss = 0.8618181943893433
In grad_steps = 107, loss = 1.1870856285095215
In grad_steps = 108, loss = 0.7215297222137451
In grad_steps = 109, loss = 0.6303670406341553
In grad_steps = 110, loss = 0.8026858568191528
In grad_steps = 111, loss = 0.6128185391426086
In grad_steps = 112, loss = 0.48127293586730957
In grad_steps = 113, loss = 0.7055166959762573
In grad_steps = 114, loss = 0.4243604242801666
In grad_steps = 115, loss = 0.8021740913391113
In grad_steps = 116, loss = 0.6475783586502075
In grad_steps = 117, loss = 0.977933406829834
In grad_steps = 118, loss = 0.32479923963546753
In grad_steps = 119, loss = 0.7211287617683411
In grad_steps = 120, loss = 0.6368980407714844
In grad_steps = 121, loss = 0.7837698459625244
In grad_steps = 122, loss = 0.4024212956428528
In grad_steps = 123, loss = 0.40935218334198
In grad_steps = 124, loss = 0.7843509912490845
In grad_steps = 125, loss = 0.6657208204269409
In grad_steps = 126, loss = 0.48096635937690735
In grad_steps = 127, loss = 0.8991043567657471
In grad_steps = 128, loss = 0.7017068862915039
In grad_steps = 129, loss = 0.5892330408096313
In grad_steps = 130, loss = 0.9546321034431458
In grad_steps = 131, loss = 0.5051583051681519
In grad_steps = 132, loss = 0.2801365852355957
In grad_steps = 133, loss = 0.4951167404651642
In grad_steps = 134, loss = 0.6790072321891785
In grad_steps = 135, loss = 0.988373339176178
In grad_steps = 136, loss = 0.9196087718009949
In grad_steps = 137, loss = 0.25888413190841675
In grad_steps = 138, loss = 0.4850549101829529
In grad_steps = 139, loss = 0.25290659070014954
In grad_steps = 140, loss = 0.25819694995880127
In grad_steps = 141, loss = 0.3382198214530945
In grad_steps = 142, loss = 0.5963623523712158
In grad_steps = 143, loss = 0.32618820667266846
In grad_steps = 144, loss = 1.3346842527389526
In grad_steps = 145, loss = 0.9315513372421265
In grad_steps = 146, loss = 1.047645092010498
In grad_steps = 147, loss = 0.5070662498474121
In grad_steps = 148, loss = 0.3182986378669739
In grad_steps = 149, loss = 1.1008176803588867
In grad_steps = 150, loss = 0.6306186318397522
In grad_steps = 151, loss = 0.2717125415802002
In grad_steps = 152, loss = 0.6882779598236084
In grad_steps = 153, loss = 0.25281059741973877
In grad_steps = 154, loss = 0.7191660404205322
In grad_steps = 155, loss = 0.5038026571273804
i = 1, Test ensemble probabilities = 
[array([[0.6541136 , 0.34588635],
       [0.66178155, 0.3382184 ],
       [0.74429244, 0.2557076 ],
       [0.7156705 , 0.2843295 ],
       [0.7945011 , 0.20549884],
       [0.8219187 , 0.17808123],
       [0.5996179 , 0.40038204],
       [0.63564837, 0.36435163],
       [0.6979162 , 0.30208376],
       [0.45069554, 0.5493044 ],
       [0.7504167 , 0.2495833 ]], dtype=float32), array([[0.644264  , 0.355736  ],
       [0.64389324, 0.35610676],
       [0.7091238 , 0.2908762 ],
       [0.73661417, 0.26338583],
       [0.79958785, 0.20041215],
       [0.8298992 , 0.17010076],
       [0.5878789 , 0.41212112],
       [0.66777635, 0.33222365],
       [0.66745555, 0.3325445 ],
       [0.29094547, 0.7090545 ],
       [0.6422839 , 0.35771605]], dtype=float32)]
i = 1, Test true class= 
[0 0 0 1 0 1 0 1 1 0 0]
In grad_steps = 0, loss = 0.311132550239563
In grad_steps = 1, loss = 1.9035016298294067
In grad_steps = 2, loss = 0.7791434526443481
In grad_steps = 3, loss = 0.8427377939224243
In grad_steps = 4, loss = 0.814883828163147
In grad_steps = 5, loss = 0.6162523031234741
In grad_steps = 6, loss = 0.32836902141571045
In grad_steps = 7, loss = 1.3611184358596802
In grad_steps = 8, loss = 0.8049519062042236
In grad_steps = 9, loss = 0.47598570585250854
In grad_steps = 10, loss = 1.4737930297851562
In grad_steps = 11, loss = 0.9495507478713989
In grad_steps = 12, loss = 0.6616919636726379
In grad_steps = 13, loss = 1.1735124588012695
In grad_steps = 14, loss = 0.39076727628707886
In grad_steps = 15, loss = 0.22960804402828217
In grad_steps = 16, loss = 0.891757607460022
In grad_steps = 17, loss = 0.9910615682601929
In grad_steps = 18, loss = 1.040966510772705
In grad_steps = 19, loss = 1.009218454360962
In grad_steps = 20, loss = 0.34650272130966187
In grad_steps = 21, loss = 0.7749229669570923
In grad_steps = 22, loss = 0.3797582983970642
In grad_steps = 23, loss = 0.4484596848487854
In grad_steps = 24, loss = 0.5083888173103333
In grad_steps = 25, loss = 0.7914503812789917
In grad_steps = 26, loss = 0.7407956123352051
In grad_steps = 27, loss = 1.3760595321655273
In grad_steps = 28, loss = 0.8164342641830444
In grad_steps = 29, loss = 1.0249638557434082
In grad_steps = 30, loss = 0.710024893283844
In grad_steps = 31, loss = 0.7161182165145874
In grad_steps = 32, loss = 0.8968808650970459
In grad_steps = 33, loss = 0.6441291570663452
In grad_steps = 34, loss = 0.5241082310676575
In grad_steps = 35, loss = 0.6654868125915527
In grad_steps = 36, loss = 0.5121920108795166
In grad_steps = 37, loss = 0.8028610944747925
In grad_steps = 38, loss = 0.7329325079917908
In grad_steps = 39, loss = 1.029329538345337
In grad_steps = 40, loss = 0.4085381031036377
In grad_steps = 41, loss = 0.7332839369773865
In grad_steps = 42, loss = 0.7305446863174438
In grad_steps = 43, loss = 0.6994494199752808
In grad_steps = 44, loss = 0.5239226818084717
In grad_steps = 45, loss = 0.4984530210494995
In grad_steps = 46, loss = 0.8670388460159302
In grad_steps = 47, loss = 0.6903824210166931
In grad_steps = 48, loss = 0.5617474317550659
In grad_steps = 49, loss = 0.9341477751731873
In grad_steps = 50, loss = 0.824165940284729
In grad_steps = 51, loss = 0.7495481371879578
In grad_steps = 52, loss = 0.7063717842102051
In grad_steps = 53, loss = 0.637576162815094
In grad_steps = 54, loss = 0.515690803527832
In grad_steps = 55, loss = 0.6843105554580688
In grad_steps = 56, loss = 0.73309326171875
In grad_steps = 57, loss = 0.7828633785247803
In grad_steps = 58, loss = 0.7987908124923706
In grad_steps = 59, loss = 0.40331196784973145
In grad_steps = 60, loss = 0.7583332657814026
In grad_steps = 61, loss = 0.3262592554092407
In grad_steps = 62, loss = 0.3318071663379669
In grad_steps = 63, loss = 0.32297176122665405
In grad_steps = 64, loss = 0.8872746229171753
In grad_steps = 65, loss = 0.8938799500465393
In grad_steps = 66, loss = 1.7420873641967773
In grad_steps = 67, loss = 0.8879303932189941
In grad_steps = 68, loss = 1.3131682872772217
In grad_steps = 69, loss = 0.7723479270935059
In grad_steps = 70, loss = 0.7131064534187317
In grad_steps = 71, loss = 0.6096991300582886
In grad_steps = 72, loss = 0.6320558786392212
In grad_steps = 73, loss = 0.6861522793769836
In grad_steps = 74, loss = 0.6658777594566345
In grad_steps = 75, loss = 0.6105977892875671
In grad_steps = 76, loss = 0.7502903938293457
In grad_steps = 77, loss = 0.6574378609657288
In grad_steps = 78, loss = 0.8417418003082275
In grad_steps = 79, loss = 0.4403935372829437
In grad_steps = 80, loss = 0.7194786667823792
In grad_steps = 81, loss = 0.6812341213226318
In grad_steps = 82, loss = 0.7111523151397705
In grad_steps = 83, loss = 0.481679230928421
In grad_steps = 84, loss = 0.44861936569213867
In grad_steps = 85, loss = 0.9006932973861694
In grad_steps = 86, loss = 0.698266863822937
In grad_steps = 87, loss = 0.5081017017364502
In grad_steps = 88, loss = 0.9827716946601868
In grad_steps = 89, loss = 0.8114606142044067
In grad_steps = 90, loss = 0.7047159671783447
In grad_steps = 91, loss = 0.806692898273468
In grad_steps = 92, loss = 0.5388457775115967
In grad_steps = 93, loss = 0.3605388402938843
In grad_steps = 94, loss = 0.678637683391571
In grad_steps = 95, loss = 0.7864547371864319
In grad_steps = 96, loss = 0.9479320645332336
In grad_steps = 97, loss = 0.8988668322563171
In grad_steps = 98, loss = 0.2835771143436432
In grad_steps = 99, loss = 0.7190983295440674
In grad_steps = 100, loss = 0.2594251334667206
In grad_steps = 101, loss = 0.3092113435268402
In grad_steps = 102, loss = 0.3630357086658478
In grad_steps = 103, loss = 0.8164373636245728
In grad_steps = 104, loss = 0.7897799611091614
In grad_steps = 105, loss = 1.6742610931396484
In grad_steps = 106, loss = 0.8751625418663025
In grad_steps = 107, loss = 1.100530743598938
In grad_steps = 108, loss = 0.7194017171859741
In grad_steps = 109, loss = 0.6445846557617188
In grad_steps = 110, loss = 0.8329893350601196
In grad_steps = 111, loss = 0.6019471883773804
In grad_steps = 112, loss = 0.47068068385124207
In grad_steps = 113, loss = 0.7133795619010925
In grad_steps = 114, loss = 0.4187242090702057
In grad_steps = 115, loss = 0.8129031658172607
In grad_steps = 116, loss = 0.6549189686775208
In grad_steps = 117, loss = 0.9666357636451721
In grad_steps = 118, loss = 0.333197683095932
In grad_steps = 119, loss = 0.7333880662918091
In grad_steps = 120, loss = 0.6505844593048096
In grad_steps = 121, loss = 0.7797955274581909
In grad_steps = 122, loss = 0.3938864767551422
In grad_steps = 123, loss = 0.40716683864593506
In grad_steps = 124, loss = 0.8021706938743591
In grad_steps = 125, loss = 0.6594547033309937
In grad_steps = 126, loss = 0.48311811685562134
In grad_steps = 127, loss = 0.8998056650161743
In grad_steps = 128, loss = 0.7421916723251343
In grad_steps = 129, loss = 0.6385228633880615
In grad_steps = 130, loss = 0.8192846179008484
In grad_steps = 131, loss = 0.5609453916549683
In grad_steps = 132, loss = 0.3468649387359619
In grad_steps = 133, loss = 0.5151035189628601
In grad_steps = 134, loss = 0.6637982130050659
In grad_steps = 135, loss = 0.8829453587532043
In grad_steps = 136, loss = 0.8304586410522461
In grad_steps = 137, loss = 0.24289941787719727
In grad_steps = 138, loss = 0.5201829671859741
In grad_steps = 139, loss = 0.2053690105676651
In grad_steps = 140, loss = 0.18802334368228912
In grad_steps = 141, loss = 0.21240147948265076
In grad_steps = 142, loss = 0.6214892268180847
In grad_steps = 143, loss = 0.2654263973236084
In grad_steps = 144, loss = 1.7017741203308105
In grad_steps = 145, loss = 1.0297492742538452
In grad_steps = 146, loss = 0.9125416874885559
In grad_steps = 147, loss = 0.5285083651542664
In grad_steps = 148, loss = 0.3582491874694824
In grad_steps = 149, loss = 1.3025188446044922
In grad_steps = 150, loss = 0.7042616009712219
In grad_steps = 151, loss = 0.2630421221256256
In grad_steps = 152, loss = 0.6629618406295776
In grad_steps = 153, loss = 0.2978335916996002
In grad_steps = 154, loss = 0.7626758813858032
In grad_steps = 155, loss = 0.5194011330604553
i = 2, Test ensemble probabilities = 
[array([[0.6541136 , 0.34588635],
       [0.66178155, 0.3382184 ],
       [0.74429244, 0.2557076 ],
       [0.7156705 , 0.2843295 ],
       [0.7945011 , 0.20549884],
       [0.8219187 , 0.17808123],
       [0.5996179 , 0.40038204],
       [0.63564837, 0.36435163],
       [0.6979162 , 0.30208376],
       [0.45069554, 0.5493044 ],
       [0.7504167 , 0.2495833 ]], dtype=float32), array([[0.644264  , 0.355736  ],
       [0.64389324, 0.35610676],
       [0.7091238 , 0.2908762 ],
       [0.73661417, 0.26338583],
       [0.79958785, 0.20041215],
       [0.8298992 , 0.17010076],
       [0.5878789 , 0.41212112],
       [0.66777635, 0.33222365],
       [0.66745555, 0.3325445 ],
       [0.29094547, 0.7090545 ],
       [0.6422839 , 0.35771605]], dtype=float32), array([[0.5731666 , 0.4268334 ],
       [0.597156  , 0.402844  ],
       [0.6754553 , 0.32454476],
       [0.728847  , 0.27115294],
       [0.70778   , 0.29222   ],
       [0.72455823, 0.27544177],
       [0.54983795, 0.45016202],
       [0.54013896, 0.45986104],
       [0.6140786 , 0.38592148],
       [0.3385297 , 0.6614703 ],
       [0.55057573, 0.44942424]], dtype=float32)]
i = 2, Test true class= 
[0 0 0 1 0 1 0 1 1 0 0]
In grad_steps = 0, loss = 0.311132550239563
In grad_steps = 1, loss = 1.781538724899292
In grad_steps = 2, loss = 0.7765567898750305
In grad_steps = 3, loss = 0.8429173231124878
In grad_steps = 4, loss = 0.8292142152786255
In grad_steps = 5, loss = 0.6218138933181763
In grad_steps = 6, loss = 0.3235075771808624
In grad_steps = 7, loss = 1.3962998390197754
In grad_steps = 8, loss = 0.7953789234161377
In grad_steps = 9, loss = 0.4654656648635864
In grad_steps = 10, loss = 1.4579010009765625
In grad_steps = 11, loss = 0.960209846496582
In grad_steps = 12, loss = 0.6466385722160339
In grad_steps = 13, loss = 1.166993260383606
In grad_steps = 14, loss = 0.36600881814956665
In grad_steps = 15, loss = 0.21997232735157013
In grad_steps = 16, loss = 0.931747555732727
In grad_steps = 17, loss = 0.9932329654693604
In grad_steps = 18, loss = 1.051554560661316
In grad_steps = 19, loss = 0.9850772619247437
In grad_steps = 20, loss = 0.3454902172088623
In grad_steps = 21, loss = 0.772078275680542
In grad_steps = 22, loss = 0.3673512041568756
In grad_steps = 23, loss = 0.45016372203826904
In grad_steps = 24, loss = 0.5029367208480835
In grad_steps = 25, loss = 0.7727559208869934
In grad_steps = 26, loss = 0.7289175987243652
In grad_steps = 27, loss = 1.3568918704986572
In grad_steps = 28, loss = 0.8168208599090576
In grad_steps = 29, loss = 1.0423996448516846
In grad_steps = 30, loss = 0.7096202969551086
In grad_steps = 31, loss = 0.7098678350448608
In grad_steps = 32, loss = 0.8811184167861938
In grad_steps = 33, loss = 0.6617588996887207
In grad_steps = 34, loss = 0.5140384435653687
In grad_steps = 35, loss = 0.6715220808982849
In grad_steps = 36, loss = 0.5018134713172913
In grad_steps = 37, loss = 0.8047757744789124
In grad_steps = 38, loss = 0.7479040622711182
In grad_steps = 39, loss = 0.9918510317802429
In grad_steps = 40, loss = 0.40820711851119995
In grad_steps = 41, loss = 0.7382027506828308
In grad_steps = 42, loss = 0.7360972762107849
In grad_steps = 43, loss = 0.6963220834732056
In grad_steps = 44, loss = 0.5242435932159424
In grad_steps = 45, loss = 0.4847249984741211
In grad_steps = 46, loss = 0.8808538913726807
In grad_steps = 47, loss = 0.7019633054733276
In grad_steps = 48, loss = 0.5624338388442993
In grad_steps = 49, loss = 0.9461074471473694
In grad_steps = 50, loss = 0.8297132253646851
In grad_steps = 51, loss = 0.7552424669265747
In grad_steps = 52, loss = 0.7117607593536377
In grad_steps = 53, loss = 0.6295299530029297
In grad_steps = 54, loss = 0.5091811418533325
In grad_steps = 55, loss = 0.6837232708930969
In grad_steps = 56, loss = 0.7278488874435425
In grad_steps = 57, loss = 0.7952311635017395
In grad_steps = 58, loss = 0.7891204357147217
In grad_steps = 59, loss = 0.396256685256958
In grad_steps = 60, loss = 0.7413625121116638
In grad_steps = 61, loss = 0.33422553539276123
In grad_steps = 62, loss = 0.3382018208503723
In grad_steps = 63, loss = 0.33068540692329407
In grad_steps = 64, loss = 0.8827994465827942
In grad_steps = 65, loss = 0.8831011056900024
In grad_steps = 66, loss = 1.7448930740356445
In grad_steps = 67, loss = 0.8784831762313843
In grad_steps = 68, loss = 1.3237942457199097
In grad_steps = 69, loss = 0.7702121734619141
In grad_steps = 70, loss = 0.7154537439346313
In grad_steps = 71, loss = 0.618108868598938
In grad_steps = 72, loss = 0.6438299417495728
In grad_steps = 73, loss = 0.6831561326980591
In grad_steps = 74, loss = 0.6652094125747681
In grad_steps = 75, loss = 0.6009179353713989
In grad_steps = 76, loss = 0.7404688596725464
In grad_steps = 77, loss = 0.6657795310020447
In grad_steps = 78, loss = 0.8637692928314209
In grad_steps = 79, loss = 0.45046550035476685
In grad_steps = 80, loss = 0.7190589904785156
In grad_steps = 81, loss = 0.6940953135490417
In grad_steps = 82, loss = 0.7279777526855469
In grad_steps = 83, loss = 0.48553091287612915
In grad_steps = 84, loss = 0.4556611180305481
In grad_steps = 85, loss = 0.9209044575691223
In grad_steps = 86, loss = 0.711883544921875
In grad_steps = 87, loss = 0.5067483186721802
In grad_steps = 88, loss = 0.9748326539993286
In grad_steps = 89, loss = 0.8271538615226746
In grad_steps = 90, loss = 0.7152771949768066
In grad_steps = 91, loss = 0.8048895597457886
In grad_steps = 92, loss = 0.5260482430458069
In grad_steps = 93, loss = 0.3770197629928589
In grad_steps = 94, loss = 0.7033936977386475
In grad_steps = 95, loss = 0.8101855516433716
In grad_steps = 96, loss = 0.9364215135574341
In grad_steps = 97, loss = 0.8912804126739502
In grad_steps = 98, loss = 0.2920302748680115
In grad_steps = 99, loss = 0.7391877174377441
In grad_steps = 100, loss = 0.27343258261680603
In grad_steps = 101, loss = 0.3087397813796997
In grad_steps = 102, loss = 0.3536761999130249
In grad_steps = 103, loss = 0.8187289834022522
In grad_steps = 104, loss = 0.8332139849662781
In grad_steps = 105, loss = 1.646241545677185
In grad_steps = 106, loss = 0.8330476880073547
In grad_steps = 107, loss = 1.1335792541503906
In grad_steps = 108, loss = 0.7240555286407471
In grad_steps = 109, loss = 0.6723347902297974
In grad_steps = 110, loss = 0.792637825012207
In grad_steps = 111, loss = 0.6106637120246887
In grad_steps = 112, loss = 0.5009422898292542
In grad_steps = 113, loss = 0.7009413242340088
In grad_steps = 114, loss = 0.43812429904937744
In grad_steps = 115, loss = 0.7913687229156494
In grad_steps = 116, loss = 0.6356446743011475
In grad_steps = 117, loss = 0.9555213451385498
In grad_steps = 118, loss = 0.34232985973358154
In grad_steps = 119, loss = 0.7317904233932495
In grad_steps = 120, loss = 0.6530755758285522
In grad_steps = 121, loss = 0.7779358625411987
In grad_steps = 122, loss = 0.40132683515548706
In grad_steps = 123, loss = 0.4019721746444702
In grad_steps = 124, loss = 0.8331813216209412
In grad_steps = 125, loss = 0.6769242882728577
In grad_steps = 126, loss = 0.45774924755096436
In grad_steps = 127, loss = 0.9352518320083618
In grad_steps = 128, loss = 0.7647159099578857
In grad_steps = 129, loss = 0.6543712019920349
In grad_steps = 130, loss = 0.8278579711914062
In grad_steps = 131, loss = 0.5598471164703369
In grad_steps = 132, loss = 0.333187997341156
In grad_steps = 133, loss = 0.520752489566803
In grad_steps = 134, loss = 0.6852753162384033
In grad_steps = 135, loss = 0.9407708048820496
In grad_steps = 136, loss = 0.9120770692825317
In grad_steps = 137, loss = 0.2592732608318329
In grad_steps = 138, loss = 0.541115403175354
In grad_steps = 139, loss = 0.2363552302122116
In grad_steps = 140, loss = 0.22773118317127228
In grad_steps = 141, loss = 0.2900572419166565
In grad_steps = 142, loss = 0.6467519402503967
In grad_steps = 143, loss = 0.46583792567253113
In grad_steps = 144, loss = 1.6558706760406494
In grad_steps = 145, loss = 0.9192789196968079
In grad_steps = 146, loss = 1.021699070930481
In grad_steps = 147, loss = 0.5784215927124023
In grad_steps = 148, loss = 0.40329745411872864
In grad_steps = 149, loss = 0.9258345365524292
In grad_steps = 150, loss = 0.5656163096427917
In grad_steps = 151, loss = 0.32353174686431885
In grad_steps = 152, loss = 0.6744676828384399
In grad_steps = 153, loss = 0.22464951872825623
In grad_steps = 154, loss = 0.7616079449653625
In grad_steps = 155, loss = 0.5385737419128418
i = 3, Test ensemble probabilities = 
[array([[0.6541136 , 0.34588635],
       [0.66178155, 0.3382184 ],
       [0.74429244, 0.2557076 ],
       [0.7156705 , 0.2843295 ],
       [0.7945011 , 0.20549884],
       [0.8219187 , 0.17808123],
       [0.5996179 , 0.40038204],
       [0.63564837, 0.36435163],
       [0.6979162 , 0.30208376],
       [0.45069554, 0.5493044 ],
       [0.7504167 , 0.2495833 ]], dtype=float32), array([[0.644264  , 0.355736  ],
       [0.64389324, 0.35610676],
       [0.7091238 , 0.2908762 ],
       [0.73661417, 0.26338583],
       [0.79958785, 0.20041215],
       [0.8298992 , 0.17010076],
       [0.5878789 , 0.41212112],
       [0.66777635, 0.33222365],
       [0.66745555, 0.3325445 ],
       [0.29094547, 0.7090545 ],
       [0.6422839 , 0.35771605]], dtype=float32), array([[0.5731666 , 0.4268334 ],
       [0.597156  , 0.402844  ],
       [0.6754553 , 0.32454476],
       [0.728847  , 0.27115294],
       [0.70778   , 0.29222   ],
       [0.72455823, 0.27544177],
       [0.54983795, 0.45016202],
       [0.54013896, 0.45986104],
       [0.6140786 , 0.38592148],
       [0.3385297 , 0.6614703 ],
       [0.55057573, 0.44942424]], dtype=float32), array([[0.68381715, 0.31618285],
       [0.6933566 , 0.3066434 ],
       [0.7447126 , 0.25528744],
       [0.7765626 , 0.22343746],
       [0.7921604 , 0.20783956],
       [0.8078156 , 0.19218436],
       [0.61597437, 0.38402563],
       [0.6898168 , 0.3101832 ],
       [0.6919265 , 0.30807355],
       [0.50256914, 0.4974308 ],
       [0.7408242 , 0.2591758 ]], dtype=float32)]
i = 3, Test true class= 
[0 0 0 1 0 1 0 1 1 0 0]
In grad_steps = 0, loss = 0.311132550239563
In grad_steps = 1, loss = 1.8044966459274292
In grad_steps = 2, loss = 0.7739000916481018
In grad_steps = 3, loss = 0.84620201587677
In grad_steps = 4, loss = 0.822449803352356
In grad_steps = 5, loss = 0.6305075883865356
In grad_steps = 6, loss = 0.3319637179374695
In grad_steps = 7, loss = 1.3533008098602295
In grad_steps = 8, loss = 0.7836892604827881
In grad_steps = 9, loss = 0.47367966175079346
In grad_steps = 10, loss = 1.4493625164031982
In grad_steps = 11, loss = 0.9515299797058105
In grad_steps = 12, loss = 0.6547254323959351
In grad_steps = 13, loss = 1.2139757871627808
In grad_steps = 14, loss = 0.3814036250114441
In grad_steps = 15, loss = 0.2124359905719757
In grad_steps = 16, loss = 0.8930560350418091
In grad_steps = 17, loss = 0.9984115958213806
In grad_steps = 18, loss = 1.0538277626037598
In grad_steps = 19, loss = 0.9735462665557861
In grad_steps = 20, loss = 0.3648981750011444
In grad_steps = 21, loss = 0.7695197463035583
In grad_steps = 22, loss = 0.3805657625198364
In grad_steps = 23, loss = 0.43961000442504883
In grad_steps = 24, loss = 0.5257339477539062
In grad_steps = 25, loss = 0.7673277854919434
In grad_steps = 26, loss = 0.7276445627212524
In grad_steps = 27, loss = 1.3910363912582397
In grad_steps = 28, loss = 0.8305014967918396
In grad_steps = 29, loss = 1.0354526042938232
In grad_steps = 30, loss = 0.7343777418136597
In grad_steps = 31, loss = 0.7047319412231445
In grad_steps = 32, loss = 0.8759214878082275
In grad_steps = 33, loss = 0.6450604200363159
In grad_steps = 34, loss = 0.5488898754119873
In grad_steps = 35, loss = 0.6801066994667053
In grad_steps = 36, loss = 0.5073400139808655
In grad_steps = 37, loss = 0.8010119199752808
In grad_steps = 38, loss = 0.7337085604667664
In grad_steps = 39, loss = 1.0230374336242676
In grad_steps = 40, loss = 0.40238335728645325
In grad_steps = 41, loss = 0.7439471483230591
In grad_steps = 42, loss = 0.7311224937438965
In grad_steps = 43, loss = 0.71156907081604
In grad_steps = 44, loss = 0.5047574043273926
In grad_steps = 45, loss = 0.4800507426261902
In grad_steps = 46, loss = 0.891441285610199
In grad_steps = 47, loss = 0.698705792427063
In grad_steps = 48, loss = 0.5483787059783936
In grad_steps = 49, loss = 0.9493787288665771
In grad_steps = 50, loss = 0.8383982181549072
In grad_steps = 51, loss = 0.7610912322998047
In grad_steps = 52, loss = 0.7058384418487549
In grad_steps = 53, loss = 0.6329140663146973
In grad_steps = 54, loss = 0.5102874040603638
In grad_steps = 55, loss = 0.6927357912063599
In grad_steps = 56, loss = 0.7354352474212646
In grad_steps = 57, loss = 0.7820359468460083
In grad_steps = 58, loss = 0.7938789129257202
In grad_steps = 59, loss = 0.40617501735687256
In grad_steps = 60, loss = 0.756006121635437
In grad_steps = 61, loss = 0.32776862382888794
In grad_steps = 62, loss = 0.33438920974731445
In grad_steps = 63, loss = 0.3311871886253357
In grad_steps = 64, loss = 0.8771207332611084
In grad_steps = 65, loss = 0.8972679376602173
In grad_steps = 66, loss = 1.7395155429840088
In grad_steps = 67, loss = 0.8835819363594055
In grad_steps = 68, loss = 1.3153274059295654
In grad_steps = 69, loss = 0.7692831158638
In grad_steps = 70, loss = 0.713926374912262
In grad_steps = 71, loss = 0.6062138080596924
In grad_steps = 72, loss = 0.6374378204345703
In grad_steps = 73, loss = 0.6914481520652771
In grad_steps = 74, loss = 0.6691770553588867
In grad_steps = 75, loss = 0.600409746170044
In grad_steps = 76, loss = 0.7466716170310974
In grad_steps = 77, loss = 0.6657017469406128
In grad_steps = 78, loss = 0.8621200323104858
In grad_steps = 79, loss = 0.4453016221523285
In grad_steps = 80, loss = 0.7170047163963318
In grad_steps = 81, loss = 0.6848723292350769
In grad_steps = 82, loss = 0.7241524457931519
In grad_steps = 83, loss = 0.48323854804039
In grad_steps = 84, loss = 0.4503668546676636
In grad_steps = 85, loss = 0.9098556041717529
In grad_steps = 86, loss = 0.7093571424484253
In grad_steps = 87, loss = 0.5004271268844604
In grad_steps = 88, loss = 0.979346752166748
In grad_steps = 89, loss = 0.8357570767402649
In grad_steps = 90, loss = 0.7302337884902954
In grad_steps = 91, loss = 0.7785661220550537
In grad_steps = 92, loss = 0.5544651746749878
In grad_steps = 93, loss = 0.3708633780479431
In grad_steps = 94, loss = 0.676040530204773
In grad_steps = 95, loss = 0.7800388932228088
In grad_steps = 96, loss = 0.9123243093490601
In grad_steps = 97, loss = 0.8894761204719543
In grad_steps = 98, loss = 0.28895026445388794
In grad_steps = 99, loss = 0.7421584129333496
In grad_steps = 100, loss = 0.25785645842552185
In grad_steps = 101, loss = 0.2932887375354767
In grad_steps = 102, loss = 0.3397473990917206
In grad_steps = 103, loss = 0.838566780090332
In grad_steps = 104, loss = 0.8088455200195312
In grad_steps = 105, loss = 1.6850001811981201
In grad_steps = 106, loss = 0.8627927303314209
In grad_steps = 107, loss = 1.1507244110107422
In grad_steps = 108, loss = 0.7211978435516357
In grad_steps = 109, loss = 0.6455904245376587
In grad_steps = 110, loss = 0.7972699999809265
In grad_steps = 111, loss = 0.6078586578369141
In grad_steps = 112, loss = 0.4887632727622986
In grad_steps = 113, loss = 0.7005323767662048
In grad_steps = 114, loss = 0.42440807819366455
In grad_steps = 115, loss = 0.7901759743690491
In grad_steps = 116, loss = 0.6554332971572876
In grad_steps = 117, loss = 0.941581130027771
In grad_steps = 118, loss = 0.3279513716697693
In grad_steps = 119, loss = 0.721324622631073
In grad_steps = 120, loss = 0.6355545520782471
In grad_steps = 121, loss = 0.7809118032455444
In grad_steps = 122, loss = 0.39648187160491943
In grad_steps = 123, loss = 0.40086662769317627
In grad_steps = 124, loss = 0.7991961240768433
In grad_steps = 125, loss = 0.6502811312675476
In grad_steps = 126, loss = 0.4668681025505066
In grad_steps = 127, loss = 0.9248744249343872
In grad_steps = 128, loss = 0.7140598297119141
In grad_steps = 129, loss = 0.5995121002197266
In grad_steps = 130, loss = 0.9601167440414429
In grad_steps = 131, loss = 0.5012762546539307
In grad_steps = 132, loss = 0.28362083435058594
In grad_steps = 133, loss = 0.517863392829895
In grad_steps = 134, loss = 0.6810995936393738
In grad_steps = 135, loss = 0.9545236825942993
In grad_steps = 136, loss = 0.8847658634185791
In grad_steps = 137, loss = 0.26550015807151794
In grad_steps = 138, loss = 0.5007117390632629
In grad_steps = 139, loss = 0.2612898349761963
In grad_steps = 140, loss = 0.26448965072631836
In grad_steps = 141, loss = 0.3314048945903778
In grad_steps = 142, loss = 0.5886424779891968
In grad_steps = 143, loss = 0.3308239281177521
In grad_steps = 144, loss = 1.491564154624939
In grad_steps = 145, loss = 0.9653254747390747
In grad_steps = 146, loss = 1.025028944015503
In grad_steps = 147, loss = 0.5291010737419128
In grad_steps = 148, loss = 0.3980143666267395
In grad_steps = 149, loss = 1.1257960796356201
In grad_steps = 150, loss = 0.5990575551986694
In grad_steps = 151, loss = 0.2846716642379761
In grad_steps = 152, loss = 0.6604189276695251
In grad_steps = 153, loss = 0.2553449273109436
In grad_steps = 154, loss = 0.7661322951316833
In grad_steps = 155, loss = 0.5370532870292664
i = 4, Test ensemble probabilities = 
[array([[0.6541136 , 0.34588635],
       [0.66178155, 0.3382184 ],
       [0.74429244, 0.2557076 ],
       [0.7156705 , 0.2843295 ],
       [0.7945011 , 0.20549884],
       [0.8219187 , 0.17808123],
       [0.5996179 , 0.40038204],
       [0.63564837, 0.36435163],
       [0.6979162 , 0.30208376],
       [0.45069554, 0.5493044 ],
       [0.7504167 , 0.2495833 ]], dtype=float32), array([[0.644264  , 0.355736  ],
       [0.64389324, 0.35610676],
       [0.7091238 , 0.2908762 ],
       [0.73661417, 0.26338583],
       [0.79958785, 0.20041215],
       [0.8298992 , 0.17010076],
       [0.5878789 , 0.41212112],
       [0.66777635, 0.33222365],
       [0.66745555, 0.3325445 ],
       [0.29094547, 0.7090545 ],
       [0.6422839 , 0.35771605]], dtype=float32), array([[0.5731666 , 0.4268334 ],
       [0.597156  , 0.402844  ],
       [0.6754553 , 0.32454476],
       [0.728847  , 0.27115294],
       [0.70778   , 0.29222   ],
       [0.72455823, 0.27544177],
       [0.54983795, 0.45016202],
       [0.54013896, 0.45986104],
       [0.6140786 , 0.38592148],
       [0.3385297 , 0.6614703 ],
       [0.55057573, 0.44942424]], dtype=float32), array([[0.68381715, 0.31618285],
       [0.6933566 , 0.3066434 ],
       [0.7447126 , 0.25528744],
       [0.7765626 , 0.22343746],
       [0.7921604 , 0.20783956],
       [0.8078156 , 0.19218436],
       [0.61597437, 0.38402563],
       [0.6898168 , 0.3101832 ],
       [0.6919265 , 0.30807355],
       [0.50256914, 0.4974308 ],
       [0.7408242 , 0.2591758 ]], dtype=float32), array([[0.644891  , 0.35510895],
       [0.6485915 , 0.35140842],
       [0.7428373 , 0.25716272],
       [0.76715744, 0.2328426 ],
       [0.7518952 , 0.24810483],
       [0.8217706 , 0.17822939],
       [0.5806948 , 0.41930515],
       [0.6161823 , 0.38381764],
       [0.640009  , 0.35999098],
       [0.43702415, 0.5629758 ],
       [0.6637796 , 0.33622038]], dtype=float32)]
i = 4, Test true class= 
[0 0 0 1 0 1 0 1 1 0 0]
Final, Test average ensemble probabilities = 
[[0.6400505  0.35994953]
 [0.64895576 0.35104418]
 [0.72328424 0.27671573]
 [0.74497044 0.25502965]
 [0.76918495 0.23081508]
 [0.80119246 0.1988075 ]
 [0.58680075 0.4131992 ]
 [0.62991256 0.37008744]
 [0.6622771  0.33772284]
 [0.40395278 0.5960472 ]
 [0.66957605 0.33042395]]
Accuracy: 0.5455
MCC: -0.2390
AUC: 0.3214
Confusion Matrix:
tensor([[6, 1],
        [4, 0]])
Specificity: 0.8571
Precision (Macro): 0.3000
F1 Score (Macro): 0.3529
Expected Calibration Error (ECE): 0.3194
NLL loss: 0.7606
Main task is done! Can finish
