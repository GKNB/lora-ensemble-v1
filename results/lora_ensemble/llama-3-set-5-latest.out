Job is running on partition: gpu_ss11
Job is running under account: m2616_g
/pscratch/sd/t/tianle/conda/envs/lucid_surp_2024/lib/python3.11/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:11<00:33, 11.13s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:22<00:22, 11.37s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:33<00:11, 11.21s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:36<00:00,  7.71s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:36<00:00,  9.01s/it]
Llama3 has been loaded successfully.
Namespace(model_name='Llama3', config='/pscratch/sd/t/tianle/lucid/other_source/SURP_2024/src/config.json', repo_dir='/pscratch/sd/t/tianle/lucid/other_source/SURP_2024/', dataset='5', n_ensemble=5, seed=1, use_model_snapshot=False)
self.num_epochs = 2, self.batch_size = 4, self.max_length = 50
Model:  LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(128256, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)
          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)
          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)
          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)
)
Train dataset looks like:  Dataset({
    features: ['question', 'answer'],
    num_rows: 8209
})
Test dataset looks like:  Dataset({
    features: ['question', 'answer'],
    num_rows: 2053
})
Training lora instance 0
DEBUG: Total parameters: 8037076992
DEBUG: Trainable parameters: 6815744
Beginning epoch 1
In grad_steps = 0, loss = 0.7379498481750488
In grad_steps = 1, loss = 0.6394188404083252
In grad_steps = 2, loss = 1.6086053848266602
In grad_steps = 3, loss = 0.7990210652351379
In grad_steps = 4, loss = 0.6240871548652649
In grad_steps = 5, loss = 0.8225207328796387
In grad_steps = 6, loss = 0.5366025567054749
In grad_steps = 7, loss = 0.5639052987098694
In grad_steps = 8, loss = 1.1856417655944824
In grad_steps = 9, loss = 0.9803955554962158
In grad_steps = 10, loss = 0.7669354677200317
In grad_steps = 11, loss = 0.7053760290145874
In grad_steps = 12, loss = 0.7418621778488159
In grad_steps = 13, loss = 0.5871825218200684
In grad_steps = 14, loss = 0.9350128769874573
In grad_steps = 15, loss = 0.5389261245727539
In grad_steps = 16, loss = 0.7150458693504333
In grad_steps = 17, loss = 0.6940141916275024
In grad_steps = 18, loss = 0.7759019136428833
In grad_steps = 19, loss = 0.7488359212875366
In grad_steps = 20, loss = 0.6679350733757019
In grad_steps = 21, loss = 0.648770809173584
In grad_steps = 22, loss = 0.6732214689254761
In grad_steps = 23, loss = 0.7821651697158813
In grad_steps = 24, loss = 0.6338694095611572
In grad_steps = 25, loss = 0.7538588047027588
In grad_steps = 26, loss = 0.7301431894302368
In grad_steps = 27, loss = 0.7194160223007202
In grad_steps = 28, loss = 0.5682435631752014
In grad_steps = 29, loss = 0.6917126774787903
In grad_steps = 30, loss = 0.8049839735031128
In grad_steps = 31, loss = 0.6278561949729919
In grad_steps = 32, loss = 0.6454300284385681
In grad_steps = 33, loss = 0.6733579635620117
In grad_steps = 34, loss = 0.6331084966659546
In grad_steps = 35, loss = 0.6835129261016846
In grad_steps = 36, loss = 0.6539328098297119
In grad_steps = 37, loss = 0.6876205205917358
In grad_steps = 38, loss = 0.7917351126670837
In grad_steps = 39, loss = 0.689846396446228
In grad_steps = 40, loss = 0.7420438528060913
In grad_steps = 41, loss = 0.6749007105827332
In grad_steps = 42, loss = 0.6263291835784912
In grad_steps = 43, loss = 0.6783573627471924
In grad_steps = 44, loss = 0.5871785283088684
In grad_steps = 45, loss = 0.7239248752593994
In grad_steps = 46, loss = 0.7151309847831726
In grad_steps = 47, loss = 0.6681235432624817
In grad_steps = 48, loss = 0.5178239345550537
In grad_steps = 49, loss = 0.6690607070922852
In grad_steps = 50, loss = 0.7226304411888123
In grad_steps = 51, loss = 0.6789926886558533
In grad_steps = 52, loss = 0.5572187304496765
In grad_steps = 53, loss = 0.7309207320213318
In grad_steps = 54, loss = 0.6364115476608276
In grad_steps = 55, loss = 0.6237339377403259
In grad_steps = 56, loss = 0.6764610409736633
In grad_steps = 57, loss = 0.706386148929596
In grad_steps = 58, loss = 0.6956850290298462
In grad_steps = 59, loss = 0.7142273187637329
In grad_steps = 60, loss = 0.7394341230392456
In grad_steps = 61, loss = 0.6151731014251709
In grad_steps = 62, loss = 0.6348956823348999
In grad_steps = 63, loss = 0.5782378911972046
In grad_steps = 64, loss = 0.47105956077575684
In grad_steps = 65, loss = 0.519986629486084
In grad_steps = 66, loss = 0.562375009059906
In grad_steps = 67, loss = 0.38239556550979614
In grad_steps = 68, loss = 0.5151066780090332
In grad_steps = 69, loss = 0.44597285985946655
In grad_steps = 70, loss = 1.1183589696884155
In grad_steps = 71, loss = 0.32604336738586426
In grad_steps = 72, loss = 1.1567579507827759
In grad_steps = 73, loss = 0.9380468726158142
In grad_steps = 74, loss = 0.5139505863189697
In grad_steps = 75, loss = 0.7705317735671997
In grad_steps = 76, loss = 0.4118039608001709
In grad_steps = 77, loss = 0.4253004789352417
In grad_steps = 78, loss = 0.5577783584594727
In grad_steps = 79, loss = 0.6191525459289551
In grad_steps = 80, loss = 0.6343353390693665
In grad_steps = 81, loss = 0.23270098865032196
In grad_steps = 82, loss = 0.9830506443977356
In grad_steps = 83, loss = 0.7792023420333862
In grad_steps = 84, loss = 0.7355750799179077
In grad_steps = 85, loss = 0.7738212943077087
In grad_steps = 86, loss = 0.8293588757514954
In grad_steps = 87, loss = 0.6099895238876343
In grad_steps = 88, loss = 0.4699603021144867
In grad_steps = 89, loss = 0.5969550609588623
In grad_steps = 90, loss = 0.6595546007156372
In grad_steps = 91, loss = 0.6822896599769592
In grad_steps = 92, loss = 0.5962569713592529
In grad_steps = 93, loss = 0.8281679153442383
In grad_steps = 94, loss = 0.6889118552207947
In grad_steps = 95, loss = 0.32883477210998535
In grad_steps = 96, loss = 0.6820415258407593
In grad_steps = 97, loss = 0.7340154647827148
In grad_steps = 98, loss = 0.7635658979415894
In grad_steps = 99, loss = 0.522230863571167
In grad_steps = 100, loss = 0.8789324760437012
In grad_steps = 101, loss = 1.0132441520690918
In grad_steps = 102, loss = 0.817132830619812
In grad_steps = 103, loss = 0.38932639360427856
In grad_steps = 104, loss = 0.5406982898712158
In grad_steps = 105, loss = 0.6050963401794434
In grad_steps = 106, loss = 0.519942045211792
In grad_steps = 107, loss = 0.6220657229423523
In grad_steps = 108, loss = 0.516979455947876
In grad_steps = 109, loss = 0.5179119110107422
In grad_steps = 110, loss = 0.9291377067565918
In grad_steps = 111, loss = 0.3634268045425415
In grad_steps = 112, loss = 0.2805306017398834
In grad_steps = 113, loss = 0.5686755180358887
In grad_steps = 114, loss = 0.23724202811717987
In grad_steps = 115, loss = 1.0305863618850708
In grad_steps = 116, loss = 0.3503454327583313
In grad_steps = 117, loss = 0.488402396440506
In grad_steps = 118, loss = 0.5273409485816956
In grad_steps = 119, loss = 0.7009885311126709
In grad_steps = 120, loss = 0.3209722340106964
In grad_steps = 121, loss = 0.37276357412338257
In grad_steps = 122, loss = 0.35455408692359924
In grad_steps = 123, loss = 0.8594053983688354
In grad_steps = 124, loss = 0.6769056916236877
In grad_steps = 125, loss = 0.7439020872116089
In grad_steps = 126, loss = 0.1409190148115158
In grad_steps = 127, loss = 0.5567845106124878
In grad_steps = 128, loss = 0.5163819193840027
In grad_steps = 129, loss = 0.4890744090080261
In grad_steps = 130, loss = 1.001585602760315
In grad_steps = 131, loss = 0.6525400876998901
In grad_steps = 132, loss = 0.6464762687683105
In grad_steps = 133, loss = 0.4575802683830261
In grad_steps = 134, loss = 0.7005898356437683
In grad_steps = 135, loss = 0.6173796653747559
In grad_steps = 136, loss = 0.47283461689949036
In grad_steps = 137, loss = 0.30131757259368896
In grad_steps = 138, loss = 0.528216540813446
In grad_steps = 139, loss = 0.301284521818161
In grad_steps = 140, loss = 0.7875732183456421
In grad_steps = 141, loss = 0.4028704762458801
In grad_steps = 142, loss = 0.43137603998184204
In grad_steps = 143, loss = 0.18414798378944397
In grad_steps = 144, loss = 0.695746660232544
In grad_steps = 145, loss = 0.31899845600128174
In grad_steps = 146, loss = 0.680253267288208
In grad_steps = 147, loss = 0.2424483597278595
In grad_steps = 148, loss = 0.24324022233486176
In grad_steps = 149, loss = 0.37862372398376465
In grad_steps = 150, loss = 0.939297080039978
In grad_steps = 151, loss = 0.5617924928665161
In grad_steps = 152, loss = 1.4335509538650513
In grad_steps = 153, loss = 1.0071179866790771
In grad_steps = 154, loss = 0.7387982606887817
In grad_steps = 155, loss = 0.21163597702980042
In grad_steps = 156, loss = 0.517896294593811
In grad_steps = 157, loss = 0.17060187458992004
In grad_steps = 158, loss = 0.2608840763568878
In grad_steps = 159, loss = 0.5471822619438171
In grad_steps = 160, loss = 0.24341832101345062
In grad_steps = 161, loss = 0.7824559211730957
In grad_steps = 162, loss = 0.5250910520553589
In grad_steps = 163, loss = 0.36066174507141113
In grad_steps = 164, loss = 0.7167600393295288
In grad_steps = 165, loss = 0.2642931342124939
In grad_steps = 166, loss = 0.18816983699798584
In grad_steps = 167, loss = 0.6704999208450317
In grad_steps = 168, loss = 0.5953577756881714
In grad_steps = 169, loss = 0.14582280814647675
In grad_steps = 170, loss = 0.23090901970863342
In grad_steps = 171, loss = 0.5071825981140137
In grad_steps = 172, loss = 0.3521229326725006
In grad_steps = 173, loss = 0.13986913859844208
In grad_steps = 174, loss = 0.11596780270338058
In grad_steps = 175, loss = 0.6149535179138184
In grad_steps = 176, loss = 0.0587959885597229
In grad_steps = 177, loss = 0.15673430263996124
In grad_steps = 178, loss = 0.6583377122879028
In grad_steps = 179, loss = 1.339374303817749
In grad_steps = 180, loss = 0.19427929818630219
In grad_steps = 181, loss = 0.4876130521297455
In grad_steps = 182, loss = 0.7736848592758179
In grad_steps = 183, loss = 0.28073516488075256
In grad_steps = 184, loss = 0.6022171974182129
In grad_steps = 185, loss = 0.5787336826324463
In grad_steps = 186, loss = 0.4256737530231476
In grad_steps = 187, loss = 0.4530722200870514
In grad_steps = 188, loss = 0.5477904081344604
In grad_steps = 189, loss = 0.32019245624542236
In grad_steps = 190, loss = 0.1901310682296753
In grad_steps = 191, loss = 0.31981921195983887
In grad_steps = 192, loss = 0.567391574382782
In grad_steps = 193, loss = 0.49602681398391724
In grad_steps = 194, loss = 0.8807036876678467
In grad_steps = 195, loss = 0.7165429592132568
In grad_steps = 196, loss = 0.5805339813232422
In grad_steps = 197, loss = 0.5678102374076843
In grad_steps = 198, loss = 0.5684579014778137
In grad_steps = 199, loss = 1.0871576070785522
In grad_steps = 200, loss = 0.2951357364654541
In grad_steps = 201, loss = 0.7841857075691223
In grad_steps = 202, loss = 0.5326547622680664
In grad_steps = 203, loss = 0.37336310744285583
In grad_steps = 204, loss = 0.558608889579773
In grad_steps = 205, loss = 0.5677475929260254
In grad_steps = 206, loss = 0.31792566180229187
In grad_steps = 207, loss = 0.4558919072151184
In grad_steps = 208, loss = 0.31516751646995544
In grad_steps = 209, loss = 0.1656390279531479
In grad_steps = 210, loss = 0.21525955200195312
In grad_steps = 211, loss = 0.9870243072509766
In grad_steps = 212, loss = 0.4236629903316498
In grad_steps = 213, loss = 0.3551621437072754
In grad_steps = 214, loss = 0.16677680611610413
In grad_steps = 215, loss = 0.2558993101119995
In grad_steps = 216, loss = 0.4877607822418213
In grad_steps = 217, loss = 0.44365137815475464
In grad_steps = 218, loss = 1.740840196609497
In grad_steps = 219, loss = 0.4702017605304718
In grad_steps = 220, loss = 0.36885809898376465
In grad_steps = 221, loss = 0.9855698943138123
In grad_steps = 222, loss = 0.5439783334732056
In grad_steps = 223, loss = 0.5827184915542603
In grad_steps = 224, loss = 0.20632892847061157
In grad_steps = 225, loss = 0.08598411083221436
In grad_steps = 226, loss = 0.6944974660873413
In grad_steps = 227, loss = 0.5094466805458069
In grad_steps = 228, loss = 0.4479876160621643
In grad_steps = 229, loss = 0.30273300409317017
In grad_steps = 230, loss = 0.333427369594574
In grad_steps = 231, loss = 0.7299787402153015
In grad_steps = 232, loss = 0.9709720611572266
In grad_steps = 233, loss = 0.259464830160141
In grad_steps = 234, loss = 0.6512308120727539
In grad_steps = 235, loss = 0.6979150772094727
In grad_steps = 236, loss = 0.5938552618026733
In grad_steps = 237, loss = 0.2163788080215454
In grad_steps = 238, loss = 0.16420161724090576
In grad_steps = 239, loss = 0.2668050527572632
In grad_steps = 240, loss = 0.19075745344161987
In grad_steps = 241, loss = 0.5910223126411438
In grad_steps = 242, loss = 0.3368208110332489
In grad_steps = 243, loss = 0.30380743741989136
In grad_steps = 244, loss = 1.0312716960906982
In grad_steps = 245, loss = 0.26226192712783813
In grad_steps = 246, loss = 0.1851610690355301
In grad_steps = 247, loss = 0.41162237524986267
In grad_steps = 248, loss = 1.164278507232666
In grad_steps = 249, loss = 0.14671637117862701
In grad_steps = 250, loss = 0.7526406645774841
In grad_steps = 251, loss = 0.8028897643089294
In grad_steps = 252, loss = 0.20452673733234406
In grad_steps = 253, loss = 0.4996483623981476
In grad_steps = 254, loss = 0.5212675929069519
In grad_steps = 255, loss = 0.5258606672286987
In grad_steps = 256, loss = 0.9866873621940613
In grad_steps = 257, loss = 0.6013998985290527
In grad_steps = 258, loss = 0.879030168056488
In grad_steps = 259, loss = 0.6830801963806152
In grad_steps = 260, loss = 0.6492509245872498
In grad_steps = 261, loss = 1.164333462715149
In grad_steps = 262, loss = 0.5959237813949585
In grad_steps = 263, loss = 0.5728184580802917
In grad_steps = 264, loss = 0.4675576686859131
In grad_steps = 265, loss = 0.5778905749320984
In grad_steps = 266, loss = 0.8711845874786377
In grad_steps = 267, loss = 0.8848645687103271
In grad_steps = 268, loss = 0.5385332703590393
In grad_steps = 269, loss = 0.33180931210517883
In grad_steps = 270, loss = 0.4706628918647766
In grad_steps = 271, loss = 0.6294340491294861
In grad_steps = 272, loss = 0.4821919798851013
In grad_steps = 273, loss = 0.5006638765335083
In grad_steps = 274, loss = 0.4530213475227356
In grad_steps = 275, loss = 0.5947738885879517
In grad_steps = 276, loss = 0.5213202238082886
In grad_steps = 277, loss = 0.41127264499664307
In grad_steps = 278, loss = 0.44089579582214355
In grad_steps = 279, loss = 0.6293173432350159
In grad_steps = 280, loss = 0.5871003866195679
In grad_steps = 281, loss = 0.47987812757492065
In grad_steps = 282, loss = 0.5805621147155762
In grad_steps = 283, loss = 0.4948493540287018
In grad_steps = 284, loss = 0.4729740619659424
In grad_steps = 285, loss = 0.3374255895614624
In grad_steps = 286, loss = 0.2124605029821396
In grad_steps = 287, loss = 0.9199064373970032
In grad_steps = 288, loss = 1.0115914344787598
In grad_steps = 289, loss = 0.3872028589248657
In grad_steps = 290, loss = 0.32543572783470154
In grad_steps = 291, loss = 1.1517090797424316
In grad_steps = 292, loss = 0.5816513299942017
In grad_steps = 293, loss = 0.7455681562423706
In grad_steps = 294, loss = 0.3745275139808655
In grad_steps = 295, loss = 0.8653276562690735
In grad_steps = 296, loss = 0.3422934412956238
In grad_steps = 297, loss = 0.33801811933517456
In grad_steps = 298, loss = 0.33179888129234314
In grad_steps = 299, loss = 0.4691089987754822
In grad_steps = 300, loss = 0.6527613401412964
In grad_steps = 301, loss = 0.6301181316375732
In grad_steps = 302, loss = 0.3649609088897705
In grad_steps = 303, loss = 0.5536280870437622
In grad_steps = 304, loss = 0.141265869140625
In grad_steps = 305, loss = 0.22189858555793762
In grad_steps = 306, loss = 0.6216909289360046
In grad_steps = 307, loss = 0.4046128988265991
In grad_steps = 308, loss = 0.6414886713027954
In grad_steps = 309, loss = 0.5244984030723572
In grad_steps = 310, loss = 0.24000129103660583
In grad_steps = 311, loss = 1.413899540901184
In grad_steps = 312, loss = 0.2258937656879425
In grad_steps = 313, loss = 0.25537633895874023
In grad_steps = 314, loss = 0.510077953338623
In grad_steps = 315, loss = 0.10806319117546082
In grad_steps = 316, loss = 1.2165122032165527
In grad_steps = 317, loss = 0.41414135694503784
In grad_steps = 318, loss = 0.3454253673553467
In grad_steps = 319, loss = 0.16378048062324524
In grad_steps = 320, loss = 0.5016350746154785
In grad_steps = 321, loss = 0.17807728052139282
In grad_steps = 322, loss = 0.9383974075317383
In grad_steps = 323, loss = 0.33264923095703125
In grad_steps = 324, loss = 0.3828921318054199
In grad_steps = 325, loss = 0.25701504945755005
In grad_steps = 326, loss = 0.5054537057876587
In grad_steps = 327, loss = 0.24179647862911224
In grad_steps = 328, loss = 0.2947070598602295
In grad_steps = 329, loss = 1.0114785432815552
In grad_steps = 330, loss = 0.23840279877185822
In grad_steps = 331, loss = 0.08422385901212692
In grad_steps = 332, loss = 0.1029566153883934
In grad_steps = 333, loss = 0.20053930580615997
In grad_steps = 334, loss = 0.40194785594940186
In grad_steps = 335, loss = 0.2762494683265686
In grad_steps = 336, loss = 0.6408190131187439
In grad_steps = 337, loss = 0.17456509172916412
In grad_steps = 338, loss = 0.07423456013202667
In grad_steps = 339, loss = 0.4636630415916443
In grad_steps = 340, loss = 0.0572662353515625
In grad_steps = 341, loss = 0.8228269815444946
In grad_steps = 342, loss = 0.17177391052246094
In grad_steps = 343, loss = 0.3476797640323639
In grad_steps = 344, loss = 1.032879114151001
In grad_steps = 345, loss = 0.6356465816497803
In grad_steps = 346, loss = 0.3901505172252655
In grad_steps = 347, loss = 0.5269220471382141
In grad_steps = 348, loss = 0.7051215767860413
In grad_steps = 349, loss = 1.0241597890853882
In grad_steps = 350, loss = 0.20380757749080658
In grad_steps = 351, loss = 1.315981149673462
In grad_steps = 352, loss = 0.7628835439682007
In grad_steps = 353, loss = 0.6216410398483276
In grad_steps = 354, loss = 0.6696226596832275
In grad_steps = 355, loss = 0.7520132660865784
In grad_steps = 356, loss = 0.2925872206687927
In grad_steps = 357, loss = 0.2667502760887146
In grad_steps = 358, loss = 0.8495309948921204
In grad_steps = 359, loss = 0.5281244516372681
In grad_steps = 360, loss = 0.9294144511222839
In grad_steps = 361, loss = 0.7563961744308472
In grad_steps = 362, loss = 0.677875280380249
In grad_steps = 363, loss = 0.9176726937294006
In grad_steps = 364, loss = 0.34975072741508484
In grad_steps = 365, loss = 0.6910355687141418
In grad_steps = 366, loss = 0.7732230424880981
In grad_steps = 367, loss = 0.6319348216056824
In grad_steps = 368, loss = 0.48852989077568054
In grad_steps = 369, loss = 0.43061748147010803
In grad_steps = 370, loss = 0.5551757216453552
In grad_steps = 371, loss = 0.4929961860179901
In grad_steps = 372, loss = 0.4429071247577667
In grad_steps = 373, loss = 0.5621150135993958
In grad_steps = 374, loss = 0.3843477964401245
In grad_steps = 375, loss = 0.5732790231704712
In grad_steps = 376, loss = 0.4465539753437042
In grad_steps = 377, loss = 0.298895001411438
In grad_steps = 378, loss = 0.21464651823043823
In grad_steps = 379, loss = 0.17256960272789001
In grad_steps = 380, loss = 0.6337289214134216
In grad_steps = 381, loss = 0.8854714632034302
In grad_steps = 382, loss = 0.3249838054180145
In grad_steps = 383, loss = 0.34149667620658875
In grad_steps = 384, loss = 0.10359726846218109
In grad_steps = 385, loss = 0.04205545037984848
In grad_steps = 386, loss = 0.1356942057609558
In grad_steps = 387, loss = 1.7879618406295776
In grad_steps = 388, loss = 0.7409406304359436
In grad_steps = 389, loss = 0.4804959297180176
In grad_steps = 390, loss = 0.28019118309020996
In grad_steps = 391, loss = 0.28582504391670227
In grad_steps = 392, loss = 0.5428978204727173
In grad_steps = 393, loss = 0.6213860511779785
In grad_steps = 394, loss = 0.33937087655067444
In grad_steps = 395, loss = 1.0517570972442627
In grad_steps = 396, loss = 0.6046134233474731
In grad_steps = 397, loss = 0.1276267021894455
In grad_steps = 398, loss = 0.6973820924758911
In grad_steps = 399, loss = 0.20798787474632263
In grad_steps = 400, loss = 1.0762282609939575
In grad_steps = 401, loss = 0.273413747549057
In grad_steps = 402, loss = 0.24040566384792328
In grad_steps = 403, loss = 0.7213014364242554
In grad_steps = 404, loss = 0.28800100088119507
In grad_steps = 405, loss = 0.3672124743461609
In grad_steps = 406, loss = 0.8121741414070129
In grad_steps = 407, loss = 0.3229900002479553
In grad_steps = 408, loss = 0.7100598812103271
In grad_steps = 409, loss = 0.5240744352340698
In grad_steps = 410, loss = 0.32236483693122864
In grad_steps = 411, loss = 0.2890963852405548
In grad_steps = 412, loss = 0.7051663398742676
In grad_steps = 413, loss = 0.8604574203491211
In grad_steps = 414, loss = 1.2392438650131226
In grad_steps = 415, loss = 0.517064094543457
In grad_steps = 416, loss = 1.1574170589447021
In grad_steps = 417, loss = 0.2778659760951996
In grad_steps = 418, loss = 0.18944282829761505
In grad_steps = 419, loss = 0.26379483938217163
In grad_steps = 420, loss = 0.3088036775588989
In grad_steps = 421, loss = 0.15666908025741577
In grad_steps = 422, loss = 0.43526118993759155
In grad_steps = 423, loss = 0.8713172674179077
In grad_steps = 424, loss = 0.6247975826263428
In grad_steps = 425, loss = 0.14432202279567719
In grad_steps = 426, loss = 0.11845248937606812
In grad_steps = 427, loss = 0.308542937040329
In grad_steps = 428, loss = 0.22954070568084717
In grad_steps = 429, loss = 0.8181197047233582
In grad_steps = 430, loss = 0.140792578458786
In grad_steps = 431, loss = 1.172730565071106
In grad_steps = 432, loss = 0.5780221223831177
In grad_steps = 433, loss = 1.0801496505737305
In grad_steps = 434, loss = 0.3215389847755432
In grad_steps = 435, loss = 0.20006237924098969
In grad_steps = 436, loss = 0.20567475259304047
In grad_steps = 437, loss = 0.3816591501235962
In grad_steps = 438, loss = 0.2297680526971817
In grad_steps = 439, loss = 0.15896010398864746
In grad_steps = 440, loss = 0.10639671981334686
In grad_steps = 441, loss = 0.07694286108016968
In grad_steps = 442, loss = 0.32162708044052124
In grad_steps = 443, loss = 0.3161003887653351
In grad_steps = 444, loss = 0.9309689402580261
In grad_steps = 445, loss = 0.13138720393180847
In grad_steps = 446, loss = 0.15391495823860168
In grad_steps = 447, loss = 0.17160241305828094
In grad_steps = 448, loss = 0.273521363735199
In grad_steps = 449, loss = 0.07298707962036133
In grad_steps = 450, loss = 0.10094110667705536
In grad_steps = 451, loss = 0.3594651520252228
In grad_steps = 452, loss = 0.2536810338497162
In grad_steps = 453, loss = 0.12308166921138763
In grad_steps = 454, loss = 0.41048547625541687
In grad_steps = 455, loss = 0.32085907459259033
In grad_steps = 456, loss = 1.2125515937805176
In grad_steps = 457, loss = 0.022806040942668915
In grad_steps = 458, loss = 0.1339910924434662
In grad_steps = 459, loss = 0.16261984407901764
In grad_steps = 460, loss = 0.0376773327589035
In grad_steps = 461, loss = 0.6867294907569885
In grad_steps = 462, loss = 0.1529178023338318
In grad_steps = 463, loss = 0.28527143597602844
In grad_steps = 464, loss = 0.38624224066734314
In grad_steps = 465, loss = 0.030476132407784462
In grad_steps = 466, loss = 0.28779733180999756
In grad_steps = 467, loss = 0.45319414138793945
In grad_steps = 468, loss = 0.43261727690696716
In grad_steps = 469, loss = 2.0686264038085938
In grad_steps = 470, loss = 0.16679351031780243
In grad_steps = 471, loss = 0.8319998979568481
In grad_steps = 472, loss = 0.5130293965339661
In grad_steps = 473, loss = 0.7741016745567322
In grad_steps = 474, loss = 0.3620726466178894
In grad_steps = 475, loss = 0.09141799807548523
In grad_steps = 476, loss = 0.1075512245297432
In grad_steps = 477, loss = 0.7887094020843506
In grad_steps = 478, loss = 0.2364722341299057
In grad_steps = 479, loss = 0.22686435282230377
In grad_steps = 480, loss = 0.18602091073989868
In grad_steps = 481, loss = 0.18056955933570862
In grad_steps = 482, loss = 0.2967456877231598
In grad_steps = 483, loss = 0.9749821424484253
In grad_steps = 484, loss = 0.2170516848564148
In grad_steps = 485, loss = 0.5754922032356262
In grad_steps = 486, loss = 0.22619150578975677
In grad_steps = 487, loss = 0.50596022605896
In grad_steps = 488, loss = 0.2182420790195465
In grad_steps = 489, loss = 0.36372798681259155
In grad_steps = 490, loss = 0.2180209457874298
In grad_steps = 491, loss = 0.1531619280576706
In grad_steps = 492, loss = 0.3172933757305145
In grad_steps = 493, loss = 0.6515443325042725
In grad_steps = 494, loss = 0.24968627095222473
In grad_steps = 495, loss = 0.17322632670402527
In grad_steps = 496, loss = 0.4562622010707855
In grad_steps = 497, loss = 0.6517536640167236
In grad_steps = 498, loss = 0.3618507981300354
In grad_steps = 499, loss = 0.5709356069564819
In grad_steps = 500, loss = 0.06206594407558441
In grad_steps = 501, loss = 0.04670435190200806
In grad_steps = 502, loss = 1.251878023147583
In grad_steps = 503, loss = 0.09251682460308075
In grad_steps = 504, loss = 0.6991734504699707
In grad_steps = 505, loss = 0.03582759201526642
In grad_steps = 506, loss = 0.26911407709121704
In grad_steps = 507, loss = 0.10498090833425522
In grad_steps = 508, loss = 0.4335576295852661
In grad_steps = 509, loss = 0.7031262516975403
In grad_steps = 510, loss = 0.5754001140594482
In grad_steps = 511, loss = 0.23452432453632355
In grad_steps = 512, loss = 0.31991100311279297
In grad_steps = 513, loss = 0.13545198738574982
In grad_steps = 514, loss = 0.3937070667743683
In grad_steps = 515, loss = 0.07686187326908112
In grad_steps = 516, loss = 0.21592405438423157
In grad_steps = 517, loss = 0.4524788558483124
In grad_steps = 518, loss = 0.13637295365333557
In grad_steps = 519, loss = 0.16184639930725098
In grad_steps = 520, loss = 0.717343807220459
In grad_steps = 521, loss = 0.3987407386302948
In grad_steps = 522, loss = 0.595445990562439
In grad_steps = 523, loss = 0.6872528195381165
In grad_steps = 524, loss = 0.6181221604347229
In grad_steps = 525, loss = 0.43174272775650024
In grad_steps = 526, loss = 0.5292227864265442
In grad_steps = 527, loss = 1.0220727920532227
In grad_steps = 528, loss = 0.5033420324325562
In grad_steps = 529, loss = 0.42010870575904846
In grad_steps = 530, loss = 0.11738321185112
In grad_steps = 531, loss = 0.1319069117307663
In grad_steps = 532, loss = 0.688274085521698
In grad_steps = 533, loss = 0.5651677250862122
In grad_steps = 534, loss = 0.36719992756843567
In grad_steps = 535, loss = 1.2544604539871216
In grad_steps = 536, loss = 0.4904661774635315
In grad_steps = 537, loss = 0.4919757544994354
In grad_steps = 538, loss = 0.6420679092407227
In grad_steps = 539, loss = 0.5989782214164734
In grad_steps = 540, loss = 0.2985953092575073
In grad_steps = 541, loss = 0.41225212812423706
In grad_steps = 542, loss = 0.37656962871551514
In grad_steps = 543, loss = 0.3749327063560486
In grad_steps = 544, loss = 0.6233174204826355
In grad_steps = 545, loss = 0.6356467008590698
In grad_steps = 546, loss = 0.4326356053352356
In grad_steps = 547, loss = 0.5125836133956909
In grad_steps = 548, loss = 0.24839863181114197
In grad_steps = 549, loss = 0.6668902039527893
In grad_steps = 550, loss = 0.23250812292099
In grad_steps = 551, loss = 0.28114476799964905
In grad_steps = 552, loss = 0.4046005606651306
In grad_steps = 553, loss = 0.2608446776866913
In grad_steps = 554, loss = 0.31206443905830383
In grad_steps = 555, loss = 0.14850075542926788
In grad_steps = 556, loss = 0.5578134059906006
In grad_steps = 557, loss = 0.6641039252281189
In grad_steps = 558, loss = 0.22844460606575012
In grad_steps = 559, loss = 0.42304152250289917
In grad_steps = 560, loss = 0.2724800705909729
In grad_steps = 561, loss = 0.24443691968917847
In grad_steps = 562, loss = 0.15929879248142242
In grad_steps = 563, loss = 0.6461144685745239
In grad_steps = 564, loss = 0.21284139156341553
In grad_steps = 565, loss = 0.15568330883979797
In grad_steps = 566, loss = 0.3648494780063629
In grad_steps = 567, loss = 1.421142816543579
In grad_steps = 568, loss = 0.4310699999332428
In grad_steps = 569, loss = 0.28717654943466187
In grad_steps = 570, loss = 0.4814535081386566
In grad_steps = 571, loss = 0.593650221824646
In grad_steps = 572, loss = 0.08804703503847122
In grad_steps = 573, loss = 0.09123548865318298
In grad_steps = 574, loss = 0.2111203372478485
In grad_steps = 575, loss = 0.055596210062503815
In grad_steps = 576, loss = 0.43399739265441895
In grad_steps = 577, loss = 0.07724300026893616
In grad_steps = 578, loss = 1.8206511735916138
In grad_steps = 579, loss = 0.16500990092754364
In grad_steps = 580, loss = 0.38383764028549194
In grad_steps = 581, loss = 0.6092219948768616
In grad_steps = 582, loss = 0.6522912979125977
In grad_steps = 583, loss = 0.27458488941192627
In grad_steps = 584, loss = 0.4651925265789032
In grad_steps = 585, loss = 0.5985080599784851
In grad_steps = 586, loss = 0.23669949173927307
In grad_steps = 587, loss = 0.2625541090965271
In grad_steps = 588, loss = 0.2777847349643707
In grad_steps = 589, loss = 0.2408231794834137
In grad_steps = 590, loss = 0.2118191421031952
In grad_steps = 591, loss = 0.2065420001745224
In grad_steps = 592, loss = 0.39553770422935486
In grad_steps = 593, loss = 0.2860429883003235
In grad_steps = 594, loss = 0.38499563932418823
In grad_steps = 595, loss = 0.7013947367668152
In grad_steps = 596, loss = 0.5716456174850464
In grad_steps = 597, loss = 0.451280802488327
In grad_steps = 598, loss = 0.30753642320632935
In grad_steps = 599, loss = 0.8990355730056763
In grad_steps = 600, loss = 1.039095401763916
In grad_steps = 601, loss = 0.37729281187057495
In grad_steps = 602, loss = 0.5982848405838013
In grad_steps = 603, loss = 0.44217389822006226
In grad_steps = 604, loss = 0.7316550016403198
In grad_steps = 605, loss = 1.1600472927093506
In grad_steps = 606, loss = 0.22749361395835876
In grad_steps = 607, loss = 0.1567888855934143
In grad_steps = 608, loss = 0.46859821677207947
In grad_steps = 609, loss = 0.19317519664764404
In grad_steps = 610, loss = 0.1898479163646698
In grad_steps = 611, loss = 0.3011402487754822
In grad_steps = 612, loss = 0.3041613698005676
In grad_steps = 613, loss = 0.21737557649612427
In grad_steps = 614, loss = 0.23072895407676697
In grad_steps = 615, loss = 0.675275981426239
In grad_steps = 616, loss = 0.369367778301239
In grad_steps = 617, loss = 0.23348098993301392
In grad_steps = 618, loss = 0.4048861563205719
In grad_steps = 619, loss = 0.12617234885692596
In grad_steps = 620, loss = 0.3730871379375458
In grad_steps = 621, loss = 0.25596117973327637
In grad_steps = 622, loss = 0.0622427761554718
In grad_steps = 623, loss = 0.1418839395046234
In grad_steps = 624, loss = 0.033796895295381546
In grad_steps = 625, loss = 0.16619767248630524
In grad_steps = 626, loss = 0.17172861099243164
In grad_steps = 627, loss = 0.35849881172180176
In grad_steps = 628, loss = 0.1418844759464264
In grad_steps = 629, loss = 0.6882179975509644
In grad_steps = 630, loss = 0.45192334055900574
In grad_steps = 631, loss = 0.7533491849899292
In grad_steps = 632, loss = 0.4672001302242279
In grad_steps = 633, loss = 0.4470618665218353
In grad_steps = 634, loss = 0.326953649520874
In grad_steps = 635, loss = 1.001526117324829
In grad_steps = 636, loss = 0.25483137369155884
In grad_steps = 637, loss = 0.41835668683052063
In grad_steps = 638, loss = 0.18695497512817383
In grad_steps = 639, loss = 0.9958270192146301
In grad_steps = 640, loss = 1.1826081275939941
In grad_steps = 641, loss = 0.577160656452179
In grad_steps = 642, loss = 0.17218926548957825
In grad_steps = 643, loss = 0.10509708523750305
In grad_steps = 644, loss = 0.5189281702041626
In grad_steps = 645, loss = 0.5911752581596375
In grad_steps = 646, loss = 0.4526428282260895
In grad_steps = 647, loss = 0.5014708638191223
In grad_steps = 648, loss = 0.24564188718795776
In grad_steps = 649, loss = 0.36546269059181213
In grad_steps = 650, loss = 0.39315366744995117
In grad_steps = 651, loss = 0.7000023126602173
In grad_steps = 652, loss = 0.33530178666114807
In grad_steps = 653, loss = 0.2793705463409424
In grad_steps = 654, loss = 0.6652622222900391
In grad_steps = 655, loss = 0.4926520586013794
In grad_steps = 656, loss = 0.4255906045436859
In grad_steps = 657, loss = 0.6009702086448669
In grad_steps = 658, loss = 0.2517448663711548
In grad_steps = 659, loss = 0.48516201972961426
In grad_steps = 660, loss = 0.7880984544754028
In grad_steps = 661, loss = 0.9477522373199463
In grad_steps = 662, loss = 0.5567378997802734
In grad_steps = 663, loss = 0.26690101623535156
In grad_steps = 664, loss = 1.0156581401824951
In grad_steps = 665, loss = 0.7274951934814453
In grad_steps = 666, loss = 0.1346597969532013
In grad_steps = 667, loss = 0.582872748374939
In grad_steps = 668, loss = 0.6338557600975037
In grad_steps = 669, loss = 0.2539602220058441
In grad_steps = 670, loss = 0.38048282265663147
In grad_steps = 671, loss = 0.6455397605895996
In grad_steps = 672, loss = 0.5376520156860352
In grad_steps = 673, loss = 0.7845332026481628
In grad_steps = 674, loss = 0.19692730903625488
In grad_steps = 675, loss = 0.900739848613739
In grad_steps = 676, loss = 0.6872430443763733
In grad_steps = 677, loss = 0.29272913932800293
In grad_steps = 678, loss = 0.28354597091674805
In grad_steps = 679, loss = 0.36327114701271057
In grad_steps = 680, loss = 0.3829527497291565
In grad_steps = 681, loss = 0.320313960313797
In grad_steps = 682, loss = 0.8265845775604248
In grad_steps = 683, loss = 0.24031546711921692
In grad_steps = 684, loss = 0.4652400016784668
In grad_steps = 685, loss = 0.21576900780200958
In grad_steps = 686, loss = 0.21132110059261322
In grad_steps = 687, loss = 0.38431915640830994
In grad_steps = 688, loss = 0.19072861969470978
In grad_steps = 689, loss = 0.3827280104160309
In grad_steps = 690, loss = 0.14653101563453674
In grad_steps = 691, loss = 0.7694439888000488
In grad_steps = 692, loss = 0.07430396974086761
In grad_steps = 693, loss = 0.9189180135726929
In grad_steps = 694, loss = 0.5688577890396118
In grad_steps = 695, loss = 0.13649167120456696
In grad_steps = 696, loss = 0.2056960016489029
In grad_steps = 697, loss = 0.24511834979057312
In grad_steps = 698, loss = 0.2563430964946747
In grad_steps = 699, loss = 0.22031982243061066
In grad_steps = 700, loss = 0.1700989156961441
In grad_steps = 701, loss = 0.8927160501480103
In grad_steps = 702, loss = 0.049685075879096985
In grad_steps = 703, loss = 0.11293640732765198
In grad_steps = 704, loss = 0.04656632989645004
In grad_steps = 705, loss = 1.4977682828903198
In grad_steps = 706, loss = 0.26750120520591736
In grad_steps = 707, loss = 0.10218845307826996
In grad_steps = 708, loss = 0.0871553122997284
In grad_steps = 709, loss = 0.5387840270996094
In grad_steps = 710, loss = 0.12930214405059814
In grad_steps = 711, loss = 0.2305685132741928
In grad_steps = 712, loss = 0.05449678376317024
In grad_steps = 713, loss = 3.1795921325683594
In grad_steps = 714, loss = 0.07344500720500946
In grad_steps = 715, loss = 0.6762397885322571
In grad_steps = 716, loss = 0.24422284960746765
In grad_steps = 717, loss = 0.3361465334892273
In grad_steps = 718, loss = 0.07052811980247498
In grad_steps = 719, loss = 0.37904906272888184
In grad_steps = 720, loss = 1.9782164096832275
In grad_steps = 721, loss = 0.5218818783760071
In grad_steps = 722, loss = 0.22167587280273438
In grad_steps = 723, loss = 0.6145248413085938
In grad_steps = 724, loss = 0.1319606453180313
In grad_steps = 725, loss = 0.19543993473052979
In grad_steps = 726, loss = 0.31397974491119385
In grad_steps = 727, loss = 0.3825555443763733
In grad_steps = 728, loss = 0.3619411885738373
In grad_steps = 729, loss = 0.2430870532989502
In grad_steps = 730, loss = 0.18199744820594788
In grad_steps = 731, loss = 0.29951804876327515
In grad_steps = 732, loss = 0.20435649156570435
In grad_steps = 733, loss = 0.4527263939380646
In grad_steps = 734, loss = 0.33974921703338623
In grad_steps = 735, loss = 0.5398923754692078
In grad_steps = 736, loss = 1.654845118522644
In grad_steps = 737, loss = 0.5183671116828918
In grad_steps = 738, loss = 0.4628978371620178
In grad_steps = 739, loss = 0.175444558262825
In grad_steps = 740, loss = 0.4208439886569977
In grad_steps = 741, loss = 0.2539908289909363
In grad_steps = 742, loss = 0.42972344160079956
In grad_steps = 743, loss = 0.327764630317688
In grad_steps = 744, loss = 0.2404695451259613
In grad_steps = 745, loss = 0.037755876779556274
In grad_steps = 746, loss = 0.5129904747009277
In grad_steps = 747, loss = 0.26429852843284607
In grad_steps = 748, loss = 0.25882428884506226
In grad_steps = 749, loss = 0.3097785711288452
In grad_steps = 750, loss = 1.3319904804229736
In grad_steps = 751, loss = 0.6962623000144958
In grad_steps = 752, loss = 0.3984520435333252
In grad_steps = 753, loss = 0.10117215663194656
In grad_steps = 754, loss = 0.565455436706543
In grad_steps = 755, loss = 0.1267038881778717
In grad_steps = 756, loss = 0.08353467285633087
In grad_steps = 757, loss = 0.09667949378490448
In grad_steps = 758, loss = 0.26198825240135193
In grad_steps = 759, loss = 0.8556379079818726
In grad_steps = 760, loss = 0.1350724697113037
In grad_steps = 761, loss = 0.045106612145900726
In grad_steps = 762, loss = 0.30770158767700195
In grad_steps = 763, loss = 0.5078423619270325
In grad_steps = 764, loss = 0.5459564924240112
In grad_steps = 765, loss = 0.10848090052604675
In grad_steps = 766, loss = 0.19078728556632996
In grad_steps = 767, loss = 0.3969511389732361
In grad_steps = 768, loss = 0.04650552570819855
In grad_steps = 769, loss = 0.8241709470748901
In grad_steps = 770, loss = 0.823072075843811
In grad_steps = 771, loss = 0.7100963592529297
In grad_steps = 772, loss = 0.443993479013443
In grad_steps = 773, loss = 0.18112564086914062
In grad_steps = 774, loss = 0.16945794224739075
In grad_steps = 775, loss = 0.08694516867399216
In grad_steps = 776, loss = 0.0727929174900055
In grad_steps = 777, loss = 0.6366070508956909
In grad_steps = 778, loss = 0.6102855205535889
In grad_steps = 779, loss = 0.7632638216018677
In grad_steps = 780, loss = 0.27663347125053406
In grad_steps = 781, loss = 0.40068650245666504
In grad_steps = 782, loss = 0.4355960190296173
In grad_steps = 783, loss = 0.42497602105140686
In grad_steps = 784, loss = 0.7099125981330872
In grad_steps = 785, loss = 0.6943795680999756
In grad_steps = 786, loss = 0.44189023971557617
In grad_steps = 787, loss = 0.29954126477241516
In grad_steps = 788, loss = 0.2749825119972229
In grad_steps = 789, loss = 0.32395800948143005
In grad_steps = 790, loss = 0.6116279363632202
In grad_steps = 791, loss = 0.2626264989376068
In grad_steps = 792, loss = 0.23538798093795776
In grad_steps = 793, loss = 0.23040612041950226
In grad_steps = 794, loss = 0.17525100708007812
In grad_steps = 795, loss = 0.28501835465431213
In grad_steps = 796, loss = 0.7089527249336243
In grad_steps = 797, loss = 0.1062072217464447
In grad_steps = 798, loss = 0.12230633199214935
In grad_steps = 799, loss = 0.23493222892284393
In grad_steps = 800, loss = 0.15958738327026367
In grad_steps = 801, loss = 0.08490312844514847
In grad_steps = 802, loss = 0.05996982008218765
In grad_steps = 803, loss = 0.11860394477844238
In grad_steps = 804, loss = 0.027854548767209053
In grad_steps = 805, loss = 0.09588842839002609
In grad_steps = 806, loss = 1.2604734897613525
In grad_steps = 807, loss = 0.20192833244800568
In grad_steps = 808, loss = 0.11484192311763763
In grad_steps = 809, loss = 0.7700058817863464
In grad_steps = 810, loss = 0.09768325090408325
In grad_steps = 811, loss = 0.02091779187321663
In grad_steps = 812, loss = 0.5819597244262695
In grad_steps = 813, loss = 0.04640117287635803
In grad_steps = 814, loss = 0.11355949938297272
In grad_steps = 815, loss = 0.03501933813095093
In grad_steps = 816, loss = 0.03321192041039467
In grad_steps = 817, loss = 0.5790535807609558
In grad_steps = 818, loss = 0.7977889776229858
In grad_steps = 819, loss = 0.5202333331108093
In grad_steps = 820, loss = 0.097450852394104
In grad_steps = 821, loss = 0.46994367241859436
In grad_steps = 822, loss = 0.9096150398254395
In grad_steps = 823, loss = 0.8486512899398804
In grad_steps = 824, loss = 0.23187802731990814
In grad_steps = 825, loss = 0.17322704195976257
In grad_steps = 826, loss = 0.183685302734375
In grad_steps = 827, loss = 0.38289737701416016
In grad_steps = 828, loss = 0.42960482835769653
In grad_steps = 829, loss = 0.29629701375961304
In grad_steps = 830, loss = 0.4846835434436798
In grad_steps = 831, loss = 0.14830216765403748
In grad_steps = 832, loss = 0.24183805286884308
In grad_steps = 833, loss = 0.6634200215339661
In grad_steps = 834, loss = 0.202193945646286
In grad_steps = 835, loss = 0.2076108157634735
In grad_steps = 836, loss = 0.5759150981903076
In grad_steps = 837, loss = 0.47053706645965576
In grad_steps = 838, loss = 1.1492598056793213
In grad_steps = 839, loss = 0.30716603994369507
In grad_steps = 840, loss = 0.09651871770620346
In grad_steps = 841, loss = 0.16332823038101196
In grad_steps = 842, loss = 0.2723602056503296
In grad_steps = 843, loss = 0.7576208710670471
In grad_steps = 844, loss = 0.35985296964645386
In grad_steps = 845, loss = 0.4159984290599823
In grad_steps = 846, loss = 0.5648027658462524
In grad_steps = 847, loss = 0.4394417405128479
In grad_steps = 848, loss = 0.09214860200881958
In grad_steps = 849, loss = 0.25100937485694885
In grad_steps = 850, loss = 0.20705446600914001
In grad_steps = 851, loss = 0.14802028238773346
In grad_steps = 852, loss = 0.6928234696388245
In grad_steps = 853, loss = 1.1068974733352661
In grad_steps = 854, loss = 0.22391293942928314
In grad_steps = 855, loss = 0.4146887958049774
In grad_steps = 856, loss = 0.13956710696220398
In grad_steps = 857, loss = 0.16534441709518433
In grad_steps = 858, loss = 0.5209934711456299
In grad_steps = 859, loss = 0.23605358600616455
In grad_steps = 860, loss = 0.26350024342536926
In grad_steps = 861, loss = 1.0114573240280151
In grad_steps = 862, loss = 0.06520774215459824
In grad_steps = 863, loss = 0.25406065583229065
In grad_steps = 864, loss = 0.6239652633666992
In grad_steps = 865, loss = 0.06194353848695755
In grad_steps = 866, loss = 0.5538272857666016
In grad_steps = 867, loss = 0.09158198535442352
In grad_steps = 868, loss = 0.21336328983306885
In grad_steps = 869, loss = 0.11724153161048889
In grad_steps = 870, loss = 0.6490064263343811
In grad_steps = 871, loss = 0.060828354209661484
In grad_steps = 872, loss = 0.2903147041797638
In grad_steps = 873, loss = 0.34275805950164795
In grad_steps = 874, loss = 0.5526791214942932
In grad_steps = 875, loss = 0.2543303072452545
In grad_steps = 876, loss = 0.12603016197681427
In grad_steps = 877, loss = 0.06917933374643326
In grad_steps = 878, loss = 0.7336257696151733
In grad_steps = 879, loss = 0.26381951570510864
In grad_steps = 880, loss = 0.20654508471488953
In grad_steps = 881, loss = 0.7626392841339111
In grad_steps = 882, loss = 0.1094123050570488
In grad_steps = 883, loss = 0.6844771504402161
In grad_steps = 884, loss = 0.688963770866394
In grad_steps = 885, loss = 0.13333070278167725
In grad_steps = 886, loss = 0.3137781322002411
In grad_steps = 887, loss = 0.6186814308166504
In grad_steps = 888, loss = 0.49670472741127014
In grad_steps = 889, loss = 0.21195289492607117
In grad_steps = 890, loss = 0.14580346643924713
In grad_steps = 891, loss = 0.8743820786476135
In grad_steps = 892, loss = 0.4959678053855896
In grad_steps = 893, loss = 0.7498335242271423
In grad_steps = 894, loss = 0.10055798292160034
In grad_steps = 895, loss = 0.08753243088722229
In grad_steps = 896, loss = 0.5147805213928223
In grad_steps = 897, loss = 0.18294617533683777
In grad_steps = 898, loss = 0.4841129779815674
In grad_steps = 899, loss = 0.47469186782836914
In grad_steps = 900, loss = 0.4844506084918976
In grad_steps = 901, loss = 0.5324634909629822
In grad_steps = 902, loss = 0.1547674536705017
In grad_steps = 903, loss = 0.6658022403717041
In grad_steps = 904, loss = 0.4864952862262726
In grad_steps = 905, loss = 0.14358888566493988
In grad_steps = 906, loss = 0.5745851993560791
In grad_steps = 907, loss = 0.3486590087413788
In grad_steps = 908, loss = 0.17798149585723877
In grad_steps = 909, loss = 0.6469947099685669
In grad_steps = 910, loss = 0.18525730073451996
In grad_steps = 911, loss = 0.47508731484413147
In grad_steps = 912, loss = 0.21164508163928986
In grad_steps = 913, loss = 0.6521353721618652
In grad_steps = 914, loss = 0.49290573596954346
In grad_steps = 915, loss = 0.7230384349822998
In grad_steps = 916, loss = 0.2687433362007141
In grad_steps = 917, loss = 0.5659530162811279
In grad_steps = 918, loss = 0.4208296239376068
In grad_steps = 919, loss = 0.17147837579250336
In grad_steps = 920, loss = 0.4910871088504791
In grad_steps = 921, loss = 0.8966891765594482
In grad_steps = 922, loss = 0.4313581883907318
In grad_steps = 923, loss = 0.22908739745616913
In grad_steps = 924, loss = 0.1445087045431137
In grad_steps = 925, loss = 0.2021198272705078
In grad_steps = 926, loss = 0.5064481496810913
In grad_steps = 927, loss = 0.07661014795303345
In grad_steps = 928, loss = 0.7684818506240845
In grad_steps = 929, loss = 0.11647801101207733
In grad_steps = 930, loss = 0.7077031135559082
In grad_steps = 931, loss = 0.12882141768932343
In grad_steps = 932, loss = 0.0945703387260437
In grad_steps = 933, loss = 0.6392761468887329
In grad_steps = 934, loss = 0.23996369540691376
In grad_steps = 935, loss = 0.8604622483253479
In grad_steps = 936, loss = 0.41877231001853943
In grad_steps = 937, loss = 0.11502866446971893
In grad_steps = 938, loss = 0.338269978761673
In grad_steps = 939, loss = 0.3138437867164612
In grad_steps = 940, loss = 0.14546865224838257
In grad_steps = 941, loss = 0.2726813554763794
In grad_steps = 942, loss = 0.26848354935646057
In grad_steps = 943, loss = 1.381227731704712
In grad_steps = 944, loss = 0.49988704919815063
In grad_steps = 945, loss = 0.1464393436908722
In grad_steps = 946, loss = 0.6819314360618591
In grad_steps = 947, loss = 0.33276820182800293
In grad_steps = 948, loss = 0.35381728410720825
In grad_steps = 949, loss = 0.49206629395484924
In grad_steps = 950, loss = 0.8542412519454956
In grad_steps = 951, loss = 1.0755681991577148
In grad_steps = 952, loss = 0.10771681368350983
In grad_steps = 953, loss = 0.3817543387413025
In grad_steps = 954, loss = 0.19232246279716492
In grad_steps = 955, loss = 0.23404285311698914
In grad_steps = 956, loss = 0.25649234652519226
In grad_steps = 957, loss = 0.4443553686141968
In grad_steps = 958, loss = 0.4177784323692322
In grad_steps = 959, loss = 0.9748044610023499
In grad_steps = 960, loss = 0.2496839016675949
In grad_steps = 961, loss = 0.4680868089199066
In grad_steps = 962, loss = 0.16136708855628967
In grad_steps = 963, loss = 0.20850512385368347
In grad_steps = 964, loss = 0.253647118806839
In grad_steps = 965, loss = 0.30925750732421875
In grad_steps = 966, loss = 0.3551334738731384
In grad_steps = 967, loss = 0.7970792055130005
In grad_steps = 968, loss = 0.5695432424545288
In grad_steps = 969, loss = 0.554008424282074
In grad_steps = 970, loss = 0.0945199579000473
In grad_steps = 971, loss = 0.3075411319732666
In grad_steps = 972, loss = 0.5892184376716614
In grad_steps = 973, loss = 0.1917925775051117
In grad_steps = 974, loss = 0.351048082113266
In grad_steps = 975, loss = 0.1592368185520172
In grad_steps = 976, loss = 0.27334341406822205
In grad_steps = 977, loss = 0.35105204582214355
In grad_steps = 978, loss = 1.1535625457763672
In grad_steps = 979, loss = 0.8499041795730591
In grad_steps = 980, loss = 0.9205424785614014
In grad_steps = 981, loss = 0.46910297870635986
In grad_steps = 982, loss = 0.4600057005882263
In grad_steps = 983, loss = 0.381987988948822
In grad_steps = 984, loss = 1.054467797279358
In grad_steps = 985, loss = 0.3870876133441925
In grad_steps = 986, loss = 0.1670132726430893
In grad_steps = 987, loss = 0.5515154004096985
In grad_steps = 988, loss = 0.6059436798095703
In grad_steps = 989, loss = 0.4950583279132843
In grad_steps = 990, loss = 0.914776086807251
In grad_steps = 991, loss = 0.7434340715408325
In grad_steps = 992, loss = 0.38304030895233154
In grad_steps = 993, loss = 0.7039761543273926
In grad_steps = 994, loss = 0.555188000202179
In grad_steps = 995, loss = 0.2676093578338623
In grad_steps = 996, loss = 0.8279751539230347
In grad_steps = 997, loss = 0.3229949474334717
In grad_steps = 998, loss = 0.3695853352546692
In grad_steps = 999, loss = 0.11743060499429703
In grad_steps = 1000, loss = 0.13089318573474884
In grad_steps = 1001, loss = 0.6286524534225464
In grad_steps = 1002, loss = 0.31894510984420776
In grad_steps = 1003, loss = 0.23030826449394226
In grad_steps = 1004, loss = 0.44030019640922546
In grad_steps = 1005, loss = 0.7824958562850952
In grad_steps = 1006, loss = 0.26884862780570984
In grad_steps = 1007, loss = 0.3719315528869629
In grad_steps = 1008, loss = 0.46800708770751953
In grad_steps = 1009, loss = 0.9690113067626953
In grad_steps = 1010, loss = 0.6982932090759277
In grad_steps = 1011, loss = 0.1216149777173996
In grad_steps = 1012, loss = 0.21305838227272034
In grad_steps = 1013, loss = 0.5049178600311279
In grad_steps = 1014, loss = 0.2944662570953369
In grad_steps = 1015, loss = 0.4749080240726471
In grad_steps = 1016, loss = 0.10863952338695526
In grad_steps = 1017, loss = 0.11606373637914658
In grad_steps = 1018, loss = 0.2644059658050537
In grad_steps = 1019, loss = 0.7567907571792603
In grad_steps = 1020, loss = 1.4162654876708984
In grad_steps = 1021, loss = 0.40113702416419983
In grad_steps = 1022, loss = 0.5225772857666016
In grad_steps = 1023, loss = 0.20296084880828857
In grad_steps = 1024, loss = 0.23353201150894165
In grad_steps = 1025, loss = 0.5791301727294922
In grad_steps = 1026, loss = 0.29991617798805237
In grad_steps = 1027, loss = 0.7662449479103088
In grad_steps = 1028, loss = 0.1829751878976822
In grad_steps = 1029, loss = 0.2100343108177185
In grad_steps = 1030, loss = 0.170871764421463
In grad_steps = 1031, loss = 0.5027294754981995
In grad_steps = 1032, loss = 0.5179504156112671
In grad_steps = 1033, loss = 0.41236019134521484
In grad_steps = 1034, loss = 0.21555858850479126
In grad_steps = 1035, loss = 0.4592485725879669
In grad_steps = 1036, loss = 0.6141158938407898
In grad_steps = 1037, loss = 0.16564179956912994
In grad_steps = 1038, loss = 0.4282710552215576
In grad_steps = 1039, loss = 0.6850401163101196
In grad_steps = 1040, loss = 0.26241767406463623
In grad_steps = 1041, loss = 0.622951865196228
In grad_steps = 1042, loss = 0.45112407207489014
In grad_steps = 1043, loss = 0.5323868989944458
In grad_steps = 1044, loss = 0.2530243992805481
In grad_steps = 1045, loss = 0.7857452630996704
In grad_steps = 1046, loss = 0.05461699515581131
In grad_steps = 1047, loss = 0.07977350056171417
In grad_steps = 1048, loss = 0.3830913305282593
In grad_steps = 1049, loss = 0.2511182725429535
In grad_steps = 1050, loss = 1.3056505918502808
In grad_steps = 1051, loss = 0.0999317616224289
In grad_steps = 1052, loss = 0.1510944366455078
In grad_steps = 1053, loss = 0.6178889870643616
In grad_steps = 1054, loss = 0.12206166237592697
In grad_steps = 1055, loss = 0.3247460424900055
In grad_steps = 1056, loss = 0.8435416221618652
In grad_steps = 1057, loss = 0.4269678592681885
In grad_steps = 1058, loss = 0.3352767527103424
In grad_steps = 1059, loss = 0.19079843163490295
In grad_steps = 1060, loss = 0.3114578127861023
In grad_steps = 1061, loss = 0.24526020884513855
In grad_steps = 1062, loss = 0.8885724544525146
In grad_steps = 1063, loss = 0.20791687071323395
In grad_steps = 1064, loss = 0.26518455147743225
In grad_steps = 1065, loss = 0.048332951962947845
In grad_steps = 1066, loss = 0.6385253071784973
In grad_steps = 1067, loss = 0.6534390449523926
In grad_steps = 1068, loss = 0.11928948760032654
In grad_steps = 1069, loss = 0.279899924993515
In grad_steps = 1070, loss = 0.3095914423465729
In grad_steps = 1071, loss = 0.08964981138706207
In grad_steps = 1072, loss = 0.08748844265937805
In grad_steps = 1073, loss = 0.3155543804168701
In grad_steps = 1074, loss = 0.337022989988327
In grad_steps = 1075, loss = 0.5589107275009155
In grad_steps = 1076, loss = 0.19912651181221008
In grad_steps = 1077, loss = 0.14589843153953552
In grad_steps = 1078, loss = 0.7247156500816345
In grad_steps = 1079, loss = 0.15060925483703613
In grad_steps = 1080, loss = 0.13373242318630219
In grad_steps = 1081, loss = 1.0228923559188843
In grad_steps = 1082, loss = 0.1200849786400795
In grad_steps = 1083, loss = 0.032887645065784454
In grad_steps = 1084, loss = 0.5656580924987793
In grad_steps = 1085, loss = 1.1393496990203857
In grad_steps = 1086, loss = 0.11597000807523727
In grad_steps = 1087, loss = 0.7982842326164246
In grad_steps = 1088, loss = 0.3446401357650757
In grad_steps = 1089, loss = 0.16845864057540894
In grad_steps = 1090, loss = 0.07433852553367615
In grad_steps = 1091, loss = 0.2206585705280304
In grad_steps = 1092, loss = 0.057036392390728
In grad_steps = 1093, loss = 0.677042543888092
In grad_steps = 1094, loss = 0.07489397376775742
In grad_steps = 1095, loss = 0.2954767048358917
In grad_steps = 1096, loss = 0.10721983015537262
In grad_steps = 1097, loss = 0.09664012491703033
In grad_steps = 1098, loss = 0.37061718106269836
In grad_steps = 1099, loss = 0.41629984974861145
In grad_steps = 1100, loss = 0.1975487470626831
In grad_steps = 1101, loss = 0.4399231970310211
In grad_steps = 1102, loss = 0.8825562000274658
In grad_steps = 1103, loss = 0.028320057317614555
In grad_steps = 1104, loss = 1.314655065536499
In grad_steps = 1105, loss = 0.24749840795993805
In grad_steps = 1106, loss = 0.24528571963310242
In grad_steps = 1107, loss = 0.11886067688465118
In grad_steps = 1108, loss = 0.5021889805793762
In grad_steps = 1109, loss = 0.2300628125667572
In grad_steps = 1110, loss = 0.2919745445251465
In grad_steps = 1111, loss = 0.4376910924911499
In grad_steps = 1112, loss = 0.8482386469841003
In grad_steps = 1113, loss = 0.3396286964416504
In grad_steps = 1114, loss = 0.3017401099205017
In grad_steps = 1115, loss = 0.08168517053127289
In grad_steps = 1116, loss = 0.6797338128089905
In grad_steps = 1117, loss = 0.2274230569601059
In grad_steps = 1118, loss = 0.3611936569213867
In grad_steps = 1119, loss = 0.5024271607398987
In grad_steps = 1120, loss = 0.09161090850830078
In grad_steps = 1121, loss = 0.15166965126991272
In grad_steps = 1122, loss = 0.37240129709243774
In grad_steps = 1123, loss = 0.6504203677177429
In grad_steps = 1124, loss = 0.4212300777435303
In grad_steps = 1125, loss = 0.1975717842578888
In grad_steps = 1126, loss = 0.14890000224113464
In grad_steps = 1127, loss = 0.16419953107833862
In grad_steps = 1128, loss = 0.5567127466201782
In grad_steps = 1129, loss = 0.2541026175022125
In grad_steps = 1130, loss = 0.09399964660406113
In grad_steps = 1131, loss = 0.17313939332962036
In grad_steps = 1132, loss = 0.19939292967319489
In grad_steps = 1133, loss = 0.6831694841384888
In grad_steps = 1134, loss = 0.26082414388656616
In grad_steps = 1135, loss = 0.6005104780197144
In grad_steps = 1136, loss = 0.8395082354545593
In grad_steps = 1137, loss = 0.3687746822834015
In grad_steps = 1138, loss = 0.22235681116580963
In grad_steps = 1139, loss = 0.5600906014442444
In grad_steps = 1140, loss = 0.1189301460981369
In grad_steps = 1141, loss = 1.4030182361602783
In grad_steps = 1142, loss = 0.20100083947181702
In grad_steps = 1143, loss = 0.6961539387702942
In grad_steps = 1144, loss = 0.015575832687318325
In grad_steps = 1145, loss = 0.5071576237678528
In grad_steps = 1146, loss = 1.525733232498169
In grad_steps = 1147, loss = 0.5901561975479126
In grad_steps = 1148, loss = 0.35479632019996643
In grad_steps = 1149, loss = 0.07635288685560226
In grad_steps = 1150, loss = 0.46408843994140625
In grad_steps = 1151, loss = 0.3517652451992035
In grad_steps = 1152, loss = 0.3764582872390747
In grad_steps = 1153, loss = 0.6964631080627441
In grad_steps = 1154, loss = 0.45376846194267273
In grad_steps = 1155, loss = 0.4235271215438843
In grad_steps = 1156, loss = 0.3846299648284912
In grad_steps = 1157, loss = 0.22471073269844055
In grad_steps = 1158, loss = 0.398348867893219
In grad_steps = 1159, loss = 0.12949888408184052
In grad_steps = 1160, loss = 0.3324819803237915
In grad_steps = 1161, loss = 0.4791654348373413
In grad_steps = 1162, loss = 0.31352707743644714
In grad_steps = 1163, loss = 0.29684165120124817
In grad_steps = 1164, loss = 0.1711297631263733
In grad_steps = 1165, loss = 0.23468750715255737
In grad_steps = 1166, loss = 1.531085729598999
In grad_steps = 1167, loss = 0.5053423643112183
In grad_steps = 1168, loss = 0.4821692705154419
In grad_steps = 1169, loss = 0.06381087750196457
In grad_steps = 1170, loss = 0.20207436382770538
In grad_steps = 1171, loss = 0.33572131395339966
In grad_steps = 1172, loss = 0.37718990445137024
In grad_steps = 1173, loss = 0.2564322352409363
In grad_steps = 1174, loss = 0.2216169238090515
In grad_steps = 1175, loss = 0.6196023225784302
In grad_steps = 1176, loss = 0.520023763179779
In grad_steps = 1177, loss = 0.07736168801784515
In grad_steps = 1178, loss = 0.8035162687301636
In grad_steps = 1179, loss = 0.07570892572402954
In grad_steps = 1180, loss = 0.29598361253738403
In grad_steps = 1181, loss = 0.4492985010147095
In grad_steps = 1182, loss = 1.0347245931625366
In grad_steps = 1183, loss = 0.46702319383621216
In grad_steps = 1184, loss = 0.7808973789215088
In grad_steps = 1185, loss = 0.36996495723724365
In grad_steps = 1186, loss = 0.29041624069213867
In grad_steps = 1187, loss = 0.17429521679878235
In grad_steps = 1188, loss = 0.9194490909576416
In grad_steps = 1189, loss = 0.2766488790512085
In grad_steps = 1190, loss = 0.12203877419233322
In grad_steps = 1191, loss = 0.2559317648410797
In grad_steps = 1192, loss = 0.293987512588501
In grad_steps = 1193, loss = 0.1328456550836563
In grad_steps = 1194, loss = 0.20850934088230133
In grad_steps = 1195, loss = 0.591296911239624
In grad_steps = 1196, loss = 0.14072683453559875
In grad_steps = 1197, loss = 0.8322888612747192
In grad_steps = 1198, loss = 0.10836289077997208
In grad_steps = 1199, loss = 1.016806960105896
In grad_steps = 1200, loss = 0.13787567615509033
In grad_steps = 1201, loss = 0.10376562178134918
In grad_steps = 1202, loss = 0.29962167143821716
In grad_steps = 1203, loss = 0.06283657252788544
In grad_steps = 1204, loss = 0.09583719074726105
In grad_steps = 1205, loss = 1.221676230430603
In grad_steps = 1206, loss = 0.9819344878196716
In grad_steps = 1207, loss = 0.1695917546749115
In grad_steps = 1208, loss = 0.4295407831668854
In grad_steps = 1209, loss = 0.6368955969810486
In grad_steps = 1210, loss = 0.5144525766372681
In grad_steps = 1211, loss = 0.34696757793426514
In grad_steps = 1212, loss = 0.19439511001110077
In grad_steps = 1213, loss = 0.7138681411743164
In grad_steps = 1214, loss = 0.2916161119937897
In grad_steps = 1215, loss = 0.26085296273231506
In grad_steps = 1216, loss = 0.18856710195541382
In grad_steps = 1217, loss = 0.04941406473517418
In grad_steps = 1218, loss = 0.6695275902748108
In grad_steps = 1219, loss = 0.22618702054023743
In grad_steps = 1220, loss = 0.44725143909454346
In grad_steps = 1221, loss = 0.3868289291858673
In grad_steps = 1222, loss = 0.3794711232185364
In grad_steps = 1223, loss = 0.3500573933124542
In grad_steps = 1224, loss = 0.8657166957855225
In grad_steps = 1225, loss = 0.13267427682876587
In grad_steps = 1226, loss = 0.15989209711551666
In grad_steps = 1227, loss = 0.14466479420661926
In grad_steps = 1228, loss = 0.22468668222427368
In grad_steps = 1229, loss = 1.3660613298416138
In grad_steps = 1230, loss = 0.6006951332092285
In grad_steps = 1231, loss = 0.09826468676328659
In grad_steps = 1232, loss = 0.034020550549030304
In grad_steps = 1233, loss = 0.5175442695617676
In grad_steps = 1234, loss = 0.3281630277633667
In grad_steps = 1235, loss = 0.33863043785095215
In grad_steps = 1236, loss = 0.5980330109596252
In grad_steps = 1237, loss = 0.6013315320014954
In grad_steps = 1238, loss = 0.19146054983139038
In grad_steps = 1239, loss = 0.38230836391448975
In grad_steps = 1240, loss = 0.2887604534626007
In grad_steps = 1241, loss = 0.08745409548282623
In grad_steps = 1242, loss = 0.4989836812019348
In grad_steps = 1243, loss = 0.15518324077129364
In grad_steps = 1244, loss = 0.7236133217811584
In grad_steps = 1245, loss = 0.5751458406448364
In grad_steps = 1246, loss = 0.15532270073890686
In grad_steps = 1247, loss = 0.2892736792564392
In grad_steps = 1248, loss = 0.5011975169181824
In grad_steps = 1249, loss = 0.3849959671497345
In grad_steps = 1250, loss = 0.08853670954704285
In grad_steps = 1251, loss = 0.20806393027305603
In grad_steps = 1252, loss = 0.07038363814353943
In grad_steps = 1253, loss = 0.07302263379096985
In grad_steps = 1254, loss = 0.26440557837486267
In grad_steps = 1255, loss = 0.8709398508071899
In grad_steps = 1256, loss = 0.9457879662513733
In grad_steps = 1257, loss = 0.6153379678726196
In grad_steps = 1258, loss = 0.5005883574485779
In grad_steps = 1259, loss = 0.08363920450210571
In grad_steps = 1260, loss = 0.21658335626125336
In grad_steps = 1261, loss = 1.14930260181427
In grad_steps = 1262, loss = 0.5037795305252075
In grad_steps = 1263, loss = 0.059810154139995575
In grad_steps = 1264, loss = 0.2768332362174988
In grad_steps = 1265, loss = 0.18868127465248108
In grad_steps = 1266, loss = 0.7513310313224792
In grad_steps = 1267, loss = 0.7616552114486694
In grad_steps = 1268, loss = 0.7103238701820374
In grad_steps = 1269, loss = 0.07935038208961487
In grad_steps = 1270, loss = 0.09337623417377472
In grad_steps = 1271, loss = 0.5371045470237732
In grad_steps = 1272, loss = 0.57036292552948
In grad_steps = 1273, loss = 0.2478340119123459
In grad_steps = 1274, loss = 0.27575257420539856
In grad_steps = 1275, loss = 0.17182472348213196
In grad_steps = 1276, loss = 0.342911958694458
In grad_steps = 1277, loss = 0.11612749844789505
In grad_steps = 1278, loss = 0.3622565269470215
In grad_steps = 1279, loss = 0.14765989780426025
In grad_steps = 1280, loss = 0.13649515807628632
In grad_steps = 1281, loss = 0.4221861958503723
In grad_steps = 1282, loss = 0.4841122627258301
In grad_steps = 1283, loss = 0.6600573062896729
In grad_steps = 1284, loss = 0.11798863112926483
In grad_steps = 1285, loss = 0.26725009083747864
In grad_steps = 1286, loss = 0.27072441577911377
In grad_steps = 1287, loss = 0.10616160184144974
In grad_steps = 1288, loss = 0.34724488854408264
In grad_steps = 1289, loss = 0.08757950365543365
In grad_steps = 1290, loss = 0.041629135608673096
In grad_steps = 1291, loss = 0.2796056866645813
In grad_steps = 1292, loss = 0.793869137763977
In grad_steps = 1293, loss = 0.11349114030599594
In grad_steps = 1294, loss = 0.6017357707023621
In grad_steps = 1295, loss = 0.43224772810935974
In grad_steps = 1296, loss = 0.05428604036569595
In grad_steps = 1297, loss = 0.1333739459514618
In grad_steps = 1298, loss = 0.09657718241214752
In grad_steps = 1299, loss = 0.9122571349143982
In grad_steps = 1300, loss = 0.1378064900636673
In grad_steps = 1301, loss = 0.09864357858896255
In grad_steps = 1302, loss = 0.034221332520246506
In grad_steps = 1303, loss = 0.8105373382568359
In grad_steps = 1304, loss = 0.02826175093650818
In grad_steps = 1305, loss = 0.8888922929763794
In grad_steps = 1306, loss = 0.8313779234886169
In grad_steps = 1307, loss = 0.2818251848220825
In grad_steps = 1308, loss = 0.8648971319198608
In grad_steps = 1309, loss = 0.3661165237426758
In grad_steps = 1310, loss = 0.17299002408981323
In grad_steps = 1311, loss = 0.19655200839042664
In grad_steps = 1312, loss = 0.1561414897441864
In grad_steps = 1313, loss = 0.1180761307477951
In grad_steps = 1314, loss = 0.09917852282524109
In grad_steps = 1315, loss = 0.16537652909755707
In grad_steps = 1316, loss = 1.011513352394104
In grad_steps = 1317, loss = 0.8384393453598022
In grad_steps = 1318, loss = 0.35912850499153137
In grad_steps = 1319, loss = 0.31004947423934937
In grad_steps = 1320, loss = 0.4207533299922943
In grad_steps = 1321, loss = 0.2664419412612915
In grad_steps = 1322, loss = 0.5593984723091125
In grad_steps = 1323, loss = 0.41819462180137634
In grad_steps = 1324, loss = 0.07045088708400726
In grad_steps = 1325, loss = 0.1288178712129593
In grad_steps = 1326, loss = 0.17016878724098206
In grad_steps = 1327, loss = 0.653201162815094
In grad_steps = 1328, loss = 0.06505221873521805
In grad_steps = 1329, loss = 0.6771852970123291
In grad_steps = 1330, loss = 0.7357646822929382
In grad_steps = 1331, loss = 0.3437266945838928
In grad_steps = 1332, loss = 0.10081569850444794
In grad_steps = 1333, loss = 0.399678111076355
In grad_steps = 1334, loss = 0.09536799043416977
In grad_steps = 1335, loss = 0.3352550268173218
In grad_steps = 1336, loss = 0.2429947555065155
In grad_steps = 1337, loss = 0.16447573900222778
In grad_steps = 1338, loss = 0.03022024966776371
In grad_steps = 1339, loss = 1.1857181787490845
In grad_steps = 1340, loss = 0.6656007170677185
In grad_steps = 1341, loss = 0.21336226165294647
In grad_steps = 1342, loss = 0.38941511511802673
In grad_steps = 1343, loss = 0.567037045955658
In grad_steps = 1344, loss = 0.17112433910369873
In grad_steps = 1345, loss = 1.1128623485565186
In grad_steps = 1346, loss = 0.17103402316570282
In grad_steps = 1347, loss = 0.33502352237701416
In grad_steps = 1348, loss = 0.5116584300994873
In grad_steps = 1349, loss = 0.022122051566839218
In grad_steps = 1350, loss = 0.5487164258956909
In grad_steps = 1351, loss = 0.30616143345832825
In grad_steps = 1352, loss = 0.12508106231689453
In grad_steps = 1353, loss = 0.7803895473480225
In grad_steps = 1354, loss = 0.1631276160478592
In grad_steps = 1355, loss = 0.7016474008560181
In grad_steps = 1356, loss = 0.14812071621418
In grad_steps = 1357, loss = 0.4322618246078491
In grad_steps = 1358, loss = 0.2904554307460785
In grad_steps = 1359, loss = 0.15243259072303772
In grad_steps = 1360, loss = 0.20980288088321686
In grad_steps = 1361, loss = 0.7143925428390503
In grad_steps = 1362, loss = 0.07524651288986206
In grad_steps = 1363, loss = 0.14418205618858337
In grad_steps = 1364, loss = 0.3326278626918793
In grad_steps = 1365, loss = 0.29459044337272644
In grad_steps = 1366, loss = 0.4338182806968689
In grad_steps = 1367, loss = 0.060382939875125885
In grad_steps = 1368, loss = 0.5467469096183777
In grad_steps = 1369, loss = 0.18329177796840668
In grad_steps = 1370, loss = 0.06377201527357101
In grad_steps = 1371, loss = 0.4056794047355652
In grad_steps = 1372, loss = 0.5449250340461731
In grad_steps = 1373, loss = 0.05784297361969948
In grad_steps = 1374, loss = 0.8147830963134766
In grad_steps = 1375, loss = 0.04102884978055954
In grad_steps = 1376, loss = 0.7109196186065674
In grad_steps = 1377, loss = 0.053494881838560104
In grad_steps = 1378, loss = 0.6213013529777527
In grad_steps = 1379, loss = 0.16893813014030457
In grad_steps = 1380, loss = 0.18142960965633392
In grad_steps = 1381, loss = 0.055941514670848846
In grad_steps = 1382, loss = 0.19136551022529602
In grad_steps = 1383, loss = 0.731421947479248
In grad_steps = 1384, loss = 0.4768809378147125
In grad_steps = 1385, loss = 0.6550293564796448
In grad_steps = 1386, loss = 0.5529817342758179
In grad_steps = 1387, loss = 0.19654864072799683
In grad_steps = 1388, loss = 0.06291132420301437
In grad_steps = 1389, loss = 0.11362958699464798
In grad_steps = 1390, loss = 0.1701609194278717
In grad_steps = 1391, loss = 0.6854302883148193
In grad_steps = 1392, loss = 0.12276293337345123
In grad_steps = 1393, loss = 0.2936277687549591
In grad_steps = 1394, loss = 0.19144147634506226
In grad_steps = 1395, loss = 0.8939336538314819
In grad_steps = 1396, loss = 0.6658667325973511
In grad_steps = 1397, loss = 0.07472968846559525
In grad_steps = 1398, loss = 0.5526614189147949
In grad_steps = 1399, loss = 0.0910092145204544
In grad_steps = 1400, loss = 0.6413145661354065
In grad_steps = 1401, loss = 0.3058096468448639
In grad_steps = 1402, loss = 0.20814503729343414
In grad_steps = 1403, loss = 0.5332008004188538
In grad_steps = 1404, loss = 0.6822114586830139
In grad_steps = 1405, loss = 0.15427836775779724
In grad_steps = 1406, loss = 0.5298604965209961
In grad_steps = 1407, loss = 0.24839721620082855
In grad_steps = 1408, loss = 0.4729880690574646
In grad_steps = 1409, loss = 0.2516513168811798
In grad_steps = 1410, loss = 0.4146810472011566
In grad_steps = 1411, loss = 0.46997761726379395
In grad_steps = 1412, loss = 0.23467327654361725
In grad_steps = 1413, loss = 1.0354390144348145
In grad_steps = 1414, loss = 0.3303947448730469
In grad_steps = 1415, loss = 0.17396152019500732
In grad_steps = 1416, loss = 0.29671406745910645
In grad_steps = 1417, loss = 0.1825244426727295
In grad_steps = 1418, loss = 0.05944336578249931
In grad_steps = 1419, loss = 0.11758122593164444
In grad_steps = 1420, loss = 0.32132765650749207
In grad_steps = 1421, loss = 0.0884404107928276
In grad_steps = 1422, loss = 0.33699122071266174
In grad_steps = 1423, loss = 0.9122779369354248
In grad_steps = 1424, loss = 0.06135883554816246
In grad_steps = 1425, loss = 0.40441569685935974
In grad_steps = 1426, loss = 0.08655032515525818
In grad_steps = 1427, loss = 0.24668923020362854
In grad_steps = 1428, loss = 0.618226170539856
In grad_steps = 1429, loss = 0.32902777194976807
In grad_steps = 1430, loss = 0.156717911362648
In grad_steps = 1431, loss = 0.8028956651687622
In grad_steps = 1432, loss = 0.044480644166469574
In grad_steps = 1433, loss = 0.057143811136484146
In grad_steps = 1434, loss = 0.5166155099868774
In grad_steps = 1435, loss = 0.3675939440727234
In grad_steps = 1436, loss = 0.3125479221343994
In grad_steps = 1437, loss = 0.8948229551315308
In grad_steps = 1438, loss = 0.24105197191238403
In grad_steps = 1439, loss = 0.03607454523444176
In grad_steps = 1440, loss = 0.3475092649459839
In grad_steps = 1441, loss = 1.0314700603485107
In grad_steps = 1442, loss = 0.1944911777973175
In grad_steps = 1443, loss = 1.6472363471984863
In grad_steps = 1444, loss = 0.06426320970058441
In grad_steps = 1445, loss = 0.17657050490379333
In grad_steps = 1446, loss = 0.2698996365070343
In grad_steps = 1447, loss = 0.021675100550055504
In grad_steps = 1448, loss = 0.4588086009025574
In grad_steps = 1449, loss = 0.37661460041999817
In grad_steps = 1450, loss = 0.15073677897453308
In grad_steps = 1451, loss = 0.825785756111145
In grad_steps = 1452, loss = 0.13234417140483856
In grad_steps = 1453, loss = 0.7985254526138306
In grad_steps = 1454, loss = 0.18788933753967285
In grad_steps = 1455, loss = 0.7340941429138184
In grad_steps = 1456, loss = 0.30171674489974976
In grad_steps = 1457, loss = 0.44447895884513855
In grad_steps = 1458, loss = 0.20837637782096863
In grad_steps = 1459, loss = 0.40649592876434326
In grad_steps = 1460, loss = 0.08497295528650284
In grad_steps = 1461, loss = 0.4773064851760864
In grad_steps = 1462, loss = 0.1827678382396698
In grad_steps = 1463, loss = 0.9195526838302612
In grad_steps = 1464, loss = 0.31689223647117615
In grad_steps = 1465, loss = 0.47749629616737366
In grad_steps = 1466, loss = 0.1243918240070343
In grad_steps = 1467, loss = 0.5822151899337769
In grad_steps = 1468, loss = 0.08501690626144409
In grad_steps = 1469, loss = 0.3407607078552246
In grad_steps = 1470, loss = 0.3300098478794098
In grad_steps = 1471, loss = 0.11461154371500015
In grad_steps = 1472, loss = 0.5685681104660034
In grad_steps = 1473, loss = 0.1209644079208374
In grad_steps = 1474, loss = 0.9208727478981018
In grad_steps = 1475, loss = 0.4262416958808899
In grad_steps = 1476, loss = 0.3988568186759949
In grad_steps = 1477, loss = 0.2070116400718689
In grad_steps = 1478, loss = 0.6588098406791687
In grad_steps = 1479, loss = 0.28903210163116455
In grad_steps = 1480, loss = 0.1695910096168518
In grad_steps = 1481, loss = 0.1935596615076065
In grad_steps = 1482, loss = 0.14508488774299622
In grad_steps = 1483, loss = 0.29237455129623413
In grad_steps = 1484, loss = 0.4238724708557129
In grad_steps = 1485, loss = 0.21433080732822418
In grad_steps = 1486, loss = 1.0624635219573975
In grad_steps = 1487, loss = 0.13643239438533783
In grad_steps = 1488, loss = 0.7031100392341614
In grad_steps = 1489, loss = 0.3779347836971283
In grad_steps = 1490, loss = 0.5382024049758911
In grad_steps = 1491, loss = 0.5720532536506653
In grad_steps = 1492, loss = 0.05401112884283066
In grad_steps = 1493, loss = 0.5631613731384277
In grad_steps = 1494, loss = 0.5094811916351318
In grad_steps = 1495, loss = 0.12041331827640533
In grad_steps = 1496, loss = 0.08206073194742203
In grad_steps = 1497, loss = 0.2053063064813614
In grad_steps = 1498, loss = 0.38944581151008606
In grad_steps = 1499, loss = 0.8821109533309937
In grad_steps = 1500, loss = 0.22660133242607117
In grad_steps = 1501, loss = 0.2843218743801117
In grad_steps = 1502, loss = 0.5423926115036011
In grad_steps = 1503, loss = 0.42215925455093384
In grad_steps = 1504, loss = 0.2981811463832855
In grad_steps = 1505, loss = 0.6215839982032776
In grad_steps = 1506, loss = 0.2780821621417999
In grad_steps = 1507, loss = 0.6319608688354492
In grad_steps = 1508, loss = 0.18229086697101593
In grad_steps = 1509, loss = 0.6678355932235718
In grad_steps = 1510, loss = 0.899112343788147
In grad_steps = 1511, loss = 0.5424187779426575
In grad_steps = 1512, loss = 0.9142102599143982
In grad_steps = 1513, loss = 0.6949471235275269
In grad_steps = 1514, loss = 0.21417811512947083
In grad_steps = 1515, loss = 0.3349318504333496
In grad_steps = 1516, loss = 0.1536034792661667
In grad_steps = 1517, loss = 0.08253545314073563
In grad_steps = 1518, loss = 0.09731777757406235
In grad_steps = 1519, loss = 0.3990522027015686
In grad_steps = 1520, loss = 0.5255523920059204
In grad_steps = 1521, loss = 1.2017719745635986
In grad_steps = 1522, loss = 0.20228256285190582
In grad_steps = 1523, loss = 1.043938398361206
In grad_steps = 1524, loss = 0.5682182908058167
In grad_steps = 1525, loss = 0.272257924079895
In grad_steps = 1526, loss = 0.3806605935096741
In grad_steps = 1527, loss = 0.5852155089378357
In grad_steps = 1528, loss = 0.6722329258918762
In grad_steps = 1529, loss = 0.1482113003730774
In grad_steps = 1530, loss = 0.6317358613014221
In grad_steps = 1531, loss = 0.6268272995948792
In grad_steps = 1532, loss = 0.3353056311607361
In grad_steps = 1533, loss = 0.212474524974823
In grad_steps = 1534, loss = 0.23030681908130646
In grad_steps = 1535, loss = 0.28306394815444946
In grad_steps = 1536, loss = 0.1656537503004074
In grad_steps = 1537, loss = 0.527262806892395
In grad_steps = 1538, loss = 0.15184585750102997
In grad_steps = 1539, loss = 0.06864991039037704
In grad_steps = 1540, loss = 0.17050856351852417
In grad_steps = 1541, loss = 0.5437960028648376
In grad_steps = 1542, loss = 0.2243175059556961
In grad_steps = 1543, loss = 0.22210969030857086
In grad_steps = 1544, loss = 0.09318212419748306
In grad_steps = 1545, loss = 0.05469564348459244
In grad_steps = 1546, loss = 0.25644955039024353
In grad_steps = 1547, loss = 1.2168725728988647
In grad_steps = 1548, loss = 0.06846455484628677
In grad_steps = 1549, loss = 0.02150573953986168
In grad_steps = 1550, loss = 0.12029345333576202
In grad_steps = 1551, loss = 0.52154940366745
In grad_steps = 1552, loss = 0.14194761216640472
In grad_steps = 1553, loss = 0.041520535945892334
In grad_steps = 1554, loss = 0.5526369214057922
In grad_steps = 1555, loss = 0.158762127161026
In grad_steps = 1556, loss = 1.1890698671340942
In grad_steps = 1557, loss = 0.19785861670970917
In grad_steps = 1558, loss = 0.6219989657402039
In grad_steps = 1559, loss = 0.6687031984329224
In grad_steps = 1560, loss = 0.18438521027565002
In grad_steps = 1561, loss = 0.48883485794067383
In grad_steps = 1562, loss = 0.415313184261322
In grad_steps = 1563, loss = 0.08995456993579865
In grad_steps = 1564, loss = 1.4990960359573364
In grad_steps = 1565, loss = 0.6058145761489868
In grad_steps = 1566, loss = 0.2159471958875656
In grad_steps = 1567, loss = 0.4918856620788574
In grad_steps = 1568, loss = 0.3082248866558075
In grad_steps = 1569, loss = 0.2585814595222473
In grad_steps = 1570, loss = 0.6078107953071594
In grad_steps = 1571, loss = 0.7343282699584961
In grad_steps = 1572, loss = 0.24240706861019135
In grad_steps = 1573, loss = 0.227254256606102
In grad_steps = 1574, loss = 0.6098663806915283
In grad_steps = 1575, loss = 0.7288541793823242
In grad_steps = 1576, loss = 0.3028486371040344
In grad_steps = 1577, loss = 0.47968119382858276
In grad_steps = 1578, loss = 0.08833400905132294
In grad_steps = 1579, loss = 0.5237144231796265
In grad_steps = 1580, loss = 0.18184229731559753
In grad_steps = 1581, loss = 0.3053342401981354
In grad_steps = 1582, loss = 0.2523462474346161
In grad_steps = 1583, loss = 0.17547063529491425
In grad_steps = 1584, loss = 0.18221430480480194
In grad_steps = 1585, loss = 0.16484937071800232
In grad_steps = 1586, loss = 0.16506898403167725
In grad_steps = 1587, loss = 0.19513283669948578
In grad_steps = 1588, loss = 0.8880082964897156
In grad_steps = 1589, loss = 0.7067072987556458
In grad_steps = 1590, loss = 0.34817034006118774
In grad_steps = 1591, loss = 0.9668355584144592
In grad_steps = 1592, loss = 0.7832337021827698
In grad_steps = 1593, loss = 0.048810552805662155
In grad_steps = 1594, loss = 0.1251886934041977
In grad_steps = 1595, loss = 1.0290321111679077
In grad_steps = 1596, loss = 0.1129620224237442
In grad_steps = 1597, loss = 0.116579070687294
In grad_steps = 1598, loss = 0.08718381822109222
In grad_steps = 1599, loss = 0.20389100909233093
In grad_steps = 1600, loss = 0.4602864384651184
In grad_steps = 1601, loss = 0.22456884384155273
In grad_steps = 1602, loss = 0.17937104403972626
In grad_steps = 1603, loss = 0.5626581311225891
In grad_steps = 1604, loss = 0.3529645800590515
In grad_steps = 1605, loss = 0.11990059912204742
In grad_steps = 1606, loss = 0.26519110798835754
In grad_steps = 1607, loss = 0.05712214112281799
In grad_steps = 1608, loss = 0.5467520952224731
In grad_steps = 1609, loss = 0.5174617767333984
In grad_steps = 1610, loss = 0.2041487991809845
In grad_steps = 1611, loss = 0.2871767282485962
In grad_steps = 1612, loss = 0.3202376365661621
In grad_steps = 1613, loss = 0.26145902276039124
In grad_steps = 1614, loss = 0.10377124696969986
In grad_steps = 1615, loss = 0.08870638906955719
In grad_steps = 1616, loss = 0.08674214780330658
In grad_steps = 1617, loss = 0.5421542525291443
In grad_steps = 1618, loss = 0.8198040723800659
In grad_steps = 1619, loss = 0.23755691945552826
In grad_steps = 1620, loss = 0.41336700320243835
In grad_steps = 1621, loss = 0.30334633588790894
In grad_steps = 1622, loss = 1.174432396888733
In grad_steps = 1623, loss = 0.7526342868804932
In grad_steps = 1624, loss = 0.6198357939720154
In grad_steps = 1625, loss = 1.1975135803222656
In grad_steps = 1626, loss = 0.14526185393333435
In grad_steps = 1627, loss = 0.12822593748569489
In grad_steps = 1628, loss = 0.22687309980392456
In grad_steps = 1629, loss = 0.1772974729537964
In grad_steps = 1630, loss = 0.40992364287376404
In grad_steps = 1631, loss = 0.1662849634885788
In grad_steps = 1632, loss = 0.36367732286453247
In grad_steps = 1633, loss = 0.6028080582618713
In grad_steps = 1634, loss = 0.2745084762573242
In grad_steps = 1635, loss = 0.2112780064344406
In grad_steps = 1636, loss = 0.29857876896858215
In grad_steps = 1637, loss = 0.2814991772174835
In grad_steps = 1638, loss = 0.16936078667640686
In grad_steps = 1639, loss = 0.17996221780776978
In grad_steps = 1640, loss = 0.2801108956336975
In grad_steps = 1641, loss = 0.3805670440196991
In grad_steps = 1642, loss = 0.06336235255002975
In grad_steps = 1643, loss = 1.1519083976745605
In grad_steps = 1644, loss = 0.2689594030380249
In grad_steps = 1645, loss = 0.2629798650741577
In grad_steps = 1646, loss = 0.2420666366815567
In grad_steps = 1647, loss = 0.9467959403991699
In grad_steps = 1648, loss = 0.0645824447274208
In grad_steps = 1649, loss = 1.4489734172821045
In grad_steps = 1650, loss = 0.3219347894191742
In grad_steps = 1651, loss = 0.8260204195976257
In grad_steps = 1652, loss = 0.09546593576669693
In grad_steps = 1653, loss = 0.3298940360546112
In grad_steps = 1654, loss = 0.7279129028320312
In grad_steps = 1655, loss = 0.7135224342346191
In grad_steps = 1656, loss = 0.25193753838539124
In grad_steps = 1657, loss = 0.42730283737182617
In grad_steps = 1658, loss = 0.22511157393455505
In grad_steps = 1659, loss = 0.11593465507030487
In grad_steps = 1660, loss = 0.5300933122634888
In grad_steps = 1661, loss = 0.2960384488105774
In grad_steps = 1662, loss = 0.4274972081184387
In grad_steps = 1663, loss = 0.30773407220840454
In grad_steps = 1664, loss = 0.40858423709869385
In grad_steps = 1665, loss = 0.4198951721191406
In grad_steps = 1666, loss = 0.32295069098472595
In grad_steps = 1667, loss = 0.3077564835548401
In grad_steps = 1668, loss = 0.29315921664237976
In grad_steps = 1669, loss = 0.45180046558380127
In grad_steps = 1670, loss = 0.1681492030620575
In grad_steps = 1671, loss = 0.12341347336769104
In grad_steps = 1672, loss = 0.16920709609985352
In grad_steps = 1673, loss = 0.057571228593587875
In grad_steps = 1674, loss = 0.055232707411050797
In grad_steps = 1675, loss = 0.2223060429096222
In grad_steps = 1676, loss = 0.20098188519477844
In grad_steps = 1677, loss = 0.12626893818378448
In grad_steps = 1678, loss = 0.7965701818466187
In grad_steps = 1679, loss = 1.4387257099151611
In grad_steps = 1680, loss = 0.1153571605682373
In grad_steps = 1681, loss = 0.13430219888687134
In grad_steps = 1682, loss = 0.27901938557624817
In grad_steps = 1683, loss = 0.4073292016983032
In grad_steps = 1684, loss = 0.8152329325675964
In grad_steps = 1685, loss = 0.19919933378696442
In grad_steps = 1686, loss = 0.08552296459674835
In grad_steps = 1687, loss = 0.918066680431366
In grad_steps = 1688, loss = 0.3695763349533081
In grad_steps = 1689, loss = 1.3398367166519165
In grad_steps = 1690, loss = 0.16930806636810303
In grad_steps = 1691, loss = 0.4879626929759979
In grad_steps = 1692, loss = 0.22815954685211182
In grad_steps = 1693, loss = 0.2155013084411621
In grad_steps = 1694, loss = 0.10824787616729736
In grad_steps = 1695, loss = 0.541355311870575
In grad_steps = 1696, loss = 0.10177969932556152
In grad_steps = 1697, loss = 0.4746282994747162
In grad_steps = 1698, loss = 0.3263566195964813
In grad_steps = 1699, loss = 0.3092907667160034
In grad_steps = 1700, loss = 0.5157544612884521
In grad_steps = 1701, loss = 0.6992032527923584
In grad_steps = 1702, loss = 0.5171443223953247
In grad_steps = 1703, loss = 0.22601327300071716
In grad_steps = 1704, loss = 0.07527673244476318
In grad_steps = 1705, loss = 0.34635430574417114
In grad_steps = 1706, loss = 0.17386993765830994
In grad_steps = 1707, loss = 0.4116757810115814
In grad_steps = 1708, loss = 0.044233180582523346
In grad_steps = 1709, loss = 0.1261272132396698
In grad_steps = 1710, loss = 1.1321263313293457
In grad_steps = 1711, loss = 0.27723488211631775
In grad_steps = 1712, loss = 0.34398573637008667
In grad_steps = 1713, loss = 0.06099017336964607
In grad_steps = 1714, loss = 0.09556092321872711
In grad_steps = 1715, loss = 0.21468789875507355
In grad_steps = 1716, loss = 0.1874607503414154
In grad_steps = 1717, loss = 0.027636555954813957
In grad_steps = 1718, loss = 0.8726837635040283
In grad_steps = 1719, loss = 0.020436247810721397
In grad_steps = 1720, loss = 0.5992193222045898
In grad_steps = 1721, loss = 0.5991039276123047
In grad_steps = 1722, loss = 0.03500153124332428
In grad_steps = 1723, loss = 0.11323366314172745
In grad_steps = 1724, loss = 0.03692201152443886
In grad_steps = 1725, loss = 0.03890374302864075
In grad_steps = 1726, loss = 0.052544742822647095
In grad_steps = 1727, loss = 0.42304039001464844
In grad_steps = 1728, loss = 0.3069440424442291
In grad_steps = 1729, loss = 0.02259005792438984
In grad_steps = 1730, loss = 0.11726962774991989
In grad_steps = 1731, loss = 0.8190277814865112
In grad_steps = 1732, loss = 0.89033043384552
In grad_steps = 1733, loss = 0.41626405715942383
In grad_steps = 1734, loss = 0.6631890535354614
In grad_steps = 1735, loss = 0.08731390535831451
In grad_steps = 1736, loss = 0.23935018479824066
In grad_steps = 1737, loss = 0.12614379823207855
In grad_steps = 1738, loss = 0.8476564884185791
In grad_steps = 1739, loss = 0.4146397113800049
In grad_steps = 1740, loss = 0.2772235572338104
In grad_steps = 1741, loss = 0.10673749446868896
In grad_steps = 1742, loss = 0.4693607985973358
In grad_steps = 1743, loss = 0.46480798721313477
In grad_steps = 1744, loss = 0.12862452864646912
In grad_steps = 1745, loss = 0.4333423376083374
In grad_steps = 1746, loss = 0.09553328901529312
In grad_steps = 1747, loss = 0.2257786989212036
In grad_steps = 1748, loss = 0.19038671255111694
In grad_steps = 1749, loss = 1.0094058513641357
In grad_steps = 1750, loss = 0.667462944984436
In grad_steps = 1751, loss = 0.18712051212787628
In grad_steps = 1752, loss = 0.26448363065719604
In grad_steps = 1753, loss = 0.12376464158296585
In grad_steps = 1754, loss = 0.34821978211402893
In grad_steps = 1755, loss = 0.919639527797699
In grad_steps = 1756, loss = 0.47005993127822876
In grad_steps = 1757, loss = 0.23737230896949768
In grad_steps = 1758, loss = 0.9425831437110901
In grad_steps = 1759, loss = 0.08394211530685425
In grad_steps = 1760, loss = 0.16872859001159668
In grad_steps = 1761, loss = 0.35465550422668457
In grad_steps = 1762, loss = 0.5817471742630005
In grad_steps = 1763, loss = 0.6774200201034546
In grad_steps = 1764, loss = 0.349093496799469
In grad_steps = 1765, loss = 0.7784658670425415
In grad_steps = 1766, loss = 0.18342813849449158
In grad_steps = 1767, loss = 0.2741045355796814
In grad_steps = 1768, loss = 0.6217776536941528
In grad_steps = 1769, loss = 0.4065342843532562
In grad_steps = 1770, loss = 0.3415544033050537
In grad_steps = 1771, loss = 0.6071417331695557
In grad_steps = 1772, loss = 0.27363812923431396
In grad_steps = 1773, loss = 0.36738961935043335
In grad_steps = 1774, loss = 0.38956552743911743
In grad_steps = 1775, loss = 0.2282886505126953
In grad_steps = 1776, loss = 0.16198326647281647
In grad_steps = 1777, loss = 0.10993309319019318
In grad_steps = 1778, loss = 0.5029889941215515
In grad_steps = 1779, loss = 0.3006608486175537
In grad_steps = 1780, loss = 0.2515562176704407
In grad_steps = 1781, loss = 0.2068924754858017
In grad_steps = 1782, loss = 0.1834709793329239
In grad_steps = 1783, loss = 0.32187771797180176
In grad_steps = 1784, loss = 0.48680052161216736
In grad_steps = 1785, loss = 0.08287622034549713
In grad_steps = 1786, loss = 0.036865003407001495
In grad_steps = 1787, loss = 0.04365089535713196
In grad_steps = 1788, loss = 0.1558954268693924
In grad_steps = 1789, loss = 0.11330753564834595
In grad_steps = 1790, loss = 0.21381130814552307
In grad_steps = 1791, loss = 1.0047451257705688
In grad_steps = 1792, loss = 0.6946103572845459
In grad_steps = 1793, loss = 0.980251133441925
In grad_steps = 1794, loss = 0.49848443269729614
In grad_steps = 1795, loss = 1.5737645626068115
In grad_steps = 1796, loss = 0.11730563640594482
In grad_steps = 1797, loss = 0.2861485779285431
In grad_steps = 1798, loss = 0.17455659806728363
In grad_steps = 1799, loss = 0.23385469615459442
In grad_steps = 1800, loss = 0.01465598214417696
In grad_steps = 1801, loss = 0.5891413688659668
In grad_steps = 1802, loss = 0.030671589076519012
In grad_steps = 1803, loss = 1.0000274181365967
In grad_steps = 1804, loss = 0.1427769511938095
In grad_steps = 1805, loss = 0.3554655909538269
In grad_steps = 1806, loss = 0.2231081873178482
In grad_steps = 1807, loss = 0.23444348573684692
In grad_steps = 1808, loss = 0.9719183444976807
In grad_steps = 1809, loss = 0.27427807450294495
In grad_steps = 1810, loss = 0.5021519660949707
In grad_steps = 1811, loss = 0.7190772891044617
In grad_steps = 1812, loss = 0.14644023776054382
In grad_steps = 1813, loss = 0.5096750259399414
In grad_steps = 1814, loss = 0.2416841983795166
In grad_steps = 1815, loss = 1.1903201341629028
In grad_steps = 1816, loss = 0.39008376002311707
In grad_steps = 1817, loss = 0.1017846018075943
In grad_steps = 1818, loss = 0.2211342453956604
In grad_steps = 1819, loss = 0.1304006427526474
In grad_steps = 1820, loss = 0.5934370756149292
In grad_steps = 1821, loss = 0.15124061703681946
In grad_steps = 1822, loss = 0.3945828974246979
In grad_steps = 1823, loss = 0.1832800805568695
In grad_steps = 1824, loss = 0.26408880949020386
In grad_steps = 1825, loss = 0.29264819622039795
In grad_steps = 1826, loss = 0.2172849029302597
In grad_steps = 1827, loss = 0.1530572474002838
In grad_steps = 1828, loss = 0.6551305055618286
In grad_steps = 1829, loss = 0.21471798419952393
In grad_steps = 1830, loss = 0.7995807528495789
In grad_steps = 1831, loss = 0.2066752314567566
In grad_steps = 1832, loss = 0.3502127230167389
In grad_steps = 1833, loss = 0.10076141357421875
In grad_steps = 1834, loss = 0.07084978371858597
In grad_steps = 1835, loss = 0.025225220248103142
In grad_steps = 1836, loss = 0.05630284547805786
In grad_steps = 1837, loss = 1.1085176467895508
In grad_steps = 1838, loss = 0.664613664150238
In grad_steps = 1839, loss = 0.7005288600921631
In grad_steps = 1840, loss = 0.2421318143606186
In grad_steps = 1841, loss = 0.12687772512435913
In grad_steps = 1842, loss = 0.18639785051345825
In grad_steps = 1843, loss = 0.0480475015938282
In grad_steps = 1844, loss = 0.2811200022697449
In grad_steps = 1845, loss = 0.06311158835887909
In grad_steps = 1846, loss = 0.06769856810569763
In grad_steps = 1847, loss = 0.1906488835811615
In grad_steps = 1848, loss = 0.194534569978714
In grad_steps = 1849, loss = 0.38330668210983276
In grad_steps = 1850, loss = 0.06866778433322906
In grad_steps = 1851, loss = 0.09283071756362915
In grad_steps = 1852, loss = 0.11292107403278351
In grad_steps = 1853, loss = 1.501956820487976
In grad_steps = 1854, loss = 0.2572181224822998
In grad_steps = 1855, loss = 0.20284190773963928
In grad_steps = 1856, loss = 0.2095896601676941
In grad_steps = 1857, loss = 0.47674036026000977
In grad_steps = 1858, loss = 0.06220513954758644
In grad_steps = 1859, loss = 0.2489762306213379
In grad_steps = 1860, loss = 0.2572374939918518
In grad_steps = 1861, loss = 0.7372753620147705
In grad_steps = 1862, loss = 0.04995111748576164
In grad_steps = 1863, loss = 1.5757020711898804
In grad_steps = 1864, loss = 0.04936973378062248
In grad_steps = 1865, loss = 0.24734263122081757
In grad_steps = 1866, loss = 0.9919995665550232
In grad_steps = 1867, loss = 0.04693454131484032
In grad_steps = 1868, loss = 0.20048320293426514
In grad_steps = 1869, loss = 0.14478866755962372
In grad_steps = 1870, loss = 0.8132273554801941
In grad_steps = 1871, loss = 1.0052680969238281
In grad_steps = 1872, loss = 0.06644336134195328
In grad_steps = 1873, loss = 0.12497012317180634
In grad_steps = 1874, loss = 0.48569220304489136
In grad_steps = 1875, loss = 1.1351884603500366
In grad_steps = 1876, loss = 0.5482901930809021
In grad_steps = 1877, loss = 0.3822697103023529
In grad_steps = 1878, loss = 1.5096909999847412
In grad_steps = 1879, loss = 0.04967466741800308
In grad_steps = 1880, loss = 0.6279476284980774
In grad_steps = 1881, loss = 0.5748332738876343
In grad_steps = 1882, loss = 0.44738849997520447
In grad_steps = 1883, loss = 0.5960400104522705
In grad_steps = 1884, loss = 0.4097297191619873
In grad_steps = 1885, loss = 0.5062850713729858
In grad_steps = 1886, loss = 0.5968809723854065
In grad_steps = 1887, loss = 0.9673546552658081
In grad_steps = 1888, loss = 0.719578206539154
In grad_steps = 1889, loss = 0.40935057401657104
In grad_steps = 1890, loss = 0.2815682590007782
In grad_steps = 1891, loss = 0.5518001317977905
In grad_steps = 1892, loss = 0.4105038344860077
In grad_steps = 1893, loss = 0.2778983414173126
In grad_steps = 1894, loss = 0.3084515929222107
In grad_steps = 1895, loss = 0.2804889678955078
In grad_steps = 1896, loss = 0.34273889660835266
In grad_steps = 1897, loss = 0.18892525136470795
In grad_steps = 1898, loss = 0.27415651082992554
In grad_steps = 1899, loss = 0.2062227725982666
In grad_steps = 1900, loss = 0.08280519396066666
In grad_steps = 1901, loss = 0.10999474674463272
In grad_steps = 1902, loss = 0.04754975810647011
In grad_steps = 1903, loss = 0.5503441095352173
In grad_steps = 1904, loss = 0.9069697856903076
In grad_steps = 1905, loss = 0.6659382581710815
In grad_steps = 1906, loss = 0.017721008509397507
In grad_steps = 1907, loss = 0.10774657130241394
In grad_steps = 1908, loss = 0.01246244739741087
In grad_steps = 1909, loss = 0.24175673723220825
In grad_steps = 1910, loss = 0.03497656434774399
In grad_steps = 1911, loss = 0.15061084926128387
In grad_steps = 1912, loss = 0.02745874971151352
In grad_steps = 1913, loss = 0.9743342995643616
In grad_steps = 1914, loss = 0.7737851142883301
In grad_steps = 1915, loss = 0.3567703366279602
In grad_steps = 1916, loss = 0.08399249613285065
In grad_steps = 1917, loss = 0.08561375737190247
In grad_steps = 1918, loss = 0.44421127438545227
In grad_steps = 1919, loss = 0.24918071925640106
In grad_steps = 1920, loss = 0.07083781063556671
In grad_steps = 1921, loss = 0.11898927390575409
In grad_steps = 1922, loss = 0.016477759927511215
In grad_steps = 1923, loss = 0.3633038103580475
In grad_steps = 1924, loss = 1.0717377662658691
In grad_steps = 1925, loss = 0.10615092515945435
In grad_steps = 1926, loss = 0.07267843931913376
In grad_steps = 1927, loss = 0.08865238726139069
In grad_steps = 1928, loss = 0.2228996753692627
In grad_steps = 1929, loss = 0.14223873615264893
In grad_steps = 1930, loss = 0.8547478914260864
In grad_steps = 1931, loss = 0.7477657794952393
In grad_steps = 1932, loss = 0.21445146203041077
In grad_steps = 1933, loss = 0.5636566281318665
In grad_steps = 1934, loss = 0.06954032182693481
In grad_steps = 1935, loss = 0.20406104624271393
In grad_steps = 1936, loss = 0.049780331552028656
In grad_steps = 1937, loss = 0.20084050297737122
In grad_steps = 1938, loss = 0.05662884935736656
In grad_steps = 1939, loss = 0.917782723903656
In grad_steps = 1940, loss = 0.38778677582740784
In grad_steps = 1941, loss = 0.24324695765972137
In grad_steps = 1942, loss = 0.09481020271778107
In grad_steps = 1943, loss = 0.21173936128616333
In grad_steps = 1944, loss = 0.8218567371368408
In grad_steps = 1945, loss = 1.372084140777588
In grad_steps = 1946, loss = 0.6616744995117188
In grad_steps = 1947, loss = 0.747870683670044
In grad_steps = 1948, loss = 0.556000828742981
In grad_steps = 1949, loss = 0.3079608976840973
In grad_steps = 1950, loss = 0.5857135653495789
In grad_steps = 1951, loss = 0.45311078429222107
In grad_steps = 1952, loss = 0.6277983784675598
In grad_steps = 1953, loss = 0.20800666511058807
In grad_steps = 1954, loss = 0.3666854500770569
In grad_steps = 1955, loss = 0.5292304158210754
In grad_steps = 1956, loss = 0.4457113742828369
In grad_steps = 1957, loss = 0.7478411197662354
In grad_steps = 1958, loss = 0.11052936315536499
In grad_steps = 1959, loss = 0.21356317400932312
In grad_steps = 1960, loss = 0.24130792915821075
In grad_steps = 1961, loss = 0.279675155878067
In grad_steps = 1962, loss = 0.5317900776863098
In grad_steps = 1963, loss = 0.6346956491470337
In grad_steps = 1964, loss = 0.14529277384281158
In grad_steps = 1965, loss = 0.388854444026947
In grad_steps = 1966, loss = 0.21407142281532288
In grad_steps = 1967, loss = 0.12420020252466202
In grad_steps = 1968, loss = 0.39690110087394714
In grad_steps = 1969, loss = 1.160176396369934
In grad_steps = 1970, loss = 0.7086527943611145
In grad_steps = 1971, loss = 1.008518934249878
In grad_steps = 1972, loss = 0.19418181478977203
In grad_steps = 1973, loss = 0.08772027492523193
In grad_steps = 1974, loss = 0.28134286403656006
In grad_steps = 1975, loss = 0.07242269814014435
In grad_steps = 1976, loss = 0.4219156801700592
In grad_steps = 1977, loss = 0.22034689784049988
In grad_steps = 1978, loss = 0.2695603668689728
In grad_steps = 1979, loss = 0.5317422151565552
In grad_steps = 1980, loss = 0.16787271201610565
In grad_steps = 1981, loss = 0.31531232595443726
In grad_steps = 1982, loss = 0.3753950595855713
In grad_steps = 1983, loss = 0.07783076912164688
In grad_steps = 1984, loss = 0.24108323454856873
In grad_steps = 1985, loss = 0.8036178946495056
In grad_steps = 1986, loss = 0.30904725193977356
In grad_steps = 1987, loss = 0.27321624755859375
In grad_steps = 1988, loss = 1.061063528060913
In grad_steps = 1989, loss = 0.6681846976280212
In grad_steps = 1990, loss = 0.08525046706199646
In grad_steps = 1991, loss = 0.4878559708595276
In grad_steps = 1992, loss = 0.20667406916618347
In grad_steps = 1993, loss = 0.2571272850036621
In grad_steps = 1994, loss = 0.6339836120605469
In grad_steps = 1995, loss = 0.27272579073905945
In grad_steps = 1996, loss = 0.29312920570373535
In grad_steps = 1997, loss = 0.2682769298553467
In grad_steps = 1998, loss = 0.0960751622915268
In grad_steps = 1999, loss = 0.15623289346694946
In grad_steps = 2000, loss = 0.17014431953430176
In grad_steps = 2001, loss = 0.67044997215271
In grad_steps = 2002, loss = 0.20950594544410706
In grad_steps = 2003, loss = 0.18055659532546997
In grad_steps = 2004, loss = 0.13038256764411926
In grad_steps = 2005, loss = 0.1692039519548416
In grad_steps = 2006, loss = 0.15836632251739502
In grad_steps = 2007, loss = 0.1228504553437233
In grad_steps = 2008, loss = 0.11106741428375244
In grad_steps = 2009, loss = 0.03089744783937931
In grad_steps = 2010, loss = 0.19708457589149475
In grad_steps = 2011, loss = 0.30435559153556824
In grad_steps = 2012, loss = 0.6429109573364258
In grad_steps = 2013, loss = 0.45588359236717224
In grad_steps = 2014, loss = 0.10952620953321457
In grad_steps = 2015, loss = 0.16050413250923157
In grad_steps = 2016, loss = 0.08875510096549988
In grad_steps = 2017, loss = 1.0359829664230347
In grad_steps = 2018, loss = 0.55975341796875
In grad_steps = 2019, loss = 0.31868666410446167
In grad_steps = 2020, loss = 0.1189384013414383
In grad_steps = 2021, loss = 0.0747406929731369
In grad_steps = 2022, loss = 0.014775341376662254
In grad_steps = 2023, loss = 0.2615858018398285
In grad_steps = 2024, loss = 0.6404580473899841
In grad_steps = 2025, loss = 0.2976064682006836
In grad_steps = 2026, loss = 0.35454481840133667
In grad_steps = 2027, loss = 0.277910053730011
In grad_steps = 2028, loss = 0.4326760172843933
In grad_steps = 2029, loss = 0.12021826207637787
In grad_steps = 2030, loss = 0.05698734149336815
In grad_steps = 2031, loss = 0.02434593439102173
In grad_steps = 2032, loss = 0.9870508313179016
In grad_steps = 2033, loss = 0.4051288962364197
In grad_steps = 2034, loss = 0.31076523661613464
In grad_steps = 2035, loss = 0.024724379181861877
In grad_steps = 2036, loss = 0.03402067720890045
In grad_steps = 2037, loss = 0.07128211855888367
In grad_steps = 2038, loss = 0.796572744846344
In grad_steps = 2039, loss = 0.06901535391807556
In grad_steps = 2040, loss = 0.024814778938889503
In grad_steps = 2041, loss = 0.5434852242469788
In grad_steps = 2042, loss = 0.0635584220290184
In grad_steps = 2043, loss = 0.19532664120197296
In grad_steps = 2044, loss = 0.6957928538322449
In grad_steps = 2045, loss = 0.6564156413078308
In grad_steps = 2046, loss = 0.03141465410590172
In grad_steps = 2047, loss = 0.09592479467391968
In grad_steps = 2048, loss = 0.24224822223186493
In grad_steps = 2049, loss = 0.7732319831848145
In grad_steps = 2050, loss = 0.3987226188182831
In grad_steps = 2051, loss = 0.15632614493370056
In grad_steps = 2052, loss = 0.0067300316877663136
Beginning epoch 2
In grad_steps = 2053, loss = 0.2657751739025116
In grad_steps = 2054, loss = 0.05365518480539322
In grad_steps = 2055, loss = 0.17879566550254822
In grad_steps = 2056, loss = 0.3838666081428528
In grad_steps = 2057, loss = 0.17415285110473633
In grad_steps = 2058, loss = 0.052443355321884155
In grad_steps = 2059, loss = 0.07458566874265671
In grad_steps = 2060, loss = 0.09446936845779419
In grad_steps = 2061, loss = 0.9164255261421204
In grad_steps = 2062, loss = 0.6429449319839478
In grad_steps = 2063, loss = 0.9127913117408752
In grad_steps = 2064, loss = 0.08134480565786362
In grad_steps = 2065, loss = 0.5284833312034607
In grad_steps = 2066, loss = 0.05237318202853203
In grad_steps = 2067, loss = 0.38813307881355286
In grad_steps = 2068, loss = 0.5874322056770325
In grad_steps = 2069, loss = 0.12120917439460754
In grad_steps = 2070, loss = 0.44558820128440857
In grad_steps = 2071, loss = 0.3898603320121765
In grad_steps = 2072, loss = 0.4682183265686035
In grad_steps = 2073, loss = 0.06711538881063461
In grad_steps = 2074, loss = 0.6100115776062012
In grad_steps = 2075, loss = 0.9070661664009094
In grad_steps = 2076, loss = 0.1377621740102768
In grad_steps = 2077, loss = 0.01688695140182972
In grad_steps = 2078, loss = 0.33547845482826233
In grad_steps = 2079, loss = 0.32661670446395874
In grad_steps = 2080, loss = 0.8808005452156067
In grad_steps = 2081, loss = 0.1274772584438324
In grad_steps = 2082, loss = 0.11699505150318146
In grad_steps = 2083, loss = 0.48762136697769165
In grad_steps = 2084, loss = 0.4247080087661743
In grad_steps = 2085, loss = 0.22470207512378693
In grad_steps = 2086, loss = 0.5367956757545471
In grad_steps = 2087, loss = 0.1570100486278534
In grad_steps = 2088, loss = 0.3323743939399719
In grad_steps = 2089, loss = 0.3333479166030884
In grad_steps = 2090, loss = 0.11649075150489807
In grad_steps = 2091, loss = 0.15869933366775513
In grad_steps = 2092, loss = 0.2854943871498108
In grad_steps = 2093, loss = 0.39933788776397705
In grad_steps = 2094, loss = 0.05547236651182175
In grad_steps = 2095, loss = 0.1257823407649994
In grad_steps = 2096, loss = 0.1741366684436798
In grad_steps = 2097, loss = 0.2239736169576645
In grad_steps = 2098, loss = 0.3058251440525055
In grad_steps = 2099, loss = 0.7864293456077576
In grad_steps = 2100, loss = 0.4334719181060791
In grad_steps = 2101, loss = 0.12139326333999634
In grad_steps = 2102, loss = 0.12472784519195557
In grad_steps = 2103, loss = 0.150028795003891
In grad_steps = 2104, loss = 0.03279057890176773
In grad_steps = 2105, loss = 0.06473752856254578
In grad_steps = 2106, loss = 0.009437495842576027
In grad_steps = 2107, loss = 0.04021777957677841
In grad_steps = 2108, loss = 0.1179855540394783
In grad_steps = 2109, loss = 0.5599430203437805
In grad_steps = 2110, loss = 0.4883567690849304
In grad_steps = 2111, loss = 0.023699553683400154
In grad_steps = 2112, loss = 1.0541061162948608
In grad_steps = 2113, loss = 2.1109273433685303
In grad_steps = 2114, loss = 0.05843217298388481
In grad_steps = 2115, loss = 0.18314716219902039
In grad_steps = 2116, loss = 0.07242222875356674
In grad_steps = 2117, loss = 0.10047605633735657
In grad_steps = 2118, loss = 0.11934220790863037
In grad_steps = 2119, loss = 0.6363216042518616
In grad_steps = 2120, loss = 0.03668659180402756
In grad_steps = 2121, loss = 0.41322606801986694
In grad_steps = 2122, loss = 0.1302599310874939
In grad_steps = 2123, loss = 0.079615518450737
In grad_steps = 2124, loss = 0.08929985761642456
In grad_steps = 2125, loss = 0.18522310256958008
In grad_steps = 2126, loss = 0.1938098818063736
In grad_steps = 2127, loss = 0.04732054844498634
In grad_steps = 2128, loss = 0.5317838788032532
In grad_steps = 2129, loss = 0.07219603657722473
In grad_steps = 2130, loss = 0.3755061626434326
In grad_steps = 2131, loss = 0.36090287566185
In grad_steps = 2132, loss = 0.4595988988876343
In grad_steps = 2133, loss = 0.4572962522506714
In grad_steps = 2134, loss = 0.1583690643310547
In grad_steps = 2135, loss = 0.0971732884645462
In grad_steps = 2136, loss = 0.3447199761867523
In grad_steps = 2137, loss = 0.09067130088806152
In grad_steps = 2138, loss = 0.9723746180534363
In grad_steps = 2139, loss = 0.18555709719657898
In grad_steps = 2140, loss = 0.1289212703704834
In grad_steps = 2141, loss = 0.7509641647338867
In grad_steps = 2142, loss = 0.12423321604728699
In grad_steps = 2143, loss = 0.8852606415748596
In grad_steps = 2144, loss = 0.5928471684455872
In grad_steps = 2145, loss = 0.4326525628566742
In grad_steps = 2146, loss = 0.47308945655822754
In grad_steps = 2147, loss = 0.4562872052192688
In grad_steps = 2148, loss = 0.053892794996500015
In grad_steps = 2149, loss = 0.3107103705406189
In grad_steps = 2150, loss = 0.5828601121902466
In grad_steps = 2151, loss = 0.5067621469497681
In grad_steps = 2152, loss = 0.3065154552459717
In grad_steps = 2153, loss = 0.653695821762085
In grad_steps = 2154, loss = 0.23459061980247498
In grad_steps = 2155, loss = 0.4975007474422455
In grad_steps = 2156, loss = 0.0948958545923233
In grad_steps = 2157, loss = 0.31198400259017944
In grad_steps = 2158, loss = 0.2763441801071167
In grad_steps = 2159, loss = 0.19194167852401733
In grad_steps = 2160, loss = 0.17246313393115997
In grad_steps = 2161, loss = 0.45809197425842285
In grad_steps = 2162, loss = 0.057314783334732056
In grad_steps = 2163, loss = 0.1398790180683136
In grad_steps = 2164, loss = 0.023238370195031166
In grad_steps = 2165, loss = 0.04362080618739128
In grad_steps = 2166, loss = 0.4630664885044098
In grad_steps = 2167, loss = 0.06035125255584717
In grad_steps = 2168, loss = 0.6780650019645691
In grad_steps = 2169, loss = 0.2263319194316864
In grad_steps = 2170, loss = 0.08497601747512817
In grad_steps = 2171, loss = 0.08355586975812912
In grad_steps = 2172, loss = 0.24115729331970215
In grad_steps = 2173, loss = 0.16981950402259827
In grad_steps = 2174, loss = 0.16621115803718567
In grad_steps = 2175, loss = 0.019694652408361435
In grad_steps = 2176, loss = 0.2472289800643921
In grad_steps = 2177, loss = 0.19440080225467682
In grad_steps = 2178, loss = 0.23983821272850037
In grad_steps = 2179, loss = 0.010979460552334785
In grad_steps = 2180, loss = 1.145980715751648
In grad_steps = 2181, loss = 0.26262664794921875
In grad_steps = 2182, loss = 0.04954865574836731
In grad_steps = 2183, loss = 0.07877969741821289
In grad_steps = 2184, loss = 0.1477046012878418
In grad_steps = 2185, loss = 1.2371821403503418
In grad_steps = 2186, loss = 0.41255176067352295
In grad_steps = 2187, loss = 0.3688722848892212
In grad_steps = 2188, loss = 0.25545936822891235
In grad_steps = 2189, loss = 0.027235601097345352
In grad_steps = 2190, loss = 0.05550907924771309
In grad_steps = 2191, loss = 0.9816949367523193
In grad_steps = 2192, loss = 0.1612393856048584
In grad_steps = 2193, loss = 0.8626575469970703
In grad_steps = 2194, loss = 0.050094909965991974
In grad_steps = 2195, loss = 0.1678084135055542
In grad_steps = 2196, loss = 0.07730385661125183
In grad_steps = 2197, loss = 0.4977947771549225
In grad_steps = 2198, loss = 0.11163236200809479
In grad_steps = 2199, loss = 0.3017319142818451
In grad_steps = 2200, loss = 0.2509193420410156
In grad_steps = 2201, loss = 0.12633222341537476
In grad_steps = 2202, loss = 0.14640316367149353
In grad_steps = 2203, loss = 0.51789391040802
In grad_steps = 2204, loss = 0.04496043175458908
In grad_steps = 2205, loss = 1.1990622282028198
In grad_steps = 2206, loss = 0.22868508100509644
In grad_steps = 2207, loss = 0.3883957266807556
In grad_steps = 2208, loss = 0.08570823818445206
In grad_steps = 2209, loss = 0.16776897013187408
In grad_steps = 2210, loss = 0.059763915836811066
In grad_steps = 2211, loss = 0.10642380267381668
In grad_steps = 2212, loss = 0.17168043553829193
In grad_steps = 2213, loss = 0.10454817861318588
In grad_steps = 2214, loss = 0.32202455401420593
In grad_steps = 2215, loss = 0.30001357197761536
In grad_steps = 2216, loss = 0.07390552759170532
In grad_steps = 2217, loss = 0.18596836924552917
In grad_steps = 2218, loss = 0.03224232792854309
In grad_steps = 2219, loss = 0.019974661991000175
In grad_steps = 2220, loss = 0.03172799199819565
In grad_steps = 2221, loss = 0.5375304818153381
In grad_steps = 2222, loss = 0.03616980463266373
In grad_steps = 2223, loss = 0.06602813303470612
In grad_steps = 2224, loss = 0.3358865976333618
In grad_steps = 2225, loss = 0.18035365641117096
In grad_steps = 2226, loss = 1.1387240886688232
In grad_steps = 2227, loss = 0.01373477652668953
In grad_steps = 2228, loss = 0.980227530002594
In grad_steps = 2229, loss = 0.013232743367552757
In grad_steps = 2230, loss = 0.02177879959344864
In grad_steps = 2231, loss = 0.07000185549259186
In grad_steps = 2232, loss = 0.6257683038711548
In grad_steps = 2233, loss = 0.158521369099617
In grad_steps = 2234, loss = 0.18312130868434906
In grad_steps = 2235, loss = 0.08577118068933487
In grad_steps = 2236, loss = 1.0683802366256714
In grad_steps = 2237, loss = 0.07923951745033264
In grad_steps = 2238, loss = 0.5039727687835693
In grad_steps = 2239, loss = 0.04406874626874924
In grad_steps = 2240, loss = 0.5626152753829956
In grad_steps = 2241, loss = 0.2847754955291748
In grad_steps = 2242, loss = 0.2501611113548279
In grad_steps = 2243, loss = 0.2261175662279129
In grad_steps = 2244, loss = 0.8175098299980164
In grad_steps = 2245, loss = 0.6192863583564758
In grad_steps = 2246, loss = 0.63709956407547
In grad_steps = 2247, loss = 0.1695144921541214
In grad_steps = 2248, loss = 0.218338280916214
In grad_steps = 2249, loss = 0.10568386316299438
In grad_steps = 2250, loss = 0.4346773326396942
In grad_steps = 2251, loss = 0.4108272194862366
In grad_steps = 2252, loss = 1.1012840270996094
In grad_steps = 2253, loss = 0.06344112753868103
In grad_steps = 2254, loss = 0.7426019906997681
In grad_steps = 2255, loss = 0.062276482582092285
In grad_steps = 2256, loss = 0.510476291179657
In grad_steps = 2257, loss = 0.26192185282707214
In grad_steps = 2258, loss = 0.26995301246643066
In grad_steps = 2259, loss = 0.13494499027729034
In grad_steps = 2260, loss = 0.5099056959152222
In grad_steps = 2261, loss = 0.19308163225650787
In grad_steps = 2262, loss = 0.1322309672832489
In grad_steps = 2263, loss = 0.3988340198993683
In grad_steps = 2264, loss = 0.3178637623786926
In grad_steps = 2265, loss = 0.32772430777549744
In grad_steps = 2266, loss = 0.34070056676864624
In grad_steps = 2267, loss = 0.11710815131664276
In grad_steps = 2268, loss = 0.20755156874656677
In grad_steps = 2269, loss = 0.7719959020614624
In grad_steps = 2270, loss = 0.7179890275001526
In grad_steps = 2271, loss = 0.5817902088165283
In grad_steps = 2272, loss = 0.3435876965522766
In grad_steps = 2273, loss = 0.1559300273656845
In grad_steps = 2274, loss = 0.6149101257324219
In grad_steps = 2275, loss = 0.41407305002212524
In grad_steps = 2276, loss = 0.31170305609703064
In grad_steps = 2277, loss = 0.12565849721431732
In grad_steps = 2278, loss = 0.2491593360900879
In grad_steps = 2279, loss = 0.24873001873493195
In grad_steps = 2280, loss = 0.12898173928260803
In grad_steps = 2281, loss = 0.2504900097846985
In grad_steps = 2282, loss = 0.37724003195762634
In grad_steps = 2283, loss = 0.15114735066890717
In grad_steps = 2284, loss = 0.504548966884613
In grad_steps = 2285, loss = 1.1686055660247803
In grad_steps = 2286, loss = 0.06498327851295471
In grad_steps = 2287, loss = 0.9146991968154907
In grad_steps = 2288, loss = 0.3819437026977539
In grad_steps = 2289, loss = 0.707382082939148
In grad_steps = 2290, loss = 0.13206341862678528
In grad_steps = 2291, loss = 0.09711083024740219
In grad_steps = 2292, loss = 0.2425290048122406
In grad_steps = 2293, loss = 0.08300948888063431
In grad_steps = 2294, loss = 0.26814624667167664
In grad_steps = 2295, loss = 0.16544032096862793
In grad_steps = 2296, loss = 0.143876850605011
In grad_steps = 2297, loss = 0.17088493704795837
In grad_steps = 2298, loss = 0.12107241153717041
In grad_steps = 2299, loss = 0.07082603871822357
In grad_steps = 2300, loss = 0.11321911215782166
In grad_steps = 2301, loss = 1.1188760995864868
In grad_steps = 2302, loss = 0.15065789222717285
In grad_steps = 2303, loss = 0.21699020266532898
In grad_steps = 2304, loss = 0.09813226759433746
In grad_steps = 2305, loss = 0.04186609387397766
In grad_steps = 2306, loss = 0.0721772164106369
In grad_steps = 2307, loss = 0.05205154046416283
In grad_steps = 2308, loss = 0.49787572026252747
In grad_steps = 2309, loss = 0.8355166912078857
In grad_steps = 2310, loss = 0.1410141885280609
In grad_steps = 2311, loss = 0.6064422130584717
In grad_steps = 2312, loss = 0.15848025679588318
In grad_steps = 2313, loss = 0.4640163481235504
In grad_steps = 2314, loss = 1.1301233768463135
In grad_steps = 2315, loss = 0.7233517169952393
In grad_steps = 2316, loss = 0.36595842242240906
In grad_steps = 2317, loss = 0.13083745539188385
In grad_steps = 2318, loss = 0.2984682321548462
In grad_steps = 2319, loss = 0.24890299141407013
In grad_steps = 2320, loss = 1.0530178546905518
In grad_steps = 2321, loss = 0.1838037222623825
In grad_steps = 2322, loss = 0.25867220759391785
In grad_steps = 2323, loss = 0.18780499696731567
In grad_steps = 2324, loss = 0.7415565252304077
In grad_steps = 2325, loss = 0.20692414045333862
In grad_steps = 2326, loss = 0.13321536779403687
In grad_steps = 2327, loss = 0.5038068294525146
In grad_steps = 2328, loss = 0.3852737843990326
In grad_steps = 2329, loss = 0.19520357251167297
In grad_steps = 2330, loss = 0.09643568098545074
In grad_steps = 2331, loss = 0.5480445623397827
In grad_steps = 2332, loss = 0.12679778039455414
In grad_steps = 2333, loss = 0.4921490550041199
In grad_steps = 2334, loss = 0.09319287538528442
In grad_steps = 2335, loss = 0.538222074508667
In grad_steps = 2336, loss = 0.6575713753700256
In grad_steps = 2337, loss = 0.18275335431098938
In grad_steps = 2338, loss = 0.14315958321094513
In grad_steps = 2339, loss = 0.06808964163064957
In grad_steps = 2340, loss = 0.28736114501953125
In grad_steps = 2341, loss = 0.489533394575119
In grad_steps = 2342, loss = 0.4313678741455078
In grad_steps = 2343, loss = 0.21517986059188843
In grad_steps = 2344, loss = 0.22966016829013824
In grad_steps = 2345, loss = 0.4433313012123108
In grad_steps = 2346, loss = 0.0952630490064621
In grad_steps = 2347, loss = 0.26586639881134033
In grad_steps = 2348, loss = 0.46052464842796326
In grad_steps = 2349, loss = 0.7257136106491089
In grad_steps = 2350, loss = 0.5837908983230591
In grad_steps = 2351, loss = 0.030659710988402367
In grad_steps = 2352, loss = 0.0955272912979126
In grad_steps = 2353, loss = 0.6907048225402832
In grad_steps = 2354, loss = 0.1994996815919876
In grad_steps = 2355, loss = 0.23571959137916565
In grad_steps = 2356, loss = 0.12642836570739746
In grad_steps = 2357, loss = 0.028563423082232475
In grad_steps = 2358, loss = 0.038370441645383835
In grad_steps = 2359, loss = 0.14598573744297028
In grad_steps = 2360, loss = 0.9672752618789673
In grad_steps = 2361, loss = 1.138527750968933
In grad_steps = 2362, loss = 0.06496009230613708
In grad_steps = 2363, loss = 0.32003921270370483
In grad_steps = 2364, loss = 0.2498118281364441
In grad_steps = 2365, loss = 0.038888320326805115
In grad_steps = 2366, loss = 0.06113487109541893
In grad_steps = 2367, loss = 0.11139465868473053
In grad_steps = 2368, loss = 0.01983138732612133
In grad_steps = 2369, loss = 1.353281021118164
In grad_steps = 2370, loss = 0.516258955001831
In grad_steps = 2371, loss = 0.2706717252731323
In grad_steps = 2372, loss = 0.03284480795264244
In grad_steps = 2373, loss = 0.17568564414978027
In grad_steps = 2374, loss = 0.14922811090946198
In grad_steps = 2375, loss = 0.6503854393959045
In grad_steps = 2376, loss = 0.19628256559371948
In grad_steps = 2377, loss = 0.05331381410360336
In grad_steps = 2378, loss = 0.5893529653549194
In grad_steps = 2379, loss = 0.19608916342258453
In grad_steps = 2380, loss = 0.16931700706481934
In grad_steps = 2381, loss = 0.02682901918888092
In grad_steps = 2382, loss = 0.649677038192749
In grad_steps = 2383, loss = 0.2867051959037781
In grad_steps = 2384, loss = 0.04729944467544556
In grad_steps = 2385, loss = 0.03520432114601135
In grad_steps = 2386, loss = 0.3315046429634094
In grad_steps = 2387, loss = 0.5459636449813843
In grad_steps = 2388, loss = 0.03734719008207321
In grad_steps = 2389, loss = 0.47748658061027527
In grad_steps = 2390, loss = 0.04059603810310364
In grad_steps = 2391, loss = 0.04732682928442955
In grad_steps = 2392, loss = 0.6647136807441711
In grad_steps = 2393, loss = 0.03594577684998512
In grad_steps = 2394, loss = 0.6468108296394348
In grad_steps = 2395, loss = 0.1797415018081665
In grad_steps = 2396, loss = 0.5892972350120544
In grad_steps = 2397, loss = 0.15940317511558533
In grad_steps = 2398, loss = 0.7570990324020386
In grad_steps = 2399, loss = 0.1988348364830017
In grad_steps = 2400, loss = 0.3146836459636688
In grad_steps = 2401, loss = 0.29001688957214355
In grad_steps = 2402, loss = 0.20318524539470673
In grad_steps = 2403, loss = 0.11749179661273956
In grad_steps = 2404, loss = 0.10413403809070587
In grad_steps = 2405, loss = 0.2664642333984375
In grad_steps = 2406, loss = 0.819757342338562
In grad_steps = 2407, loss = 0.18753308057785034
In grad_steps = 2408, loss = 0.6026955246925354
In grad_steps = 2409, loss = 0.7254329919815063
In grad_steps = 2410, loss = 0.2735207676887512
In grad_steps = 2411, loss = 0.2902553975582123
In grad_steps = 2412, loss = 0.6731716394424438
In grad_steps = 2413, loss = 0.8384609818458557
In grad_steps = 2414, loss = 0.12868215143680573
In grad_steps = 2415, loss = 0.45728087425231934
In grad_steps = 2416, loss = 0.8614059090614319
In grad_steps = 2417, loss = 0.10268855839967728
In grad_steps = 2418, loss = 0.7586493492126465
In grad_steps = 2419, loss = 0.3592725992202759
In grad_steps = 2420, loss = 0.2213984876871109
In grad_steps = 2421, loss = 0.4235284924507141
In grad_steps = 2422, loss = 0.25245463848114014
In grad_steps = 2423, loss = 0.6726090908050537
In grad_steps = 2424, loss = 0.12250122427940369
In grad_steps = 2425, loss = 0.16691964864730835
In grad_steps = 2426, loss = 0.41033005714416504
In grad_steps = 2427, loss = 0.500339150428772
In grad_steps = 2428, loss = 0.26757216453552246
In grad_steps = 2429, loss = 0.2777632474899292
In grad_steps = 2430, loss = 0.23144522309303284
In grad_steps = 2431, loss = 0.2836408019065857
In grad_steps = 2432, loss = 0.24035842716693878
In grad_steps = 2433, loss = 0.47843027114868164
In grad_steps = 2434, loss = 0.36298424005508423
In grad_steps = 2435, loss = 0.1851034164428711
In grad_steps = 2436, loss = 0.06843224167823792
In grad_steps = 2437, loss = 0.08945895731449127
In grad_steps = 2438, loss = 0.061105404049158096
In grad_steps = 2439, loss = 0.039068374782800674
In grad_steps = 2440, loss = 0.7527620792388916
In grad_steps = 2441, loss = 0.20726266503334045
In grad_steps = 2442, loss = 0.3475230634212494
In grad_steps = 2443, loss = 0.349835067987442
In grad_steps = 2444, loss = 0.6223943829536438
In grad_steps = 2445, loss = 0.31241127848625183
In grad_steps = 2446, loss = 0.07785233855247498
In grad_steps = 2447, loss = 0.15689435601234436
In grad_steps = 2448, loss = 1.3836780786514282
In grad_steps = 2449, loss = 0.4501044750213623
In grad_steps = 2450, loss = 0.007039024494588375
In grad_steps = 2451, loss = 1.5277040004730225
In grad_steps = 2452, loss = 0.14387774467468262
In grad_steps = 2453, loss = 0.7107214331626892
In grad_steps = 2454, loss = 0.06874032318592072
In grad_steps = 2455, loss = 0.2365821748971939
In grad_steps = 2456, loss = 0.0733882486820221
In grad_steps = 2457, loss = 0.05387326702475548
In grad_steps = 2458, loss = 0.08425627648830414
In grad_steps = 2459, loss = 0.288324773311615
In grad_steps = 2460, loss = 0.0455511212348938
In grad_steps = 2461, loss = 0.055578649044036865
In grad_steps = 2462, loss = 0.3816356360912323
In grad_steps = 2463, loss = 0.11568023264408112
In grad_steps = 2464, loss = 0.1747693419456482
In grad_steps = 2465, loss = 0.127298966050148
In grad_steps = 2466, loss = 0.3063741624355316
In grad_steps = 2467, loss = 0.12703411281108856
In grad_steps = 2468, loss = 0.7698637247085571
In grad_steps = 2469, loss = 1.2584879398345947
In grad_steps = 2470, loss = 0.02766641229391098
In grad_steps = 2471, loss = 0.11779774725437164
In grad_steps = 2472, loss = 0.09312885999679565
In grad_steps = 2473, loss = 0.8016932606697083
In grad_steps = 2474, loss = 0.09684762358665466
In grad_steps = 2475, loss = 0.09127908945083618
In grad_steps = 2476, loss = 0.1861869990825653
In grad_steps = 2477, loss = 0.08584170043468475
In grad_steps = 2478, loss = 0.050083644688129425
In grad_steps = 2479, loss = 0.14939367771148682
In grad_steps = 2480, loss = 0.08005057275295258
In grad_steps = 2481, loss = 0.09385403990745544
In grad_steps = 2482, loss = 0.6166650056838989
In grad_steps = 2483, loss = 0.05241283029317856
In grad_steps = 2484, loss = 0.3366774916648865
In grad_steps = 2485, loss = 0.1432172805070877
In grad_steps = 2486, loss = 0.2718750238418579
In grad_steps = 2487, loss = 0.06361155211925507
In grad_steps = 2488, loss = 0.054228559136390686
In grad_steps = 2489, loss = 0.03980875760316849
In grad_steps = 2490, loss = 0.10037706792354584
In grad_steps = 2491, loss = 0.03909517824649811
In grad_steps = 2492, loss = 0.21952471137046814
In grad_steps = 2493, loss = 0.025517089292407036
In grad_steps = 2494, loss = 0.020861804485321045
In grad_steps = 2495, loss = 0.48876476287841797
In grad_steps = 2496, loss = 0.03872855007648468
In grad_steps = 2497, loss = 0.46690356731414795
In grad_steps = 2498, loss = 0.07602658867835999
In grad_steps = 2499, loss = 0.12591004371643066
In grad_steps = 2500, loss = 0.04801018908619881
In grad_steps = 2501, loss = 0.02806275337934494
In grad_steps = 2502, loss = 0.02113126404583454
In grad_steps = 2503, loss = 0.0166455265134573
In grad_steps = 2504, loss = 0.015775304287672043
In grad_steps = 2505, loss = 0.4174392819404602
In grad_steps = 2506, loss = 0.014547568745911121
In grad_steps = 2507, loss = 0.16098928451538086
In grad_steps = 2508, loss = 0.022290561348199844
In grad_steps = 2509, loss = 1.144107460975647
In grad_steps = 2510, loss = 0.011097686365246773
In grad_steps = 2511, loss = 0.010735573247075081
In grad_steps = 2512, loss = 0.020821403712034225
In grad_steps = 2513, loss = 0.014003198593854904
In grad_steps = 2514, loss = 0.12017732858657837
In grad_steps = 2515, loss = 0.8396145105361938
In grad_steps = 2516, loss = 0.09767252206802368
In grad_steps = 2517, loss = 0.38067466020584106
In grad_steps = 2518, loss = 0.014676686376333237
In grad_steps = 2519, loss = 0.6435002088546753
In grad_steps = 2520, loss = 0.21315599977970123
In grad_steps = 2521, loss = 0.04735281318426132
In grad_steps = 2522, loss = 0.641173243522644
In grad_steps = 2523, loss = 0.04169861972332001
In grad_steps = 2524, loss = 0.36725181341171265
In grad_steps = 2525, loss = 0.6300227046012878
In grad_steps = 2526, loss = 0.5040419101715088
In grad_steps = 2527, loss = 1.3865735530853271
In grad_steps = 2528, loss = 0.237884983420372
In grad_steps = 2529, loss = 0.3454008102416992
In grad_steps = 2530, loss = 0.3896026015281677
In grad_steps = 2531, loss = 0.4331081509590149
In grad_steps = 2532, loss = 0.22513920068740845
In grad_steps = 2533, loss = 0.13789239525794983
In grad_steps = 2534, loss = 0.11532777547836304
In grad_steps = 2535, loss = 0.086184062063694
In grad_steps = 2536, loss = 0.4405025839805603
In grad_steps = 2537, loss = 0.07997169345617294
In grad_steps = 2538, loss = 0.14864961802959442
In grad_steps = 2539, loss = 0.19262082874774933
In grad_steps = 2540, loss = 0.8023492693901062
In grad_steps = 2541, loss = 0.07233217358589172
In grad_steps = 2542, loss = 0.13336428999900818
In grad_steps = 2543, loss = 0.25262558460235596
In grad_steps = 2544, loss = 0.3880518972873688
In grad_steps = 2545, loss = 0.20700998604297638
In grad_steps = 2546, loss = 0.1571400910615921
In grad_steps = 2547, loss = 0.14575843513011932
In grad_steps = 2548, loss = 0.23526310920715332
In grad_steps = 2549, loss = 0.2970771789550781
In grad_steps = 2550, loss = 0.6856787204742432
In grad_steps = 2551, loss = 0.3846707344055176
In grad_steps = 2552, loss = 0.01595468632876873
In grad_steps = 2553, loss = 0.030495110899209976
In grad_steps = 2554, loss = 0.01892233081161976
In grad_steps = 2555, loss = 1.4357746839523315
In grad_steps = 2556, loss = 0.0702945739030838
In grad_steps = 2557, loss = 0.590015172958374
In grad_steps = 2558, loss = 0.01862681657075882
In grad_steps = 2559, loss = 0.43875735998153687
In grad_steps = 2560, loss = 0.0831114873290062
In grad_steps = 2561, loss = 0.17776335775852203
In grad_steps = 2562, loss = 0.5158747434616089
In grad_steps = 2563, loss = 0.05937657877802849
In grad_steps = 2564, loss = 0.07073534280061722
In grad_steps = 2565, loss = 0.05602763593196869
In grad_steps = 2566, loss = 0.053649820387363434
In grad_steps = 2567, loss = 0.11144159734249115
In grad_steps = 2568, loss = 0.026264775544404984
In grad_steps = 2569, loss = 0.060598134994506836
In grad_steps = 2570, loss = 0.1345261186361313
In grad_steps = 2571, loss = 0.14315271377563477
In grad_steps = 2572, loss = 0.05021186172962189
In grad_steps = 2573, loss = 0.4988265037536621
In grad_steps = 2574, loss = 0.3089720904827118
In grad_steps = 2575, loss = 0.8196790218353271
In grad_steps = 2576, loss = 0.7030011415481567
In grad_steps = 2577, loss = 0.3720340132713318
In grad_steps = 2578, loss = 0.27880313992500305
In grad_steps = 2579, loss = 0.14850829541683197
In grad_steps = 2580, loss = 0.20597130060195923
In grad_steps = 2581, loss = 0.6404823064804077
In grad_steps = 2582, loss = 0.08699777722358704
In grad_steps = 2583, loss = 0.09070601314306259
In grad_steps = 2584, loss = 0.03412124887108803
In grad_steps = 2585, loss = 1.7873919010162354
In grad_steps = 2586, loss = 1.064956545829773
In grad_steps = 2587, loss = 0.0991954579949379
In grad_steps = 2588, loss = 1.4809269905090332
In grad_steps = 2589, loss = 0.44012945890426636
In grad_steps = 2590, loss = 0.6758185625076294
In grad_steps = 2591, loss = 0.7796208262443542
In grad_steps = 2592, loss = 0.4768647253513336
In grad_steps = 2593, loss = 0.18892961740493774
In grad_steps = 2594, loss = 0.3449351489543915
In grad_steps = 2595, loss = 0.27520594000816345
In grad_steps = 2596, loss = 0.20092478394508362
In grad_steps = 2597, loss = 0.49887335300445557
In grad_steps = 2598, loss = 0.5229812264442444
In grad_steps = 2599, loss = 0.3545885980129242
In grad_steps = 2600, loss = 0.3866710960865021
In grad_steps = 2601, loss = 0.3381243348121643
In grad_steps = 2602, loss = 0.5641111135482788
In grad_steps = 2603, loss = 0.2706744074821472
In grad_steps = 2604, loss = 0.3877365291118622
In grad_steps = 2605, loss = 0.2545372545719147
In grad_steps = 2606, loss = 0.3509429395198822
In grad_steps = 2607, loss = 0.07286893576383591
In grad_steps = 2608, loss = 0.13219504058361053
In grad_steps = 2609, loss = 0.33397072553634644
In grad_steps = 2610, loss = 0.5741768479347229
In grad_steps = 2611, loss = 0.02492540143430233
In grad_steps = 2612, loss = 0.20442534983158112
In grad_steps = 2613, loss = 0.10895577073097229
In grad_steps = 2614, loss = 0.13911208510398865
In grad_steps = 2615, loss = 0.23737545311450958
In grad_steps = 2616, loss = 0.034968070685863495
In grad_steps = 2617, loss = 0.12109117209911346
In grad_steps = 2618, loss = 0.2001255750656128
In grad_steps = 2619, loss = 0.20237499475479126
In grad_steps = 2620, loss = 1.6737618446350098
In grad_steps = 2621, loss = 0.24358569085597992
In grad_steps = 2622, loss = 0.18992455303668976
In grad_steps = 2623, loss = 0.10824640095233917
In grad_steps = 2624, loss = 0.7296155095100403
In grad_steps = 2625, loss = 0.04810174182057381
In grad_steps = 2626, loss = 0.04945933073759079
In grad_steps = 2627, loss = 0.36257100105285645
In grad_steps = 2628, loss = 0.05713047832250595
In grad_steps = 2629, loss = 0.9728727340698242
In grad_steps = 2630, loss = 0.3763461410999298
In grad_steps = 2631, loss = 1.1092791557312012
In grad_steps = 2632, loss = 0.08347433060407639
In grad_steps = 2633, loss = 0.29810479283332825
In grad_steps = 2634, loss = 0.13351228833198547
In grad_steps = 2635, loss = 0.5414177179336548
In grad_steps = 2636, loss = 0.162555530667305
In grad_steps = 2637, loss = 0.18190892040729523
In grad_steps = 2638, loss = 0.4208204448223114
In grad_steps = 2639, loss = 0.1866917908191681
In grad_steps = 2640, loss = 0.24301937222480774
In grad_steps = 2641, loss = 0.09603633731603622
In grad_steps = 2642, loss = 0.23805086314678192
In grad_steps = 2643, loss = 0.4814000427722931
In grad_steps = 2644, loss = 0.1193477064371109
In grad_steps = 2645, loss = 0.162187859416008
In grad_steps = 2646, loss = 0.35628628730773926
In grad_steps = 2647, loss = 0.5256589651107788
In grad_steps = 2648, loss = 0.4470967948436737
In grad_steps = 2649, loss = 0.3204088807106018
In grad_steps = 2650, loss = 0.347417950630188
In grad_steps = 2651, loss = 0.11642129719257355
In grad_steps = 2652, loss = 0.5787652134895325
In grad_steps = 2653, loss = 0.3156569302082062
In grad_steps = 2654, loss = 0.18590931594371796
In grad_steps = 2655, loss = 0.4512760043144226
In grad_steps = 2656, loss = 0.06400761008262634
In grad_steps = 2657, loss = 0.6092497110366821
In grad_steps = 2658, loss = 1.282073974609375
In grad_steps = 2659, loss = 0.18022745847702026
In grad_steps = 2660, loss = 0.14918597042560577
In grad_steps = 2661, loss = 0.7557746767997742
In grad_steps = 2662, loss = 0.064004085958004
In grad_steps = 2663, loss = 0.22992266714572906
In grad_steps = 2664, loss = 0.2201795130968094
In grad_steps = 2665, loss = 0.6928602457046509
In grad_steps = 2666, loss = 0.11577402800321579
In grad_steps = 2667, loss = 0.06873700767755508
In grad_steps = 2668, loss = 0.25766196846961975
In grad_steps = 2669, loss = 0.09762246906757355
In grad_steps = 2670, loss = 0.07708846777677536
In grad_steps = 2671, loss = 0.20312902331352234
In grad_steps = 2672, loss = 0.13619014620780945
In grad_steps = 2673, loss = 0.07327764481306076
In grad_steps = 2674, loss = 0.04654432833194733
In grad_steps = 2675, loss = 0.039851114153862
In grad_steps = 2676, loss = 0.18266095221042633
In grad_steps = 2677, loss = 0.03502286598086357
In grad_steps = 2678, loss = 0.018815988674759865
In grad_steps = 2679, loss = 0.3910229504108429
In grad_steps = 2680, loss = 0.12062308192253113
In grad_steps = 2681, loss = 0.09694014489650726
In grad_steps = 2682, loss = 0.7317436933517456
In grad_steps = 2683, loss = 0.020556120201945305
In grad_steps = 2684, loss = 0.1208118349313736
In grad_steps = 2685, loss = 0.22846734523773193
In grad_steps = 2686, loss = 0.22933071851730347
In grad_steps = 2687, loss = 0.1615142971277237
In grad_steps = 2688, loss = 0.13700231909751892
In grad_steps = 2689, loss = 0.1664448380470276
In grad_steps = 2690, loss = 1.4180821180343628
In grad_steps = 2691, loss = 0.1868562251329422
In grad_steps = 2692, loss = 0.08928379416465759
In grad_steps = 2693, loss = 1.3263307809829712
In grad_steps = 2694, loss = 0.539299726486206
In grad_steps = 2695, loss = 0.4572709798812866
In grad_steps = 2696, loss = 0.027230974286794662
In grad_steps = 2697, loss = 0.22037193179130554
In grad_steps = 2698, loss = 0.4681706130504608
In grad_steps = 2699, loss = 0.11948303133249283
In grad_steps = 2700, loss = 1.0280576944351196
In grad_steps = 2701, loss = 0.029973212629556656
In grad_steps = 2702, loss = 0.21851061284542084
In grad_steps = 2703, loss = 0.613874077796936
In grad_steps = 2704, loss = 0.2022654116153717
In grad_steps = 2705, loss = 0.24789626896381378
In grad_steps = 2706, loss = 0.23568321764469147
In grad_steps = 2707, loss = 0.7339064478874207
In grad_steps = 2708, loss = 0.6147439479827881
In grad_steps = 2709, loss = 0.4328169822692871
In grad_steps = 2710, loss = 0.4231918752193451
In grad_steps = 2711, loss = 0.15479113161563873
In grad_steps = 2712, loss = 0.3875807821750641
In grad_steps = 2713, loss = 0.5271162390708923
In grad_steps = 2714, loss = 0.6396158933639526
In grad_steps = 2715, loss = 0.5292555093765259
In grad_steps = 2716, loss = 0.11731887608766556
In grad_steps = 2717, loss = 0.8055679798126221
In grad_steps = 2718, loss = 0.4599531590938568
In grad_steps = 2719, loss = 0.19363617897033691
In grad_steps = 2720, loss = 0.4097815752029419
In grad_steps = 2721, loss = 0.7349023818969727
In grad_steps = 2722, loss = 0.24407893419265747
In grad_steps = 2723, loss = 0.19409441947937012
In grad_steps = 2724, loss = 0.3962472379207611
In grad_steps = 2725, loss = 0.1332111656665802
In grad_steps = 2726, loss = 0.8614611029624939
In grad_steps = 2727, loss = 0.037939541041851044
In grad_steps = 2728, loss = 0.8966262340545654
In grad_steps = 2729, loss = 0.6737330555915833
In grad_steps = 2730, loss = 0.06781511008739471
In grad_steps = 2731, loss = 0.07947678864002228
In grad_steps = 2732, loss = 0.14528048038482666
In grad_steps = 2733, loss = 0.24979953467845917
In grad_steps = 2734, loss = 0.14052195847034454
In grad_steps = 2735, loss = 0.9084463119506836
In grad_steps = 2736, loss = 0.08182277530431747
In grad_steps = 2737, loss = 0.23333217203617096
In grad_steps = 2738, loss = 0.24121537804603577
In grad_steps = 2739, loss = 0.16059435904026031
In grad_steps = 2740, loss = 0.09269639849662781
In grad_steps = 2741, loss = 0.08449360728263855
In grad_steps = 2742, loss = 0.3041556477546692
In grad_steps = 2743, loss = 0.11778655648231506
In grad_steps = 2744, loss = 0.18048711121082306
In grad_steps = 2745, loss = 0.02298203855752945
In grad_steps = 2746, loss = 0.989719033241272
In grad_steps = 2747, loss = 0.2932623624801636
In grad_steps = 2748, loss = 0.058560922741889954
In grad_steps = 2749, loss = 0.06659302860498428
In grad_steps = 2750, loss = 0.07874937355518341
In grad_steps = 2751, loss = 0.08909530937671661
In grad_steps = 2752, loss = 0.11041092872619629
In grad_steps = 2753, loss = 0.29280897974967957
In grad_steps = 2754, loss = 0.28517937660217285
In grad_steps = 2755, loss = 0.02151445299386978
In grad_steps = 2756, loss = 0.043944474309682846
In grad_steps = 2757, loss = 0.020370639860630035
In grad_steps = 2758, loss = 1.144910454750061
In grad_steps = 2759, loss = 0.19067659974098206
In grad_steps = 2760, loss = 0.029930830001831055
In grad_steps = 2761, loss = 0.09484875947237015
In grad_steps = 2762, loss = 0.14903615415096283
In grad_steps = 2763, loss = 0.07502523809671402
In grad_steps = 2764, loss = 0.01771276816725731
In grad_steps = 2765, loss = 0.05551345273852348
In grad_steps = 2766, loss = 1.9914336204528809
In grad_steps = 2767, loss = 0.018056806176900864
In grad_steps = 2768, loss = 0.8198192715644836
In grad_steps = 2769, loss = 0.04426721855998039
In grad_steps = 2770, loss = 0.3795055150985718
In grad_steps = 2771, loss = 0.026124514639377594
In grad_steps = 2772, loss = 0.2859526574611664
In grad_steps = 2773, loss = 1.400284767150879
In grad_steps = 2774, loss = 0.606997013092041
In grad_steps = 2775, loss = 0.06399514526128769
In grad_steps = 2776, loss = 0.12601426243782043
In grad_steps = 2777, loss = 0.03506966680288315
In grad_steps = 2778, loss = 0.1382974237203598
In grad_steps = 2779, loss = 0.11248009651899338
In grad_steps = 2780, loss = 0.2166542410850525
In grad_steps = 2781, loss = 0.1165027767419815
In grad_steps = 2782, loss = 0.09961432963609695
In grad_steps = 2783, loss = 0.05340166017413139
In grad_steps = 2784, loss = 0.08394399285316467
In grad_steps = 2785, loss = 0.10679652541875839
In grad_steps = 2786, loss = 0.13400012254714966
In grad_steps = 2787, loss = 0.22709447145462036
In grad_steps = 2788, loss = 0.5470192432403564
In grad_steps = 2789, loss = 1.0769455432891846
In grad_steps = 2790, loss = 0.2800907492637634
In grad_steps = 2791, loss = 0.3102266788482666
In grad_steps = 2792, loss = 0.09106473624706268
In grad_steps = 2793, loss = 0.1704779714345932
In grad_steps = 2794, loss = 0.21571530401706696
In grad_steps = 2795, loss = 0.24765005707740784
In grad_steps = 2796, loss = 0.12105275690555573
In grad_steps = 2797, loss = 0.15710964798927307
In grad_steps = 2798, loss = 0.016927458345890045
In grad_steps = 2799, loss = 0.11347256600856781
In grad_steps = 2800, loss = 0.07999079674482346
In grad_steps = 2801, loss = 0.03950497508049011
In grad_steps = 2802, loss = 0.19789044559001923
In grad_steps = 2803, loss = 1.1640996932983398
In grad_steps = 2804, loss = 0.26736053824424744
In grad_steps = 2805, loss = 0.3783366084098816
In grad_steps = 2806, loss = 0.09057680517435074
In grad_steps = 2807, loss = 0.7282626032829285
In grad_steps = 2808, loss = 0.02361680567264557
In grad_steps = 2809, loss = 0.0433267317712307
In grad_steps = 2810, loss = 0.08870457857847214
In grad_steps = 2811, loss = 0.04615674912929535
In grad_steps = 2812, loss = 0.03926229104399681
In grad_steps = 2813, loss = 0.02021685615181923
In grad_steps = 2814, loss = 0.028587812557816505
In grad_steps = 2815, loss = 0.06012512370944023
In grad_steps = 2816, loss = 0.16183865070343018
In grad_steps = 2817, loss = 0.04340776801109314
In grad_steps = 2818, loss = 0.12090840935707092
In grad_steps = 2819, loss = 0.5320947766304016
In grad_steps = 2820, loss = 0.02433975785970688
In grad_steps = 2821, loss = 0.1698375940322876
In grad_steps = 2822, loss = 0.21804305911064148
In grad_steps = 2823, loss = 0.08424650877714157
In grad_steps = 2824, loss = 0.3631894290447235
In grad_steps = 2825, loss = 0.13134057819843292
In grad_steps = 2826, loss = 0.041321367025375366
In grad_steps = 2827, loss = 0.03926782310009003
In grad_steps = 2828, loss = 0.017118332907557487
In grad_steps = 2829, loss = 0.014275806955993176
In grad_steps = 2830, loss = 0.5232488512992859
In grad_steps = 2831, loss = 0.840366005897522
In grad_steps = 2832, loss = 0.19995880126953125
In grad_steps = 2833, loss = 0.11320580542087555
In grad_steps = 2834, loss = 1.0904582738876343
In grad_steps = 2835, loss = 0.4979000687599182
In grad_steps = 2836, loss = 0.39271992444992065
In grad_steps = 2837, loss = 0.26944732666015625
In grad_steps = 2838, loss = 0.7812389731407166
In grad_steps = 2839, loss = 0.40091004967689514
In grad_steps = 2840, loss = 0.08622550219297409
In grad_steps = 2841, loss = 0.0348307304084301
In grad_steps = 2842, loss = 0.27521470189094543
In grad_steps = 2843, loss = 0.3985665738582611
In grad_steps = 2844, loss = 0.11869064718484879
In grad_steps = 2845, loss = 0.14239846169948578
In grad_steps = 2846, loss = 0.09025627374649048
In grad_steps = 2847, loss = 0.10745634138584137
In grad_steps = 2848, loss = 0.025958610698580742
In grad_steps = 2849, loss = 0.7720082998275757
In grad_steps = 2850, loss = 0.056591033935546875
In grad_steps = 2851, loss = 0.10015031695365906
In grad_steps = 2852, loss = 0.08704293519258499
In grad_steps = 2853, loss = 0.07585514336824417
In grad_steps = 2854, loss = 0.035381145775318146
In grad_steps = 2855, loss = 0.04447874799370766
In grad_steps = 2856, loss = 0.10256420075893402
In grad_steps = 2857, loss = 0.026998650282621384
In grad_steps = 2858, loss = 0.12678518891334534
In grad_steps = 2859, loss = 1.101650595664978
In grad_steps = 2860, loss = 0.05813691020011902
In grad_steps = 2861, loss = 0.1635710895061493
In grad_steps = 2862, loss = 0.29701972007751465
In grad_steps = 2863, loss = 0.06085013225674629
In grad_steps = 2864, loss = 0.027059605345129967
In grad_steps = 2865, loss = 0.20042100548744202
In grad_steps = 2866, loss = 0.033624179661273956
In grad_steps = 2867, loss = 0.13477367162704468
In grad_steps = 2868, loss = 0.021477743983268738
In grad_steps = 2869, loss = 0.016173284500837326
In grad_steps = 2870, loss = 0.11528884619474411
In grad_steps = 2871, loss = 0.019668161869049072
In grad_steps = 2872, loss = 0.33658552169799805
In grad_steps = 2873, loss = 0.02375456690788269
In grad_steps = 2874, loss = 0.7071253657341003
In grad_steps = 2875, loss = 0.05721616744995117
In grad_steps = 2876, loss = 0.11432216316461563
In grad_steps = 2877, loss = 0.020118670538067818
In grad_steps = 2878, loss = 0.11229319125413895
In grad_steps = 2879, loss = 0.030241377651691437
In grad_steps = 2880, loss = 0.05028708279132843
In grad_steps = 2881, loss = 0.3866005837917328
In grad_steps = 2882, loss = 0.04886414855718613
In grad_steps = 2883, loss = 0.6760414242744446
In grad_steps = 2884, loss = 0.011413680389523506
In grad_steps = 2885, loss = 0.10087025165557861
In grad_steps = 2886, loss = 0.44831717014312744
In grad_steps = 2887, loss = 0.2352498471736908
In grad_steps = 2888, loss = 0.544364333152771
In grad_steps = 2889, loss = 1.1949238777160645
In grad_steps = 2890, loss = 0.6733589172363281
In grad_steps = 2891, loss = 0.9150131940841675
In grad_steps = 2892, loss = 0.07085918635129929
In grad_steps = 2893, loss = 0.02248585969209671
In grad_steps = 2894, loss = 0.043231092393398285
In grad_steps = 2895, loss = 0.10972648113965988
In grad_steps = 2896, loss = 0.8390253782272339
In grad_steps = 2897, loss = 0.5606249570846558
In grad_steps = 2898, loss = 0.3527950942516327
In grad_steps = 2899, loss = 0.4993869662284851
In grad_steps = 2900, loss = 0.589569628238678
In grad_steps = 2901, loss = 0.15554852783679962
In grad_steps = 2902, loss = 0.2068786472082138
In grad_steps = 2903, loss = 0.11011096835136414
In grad_steps = 2904, loss = 0.11922720074653625
In grad_steps = 2905, loss = 0.22682809829711914
In grad_steps = 2906, loss = 0.5494028329849243
In grad_steps = 2907, loss = 0.160736083984375
In grad_steps = 2908, loss = 0.3298035264015198
In grad_steps = 2909, loss = 0.18830427527427673
In grad_steps = 2910, loss = 0.213734433054924
In grad_steps = 2911, loss = 0.2394903600215912
In grad_steps = 2912, loss = 0.26636919379234314
In grad_steps = 2913, loss = 0.10027500987052917
In grad_steps = 2914, loss = 0.9411296248435974
In grad_steps = 2915, loss = 0.025152239948511124
In grad_steps = 2916, loss = 0.4916614890098572
In grad_steps = 2917, loss = 0.04379527270793915
In grad_steps = 2918, loss = 0.016947900876402855
In grad_steps = 2919, loss = 0.09938452392816544
In grad_steps = 2920, loss = 0.18831785023212433
In grad_steps = 2921, loss = 0.42467403411865234
In grad_steps = 2922, loss = 0.009793348610401154
In grad_steps = 2923, loss = 0.8233232498168945
In grad_steps = 2924, loss = 0.007141792215406895
In grad_steps = 2925, loss = 0.17333921790122986
In grad_steps = 2926, loss = 0.19597311317920685
In grad_steps = 2927, loss = 0.7966700196266174
In grad_steps = 2928, loss = 0.027208982035517693
In grad_steps = 2929, loss = 0.044037457555532455
In grad_steps = 2930, loss = 0.03628050908446312
In grad_steps = 2931, loss = 0.6644811630249023
In grad_steps = 2932, loss = 0.5087684392929077
In grad_steps = 2933, loss = 0.024829411879181862
In grad_steps = 2934, loss = 0.6200980544090271
In grad_steps = 2935, loss = 0.09185390174388885
In grad_steps = 2936, loss = 0.6395640969276428
In grad_steps = 2937, loss = 0.4195631742477417
In grad_steps = 2938, loss = 0.2945568263530731
In grad_steps = 2939, loss = 0.1772737205028534
In grad_steps = 2940, loss = 0.7063707709312439
In grad_steps = 2941, loss = 0.12104129046201706
In grad_steps = 2942, loss = 0.12186326086521149
In grad_steps = 2943, loss = 0.30773910880088806
In grad_steps = 2944, loss = 0.5726525783538818
In grad_steps = 2945, loss = 0.3182802200317383
In grad_steps = 2946, loss = 0.5617676973342896
In grad_steps = 2947, loss = 0.1209002360701561
In grad_steps = 2948, loss = 0.07176142185926437
In grad_steps = 2949, loss = 0.37435322999954224
In grad_steps = 2950, loss = 0.18279844522476196
In grad_steps = 2951, loss = 0.5498709678649902
In grad_steps = 2952, loss = 0.36315298080444336
In grad_steps = 2953, loss = 0.3156030476093292
In grad_steps = 2954, loss = 0.3942070007324219
In grad_steps = 2955, loss = 0.17814591526985168
In grad_steps = 2956, loss = 0.7465543150901794
In grad_steps = 2957, loss = 0.4083923399448395
In grad_steps = 2958, loss = 0.1120605543255806
In grad_steps = 2959, loss = 0.3558093309402466
In grad_steps = 2960, loss = 0.14834867417812347
In grad_steps = 2961, loss = 0.045328088104724884
In grad_steps = 2962, loss = 0.15868130326271057
In grad_steps = 2963, loss = 0.1652897596359253
In grad_steps = 2964, loss = 0.681411862373352
In grad_steps = 2965, loss = 0.021310728043317795
In grad_steps = 2966, loss = 0.0984618216753006
In grad_steps = 2967, loss = 0.3111686110496521
In grad_steps = 2968, loss = 1.150173306465149
In grad_steps = 2969, loss = 0.05706709250807762
In grad_steps = 2970, loss = 0.110855832695961
In grad_steps = 2971, loss = 0.037976812571287155
In grad_steps = 2972, loss = 0.037526607513427734
In grad_steps = 2973, loss = 0.06258358806371689
In grad_steps = 2974, loss = 1.32337486743927
In grad_steps = 2975, loss = 0.409572958946228
In grad_steps = 2976, loss = 0.0800752341747284
In grad_steps = 2977, loss = 0.10684557259082794
In grad_steps = 2978, loss = 0.058206215500831604
In grad_steps = 2979, loss = 0.346794992685318
In grad_steps = 2980, loss = 0.030244648456573486
In grad_steps = 2981, loss = 0.25882506370544434
In grad_steps = 2982, loss = 0.024314967915415764
In grad_steps = 2983, loss = 0.38875889778137207
In grad_steps = 2984, loss = 0.10397255420684814
In grad_steps = 2985, loss = 0.03338002786040306
In grad_steps = 2986, loss = 0.3674919009208679
In grad_steps = 2987, loss = 0.117777518928051
In grad_steps = 2988, loss = 1.0649607181549072
In grad_steps = 2989, loss = 0.2000560760498047
In grad_steps = 2990, loss = 0.06771724671125412
In grad_steps = 2991, loss = 0.18755951523780823
In grad_steps = 2992, loss = 0.14457674324512482
In grad_steps = 2993, loss = 0.06730640679597855
In grad_steps = 2994, loss = 0.08941954374313354
In grad_steps = 2995, loss = 0.024181023240089417
In grad_steps = 2996, loss = 0.7127474546432495
In grad_steps = 2997, loss = 0.05736355111002922
In grad_steps = 2998, loss = 0.3706807792186737
In grad_steps = 2999, loss = 0.23134440183639526
In grad_steps = 3000, loss = 0.3227846026420593
In grad_steps = 3001, loss = 0.12657202780246735
In grad_steps = 3002, loss = 0.7484414577484131
In grad_steps = 3003, loss = 0.19409902393817902
In grad_steps = 3004, loss = 1.172592282295227
In grad_steps = 3005, loss = 0.008607900701463223
In grad_steps = 3006, loss = 0.07754859328269958
In grad_steps = 3007, loss = 0.1312675178050995
In grad_steps = 3008, loss = 0.15256360173225403
In grad_steps = 3009, loss = 0.15663471817970276
In grad_steps = 3010, loss = 0.08178933709859848
In grad_steps = 3011, loss = 0.6965854167938232
In grad_steps = 3012, loss = 1.2375506162643433
In grad_steps = 3013, loss = 0.07203862071037292
In grad_steps = 3014, loss = 0.4490642845630646
In grad_steps = 3015, loss = 0.07899586856365204
In grad_steps = 3016, loss = 0.19455641508102417
In grad_steps = 3017, loss = 0.25074532628059387
In grad_steps = 3018, loss = 0.11090292036533356
In grad_steps = 3019, loss = 0.13353189826011658
In grad_steps = 3020, loss = 0.36036014556884766
In grad_steps = 3021, loss = 0.2209467738866806
In grad_steps = 3022, loss = 0.18070247769355774
In grad_steps = 3023, loss = 0.09133454412221909
In grad_steps = 3024, loss = 0.0495341531932354
In grad_steps = 3025, loss = 0.38739049434661865
In grad_steps = 3026, loss = 0.07510439306497574
In grad_steps = 3027, loss = 0.06492499262094498
In grad_steps = 3028, loss = 0.07211456447839737
In grad_steps = 3029, loss = 0.09259750694036484
In grad_steps = 3030, loss = 0.16552871465682983
In grad_steps = 3031, loss = 1.0369592905044556
In grad_steps = 3032, loss = 1.149076223373413
In grad_steps = 3033, loss = 0.9552105069160461
In grad_steps = 3034, loss = 0.8347898721694946
In grad_steps = 3035, loss = 0.9984612464904785
In grad_steps = 3036, loss = 0.5780849456787109
In grad_steps = 3037, loss = 0.33380022644996643
In grad_steps = 3038, loss = 0.5067057013511658
In grad_steps = 3039, loss = 0.04807276278734207
In grad_steps = 3040, loss = 0.6147934198379517
In grad_steps = 3041, loss = 0.44329261779785156
In grad_steps = 3042, loss = 0.39892852306365967
In grad_steps = 3043, loss = 0.6241680383682251
In grad_steps = 3044, loss = 0.5688523054122925
In grad_steps = 3045, loss = 0.19372166693210602
In grad_steps = 3046, loss = 0.3220968246459961
In grad_steps = 3047, loss = 0.34244629740715027
In grad_steps = 3048, loss = 0.18418580293655396
In grad_steps = 3049, loss = 0.5067518353462219
In grad_steps = 3050, loss = 0.05862654745578766
In grad_steps = 3051, loss = 0.32015538215637207
In grad_steps = 3052, loss = 0.024652015417814255
In grad_steps = 3053, loss = 0.06666205078363419
In grad_steps = 3054, loss = 0.18503408133983612
In grad_steps = 3055, loss = 0.08965182304382324
In grad_steps = 3056, loss = 0.05916334688663483
In grad_steps = 3057, loss = 2.378748893737793
In grad_steps = 3058, loss = 0.24802015721797943
In grad_steps = 3059, loss = 0.30557048320770264
In grad_steps = 3060, loss = 0.070835642516613
In grad_steps = 3061, loss = 0.017291663214564323
In grad_steps = 3062, loss = 1.1701257228851318
In grad_steps = 3063, loss = 0.07707595825195312
In grad_steps = 3064, loss = 0.020962227135896683
In grad_steps = 3065, loss = 0.09233830124139786
In grad_steps = 3066, loss = 0.16020065546035767
In grad_steps = 3067, loss = 0.1337713748216629
In grad_steps = 3068, loss = 0.2136487513780594
In grad_steps = 3069, loss = 0.048879027366638184
In grad_steps = 3070, loss = 0.045944731682538986
In grad_steps = 3071, loss = 0.4429715871810913
In grad_steps = 3072, loss = 0.7465371489524841
In grad_steps = 3073, loss = 1.0686901807785034
In grad_steps = 3074, loss = 0.11602525413036346
In grad_steps = 3075, loss = 0.15020237863063812
In grad_steps = 3076, loss = 0.18041400611400604
In grad_steps = 3077, loss = 0.2367723137140274
In grad_steps = 3078, loss = 0.9157074689865112
In grad_steps = 3079, loss = 0.06271743029356003
In grad_steps = 3080, loss = 0.5964253544807434
In grad_steps = 3081, loss = 0.20129945874214172
In grad_steps = 3082, loss = 0.12734980881214142
In grad_steps = 3083, loss = 0.3345196843147278
In grad_steps = 3084, loss = 0.7559064626693726
In grad_steps = 3085, loss = 0.5797182321548462
In grad_steps = 3086, loss = 0.1023101806640625
In grad_steps = 3087, loss = 0.13565875589847565
In grad_steps = 3088, loss = 0.6222378611564636
In grad_steps = 3089, loss = 0.7380690574645996
In grad_steps = 3090, loss = 0.16726666688919067
In grad_steps = 3091, loss = 0.4191838502883911
In grad_steps = 3092, loss = 0.3588346540927887
In grad_steps = 3093, loss = 0.1552419662475586
In grad_steps = 3094, loss = 0.30985429883003235
In grad_steps = 3095, loss = 0.16422738134860992
In grad_steps = 3096, loss = 0.8358674049377441
In grad_steps = 3097, loss = 0.2923808693885803
In grad_steps = 3098, loss = 0.26659905910491943
In grad_steps = 3099, loss = 0.0378563329577446
In grad_steps = 3100, loss = 0.04052145779132843
In grad_steps = 3101, loss = 0.2177635282278061
In grad_steps = 3102, loss = 0.10753389447927475
In grad_steps = 3103, loss = 0.897612988948822
In grad_steps = 3104, loss = 0.0867128074169159
In grad_steps = 3105, loss = 0.22347277402877808
In grad_steps = 3106, loss = 0.5370553731918335
In grad_steps = 3107, loss = 0.03984560817480087
In grad_steps = 3108, loss = 0.08584007620811462
In grad_steps = 3109, loss = 0.2317800223827362
In grad_steps = 3110, loss = 0.2643084228038788
In grad_steps = 3111, loss = 0.09149202704429626
In grad_steps = 3112, loss = 0.1009141355752945
In grad_steps = 3113, loss = 0.5107762813568115
In grad_steps = 3114, loss = 0.08345788717269897
In grad_steps = 3115, loss = 0.3044823110103607
In grad_steps = 3116, loss = 0.059327613562345505
In grad_steps = 3117, loss = 0.07965333759784698
In grad_steps = 3118, loss = 0.017167625948786736
In grad_steps = 3119, loss = 0.4858199954032898
In grad_steps = 3120, loss = 0.806725800037384
In grad_steps = 3121, loss = 0.8317426443099976
In grad_steps = 3122, loss = 0.09759647399187088
In grad_steps = 3123, loss = 0.2161889523267746
In grad_steps = 3124, loss = 0.06256771087646484
In grad_steps = 3125, loss = 0.0472862534224987
In grad_steps = 3126, loss = 0.1378643661737442
In grad_steps = 3127, loss = 0.07518316805362701
In grad_steps = 3128, loss = 0.2986266613006592
In grad_steps = 3129, loss = 0.08904257416725159
In grad_steps = 3130, loss = 0.09603317826986313
In grad_steps = 3131, loss = 0.6478745937347412
In grad_steps = 3132, loss = 0.06601833552122116
In grad_steps = 3133, loss = 0.3324151039123535
In grad_steps = 3134, loss = 0.5660610198974609
In grad_steps = 3135, loss = 0.11670062690973282
In grad_steps = 3136, loss = 0.025591053068637848
In grad_steps = 3137, loss = 0.29612046480178833
In grad_steps = 3138, loss = 0.4009717106819153
In grad_steps = 3139, loss = 0.1014755368232727
In grad_steps = 3140, loss = 0.6186283230781555
In grad_steps = 3141, loss = 0.3516511023044586
In grad_steps = 3142, loss = 0.21480000019073486
In grad_steps = 3143, loss = 0.11671062558889389
In grad_steps = 3144, loss = 0.04825623333454132
In grad_steps = 3145, loss = 0.027671093121170998
In grad_steps = 3146, loss = 0.0857761800289154
In grad_steps = 3147, loss = 0.0351218655705452
In grad_steps = 3148, loss = 0.23858478665351868
In grad_steps = 3149, loss = 0.0160566046833992
In grad_steps = 3150, loss = 0.03638581559062004
In grad_steps = 3151, loss = 0.35080841183662415
In grad_steps = 3152, loss = 0.12968197464942932
In grad_steps = 3153, loss = 0.0737074539065361
In grad_steps = 3154, loss = 0.29093754291534424
In grad_steps = 3155, loss = 0.6757428646087646
In grad_steps = 3156, loss = 0.006831265054643154
In grad_steps = 3157, loss = 0.1812492460012436
In grad_steps = 3158, loss = 0.0628429651260376
In grad_steps = 3159, loss = 0.06687048077583313
In grad_steps = 3160, loss = 0.04001250118017197
In grad_steps = 3161, loss = 0.23301899433135986
In grad_steps = 3162, loss = 0.012004388496279716
In grad_steps = 3163, loss = 0.5604637265205383
In grad_steps = 3164, loss = 0.11407558619976044
In grad_steps = 3165, loss = 1.0928293466567993
In grad_steps = 3166, loss = 0.04529271647334099
In grad_steps = 3167, loss = 0.04302681237459183
In grad_steps = 3168, loss = 0.03422157093882561
In grad_steps = 3169, loss = 0.1862996369600296
In grad_steps = 3170, loss = 0.03098633699119091
In grad_steps = 3171, loss = 0.015307996422052383
In grad_steps = 3172, loss = 0.10355114191770554
In grad_steps = 3173, loss = 0.004764551296830177
In grad_steps = 3174, loss = 0.04064477980136871
In grad_steps = 3175, loss = 0.016467949375510216
In grad_steps = 3176, loss = 0.03332434594631195
In grad_steps = 3177, loss = 0.06377638876438141
In grad_steps = 3178, loss = 0.011387226171791553
In grad_steps = 3179, loss = 0.33459317684173584
In grad_steps = 3180, loss = 0.007946078665554523
In grad_steps = 3181, loss = 0.586780309677124
In grad_steps = 3182, loss = 0.1374034881591797
In grad_steps = 3183, loss = 0.004196952097117901
In grad_steps = 3184, loss = 0.11626388132572174
In grad_steps = 3185, loss = 0.37765833735466003
In grad_steps = 3186, loss = 1.351168155670166
In grad_steps = 3187, loss = 0.07939210534095764
In grad_steps = 3188, loss = 0.050008535385131836
In grad_steps = 3189, loss = 0.9566322565078735
In grad_steps = 3190, loss = 0.007814489305019379
In grad_steps = 3191, loss = 0.16880163550376892
In grad_steps = 3192, loss = 0.13381335139274597
In grad_steps = 3193, loss = 0.07468144595623016
In grad_steps = 3194, loss = 0.9205818176269531
In grad_steps = 3195, loss = 0.09831232577562332
In grad_steps = 3196, loss = 0.3322087824344635
In grad_steps = 3197, loss = 0.015956532210111618
In grad_steps = 3198, loss = 0.2576043903827667
In grad_steps = 3199, loss = 1.5273380279541016
In grad_steps = 3200, loss = 0.11991424858570099
In grad_steps = 3201, loss = 0.5843948721885681
In grad_steps = 3202, loss = 0.21262575685977936
In grad_steps = 3203, loss = 0.47792306542396545
In grad_steps = 3204, loss = 0.2377321571111679
In grad_steps = 3205, loss = 0.32724475860595703
In grad_steps = 3206, loss = 0.6310091614723206
In grad_steps = 3207, loss = 0.30017009377479553
In grad_steps = 3208, loss = 0.3482167422771454
In grad_steps = 3209, loss = 0.273950457572937
In grad_steps = 3210, loss = 0.19317284226417542
In grad_steps = 3211, loss = 0.13350778818130493
In grad_steps = 3212, loss = 0.11428602784872055
In grad_steps = 3213, loss = 0.13506567478179932
In grad_steps = 3214, loss = 0.5283478498458862
In grad_steps = 3215, loss = 0.12392367422580719
In grad_steps = 3216, loss = 0.22083424031734467
In grad_steps = 3217, loss = 0.16986379027366638
In grad_steps = 3218, loss = 0.21865223348140717
In grad_steps = 3219, loss = 0.638064444065094
In grad_steps = 3220, loss = 0.17929869890213013
In grad_steps = 3221, loss = 0.13230642676353455
In grad_steps = 3222, loss = 0.017743606120347977
In grad_steps = 3223, loss = 0.12991395592689514
In grad_steps = 3224, loss = 0.11276568472385406
In grad_steps = 3225, loss = 0.30432021617889404
In grad_steps = 3226, loss = 0.785531759262085
In grad_steps = 3227, loss = 0.09202614426612854
In grad_steps = 3228, loss = 0.5798549652099609
In grad_steps = 3229, loss = 0.017886467278003693
In grad_steps = 3230, loss = 0.033701129257678986
In grad_steps = 3231, loss = 0.43085646629333496
In grad_steps = 3232, loss = 0.018312646076083183
In grad_steps = 3233, loss = 0.01791231520473957
In grad_steps = 3234, loss = 0.12744517624378204
In grad_steps = 3235, loss = 0.9514339566230774
In grad_steps = 3236, loss = 0.9057615399360657
In grad_steps = 3237, loss = 1.1818398237228394
In grad_steps = 3238, loss = 0.11265289038419724
In grad_steps = 3239, loss = 0.10097421705722809
In grad_steps = 3240, loss = 0.2197776883840561
In grad_steps = 3241, loss = 0.25335168838500977
In grad_steps = 3242, loss = 0.09376309812068939
In grad_steps = 3243, loss = 0.04823843762278557
In grad_steps = 3244, loss = 0.07409752905368805
In grad_steps = 3245, loss = 0.07767285406589508
In grad_steps = 3246, loss = 0.07127609848976135
In grad_steps = 3247, loss = 0.42215728759765625
In grad_steps = 3248, loss = 0.6909509897232056
In grad_steps = 3249, loss = 0.05577019229531288
In grad_steps = 3250, loss = 0.6498730778694153
In grad_steps = 3251, loss = 0.1678675413131714
In grad_steps = 3252, loss = 0.709491491317749
In grad_steps = 3253, loss = 0.08991426974534988
In grad_steps = 3254, loss = 0.0486905500292778
In grad_steps = 3255, loss = 0.34105420112609863
In grad_steps = 3256, loss = 0.03399667143821716
In grad_steps = 3257, loss = 0.030762217938899994
In grad_steps = 3258, loss = 1.0136785507202148
In grad_steps = 3259, loss = 0.777415931224823
In grad_steps = 3260, loss = 0.08953404426574707
In grad_steps = 3261, loss = 0.07941009849309921
In grad_steps = 3262, loss = 0.27398350834846497
In grad_steps = 3263, loss = 0.3991931080818176
In grad_steps = 3264, loss = 0.8377594947814941
In grad_steps = 3265, loss = 0.031851138919591904
In grad_steps = 3266, loss = 0.6784495115280151
In grad_steps = 3267, loss = 0.6403374671936035
In grad_steps = 3268, loss = 0.07508428394794464
In grad_steps = 3269, loss = 0.6407215595245361
In grad_steps = 3270, loss = 0.03353886306285858
In grad_steps = 3271, loss = 0.5238336324691772
In grad_steps = 3272, loss = 0.1275763064622879
In grad_steps = 3273, loss = 0.18249748647212982
In grad_steps = 3274, loss = 0.32958146929740906
In grad_steps = 3275, loss = 0.39716610312461853
In grad_steps = 3276, loss = 0.44203898310661316
In grad_steps = 3277, loss = 1.0563592910766602
In grad_steps = 3278, loss = 0.06873171776533127
In grad_steps = 3279, loss = 0.11246941983699799
In grad_steps = 3280, loss = 0.13925769925117493
In grad_steps = 3281, loss = 0.21868038177490234
In grad_steps = 3282, loss = 0.698559582233429
In grad_steps = 3283, loss = 0.19148217141628265
In grad_steps = 3284, loss = 0.10307140648365021
In grad_steps = 3285, loss = 0.036113400012254715
In grad_steps = 3286, loss = 0.5973311066627502
In grad_steps = 3287, loss = 0.12068173289299011
In grad_steps = 3288, loss = 0.10622402280569077
In grad_steps = 3289, loss = 0.25798138976097107
In grad_steps = 3290, loss = 0.45307251811027527
In grad_steps = 3291, loss = 0.0944395512342453
In grad_steps = 3292, loss = 0.323763370513916
In grad_steps = 3293, loss = 0.07332171499729156
In grad_steps = 3294, loss = 0.06952796876430511
In grad_steps = 3295, loss = 0.07308036088943481
In grad_steps = 3296, loss = 0.19022107124328613
In grad_steps = 3297, loss = 0.5831331014633179
In grad_steps = 3298, loss = 0.684027373790741
In grad_steps = 3299, loss = 0.09283914417028427
In grad_steps = 3300, loss = 0.15472260117530823
In grad_steps = 3301, loss = 0.6575379371643066
In grad_steps = 3302, loss = 0.18810014426708221
In grad_steps = 3303, loss = 0.07581961154937744
In grad_steps = 3304, loss = 0.21506501734256744
In grad_steps = 3305, loss = 0.12704917788505554
In grad_steps = 3306, loss = 0.2557292580604553
In grad_steps = 3307, loss = 0.05024021118879318
In grad_steps = 3308, loss = 0.5115408897399902
In grad_steps = 3309, loss = 0.5523075461387634
In grad_steps = 3310, loss = 0.11359502375125885
In grad_steps = 3311, loss = 0.2561602294445038
In grad_steps = 3312, loss = 0.10346443951129913
In grad_steps = 3313, loss = 0.11540497839450836
In grad_steps = 3314, loss = 0.8510439395904541
In grad_steps = 3315, loss = 1.3085514307022095
In grad_steps = 3316, loss = 0.11597982794046402
In grad_steps = 3317, loss = 0.019986644387245178
In grad_steps = 3318, loss = 0.07585529237985611
In grad_steps = 3319, loss = 0.5116065144538879
In grad_steps = 3320, loss = 0.5078946948051453
In grad_steps = 3321, loss = 0.4833312928676605
In grad_steps = 3322, loss = 0.047609325498342514
In grad_steps = 3323, loss = 0.060941584408283234
In grad_steps = 3324, loss = 0.1400352567434311
In grad_steps = 3325, loss = 0.14145804941654205
In grad_steps = 3326, loss = 0.14467297494411469
In grad_steps = 3327, loss = 0.23219537734985352
In grad_steps = 3328, loss = 0.06189274042844772
In grad_steps = 3329, loss = 0.25232768058776855
In grad_steps = 3330, loss = 0.05799200385808945
In grad_steps = 3331, loss = 0.12365103513002396
In grad_steps = 3332, loss = 0.38860413432121277
In grad_steps = 3333, loss = 0.05267645791172981
In grad_steps = 3334, loss = 0.18036963045597076
In grad_steps = 3335, loss = 0.11525382101535797
In grad_steps = 3336, loss = 0.4492262601852417
In grad_steps = 3337, loss = 0.1337374746799469
In grad_steps = 3338, loss = 0.3229021430015564
In grad_steps = 3339, loss = 0.047996390610933304
In grad_steps = 3340, loss = 0.029060104861855507
In grad_steps = 3341, loss = 0.34240466356277466
In grad_steps = 3342, loss = 0.07358825206756592
In grad_steps = 3343, loss = 0.017673857510089874
In grad_steps = 3344, loss = 0.03150215372443199
In grad_steps = 3345, loss = 0.16265547275543213
In grad_steps = 3346, loss = 0.10974504798650742
In grad_steps = 3347, loss = 0.17638644576072693
In grad_steps = 3348, loss = 0.4036502540111542
In grad_steps = 3349, loss = 0.06208103150129318
In grad_steps = 3350, loss = 0.020140230655670166
In grad_steps = 3351, loss = 0.021828236058354378
In grad_steps = 3352, loss = 0.4116106629371643
In grad_steps = 3353, loss = 0.2598659098148346
In grad_steps = 3354, loss = 0.01835249550640583
In grad_steps = 3355, loss = 0.03379105404019356
In grad_steps = 3356, loss = 0.07276596873998642
In grad_steps = 3357, loss = 0.19617706537246704
In grad_steps = 3358, loss = 0.14275053143501282
In grad_steps = 3359, loss = 0.8275577425956726
In grad_steps = 3360, loss = 0.04680996760725975
In grad_steps = 3361, loss = 1.0588397979736328
In grad_steps = 3362, loss = 0.21934552490711212
In grad_steps = 3363, loss = 0.1118340715765953
In grad_steps = 3364, loss = 0.0719950795173645
In grad_steps = 3365, loss = 0.06707864254713058
In grad_steps = 3366, loss = 0.04099482297897339
In grad_steps = 3367, loss = 0.03612818568944931
In grad_steps = 3368, loss = 0.05887201800942421
In grad_steps = 3369, loss = 1.2414716482162476
In grad_steps = 3370, loss = 0.43169304728507996
In grad_steps = 3371, loss = 0.11752665042877197
In grad_steps = 3372, loss = 0.032743435353040695
In grad_steps = 3373, loss = 0.3274132013320923
In grad_steps = 3374, loss = 0.013527872040867805
In grad_steps = 3375, loss = 0.4678702652454376
In grad_steps = 3376, loss = 0.13327637314796448
In grad_steps = 3377, loss = 0.017381351441144943
In grad_steps = 3378, loss = 0.11052123457193375
In grad_steps = 3379, loss = 0.1028214693069458
In grad_steps = 3380, loss = 0.9207994341850281
In grad_steps = 3381, loss = 0.030688855797052383
In grad_steps = 3382, loss = 0.9659076929092407
In grad_steps = 3383, loss = 0.9221349954605103
In grad_steps = 3384, loss = 0.3931647539138794
In grad_steps = 3385, loss = 0.034102194011211395
In grad_steps = 3386, loss = 0.28419435024261475
In grad_steps = 3387, loss = 0.03971848636865616
In grad_steps = 3388, loss = 0.09437049925327301
In grad_steps = 3389, loss = 0.24211031198501587
In grad_steps = 3390, loss = 0.07436089217662811
In grad_steps = 3391, loss = 0.04221096262335777
In grad_steps = 3392, loss = 0.21850566565990448
In grad_steps = 3393, loss = 0.20203758776187897
In grad_steps = 3394, loss = 0.23981890082359314
In grad_steps = 3395, loss = 0.11004674434661865
In grad_steps = 3396, loss = 0.3759218752384186
In grad_steps = 3397, loss = 0.06756805628538132
In grad_steps = 3398, loss = 0.7001940011978149
In grad_steps = 3399, loss = 0.06581118702888489
In grad_steps = 3400, loss = 0.057258084416389465
In grad_steps = 3401, loss = 0.4234878718852997
In grad_steps = 3402, loss = 0.002288851421326399
In grad_steps = 3403, loss = 0.22927594184875488
In grad_steps = 3404, loss = 0.346312940120697
In grad_steps = 3405, loss = 0.2292962372303009
In grad_steps = 3406, loss = 1.084594964981079
In grad_steps = 3407, loss = 0.04133736342191696
In grad_steps = 3408, loss = 1.1446059942245483
In grad_steps = 3409, loss = 0.09867331385612488
In grad_steps = 3410, loss = 0.28628578782081604
In grad_steps = 3411, loss = 0.034041084349155426
In grad_steps = 3412, loss = 0.09312587231397629
In grad_steps = 3413, loss = 0.15730394423007965
In grad_steps = 3414, loss = 0.7339347004890442
In grad_steps = 3415, loss = 0.09433145821094513
In grad_steps = 3416, loss = 0.43884631991386414
In grad_steps = 3417, loss = 0.396869957447052
In grad_steps = 3418, loss = 0.12679967284202576
In grad_steps = 3419, loss = 0.15735436975955963
In grad_steps = 3420, loss = 0.07052293419837952
In grad_steps = 3421, loss = 0.5836488604545593
In grad_steps = 3422, loss = 0.09527936577796936
In grad_steps = 3423, loss = 0.1410609930753708
In grad_steps = 3424, loss = 0.4283824861049652
In grad_steps = 3425, loss = 0.4457789659500122
In grad_steps = 3426, loss = 0.08189233392477036
In grad_steps = 3427, loss = 0.8659592866897583
In grad_steps = 3428, loss = 0.025343310087919235
In grad_steps = 3429, loss = 0.21277308464050293
In grad_steps = 3430, loss = 0.03828166425228119
In grad_steps = 3431, loss = 0.7887941002845764
In grad_steps = 3432, loss = 0.15108844637870789
In grad_steps = 3433, loss = 0.1509849727153778
In grad_steps = 3434, loss = 0.041814886033535004
In grad_steps = 3435, loss = 0.07892466336488724
In grad_steps = 3436, loss = 0.1438293308019638
In grad_steps = 3437, loss = 0.05132384970784187
In grad_steps = 3438, loss = 0.1851484775543213
In grad_steps = 3439, loss = 0.415583997964859
In grad_steps = 3440, loss = 0.26560699939727783
In grad_steps = 3441, loss = 0.05107833817601204
In grad_steps = 3442, loss = 0.5191324949264526
In grad_steps = 3443, loss = 0.003024038625881076
In grad_steps = 3444, loss = 0.6580960154533386
In grad_steps = 3445, loss = 0.4661984443664551
In grad_steps = 3446, loss = 0.08275644481182098
In grad_steps = 3447, loss = 0.054745957255363464
In grad_steps = 3448, loss = 0.4211735129356384
In grad_steps = 3449, loss = 0.6508709788322449
In grad_steps = 3450, loss = 0.015163718722760677
In grad_steps = 3451, loss = 0.29556548595428467
In grad_steps = 3452, loss = 0.03480429947376251
In grad_steps = 3453, loss = 0.8623747229576111
In grad_steps = 3454, loss = 0.04176577925682068
In grad_steps = 3455, loss = 0.5522090196609497
In grad_steps = 3456, loss = 0.7471375465393066
In grad_steps = 3457, loss = 0.07852654159069061
In grad_steps = 3458, loss = 0.01979808509349823
In grad_steps = 3459, loss = 0.5977283716201782
In grad_steps = 3460, loss = 0.7254295349121094
In grad_steps = 3461, loss = 0.8903766870498657
In grad_steps = 3462, loss = 0.11299034208059311
In grad_steps = 3463, loss = 0.11125901341438293
In grad_steps = 3464, loss = 0.43389642238616943
In grad_steps = 3465, loss = 0.1613726019859314
In grad_steps = 3466, loss = 0.8864443898200989
In grad_steps = 3467, loss = 0.206704244017601
In grad_steps = 3468, loss = 0.20543067157268524
In grad_steps = 3469, loss = 0.27897000312805176
In grad_steps = 3470, loss = 0.15651747584342957
In grad_steps = 3471, loss = 0.08328254520893097
In grad_steps = 3472, loss = 0.11241859197616577
In grad_steps = 3473, loss = 0.15122483670711517
In grad_steps = 3474, loss = 0.16076570749282837
In grad_steps = 3475, loss = 0.17424523830413818
In grad_steps = 3476, loss = 0.262066125869751
In grad_steps = 3477, loss = 0.040304653346538544
In grad_steps = 3478, loss = 0.32639437913894653
In grad_steps = 3479, loss = 0.07381820678710938
In grad_steps = 3480, loss = 0.24587148427963257
In grad_steps = 3481, loss = 0.30069831013679504
In grad_steps = 3482, loss = 0.07851187139749527
In grad_steps = 3483, loss = 0.05869528651237488
In grad_steps = 3484, loss = 0.5908383727073669
In grad_steps = 3485, loss = 0.03923919051885605
In grad_steps = 3486, loss = 0.018441494554281235
In grad_steps = 3487, loss = 0.37003880739212036
In grad_steps = 3488, loss = 0.10855843126773834
In grad_steps = 3489, loss = 0.09593423455953598
In grad_steps = 3490, loss = 0.9565749168395996
In grad_steps = 3491, loss = 0.4104588031768799
In grad_steps = 3492, loss = 0.014325623400509357
In grad_steps = 3493, loss = 0.05267713963985443
In grad_steps = 3494, loss = 0.1969311386346817
In grad_steps = 3495, loss = 0.06097933277487755
In grad_steps = 3496, loss = 1.4289329051971436
In grad_steps = 3497, loss = 0.05996471643447876
In grad_steps = 3498, loss = 0.1288945972919464
In grad_steps = 3499, loss = 0.10689445585012436
In grad_steps = 3500, loss = 0.008938421495258808
In grad_steps = 3501, loss = 0.6580907702445984
In grad_steps = 3502, loss = 0.04565729200839996
In grad_steps = 3503, loss = 0.02861044555902481
In grad_steps = 3504, loss = 0.19208365678787231
In grad_steps = 3505, loss = 0.3271738290786743
In grad_steps = 3506, loss = 0.9232007265090942
In grad_steps = 3507, loss = 0.03489048779010773
In grad_steps = 3508, loss = 0.5550398230552673
In grad_steps = 3509, loss = 0.1332196593284607
In grad_steps = 3510, loss = 0.1688242256641388
In grad_steps = 3511, loss = 0.17710651457309723
In grad_steps = 3512, loss = 0.03897498548030853
In grad_steps = 3513, loss = 0.02957410365343094
In grad_steps = 3514, loss = 0.18431077897548676
In grad_steps = 3515, loss = 0.037527866661548615
In grad_steps = 3516, loss = 0.7450516223907471
In grad_steps = 3517, loss = 0.11992509663105011
In grad_steps = 3518, loss = 0.165040522813797
In grad_steps = 3519, loss = 0.04995609447360039
In grad_steps = 3520, loss = 0.052647873759269714
In grad_steps = 3521, loss = 0.02983463555574417
In grad_steps = 3522, loss = 0.04782767966389656
In grad_steps = 3523, loss = 0.05195244401693344
In grad_steps = 3524, loss = 0.025322087109088898
In grad_steps = 3525, loss = 0.12505528330802917
In grad_steps = 3526, loss = 0.033864621073007584
In grad_steps = 3527, loss = 1.1626378297805786
In grad_steps = 3528, loss = 0.22030720114707947
In grad_steps = 3529, loss = 0.049293678253889084
In grad_steps = 3530, loss = 0.07601924985647202
In grad_steps = 3531, loss = 0.5906751751899719
In grad_steps = 3532, loss = 0.48463115096092224
In grad_steps = 3533, loss = 0.05040179565548897
In grad_steps = 3534, loss = 0.05017436668276787
In grad_steps = 3535, loss = 0.056626323610544205
In grad_steps = 3536, loss = 0.05993472784757614
In grad_steps = 3537, loss = 0.09745819121599197
In grad_steps = 3538, loss = 0.15818606317043304
In grad_steps = 3539, loss = 1.0840879678726196
In grad_steps = 3540, loss = 0.04620036855340004
In grad_steps = 3541, loss = 0.27557873725891113
In grad_steps = 3542, loss = 0.32988911867141724
In grad_steps = 3543, loss = 0.07567724585533142
In grad_steps = 3544, loss = 0.5415104627609253
In grad_steps = 3545, loss = 0.02720373496413231
In grad_steps = 3546, loss = 0.11469265818595886
In grad_steps = 3547, loss = 0.1144014298915863
In grad_steps = 3548, loss = 0.07223843038082123
In grad_steps = 3549, loss = 0.02045508660376072
In grad_steps = 3550, loss = 0.02254263497889042
In grad_steps = 3551, loss = 0.24857372045516968
In grad_steps = 3552, loss = 0.8800804615020752
In grad_steps = 3553, loss = 0.08635646104812622
In grad_steps = 3554, loss = 0.6702452898025513
In grad_steps = 3555, loss = 0.07930943369865417
In grad_steps = 3556, loss = 0.10588178038597107
In grad_steps = 3557, loss = 0.0924176275730133
In grad_steps = 3558, loss = 0.6543810963630676
In grad_steps = 3559, loss = 0.09910069406032562
In grad_steps = 3560, loss = 0.3856167793273926
In grad_steps = 3561, loss = 0.11069835722446442
In grad_steps = 3562, loss = 0.8364605903625488
In grad_steps = 3563, loss = 0.5237857103347778
In grad_steps = 3564, loss = 0.3600323796272278
In grad_steps = 3565, loss = 0.8984377980232239
In grad_steps = 3566, loss = 0.2156374752521515
In grad_steps = 3567, loss = 0.07164914160966873
In grad_steps = 3568, loss = 0.08336042612791061
In grad_steps = 3569, loss = 0.07404220104217529
In grad_steps = 3570, loss = 0.044167958199977875
In grad_steps = 3571, loss = 0.051625944674015045
In grad_steps = 3572, loss = 0.1841893494129181
In grad_steps = 3573, loss = 0.11408144235610962
In grad_steps = 3574, loss = 1.3136136531829834
In grad_steps = 3575, loss = 0.04090336710214615
In grad_steps = 3576, loss = 0.8553473949432373
In grad_steps = 3577, loss = 0.5341346263885498
In grad_steps = 3578, loss = 0.14742165803909302
In grad_steps = 3579, loss = 0.4612025022506714
In grad_steps = 3580, loss = 0.6274699568748474
In grad_steps = 3581, loss = 0.9548119902610779
In grad_steps = 3582, loss = 0.08399888873100281
In grad_steps = 3583, loss = 0.5163593292236328
In grad_steps = 3584, loss = 0.6469570994377136
In grad_steps = 3585, loss = 0.0859072357416153
In grad_steps = 3586, loss = 0.1551591455936432
In grad_steps = 3587, loss = 0.1346057653427124
In grad_steps = 3588, loss = 0.386277973651886
In grad_steps = 3589, loss = 0.09282851964235306
In grad_steps = 3590, loss = 0.524529755115509
In grad_steps = 3591, loss = 0.191951185464859
In grad_steps = 3592, loss = 0.04185773804783821
In grad_steps = 3593, loss = 0.13769766688346863
In grad_steps = 3594, loss = 0.4690797030925751
In grad_steps = 3595, loss = 0.24000325798988342
In grad_steps = 3596, loss = 0.15976157784461975
In grad_steps = 3597, loss = 0.09044329822063446
In grad_steps = 3598, loss = 0.03372342512011528
In grad_steps = 3599, loss = 0.20074695348739624
In grad_steps = 3600, loss = 0.6741701364517212
In grad_steps = 3601, loss = 0.029105069115757942
In grad_steps = 3602, loss = 0.2217443436384201
In grad_steps = 3603, loss = 0.09650959074497223
In grad_steps = 3604, loss = 0.28301528096199036
In grad_steps = 3605, loss = 0.1610787957906723
In grad_steps = 3606, loss = 0.03199094906449318
In grad_steps = 3607, loss = 0.35446369647979736
In grad_steps = 3608, loss = 0.07201829552650452
In grad_steps = 3609, loss = 0.07785256952047348
In grad_steps = 3610, loss = 0.1347220242023468
In grad_steps = 3611, loss = 0.46932893991470337
In grad_steps = 3612, loss = 0.5481354594230652
In grad_steps = 3613, loss = 0.0921478420495987
In grad_steps = 3614, loss = 0.6922633647918701
In grad_steps = 3615, loss = 0.1975400447845459
In grad_steps = 3616, loss = 0.1286720335483551
In grad_steps = 3617, loss = 1.46396005153656
In grad_steps = 3618, loss = 0.4400503933429718
In grad_steps = 3619, loss = 0.011633733287453651
In grad_steps = 3620, loss = 0.35589349269866943
In grad_steps = 3621, loss = 0.20263953506946564
In grad_steps = 3622, loss = 0.0248765479773283
In grad_steps = 3623, loss = 0.30119019746780396
In grad_steps = 3624, loss = 1.0385197401046753
In grad_steps = 3625, loss = 0.11294578015804291
In grad_steps = 3626, loss = 0.15011364221572876
In grad_steps = 3627, loss = 0.46809619665145874
In grad_steps = 3628, loss = 1.1010380983352661
In grad_steps = 3629, loss = 0.08995533734560013
In grad_steps = 3630, loss = 0.711053729057312
In grad_steps = 3631, loss = 0.016529632732272148
In grad_steps = 3632, loss = 0.6557184457778931
In grad_steps = 3633, loss = 0.06431923806667328
In grad_steps = 3634, loss = 0.27412933111190796
In grad_steps = 3635, loss = 0.08561012148857117
In grad_steps = 3636, loss = 0.09264455735683441
In grad_steps = 3637, loss = 0.14164845645427704
In grad_steps = 3638, loss = 0.14789648354053497
In grad_steps = 3639, loss = 0.10961903631687164
In grad_steps = 3640, loss = 0.3413551449775696
In grad_steps = 3641, loss = 0.8884575366973877
In grad_steps = 3642, loss = 0.4126093089580536
In grad_steps = 3643, loss = 0.2469676434993744
In grad_steps = 3644, loss = 0.7099311351776123
In grad_steps = 3645, loss = 0.06873666495084763
In grad_steps = 3646, loss = 0.026476863771677017
In grad_steps = 3647, loss = 0.1045704334974289
In grad_steps = 3648, loss = 0.3134165108203888
In grad_steps = 3649, loss = 0.07160577923059464
In grad_steps = 3650, loss = 0.2336282730102539
In grad_steps = 3651, loss = 0.10032029449939728
In grad_steps = 3652, loss = 0.1485711634159088
In grad_steps = 3653, loss = 0.31530529260635376
In grad_steps = 3654, loss = 0.0388721264898777
In grad_steps = 3655, loss = 0.11146162450313568
In grad_steps = 3656, loss = 0.622856080532074
In grad_steps = 3657, loss = 0.14013879001140594
In grad_steps = 3658, loss = 0.17698951065540314
In grad_steps = 3659, loss = 0.15564894676208496
In grad_steps = 3660, loss = 0.08728509396314621
In grad_steps = 3661, loss = 0.5596423149108887
In grad_steps = 3662, loss = 0.18830907344818115
In grad_steps = 3663, loss = 0.4135240614414215
In grad_steps = 3664, loss = 0.16880543529987335
In grad_steps = 3665, loss = 0.06708031892776489
In grad_steps = 3666, loss = 0.23560954630374908
In grad_steps = 3667, loss = 0.03823055326938629
In grad_steps = 3668, loss = 0.025142744183540344
In grad_steps = 3669, loss = 0.018208205699920654
In grad_steps = 3670, loss = 0.07922378927469254
In grad_steps = 3671, loss = 0.7708667516708374
In grad_steps = 3672, loss = 0.02595878206193447
In grad_steps = 3673, loss = 0.35997486114501953
In grad_steps = 3674, loss = 0.06656625866889954
In grad_steps = 3675, loss = 1.068563461303711
In grad_steps = 3676, loss = 0.43319934606552124
In grad_steps = 3677, loss = 0.23363299667835236
In grad_steps = 3678, loss = 0.3018718361854553
In grad_steps = 3679, loss = 0.02401384897530079
In grad_steps = 3680, loss = 0.02586328610777855
In grad_steps = 3681, loss = 0.07374664396047592
In grad_steps = 3682, loss = 0.026403572410345078
In grad_steps = 3683, loss = 0.12202151864767075
In grad_steps = 3684, loss = 0.0811752900481224
In grad_steps = 3685, loss = 0.19084851443767548
In grad_steps = 3686, loss = 0.30826854705810547
In grad_steps = 3687, loss = 0.04067753255367279
In grad_steps = 3688, loss = 0.09730686247348785
In grad_steps = 3689, loss = 0.362896203994751
In grad_steps = 3690, loss = 0.11802002787590027
In grad_steps = 3691, loss = 0.02154535800218582
In grad_steps = 3692, loss = 0.20446638762950897
In grad_steps = 3693, loss = 0.020669467747211456
In grad_steps = 3694, loss = 0.03629758581519127
In grad_steps = 3695, loss = 0.019534889608621597
In grad_steps = 3696, loss = 0.4121771454811096
In grad_steps = 3697, loss = 0.031148837879300117
In grad_steps = 3698, loss = 0.4262107312679291
In grad_steps = 3699, loss = 0.20937030017375946
In grad_steps = 3700, loss = 0.766244113445282
In grad_steps = 3701, loss = 0.015268527902662754
In grad_steps = 3702, loss = 1.1900216341018677
In grad_steps = 3703, loss = 0.04050155356526375
In grad_steps = 3704, loss = 0.8396127223968506
In grad_steps = 3705, loss = 0.04311198368668556
In grad_steps = 3706, loss = 0.013860286213457584
In grad_steps = 3707, loss = 0.43094733357429504
In grad_steps = 3708, loss = 0.5495619177818298
In grad_steps = 3709, loss = 0.04234626516699791
In grad_steps = 3710, loss = 0.5648144483566284
In grad_steps = 3711, loss = 0.17177066206932068
In grad_steps = 3712, loss = 0.03589099645614624
In grad_steps = 3713, loss = 0.7130865454673767
In grad_steps = 3714, loss = 0.34128642082214355
In grad_steps = 3715, loss = 0.04748138412833214
In grad_steps = 3716, loss = 0.41944822669029236
In grad_steps = 3717, loss = 0.0722268596291542
In grad_steps = 3718, loss = 0.1310729831457138
In grad_steps = 3719, loss = 0.20770518481731415
In grad_steps = 3720, loss = 0.08693362772464752
In grad_steps = 3721, loss = 0.2627426087856293
In grad_steps = 3722, loss = 0.09410873055458069
In grad_steps = 3723, loss = 0.12003908306360245
In grad_steps = 3724, loss = 0.02357255108654499
In grad_steps = 3725, loss = 0.14401420950889587
In grad_steps = 3726, loss = 0.06106065213680267
In grad_steps = 3727, loss = 0.07219245284795761
In grad_steps = 3728, loss = 0.06364988535642624
In grad_steps = 3729, loss = 0.06542211771011353
In grad_steps = 3730, loss = 0.04476380720734596
In grad_steps = 3731, loss = 0.08730565756559372
In grad_steps = 3732, loss = 1.842377781867981
In grad_steps = 3733, loss = 0.10356838256120682
In grad_steps = 3734, loss = 0.049424972385168076
In grad_steps = 3735, loss = 0.02795627899467945
In grad_steps = 3736, loss = 0.0494827963411808
In grad_steps = 3737, loss = 0.544636070728302
In grad_steps = 3738, loss = 0.0973961353302002
In grad_steps = 3739, loss = 0.024690501391887665
In grad_steps = 3740, loss = 0.3923965394496918
In grad_steps = 3741, loss = 0.06837521493434906
In grad_steps = 3742, loss = 0.3745468854904175
In grad_steps = 3743, loss = 0.04440157860517502
In grad_steps = 3744, loss = 0.3209272027015686
In grad_steps = 3745, loss = 0.13623857498168945
In grad_steps = 3746, loss = 0.04755113273859024
In grad_steps = 3747, loss = 0.048752546310424805
In grad_steps = 3748, loss = 0.2504391372203827
In grad_steps = 3749, loss = 0.024081509560346603
In grad_steps = 3750, loss = 0.03219575434923172
In grad_steps = 3751, loss = 0.2362397164106369
In grad_steps = 3752, loss = 0.19484186172485352
In grad_steps = 3753, loss = 0.5540649890899658
In grad_steps = 3754, loss = 0.9863808155059814
In grad_steps = 3755, loss = 0.04240605607628822
In grad_steps = 3756, loss = 0.02216525934636593
In grad_steps = 3757, loss = 0.012897970154881477
In grad_steps = 3758, loss = 0.1472741812467575
In grad_steps = 3759, loss = 0.03344742953777313
In grad_steps = 3760, loss = 0.6061962842941284
In grad_steps = 3761, loss = 0.019177058711647987
In grad_steps = 3762, loss = 0.04149073362350464
In grad_steps = 3763, loss = 1.201141595840454
In grad_steps = 3764, loss = 0.15659651160240173
In grad_steps = 3765, loss = 0.42565464973449707
In grad_steps = 3766, loss = 0.08788905292749405
In grad_steps = 3767, loss = 0.08289052546024323
In grad_steps = 3768, loss = 0.1448567658662796
In grad_steps = 3769, loss = 0.04009149223566055
In grad_steps = 3770, loss = 0.01520963292568922
In grad_steps = 3771, loss = 0.7367056608200073
In grad_steps = 3772, loss = 0.01164674200117588
In grad_steps = 3773, loss = 0.08090278506278992
In grad_steps = 3774, loss = 0.4034353494644165
In grad_steps = 3775, loss = 0.01734345220029354
In grad_steps = 3776, loss = 0.07030119001865387
In grad_steps = 3777, loss = 0.0371963232755661
In grad_steps = 3778, loss = 0.026950515806674957
In grad_steps = 3779, loss = 0.1196192130446434
In grad_steps = 3780, loss = 0.5051401257514954
In grad_steps = 3781, loss = 0.04080543294548988
In grad_steps = 3782, loss = 0.016351766884326935
In grad_steps = 3783, loss = 0.1855323314666748
In grad_steps = 3784, loss = 0.6834567189216614
In grad_steps = 3785, loss = 0.8577144145965576
In grad_steps = 3786, loss = 0.5434637069702148
In grad_steps = 3787, loss = 0.5091392993927002
In grad_steps = 3788, loss = 0.20103617012500763
In grad_steps = 3789, loss = 0.31023329496383667
In grad_steps = 3790, loss = 0.12823492288589478
In grad_steps = 3791, loss = 0.6416503190994263
In grad_steps = 3792, loss = 0.39279308915138245
In grad_steps = 3793, loss = 0.10966979712247849
In grad_steps = 3794, loss = 0.1711447536945343
In grad_steps = 3795, loss = 0.17686083912849426
In grad_steps = 3796, loss = 0.3292407989501953
In grad_steps = 3797, loss = 0.48698297142982483
In grad_steps = 3798, loss = 0.27138450741767883
In grad_steps = 3799, loss = 0.11323459446430206
In grad_steps = 3800, loss = 0.062186069786548615
In grad_steps = 3801, loss = 0.29936081171035767
In grad_steps = 3802, loss = 1.249183177947998
In grad_steps = 3803, loss = 0.28621697425842285
In grad_steps = 3804, loss = 0.09844189882278442
In grad_steps = 3805, loss = 0.2135593295097351
In grad_steps = 3806, loss = 0.051799226552248
In grad_steps = 3807, loss = 0.21678930521011353
In grad_steps = 3808, loss = 0.5182721018791199
In grad_steps = 3809, loss = 0.2583293914794922
In grad_steps = 3810, loss = 0.17548160254955292
In grad_steps = 3811, loss = 1.0433671474456787
In grad_steps = 3812, loss = 0.09990102052688599
In grad_steps = 3813, loss = 0.13851670920848846
In grad_steps = 3814, loss = 0.28952646255493164
In grad_steps = 3815, loss = 0.3738273084163666
In grad_steps = 3816, loss = 0.49251654744148254
In grad_steps = 3817, loss = 0.05695337802171707
In grad_steps = 3818, loss = 0.9777409434318542
In grad_steps = 3819, loss = 0.10552652180194855
In grad_steps = 3820, loss = 0.14961686730384827
In grad_steps = 3821, loss = 0.2895452678203583
In grad_steps = 3822, loss = 0.35894110798835754
In grad_steps = 3823, loss = 0.45217445492744446
In grad_steps = 3824, loss = 0.5606484413146973
In grad_steps = 3825, loss = 0.20482392609119415
In grad_steps = 3826, loss = 0.2411281168460846
In grad_steps = 3827, loss = 0.15328040719032288
In grad_steps = 3828, loss = 0.08596886694431305
In grad_steps = 3829, loss = 0.035210251808166504
In grad_steps = 3830, loss = 0.2654845714569092
In grad_steps = 3831, loss = 0.3536744713783264
In grad_steps = 3832, loss = 0.1341390609741211
In grad_steps = 3833, loss = 0.1472223550081253
In grad_steps = 3834, loss = 0.04866891726851463
In grad_steps = 3835, loss = 0.12839314341545105
In grad_steps = 3836, loss = 0.21872949600219727
In grad_steps = 3837, loss = 0.3741215169429779
In grad_steps = 3838, loss = 0.06347443163394928
In grad_steps = 3839, loss = 0.01524127647280693
In grad_steps = 3840, loss = 0.02748304419219494
In grad_steps = 3841, loss = 0.050329115241765976
In grad_steps = 3842, loss = 0.017031991854310036
In grad_steps = 3843, loss = 0.2914842665195465
In grad_steps = 3844, loss = 0.4497944712638855
In grad_steps = 3845, loss = 0.12458378076553345
In grad_steps = 3846, loss = 0.9887633323669434
In grad_steps = 3847, loss = 0.2929305136203766
In grad_steps = 3848, loss = 1.0684551000595093
In grad_steps = 3849, loss = 0.044418781995773315
In grad_steps = 3850, loss = 0.6047703623771667
In grad_steps = 3851, loss = 0.22301124036312103
In grad_steps = 3852, loss = 0.0830746442079544
In grad_steps = 3853, loss = 0.03524680808186531
In grad_steps = 3854, loss = 0.1450096070766449
In grad_steps = 3855, loss = 0.010041498579084873
In grad_steps = 3856, loss = 0.3201812207698822
In grad_steps = 3857, loss = 0.11790762096643448
In grad_steps = 3858, loss = 0.12573856115341187
In grad_steps = 3859, loss = 0.05161156505346298
In grad_steps = 3860, loss = 0.1754683405160904
In grad_steps = 3861, loss = 0.41763541102409363
In grad_steps = 3862, loss = 1.0190011262893677
In grad_steps = 3863, loss = 0.3068625330924988
In grad_steps = 3864, loss = 0.5852499008178711
In grad_steps = 3865, loss = 0.09097802639007568
In grad_steps = 3866, loss = 0.6675611138343811
In grad_steps = 3867, loss = 0.20555917918682098
In grad_steps = 3868, loss = 0.5543701648712158
In grad_steps = 3869, loss = 0.22731681168079376
In grad_steps = 3870, loss = 0.04400566965341568
In grad_steps = 3871, loss = 0.08879312127828598
In grad_steps = 3872, loss = 0.12646012008190155
In grad_steps = 3873, loss = 0.4995219111442566
In grad_steps = 3874, loss = 0.06490983814001083
In grad_steps = 3875, loss = 0.23468010127544403
In grad_steps = 3876, loss = 0.16464774310588837
In grad_steps = 3877, loss = 0.22430232167243958
In grad_steps = 3878, loss = 0.39126724004745483
In grad_steps = 3879, loss = 0.3783700466156006
In grad_steps = 3880, loss = 0.08924125134944916
In grad_steps = 3881, loss = 0.6345214247703552
In grad_steps = 3882, loss = 0.04433572664856911
In grad_steps = 3883, loss = 0.8169609904289246
In grad_steps = 3884, loss = 0.04962553083896637
In grad_steps = 3885, loss = 0.26758113503456116
In grad_steps = 3886, loss = 0.07379040122032166
In grad_steps = 3887, loss = 0.05910969525575638
In grad_steps = 3888, loss = 0.012018143199384212
In grad_steps = 3889, loss = 0.050148557871580124
In grad_steps = 3890, loss = 0.5262064337730408
In grad_steps = 3891, loss = 0.32111459970474243
In grad_steps = 3892, loss = 0.2300664335489273
In grad_steps = 3893, loss = 0.21355783939361572
In grad_steps = 3894, loss = 0.2735357880592346
In grad_steps = 3895, loss = 0.219901904463768
In grad_steps = 3896, loss = 0.02079431340098381
In grad_steps = 3897, loss = 0.18155914545059204
In grad_steps = 3898, loss = 0.042275287210941315
In grad_steps = 3899, loss = 0.38488948345184326
In grad_steps = 3900, loss = 0.16240528225898743
In grad_steps = 3901, loss = 0.0634288638830185
In grad_steps = 3902, loss = 0.1955927014350891
In grad_steps = 3903, loss = 0.018848519772291183
In grad_steps = 3904, loss = 0.04132011532783508
In grad_steps = 3905, loss = 0.08474192023277283
In grad_steps = 3906, loss = 1.0311100482940674
In grad_steps = 3907, loss = 0.2763139605522156
In grad_steps = 3908, loss = 0.29044947028160095
In grad_steps = 3909, loss = 0.16626201570034027
In grad_steps = 3910, loss = 0.31455618143081665
In grad_steps = 3911, loss = 0.046487800776958466
In grad_steps = 3912, loss = 0.3385410010814667
In grad_steps = 3913, loss = 0.14091414213180542
In grad_steps = 3914, loss = 0.010698522441089153
In grad_steps = 3915, loss = 0.011000009253621101
In grad_steps = 3916, loss = 0.3256267309188843
In grad_steps = 3917, loss = 0.021016128361225128
In grad_steps = 3918, loss = 0.34241291880607605
In grad_steps = 3919, loss = 1.229615330696106
In grad_steps = 3920, loss = 0.009097347036004066
In grad_steps = 3921, loss = 0.018526989966630936
In grad_steps = 3922, loss = 0.05623764917254448
In grad_steps = 3923, loss = 0.6945136785507202
In grad_steps = 3924, loss = 0.7106114029884338
In grad_steps = 3925, loss = 0.10508208721876144
In grad_steps = 3926, loss = 0.10495036095380783
In grad_steps = 3927, loss = 0.3357144594192505
In grad_steps = 3928, loss = 1.2886580228805542
In grad_steps = 3929, loss = 0.6062882542610168
In grad_steps = 3930, loss = 0.1017284244298935
In grad_steps = 3931, loss = 1.0207083225250244
In grad_steps = 3932, loss = 0.011298306286334991
In grad_steps = 3933, loss = 0.6310946345329285
In grad_steps = 3934, loss = 0.25975924730300903
In grad_steps = 3935, loss = 0.2020592838525772
In grad_steps = 3936, loss = 0.2556833028793335
In grad_steps = 3937, loss = 0.0625196099281311
In grad_steps = 3938, loss = 0.24331805109977722
In grad_steps = 3939, loss = 0.4378507733345032
In grad_steps = 3940, loss = 0.3687567114830017
In grad_steps = 3941, loss = 0.32711687684059143
In grad_steps = 3942, loss = 0.3051888048648834
In grad_steps = 3943, loss = 0.4390397071838379
In grad_steps = 3944, loss = 0.32539987564086914
In grad_steps = 3945, loss = 0.2239753156900406
In grad_steps = 3946, loss = 0.12542298436164856
In grad_steps = 3947, loss = 0.1811494380235672
In grad_steps = 3948, loss = 0.12258026003837585
In grad_steps = 3949, loss = 0.3570842146873474
In grad_steps = 3950, loss = 0.11662474274635315
In grad_steps = 3951, loss = 0.11632554978132248
In grad_steps = 3952, loss = 0.16247205436229706
In grad_steps = 3953, loss = 0.05431487411260605
In grad_steps = 3954, loss = 0.12245845049619675
In grad_steps = 3955, loss = 0.1488221287727356
In grad_steps = 3956, loss = 0.15546634793281555
In grad_steps = 3957, loss = 0.28399237990379333
In grad_steps = 3958, loss = 0.27351322770118713
In grad_steps = 3959, loss = 0.010188761167228222
In grad_steps = 3960, loss = 0.06938246637582779
In grad_steps = 3961, loss = 0.008866965770721436
In grad_steps = 3962, loss = 0.09319464862346649
In grad_steps = 3963, loss = 0.018299903720617294
In grad_steps = 3964, loss = 0.01788950152695179
In grad_steps = 3965, loss = 0.01041172631084919
In grad_steps = 3966, loss = 0.7451155185699463
In grad_steps = 3967, loss = 0.7724745273590088
In grad_steps = 3968, loss = 0.21497803926467896
In grad_steps = 3969, loss = 0.022585541009902954
In grad_steps = 3970, loss = 0.01991015300154686
In grad_steps = 3971, loss = 0.3055904805660248
In grad_steps = 3972, loss = 0.039580706506967545
In grad_steps = 3973, loss = 0.017693091183900833
In grad_steps = 3974, loss = 0.02707330323755741
In grad_steps = 3975, loss = 0.006251884158700705
In grad_steps = 3976, loss = 0.17581093311309814
In grad_steps = 3977, loss = 0.41527047753334045
In grad_steps = 3978, loss = 0.08341782540082932
In grad_steps = 3979, loss = 0.016570573672652245
In grad_steps = 3980, loss = 0.06971849501132965
In grad_steps = 3981, loss = 0.2128448337316513
In grad_steps = 3982, loss = 0.012881968170404434
In grad_steps = 3983, loss = 0.7216997146606445
In grad_steps = 3984, loss = 0.6280460357666016
In grad_steps = 3985, loss = 0.030175676569342613
In grad_steps = 3986, loss = 0.1900465488433838
In grad_steps = 3987, loss = 0.028152870014309883
In grad_steps = 3988, loss = 0.01700262539088726
In grad_steps = 3989, loss = 0.08516024053096771
In grad_steps = 3990, loss = 0.1264648586511612
In grad_steps = 3991, loss = 0.2320590764284134
In grad_steps = 3992, loss = 0.5821219682693481
In grad_steps = 3993, loss = 0.10697554796934128
In grad_steps = 3994, loss = 0.01978975161910057
In grad_steps = 3995, loss = 0.010810240171849728
In grad_steps = 3996, loss = 0.13177847862243652
In grad_steps = 3997, loss = 0.8232709169387817
In grad_steps = 3998, loss = 1.152245044708252
In grad_steps = 3999, loss = 0.8292035460472107
In grad_steps = 4000, loss = 0.24917320907115936
In grad_steps = 4001, loss = 0.18415039777755737
In grad_steps = 4002, loss = 0.27741920948028564
In grad_steps = 4003, loss = 0.32469919323921204
In grad_steps = 4004, loss = 0.3205459415912628
In grad_steps = 4005, loss = 0.23969970643520355
In grad_steps = 4006, loss = 0.17818032205104828
In grad_steps = 4007, loss = 0.21508173644542694
In grad_steps = 4008, loss = 0.33543217182159424
In grad_steps = 4009, loss = 0.23792532086372375
In grad_steps = 4010, loss = 0.1321885585784912
In grad_steps = 4011, loss = 0.1349339783191681
In grad_steps = 4012, loss = 0.47093331813812256
In grad_steps = 4013, loss = 0.013461445458233356
In grad_steps = 4014, loss = 0.2178615927696228
In grad_steps = 4015, loss = 0.08031435310840607
In grad_steps = 4016, loss = 0.5649511814117432
In grad_steps = 4017, loss = 0.3178858458995819
In grad_steps = 4018, loss = 0.2388371080160141
In grad_steps = 4019, loss = 0.06111408770084381
In grad_steps = 4020, loss = 0.050910480320453644
In grad_steps = 4021, loss = 0.3013905882835388
In grad_steps = 4022, loss = 0.2987433671951294
In grad_steps = 4023, loss = 0.1991952508687973
In grad_steps = 4024, loss = 0.796268105506897
In grad_steps = 4025, loss = 0.00801887083798647
In grad_steps = 4026, loss = 0.019333139061927795
In grad_steps = 4027, loss = 0.06796817481517792
In grad_steps = 4028, loss = 0.03786040097475052
In grad_steps = 4029, loss = 0.5150009989738464
In grad_steps = 4030, loss = 0.1116734966635704
In grad_steps = 4031, loss = 0.17965006828308105
In grad_steps = 4032, loss = 0.5920292735099792
In grad_steps = 4033, loss = 0.28375375270843506
In grad_steps = 4034, loss = 0.09520614892244339
In grad_steps = 4035, loss = 0.2846931517124176
In grad_steps = 4036, loss = 0.04516934975981712
In grad_steps = 4037, loss = 0.1179155632853508
In grad_steps = 4038, loss = 0.35789722204208374
In grad_steps = 4039, loss = 0.06826447695493698
In grad_steps = 4040, loss = 0.01041861530393362
In grad_steps = 4041, loss = 3.136991024017334
In grad_steps = 4042, loss = 1.257435917854309
In grad_steps = 4043, loss = 0.011511456221342087
In grad_steps = 4044, loss = 0.3237345814704895
In grad_steps = 4045, loss = 0.15480674803256989
In grad_steps = 4046, loss = 0.6038063168525696
In grad_steps = 4047, loss = 0.24963533878326416
In grad_steps = 4048, loss = 0.2554611563682556
In grad_steps = 4049, loss = 0.2340155988931656
In grad_steps = 4050, loss = 0.23106428980827332
In grad_steps = 4051, loss = 0.0410398468375206
In grad_steps = 4052, loss = 0.054270800203084946
In grad_steps = 4053, loss = 0.07771362364292145
In grad_steps = 4054, loss = 0.4382894039154053
In grad_steps = 4055, loss = 0.23972710967063904
In grad_steps = 4056, loss = 0.06615111976861954
In grad_steps = 4057, loss = 0.1589706987142563
In grad_steps = 4058, loss = 0.16985692083835602
In grad_steps = 4059, loss = 0.09955655038356781
In grad_steps = 4060, loss = 0.18165595829486847
In grad_steps = 4061, loss = 0.06436531245708466
In grad_steps = 4062, loss = 0.005100126378238201
In grad_steps = 4063, loss = 0.2145601511001587
In grad_steps = 4064, loss = 0.020320681855082512
In grad_steps = 4065, loss = 0.23421229422092438
In grad_steps = 4066, loss = 1.0571482181549072
In grad_steps = 4067, loss = 0.017414581030607224
In grad_steps = 4068, loss = 0.016021553426980972
In grad_steps = 4069, loss = 0.023555779829621315
In grad_steps = 4070, loss = 0.19272014498710632
In grad_steps = 4071, loss = 0.02735939621925354
In grad_steps = 4072, loss = 0.011799824424088001
In grad_steps = 4073, loss = 0.3567703068256378
In grad_steps = 4074, loss = 0.011289851740002632
In grad_steps = 4075, loss = 0.00628660386428237
In grad_steps = 4076, loss = 0.006982182152569294
In grad_steps = 4077, loss = 0.03621160238981247
In grad_steps = 4078, loss = 0.11021792143583298
In grad_steps = 4079, loss = 0.09036775678396225
In grad_steps = 4080, loss = 1.6293272972106934
In grad_steps = 4081, loss = 1.1858036518096924
In grad_steps = 4082, loss = 0.06273500621318817
In grad_steps = 4083, loss = 0.05039878189563751
In grad_steps = 4084, loss = 0.017259769141674042
In grad_steps = 4085, loss = 0.9084361791610718
In grad_steps = 4086, loss = 0.8513249158859253
In grad_steps = 4087, loss = 0.9358504414558411
In grad_steps = 4088, loss = 0.04558053985238075
In grad_steps = 4089, loss = 0.04046742618083954
In grad_steps = 4090, loss = 0.05321855843067169
In grad_steps = 4091, loss = 0.8042207360267639
In grad_steps = 4092, loss = 0.08677011728286743
In grad_steps = 4093, loss = 0.12274013459682465
In grad_steps = 4094, loss = 0.49060192704200745
In grad_steps = 4095, loss = 0.150798961520195
In grad_steps = 4096, loss = 0.19215445220470428
In grad_steps = 4097, loss = 0.22688911855220795
In grad_steps = 4098, loss = 0.39186516404151917
In grad_steps = 4099, loss = 0.08284613490104675
In grad_steps = 4100, loss = 0.20904593169689178
In grad_steps = 4101, loss = 0.37081536650657654
In grad_steps = 4102, loss = 0.28155598044395447
In grad_steps = 4103, loss = 0.12101908773183823
In grad_steps = 4104, loss = 0.30498263239860535
In grad_steps = 4105, loss = 0.006458600051701069
Elapsed time: 2342.173635482788 seconds for ensemble 0 with 2 epochs
LoRA instance 0 evaluation complete. Data saved to /pscratch/sd/t/tianle/lucid/other_source/SURP_2024/results/tmp_lora_ensemble/Llama3-8B-set-5/test_data_instance_0_seed_1.npz.
lora instance i = 0 Successfully finished.
Training lora instance 1
DEBUG: Total parameters: 8037076992
DEBUG: Trainable parameters: 6815744
Beginning epoch 1
In grad_steps = 0, loss = 0.7379498481750488
In grad_steps = 1, loss = 0.6388602256774902
In grad_steps = 2, loss = 1.606998085975647
In grad_steps = 3, loss = 0.8036438822746277
In grad_steps = 4, loss = 0.6285702586174011
In grad_steps = 5, loss = 0.8180009722709656
In grad_steps = 6, loss = 0.5323972105979919
In grad_steps = 7, loss = 0.574597179889679
In grad_steps = 8, loss = 1.1806743144989014
In grad_steps = 9, loss = 0.955660343170166
In grad_steps = 10, loss = 0.7589036226272583
In grad_steps = 11, loss = 0.7100327014923096
In grad_steps = 12, loss = 0.7489947080612183
In grad_steps = 13, loss = 0.587338387966156
In grad_steps = 14, loss = 0.9402954578399658
In grad_steps = 15, loss = 0.5358189940452576
In grad_steps = 16, loss = 0.7135878801345825
In grad_steps = 17, loss = 0.6912459135055542
In grad_steps = 18, loss = 0.7715641856193542
In grad_steps = 19, loss = 0.7426652908325195
In grad_steps = 20, loss = 0.6717743873596191
In grad_steps = 21, loss = 0.6496249437332153
In grad_steps = 22, loss = 0.673829197883606
In grad_steps = 23, loss = 0.7795917987823486
In grad_steps = 24, loss = 0.6417447924613953
In grad_steps = 25, loss = 0.7515506744384766
In grad_steps = 26, loss = 0.7279269099235535
In grad_steps = 27, loss = 0.713259220123291
In grad_steps = 28, loss = 0.5747599601745605
In grad_steps = 29, loss = 0.6879698038101196
In grad_steps = 30, loss = 0.8094446063041687
In grad_steps = 31, loss = 0.6274048686027527
In grad_steps = 32, loss = 0.6507171392440796
In grad_steps = 33, loss = 0.6780962347984314
In grad_steps = 34, loss = 0.6255671381950378
In grad_steps = 35, loss = 0.6909005641937256
In grad_steps = 36, loss = 0.6590144634246826
In grad_steps = 37, loss = 0.691591203212738
In grad_steps = 38, loss = 0.784619927406311
In grad_steps = 39, loss = 0.6913864016532898
In grad_steps = 40, loss = 0.7409262657165527
In grad_steps = 41, loss = 0.6719299554824829
In grad_steps = 42, loss = 0.634289562702179
In grad_steps = 43, loss = 0.6738502383232117
In grad_steps = 44, loss = 0.5918151140213013
In grad_steps = 45, loss = 0.735532283782959
In grad_steps = 46, loss = 0.7161434888839722
In grad_steps = 47, loss = 0.664859414100647
In grad_steps = 48, loss = 0.5258479714393616
In grad_steps = 49, loss = 0.6683588027954102
In grad_steps = 50, loss = 0.7195569276809692
In grad_steps = 51, loss = 0.6752297878265381
In grad_steps = 52, loss = 0.5712881088256836
In grad_steps = 53, loss = 0.7391415238380432
In grad_steps = 54, loss = 0.6518704891204834
In grad_steps = 55, loss = 0.618643581867218
In grad_steps = 56, loss = 0.6793871521949768
In grad_steps = 57, loss = 0.7035812139511108
In grad_steps = 58, loss = 0.696727454662323
In grad_steps = 59, loss = 0.7080702781677246
In grad_steps = 60, loss = 0.7379111051559448
In grad_steps = 61, loss = 0.6229422092437744
In grad_steps = 62, loss = 0.6434605717658997
In grad_steps = 63, loss = 0.6033428907394409
In grad_steps = 64, loss = 0.4984915256500244
In grad_steps = 65, loss = 0.5480911135673523
In grad_steps = 66, loss = 0.601009726524353
In grad_steps = 67, loss = 0.42361968755722046
In grad_steps = 68, loss = 0.5577487945556641
In grad_steps = 69, loss = 0.49962902069091797
In grad_steps = 70, loss = 0.9071068167686462
In grad_steps = 71, loss = 0.3787592649459839
In grad_steps = 72, loss = 0.9955275058746338
In grad_steps = 73, loss = 0.8349894285202026
In grad_steps = 74, loss = 0.6777908205986023
In grad_steps = 75, loss = 0.657042384147644
In grad_steps = 76, loss = 0.4118485748767853
In grad_steps = 77, loss = 0.3957788348197937
In grad_steps = 78, loss = 0.6409724354743958
In grad_steps = 79, loss = 0.5852693319320679
In grad_steps = 80, loss = 0.6441631317138672
In grad_steps = 81, loss = 0.23228684067726135
In grad_steps = 82, loss = 0.9797186851501465
In grad_steps = 83, loss = 0.8017899394035339
In grad_steps = 84, loss = 0.7640942931175232
In grad_steps = 85, loss = 0.7637942433357239
In grad_steps = 86, loss = 0.7849913835525513
In grad_steps = 87, loss = 0.5912313461303711
In grad_steps = 88, loss = 0.4670853614807129
In grad_steps = 89, loss = 0.5983561277389526
In grad_steps = 90, loss = 0.653485894203186
In grad_steps = 91, loss = 0.6931006908416748
In grad_steps = 92, loss = 0.6048936247825623
In grad_steps = 93, loss = 0.8020186424255371
In grad_steps = 94, loss = 0.6722298860549927
In grad_steps = 95, loss = 0.35362446308135986
In grad_steps = 96, loss = 0.6928917169570923
In grad_steps = 97, loss = 0.737926721572876
In grad_steps = 98, loss = 0.7426553964614868
In grad_steps = 99, loss = 0.5271915197372437
In grad_steps = 100, loss = 0.9081339240074158
In grad_steps = 101, loss = 1.0291202068328857
In grad_steps = 102, loss = 0.8303148150444031
In grad_steps = 103, loss = 0.4105417728424072
In grad_steps = 104, loss = 0.5553690791130066
In grad_steps = 105, loss = 0.5964683890342712
In grad_steps = 106, loss = 0.549597978591919
In grad_steps = 107, loss = 0.57102370262146
In grad_steps = 108, loss = 0.46061062812805176
In grad_steps = 109, loss = 0.5618451833724976
In grad_steps = 110, loss = 1.0210492610931396
In grad_steps = 111, loss = 0.36934250593185425
In grad_steps = 112, loss = 0.27855151891708374
In grad_steps = 113, loss = 0.5583307147026062
In grad_steps = 114, loss = 0.2509855329990387
In grad_steps = 115, loss = 0.9906387329101562
In grad_steps = 116, loss = 0.3025882840156555
In grad_steps = 117, loss = 0.499918669462204
In grad_steps = 118, loss = 0.50126713514328
In grad_steps = 119, loss = 0.8049106001853943
In grad_steps = 120, loss = 0.3310428857803345
In grad_steps = 121, loss = 0.39944958686828613
In grad_steps = 122, loss = 0.3295121192932129
In grad_steps = 123, loss = 0.8411680459976196
In grad_steps = 124, loss = 0.6862530708312988
In grad_steps = 125, loss = 0.7653160095214844
In grad_steps = 126, loss = 0.14754070341587067
In grad_steps = 127, loss = 0.5991440415382385
In grad_steps = 128, loss = 0.5770344138145447
In grad_steps = 129, loss = 0.5308532118797302
In grad_steps = 130, loss = 0.9970632791519165
In grad_steps = 131, loss = 0.6695039868354797
In grad_steps = 132, loss = 0.6146116256713867
In grad_steps = 133, loss = 0.5073345303535461
In grad_steps = 134, loss = 0.6974417567253113
In grad_steps = 135, loss = 0.6243772506713867
In grad_steps = 136, loss = 0.5394459366798401
In grad_steps = 137, loss = 0.3205251395702362
In grad_steps = 138, loss = 0.5459246635437012
In grad_steps = 139, loss = 0.34017837047576904
In grad_steps = 140, loss = 0.7828441858291626
In grad_steps = 141, loss = 0.4349815845489502
In grad_steps = 142, loss = 0.4421716034412384
In grad_steps = 143, loss = 0.2175816297531128
In grad_steps = 144, loss = 0.656177818775177
In grad_steps = 145, loss = 0.347103476524353
In grad_steps = 146, loss = 0.6211597919464111
In grad_steps = 147, loss = 0.2553254961967468
In grad_steps = 148, loss = 0.262870728969574
In grad_steps = 149, loss = 0.3927682936191559
In grad_steps = 150, loss = 0.9374128580093384
In grad_steps = 151, loss = 0.5409959554672241
In grad_steps = 152, loss = 1.4220144748687744
In grad_steps = 153, loss = 1.2580337524414062
In grad_steps = 154, loss = 0.8671143651008606
In grad_steps = 155, loss = 0.23319083452224731
In grad_steps = 156, loss = 0.5333366394042969
In grad_steps = 157, loss = 0.2019217312335968
In grad_steps = 158, loss = 0.2956278920173645
In grad_steps = 159, loss = 0.5680831670761108
In grad_steps = 160, loss = 0.29011785984039307
In grad_steps = 161, loss = 0.7574613094329834
In grad_steps = 162, loss = 0.5083731412887573
In grad_steps = 163, loss = 0.36985307931900024
In grad_steps = 164, loss = 0.7130562663078308
In grad_steps = 165, loss = 0.32896149158477783
In grad_steps = 166, loss = 0.20120076835155487
In grad_steps = 167, loss = 0.642490804195404
In grad_steps = 168, loss = 0.6389490962028503
In grad_steps = 169, loss = 0.15173253417015076
In grad_steps = 170, loss = 0.2558356523513794
In grad_steps = 171, loss = 0.4175174832344055
In grad_steps = 172, loss = 0.34613344073295593
In grad_steps = 173, loss = 0.14848218858242035
In grad_steps = 174, loss = 0.1293572038412094
In grad_steps = 175, loss = 0.7958725690841675
In grad_steps = 176, loss = 0.04566412419080734
In grad_steps = 177, loss = 0.1503426879644394
In grad_steps = 178, loss = 0.41408050060272217
In grad_steps = 179, loss = 1.508523941040039
In grad_steps = 180, loss = 0.1521998941898346
In grad_steps = 181, loss = 0.6049126982688904
In grad_steps = 182, loss = 0.8529411554336548
In grad_steps = 183, loss = 0.34796011447906494
In grad_steps = 184, loss = 0.6232079267501831
In grad_steps = 185, loss = 0.5834792852401733
In grad_steps = 186, loss = 0.371600866317749
In grad_steps = 187, loss = 0.47170642018318176
In grad_steps = 188, loss = 0.5889796614646912
In grad_steps = 189, loss = 0.3523303270339966
In grad_steps = 190, loss = 0.21638241410255432
In grad_steps = 191, loss = 0.3619515299797058
In grad_steps = 192, loss = 0.5945643186569214
In grad_steps = 193, loss = 0.4888809323310852
In grad_steps = 194, loss = 0.8538304567337036
In grad_steps = 195, loss = 0.692000150680542
In grad_steps = 196, loss = 0.5929362177848816
In grad_steps = 197, loss = 0.6006134748458862
In grad_steps = 198, loss = 0.5162156820297241
In grad_steps = 199, loss = 1.019153356552124
In grad_steps = 200, loss = 0.2754823863506317
In grad_steps = 201, loss = 0.8016669750213623
In grad_steps = 202, loss = 0.568930447101593
In grad_steps = 203, loss = 0.36312755942344666
In grad_steps = 204, loss = 0.540621280670166
In grad_steps = 205, loss = 0.5848086476325989
In grad_steps = 206, loss = 0.2972986698150635
In grad_steps = 207, loss = 0.5169010162353516
In grad_steps = 208, loss = 0.2563258409500122
In grad_steps = 209, loss = 0.15964217483997345
In grad_steps = 210, loss = 0.2134421169757843
In grad_steps = 211, loss = 0.9488811492919922
In grad_steps = 212, loss = 0.5014248490333557
In grad_steps = 213, loss = 0.36652204394340515
In grad_steps = 214, loss = 0.11870316416025162
In grad_steps = 215, loss = 0.23290887475013733
In grad_steps = 216, loss = 0.5720416903495789
In grad_steps = 217, loss = 0.4437824487686157
In grad_steps = 218, loss = 1.6683523654937744
In grad_steps = 219, loss = 0.4586502015590668
In grad_steps = 220, loss = 0.3872274160385132
In grad_steps = 221, loss = 1.0584036111831665
In grad_steps = 222, loss = 0.6400336027145386
In grad_steps = 223, loss = 0.5502612590789795
In grad_steps = 224, loss = 0.21271201968193054
In grad_steps = 225, loss = 0.09162906557321548
In grad_steps = 226, loss = 0.7479212284088135
In grad_steps = 227, loss = 0.47516292333602905
In grad_steps = 228, loss = 0.3784005641937256
In grad_steps = 229, loss = 0.31329259276390076
In grad_steps = 230, loss = 0.3375283479690552
In grad_steps = 231, loss = 0.7072767615318298
In grad_steps = 232, loss = 0.9958235025405884
In grad_steps = 233, loss = 0.3140031695365906
In grad_steps = 234, loss = 0.6764629483222961
In grad_steps = 235, loss = 0.7152321338653564
In grad_steps = 236, loss = 0.5994371771812439
In grad_steps = 237, loss = 0.24228180944919586
In grad_steps = 238, loss = 0.163058802485466
In grad_steps = 239, loss = 0.3093957006931305
In grad_steps = 240, loss = 0.2209424376487732
In grad_steps = 241, loss = 0.5392516255378723
In grad_steps = 242, loss = 0.33218491077423096
In grad_steps = 243, loss = 0.38650792837142944
In grad_steps = 244, loss = 1.0618908405303955
In grad_steps = 245, loss = 0.27781182527542114
In grad_steps = 246, loss = 0.2124875783920288
In grad_steps = 247, loss = 0.4462410807609558
In grad_steps = 248, loss = 1.1955780982971191
In grad_steps = 249, loss = 0.18936210870742798
In grad_steps = 250, loss = 0.6076136231422424
In grad_steps = 251, loss = 0.8857764601707458
In grad_steps = 252, loss = 0.18873324990272522
In grad_steps = 253, loss = 0.5798968076705933
In grad_steps = 254, loss = 0.535057544708252
In grad_steps = 255, loss = 0.547011137008667
In grad_steps = 256, loss = 1.0381799936294556
In grad_steps = 257, loss = 0.6168942451477051
In grad_steps = 258, loss = 0.800761342048645
In grad_steps = 259, loss = 0.6330732703208923
In grad_steps = 260, loss = 0.6541962027549744
In grad_steps = 261, loss = 1.017907738685608
In grad_steps = 262, loss = 0.5292704701423645
In grad_steps = 263, loss = 0.5641502141952515
In grad_steps = 264, loss = 0.4740832448005676
In grad_steps = 265, loss = 0.5995175838470459
In grad_steps = 266, loss = 0.9288544654846191
In grad_steps = 267, loss = 0.9180380702018738
In grad_steps = 268, loss = 0.5269211530685425
In grad_steps = 269, loss = 0.3008500933647156
In grad_steps = 270, loss = 0.479878693819046
In grad_steps = 271, loss = 0.6447194814682007
In grad_steps = 272, loss = 0.4831717014312744
In grad_steps = 273, loss = 0.5159613490104675
In grad_steps = 274, loss = 0.4148787260055542
In grad_steps = 275, loss = 0.58301842212677
In grad_steps = 276, loss = 0.49692627787590027
In grad_steps = 277, loss = 0.3866577744483948
In grad_steps = 278, loss = 0.4387749433517456
In grad_steps = 279, loss = 0.6301646828651428
In grad_steps = 280, loss = 0.6435364484786987
In grad_steps = 281, loss = 0.4724433422088623
In grad_steps = 282, loss = 0.5539675951004028
In grad_steps = 283, loss = 0.541881263256073
In grad_steps = 284, loss = 0.4951174259185791
In grad_steps = 285, loss = 0.32587966322898865
In grad_steps = 286, loss = 0.16237783432006836
In grad_steps = 287, loss = 1.0257641077041626
In grad_steps = 288, loss = 0.9827436804771423
In grad_steps = 289, loss = 0.44016093015670776
In grad_steps = 290, loss = 0.33831125497817993
In grad_steps = 291, loss = 0.8927868604660034
In grad_steps = 292, loss = 0.5266642570495605
In grad_steps = 293, loss = 0.7382930517196655
In grad_steps = 294, loss = 0.34474390745162964
In grad_steps = 295, loss = 0.8723783493041992
In grad_steps = 296, loss = 0.3897833824157715
In grad_steps = 297, loss = 0.3226689398288727
In grad_steps = 298, loss = 0.3537347614765167
In grad_steps = 299, loss = 0.46870723366737366
In grad_steps = 300, loss = 0.6534522771835327
In grad_steps = 301, loss = 0.5969955921173096
In grad_steps = 302, loss = 0.2944139838218689
In grad_steps = 303, loss = 0.5192591547966003
In grad_steps = 304, loss = 0.1442154496908188
In grad_steps = 305, loss = 0.23190703988075256
In grad_steps = 306, loss = 0.6515713334083557
In grad_steps = 307, loss = 0.40655606985092163
In grad_steps = 308, loss = 0.6673096418380737
In grad_steps = 309, loss = 0.6437772512435913
In grad_steps = 310, loss = 0.19814828038215637
In grad_steps = 311, loss = 1.333324670791626
In grad_steps = 312, loss = 0.17514918744564056
In grad_steps = 313, loss = 0.23950907588005066
In grad_steps = 314, loss = 0.5269864201545715
In grad_steps = 315, loss = 0.12611018121242523
In grad_steps = 316, loss = 1.2285581827163696
In grad_steps = 317, loss = 0.44697046279907227
In grad_steps = 318, loss = 0.3450854420661926
In grad_steps = 319, loss = 0.1556566059589386
In grad_steps = 320, loss = 0.6425673365592957
In grad_steps = 321, loss = 0.21401174366474152
In grad_steps = 322, loss = 0.9102581739425659
In grad_steps = 323, loss = 0.33104759454727173
In grad_steps = 324, loss = 0.36081427335739136
In grad_steps = 325, loss = 0.2259504199028015
In grad_steps = 326, loss = 0.43664437532424927
In grad_steps = 327, loss = 0.22613351047039032
In grad_steps = 328, loss = 0.20798806846141815
In grad_steps = 329, loss = 1.058569073677063
In grad_steps = 330, loss = 0.2164084017276764
In grad_steps = 331, loss = 0.08364763855934143
In grad_steps = 332, loss = 0.11194367706775665
In grad_steps = 333, loss = 0.19195705652236938
In grad_steps = 334, loss = 0.5606786012649536
In grad_steps = 335, loss = 0.3056851327419281
In grad_steps = 336, loss = 0.6342867612838745
In grad_steps = 337, loss = 0.13153471052646637
In grad_steps = 338, loss = 0.09019467234611511
In grad_steps = 339, loss = 0.5163378119468689
In grad_steps = 340, loss = 0.07558079808950424
In grad_steps = 341, loss = 0.7865858674049377
In grad_steps = 342, loss = 0.16334624588489532
In grad_steps = 343, loss = 0.29268813133239746
In grad_steps = 344, loss = 0.9367839097976685
In grad_steps = 345, loss = 0.579375147819519
In grad_steps = 346, loss = 0.3669484853744507
In grad_steps = 347, loss = 0.5243242979049683
In grad_steps = 348, loss = 0.6980493068695068
In grad_steps = 349, loss = 0.9756178259849548
In grad_steps = 350, loss = 0.2632257640361786
In grad_steps = 351, loss = 1.2609102725982666
In grad_steps = 352, loss = 0.6664845943450928
In grad_steps = 353, loss = 0.6106500029563904
In grad_steps = 354, loss = 0.6787928342819214
In grad_steps = 355, loss = 0.7754414081573486
In grad_steps = 356, loss = 0.30627018213272095
In grad_steps = 357, loss = 0.258993923664093
In grad_steps = 358, loss = 0.9336410760879517
In grad_steps = 359, loss = 0.5248789191246033
In grad_steps = 360, loss = 0.8885408639907837
In grad_steps = 361, loss = 0.7876420617103577
In grad_steps = 362, loss = 0.6964451670646667
In grad_steps = 363, loss = 0.9408711791038513
In grad_steps = 364, loss = 0.35438209772109985
In grad_steps = 365, loss = 0.711911678314209
In grad_steps = 366, loss = 0.783004105091095
In grad_steps = 367, loss = 0.5989750027656555
In grad_steps = 368, loss = 0.5235151648521423
In grad_steps = 369, loss = 0.4293537735939026
In grad_steps = 370, loss = 0.5807133316993713
In grad_steps = 371, loss = 0.5167938470840454
In grad_steps = 372, loss = 0.43582621216773987
In grad_steps = 373, loss = 0.5847537517547607
In grad_steps = 374, loss = 0.4192084074020386
In grad_steps = 375, loss = 0.5886582732200623
In grad_steps = 376, loss = 0.5045891404151917
In grad_steps = 377, loss = 0.3163626194000244
In grad_steps = 378, loss = 0.28933680057525635
In grad_steps = 379, loss = 0.2023792266845703
In grad_steps = 380, loss = 0.6138637661933899
In grad_steps = 381, loss = 0.779131293296814
In grad_steps = 382, loss = 0.3569231927394867
In grad_steps = 383, loss = 0.22099220752716064
In grad_steps = 384, loss = 0.10882021486759186
In grad_steps = 385, loss = 0.05524185299873352
In grad_steps = 386, loss = 0.09438139945268631
In grad_steps = 387, loss = 1.4978301525115967
In grad_steps = 388, loss = 0.6107285618782043
In grad_steps = 389, loss = 0.4043118953704834
In grad_steps = 390, loss = 0.22149842977523804
In grad_steps = 391, loss = 0.24517248570919037
In grad_steps = 392, loss = 0.47504204511642456
In grad_steps = 393, loss = 0.718486487865448
In grad_steps = 394, loss = 0.26467788219451904
In grad_steps = 395, loss = 1.139204978942871
In grad_steps = 396, loss = 0.39815783500671387
In grad_steps = 397, loss = 0.16880695521831512
In grad_steps = 398, loss = 0.7033054232597351
In grad_steps = 399, loss = 0.1261497139930725
In grad_steps = 400, loss = 1.3012003898620605
In grad_steps = 401, loss = 0.25229084491729736
In grad_steps = 402, loss = 0.20866940915584564
In grad_steps = 403, loss = 0.7765099406242371
In grad_steps = 404, loss = 0.22546623647212982
In grad_steps = 405, loss = 0.371945321559906
In grad_steps = 406, loss = 0.7948780059814453
In grad_steps = 407, loss = 0.33882102370262146
In grad_steps = 408, loss = 0.6484415531158447
In grad_steps = 409, loss = 0.528295636177063
In grad_steps = 410, loss = 0.3207785189151764
In grad_steps = 411, loss = 0.2991236746311188
In grad_steps = 412, loss = 0.7783210277557373
In grad_steps = 413, loss = 0.9075961112976074
In grad_steps = 414, loss = 1.1477073431015015
In grad_steps = 415, loss = 0.5922825932502747
In grad_steps = 416, loss = 1.0791319608688354
In grad_steps = 417, loss = 0.309611439704895
In grad_steps = 418, loss = 0.2257826328277588
In grad_steps = 419, loss = 0.2552242875099182
In grad_steps = 420, loss = 0.33189553022384644
In grad_steps = 421, loss = 0.18862015008926392
In grad_steps = 422, loss = 0.4434378147125244
In grad_steps = 423, loss = 0.8499269485473633
In grad_steps = 424, loss = 0.6713842153549194
In grad_steps = 425, loss = 0.16219750046730042
In grad_steps = 426, loss = 0.1247156411409378
In grad_steps = 427, loss = 0.3735138475894928
In grad_steps = 428, loss = 0.27582699060440063
In grad_steps = 429, loss = 0.7649073600769043
In grad_steps = 430, loss = 0.15763352811336517
In grad_steps = 431, loss = 1.3577055931091309
In grad_steps = 432, loss = 0.5896467566490173
In grad_steps = 433, loss = 1.2088744640350342
In grad_steps = 434, loss = 0.2584559917449951
In grad_steps = 435, loss = 0.17445501685142517
In grad_steps = 436, loss = 0.1981574445962906
In grad_steps = 437, loss = 0.4322744309902191
In grad_steps = 438, loss = 0.23362833261489868
In grad_steps = 439, loss = 0.1834246665239334
In grad_steps = 440, loss = 0.11284404993057251
In grad_steps = 441, loss = 0.08721020072698593
In grad_steps = 442, loss = 0.26420605182647705
In grad_steps = 443, loss = 0.35753610730171204
In grad_steps = 444, loss = 0.7274148464202881
In grad_steps = 445, loss = 0.15233391523361206
In grad_steps = 446, loss = 0.12210262566804886
In grad_steps = 447, loss = 0.1567961424589157
In grad_steps = 448, loss = 0.24995706975460052
In grad_steps = 449, loss = 0.10044492781162262
In grad_steps = 450, loss = 0.08126865327358246
In grad_steps = 451, loss = 0.29585564136505127
In grad_steps = 452, loss = 0.1519578993320465
In grad_steps = 453, loss = 0.1729414314031601
In grad_steps = 454, loss = 0.2902069091796875
In grad_steps = 455, loss = 0.4146394431591034
In grad_steps = 456, loss = 1.0271580219268799
In grad_steps = 457, loss = 0.019101206213235855
In grad_steps = 458, loss = 0.2643020749092102
In grad_steps = 459, loss = 0.24820762872695923
In grad_steps = 460, loss = 0.027539078146219254
In grad_steps = 461, loss = 0.888412594795227
In grad_steps = 462, loss = 0.2888021767139435
In grad_steps = 463, loss = 0.34852683544158936
In grad_steps = 464, loss = 0.4953397810459137
In grad_steps = 465, loss = 0.02880971133708954
In grad_steps = 466, loss = 0.24255356192588806
In grad_steps = 467, loss = 0.5447148084640503
In grad_steps = 468, loss = 0.47892653942108154
In grad_steps = 469, loss = 1.7693753242492676
In grad_steps = 470, loss = 0.14893615245819092
In grad_steps = 471, loss = 0.914892315864563
In grad_steps = 472, loss = 0.5598142147064209
In grad_steps = 473, loss = 0.7381255626678467
In grad_steps = 474, loss = 0.25360405445098877
In grad_steps = 475, loss = 0.10070137679576874
In grad_steps = 476, loss = 0.14156362414360046
In grad_steps = 477, loss = 0.7114701271057129
In grad_steps = 478, loss = 0.22563005983829498
In grad_steps = 479, loss = 0.26639771461486816
In grad_steps = 480, loss = 0.20416505634784698
In grad_steps = 481, loss = 0.2164369523525238
In grad_steps = 482, loss = 0.32275906205177307
In grad_steps = 483, loss = 0.9343520402908325
In grad_steps = 484, loss = 0.17723128199577332
In grad_steps = 485, loss = 0.6108601093292236
In grad_steps = 486, loss = 0.26362496614456177
In grad_steps = 487, loss = 0.48897358775138855
In grad_steps = 488, loss = 0.1883983612060547
In grad_steps = 489, loss = 0.3689199686050415
In grad_steps = 490, loss = 0.22852018475532532
In grad_steps = 491, loss = 0.14019185304641724
In grad_steps = 492, loss = 0.25436311960220337
In grad_steps = 493, loss = 0.5930376648902893
In grad_steps = 494, loss = 0.2925364673137665
In grad_steps = 495, loss = 0.17472082376480103
In grad_steps = 496, loss = 0.47023746371269226
In grad_steps = 497, loss = 0.782679557800293
In grad_steps = 498, loss = 0.4398494064807892
In grad_steps = 499, loss = 0.6215221285820007
In grad_steps = 500, loss = 0.06239340454339981
In grad_steps = 501, loss = 0.05608304962515831
In grad_steps = 502, loss = 1.1795939207077026
In grad_steps = 503, loss = 0.09241072088479996
In grad_steps = 504, loss = 0.6393230557441711
In grad_steps = 505, loss = 0.04789929464459419
In grad_steps = 506, loss = 0.3205428719520569
In grad_steps = 507, loss = 0.12346060574054718
In grad_steps = 508, loss = 0.3903366029262543
In grad_steps = 509, loss = 0.7610399127006531
In grad_steps = 510, loss = 0.5032359957695007
In grad_steps = 511, loss = 0.27659282088279724
In grad_steps = 512, loss = 0.34353235363960266
In grad_steps = 513, loss = 0.12832748889923096
In grad_steps = 514, loss = 0.3572346568107605
In grad_steps = 515, loss = 0.07894790172576904
In grad_steps = 516, loss = 0.1893726885318756
In grad_steps = 517, loss = 0.3654877841472626
In grad_steps = 518, loss = 0.10754556953907013
In grad_steps = 519, loss = 0.14805495738983154
In grad_steps = 520, loss = 0.6310587525367737
In grad_steps = 521, loss = 0.3701888918876648
In grad_steps = 522, loss = 0.7191238403320312
In grad_steps = 523, loss = 0.6882773637771606
In grad_steps = 524, loss = 0.6423603892326355
In grad_steps = 525, loss = 0.4613707363605499
In grad_steps = 526, loss = 0.54146409034729
In grad_steps = 527, loss = 1.0316689014434814
In grad_steps = 528, loss = 0.5049065351486206
In grad_steps = 529, loss = 0.2710893750190735
In grad_steps = 530, loss = 0.10892439633607864
In grad_steps = 531, loss = 0.11678729951381683
In grad_steps = 532, loss = 0.5933374166488647
In grad_steps = 533, loss = 0.5664794445037842
In grad_steps = 534, loss = 0.35537028312683105
In grad_steps = 535, loss = 1.3871760368347168
In grad_steps = 536, loss = 0.46764057874679565
In grad_steps = 537, loss = 0.5163663625717163
In grad_steps = 538, loss = 0.6114006042480469
In grad_steps = 539, loss = 0.6082684993743896
In grad_steps = 540, loss = 0.26984718441963196
In grad_steps = 541, loss = 0.37044817209243774
In grad_steps = 542, loss = 0.344560444355011
In grad_steps = 543, loss = 0.3446781635284424
In grad_steps = 544, loss = 0.5712835788726807
In grad_steps = 545, loss = 0.6283395290374756
In grad_steps = 546, loss = 0.41111844778060913
In grad_steps = 547, loss = 0.5540144443511963
In grad_steps = 548, loss = 0.22324344515800476
In grad_steps = 549, loss = 0.6170412302017212
In grad_steps = 550, loss = 0.24601393938064575
In grad_steps = 551, loss = 0.29328006505966187
In grad_steps = 552, loss = 0.4143819808959961
In grad_steps = 553, loss = 0.35280296206474304
In grad_steps = 554, loss = 0.2886704206466675
In grad_steps = 555, loss = 0.1659766137599945
In grad_steps = 556, loss = 0.5238034725189209
In grad_steps = 557, loss = 0.6794351935386658
In grad_steps = 558, loss = 0.2610650062561035
In grad_steps = 559, loss = 0.5678068399429321
In grad_steps = 560, loss = 0.39933064579963684
In grad_steps = 561, loss = 0.21183909475803375
In grad_steps = 562, loss = 0.18943119049072266
In grad_steps = 563, loss = 0.5355454683303833
In grad_steps = 564, loss = 0.2758373022079468
In grad_steps = 565, loss = 0.15471772849559784
In grad_steps = 566, loss = 0.4898850619792938
In grad_steps = 567, loss = 1.2663781642913818
In grad_steps = 568, loss = 0.36449098587036133
In grad_steps = 569, loss = 0.33522844314575195
In grad_steps = 570, loss = 0.5437055826187134
In grad_steps = 571, loss = 0.5260172486305237
In grad_steps = 572, loss = 0.12035422027111053
In grad_steps = 573, loss = 0.09910716116428375
In grad_steps = 574, loss = 0.20370636880397797
In grad_steps = 575, loss = 0.09406156837940216
In grad_steps = 576, loss = 0.6616852879524231
In grad_steps = 577, loss = 0.09272781759500504
In grad_steps = 578, loss = 1.5498566627502441
In grad_steps = 579, loss = 0.1718643754720688
In grad_steps = 580, loss = 0.35922470688819885
In grad_steps = 581, loss = 0.6882739067077637
In grad_steps = 582, loss = 0.7060847878456116
In grad_steps = 583, loss = 0.28231436014175415
In grad_steps = 584, loss = 0.38989341259002686
In grad_steps = 585, loss = 0.6801738142967224
In grad_steps = 586, loss = 0.21119898557662964
In grad_steps = 587, loss = 0.20360717177391052
In grad_steps = 588, loss = 0.3315814435482025
In grad_steps = 589, loss = 0.3261006474494934
In grad_steps = 590, loss = 0.14940288662910461
In grad_steps = 591, loss = 0.2481301724910736
In grad_steps = 592, loss = 0.31209883093833923
In grad_steps = 593, loss = 0.33621424436569214
In grad_steps = 594, loss = 0.30485957860946655
In grad_steps = 595, loss = 0.7689273953437805
In grad_steps = 596, loss = 0.6328080892562866
In grad_steps = 597, loss = 0.3649553060531616
In grad_steps = 598, loss = 0.2391766905784607
In grad_steps = 599, loss = 0.7435442805290222
In grad_steps = 600, loss = 0.9459838271141052
In grad_steps = 601, loss = 0.34890127182006836
In grad_steps = 602, loss = 0.6246941685676575
In grad_steps = 603, loss = 0.432909220457077
In grad_steps = 604, loss = 0.8646392226219177
In grad_steps = 605, loss = 1.3023719787597656
In grad_steps = 606, loss = 0.23452800512313843
In grad_steps = 607, loss = 0.1822919398546219
In grad_steps = 608, loss = 0.5016602873802185
In grad_steps = 609, loss = 0.13974657654762268
In grad_steps = 610, loss = 0.17896825075149536
In grad_steps = 611, loss = 0.2842840254306793
In grad_steps = 612, loss = 0.32499009370803833
In grad_steps = 613, loss = 0.2050599753856659
In grad_steps = 614, loss = 0.22209113836288452
In grad_steps = 615, loss = 0.7824246883392334
In grad_steps = 616, loss = 0.2880123555660248
In grad_steps = 617, loss = 0.20830802619457245
In grad_steps = 618, loss = 0.4961435794830322
In grad_steps = 619, loss = 0.12191204726696014
In grad_steps = 620, loss = 0.2579597532749176
In grad_steps = 621, loss = 0.1625659167766571
In grad_steps = 622, loss = 0.08695099502801895
In grad_steps = 623, loss = 0.12506181001663208
In grad_steps = 624, loss = 0.040524404495954514
In grad_steps = 625, loss = 0.10145548731088638
In grad_steps = 626, loss = 0.32508957386016846
In grad_steps = 627, loss = 0.3151484727859497
In grad_steps = 628, loss = 0.2550376057624817
In grad_steps = 629, loss = 0.709395170211792
In grad_steps = 630, loss = 0.32150402665138245
In grad_steps = 631, loss = 0.6108168363571167
In grad_steps = 632, loss = 0.4897855222225189
In grad_steps = 633, loss = 0.46174171566963196
In grad_steps = 634, loss = 0.21703508496284485
In grad_steps = 635, loss = 1.0579557418823242
In grad_steps = 636, loss = 0.3361019492149353
In grad_steps = 637, loss = 0.24848493933677673
In grad_steps = 638, loss = 0.14848637580871582
In grad_steps = 639, loss = 1.0088783502578735
In grad_steps = 640, loss = 1.4710781574249268
In grad_steps = 641, loss = 0.694474458694458
In grad_steps = 642, loss = 0.14673994481563568
In grad_steps = 643, loss = 0.09301838278770447
In grad_steps = 644, loss = 0.512535572052002
In grad_steps = 645, loss = 0.5977663397789001
In grad_steps = 646, loss = 0.5476505756378174
In grad_steps = 647, loss = 0.45729953050613403
In grad_steps = 648, loss = 0.2028067260980606
In grad_steps = 649, loss = 0.3391503095626831
In grad_steps = 650, loss = 0.567634642124176
In grad_steps = 651, loss = 0.5604157447814941
In grad_steps = 652, loss = 0.3389018774032593
In grad_steps = 653, loss = 0.30743664503097534
In grad_steps = 654, loss = 0.6230451464653015
In grad_steps = 655, loss = 0.4234205186367035
In grad_steps = 656, loss = 0.4830908477306366
In grad_steps = 657, loss = 0.7345750331878662
In grad_steps = 658, loss = 0.3020205497741699
In grad_steps = 659, loss = 0.5441539287567139
In grad_steps = 660, loss = 0.8490096926689148
In grad_steps = 661, loss = 0.8477118015289307
In grad_steps = 662, loss = 0.4540629982948303
In grad_steps = 663, loss = 0.27913421392440796
In grad_steps = 664, loss = 0.865756094455719
In grad_steps = 665, loss = 0.6941948533058167
In grad_steps = 666, loss = 0.176320418715477
In grad_steps = 667, loss = 0.4516676366329193
In grad_steps = 668, loss = 0.5866936445236206
In grad_steps = 669, loss = 0.2977420687675476
In grad_steps = 670, loss = 0.37239670753479004
In grad_steps = 671, loss = 0.5901598930358887
In grad_steps = 672, loss = 0.45425400137901306
In grad_steps = 673, loss = 0.770206093788147
In grad_steps = 674, loss = 0.2163369059562683
In grad_steps = 675, loss = 0.9674567580223083
In grad_steps = 676, loss = 0.6171137690544128
In grad_steps = 677, loss = 0.2622397840023041
In grad_steps = 678, loss = 0.25755760073661804
In grad_steps = 679, loss = 0.2790212035179138
In grad_steps = 680, loss = 0.35870498418807983
In grad_steps = 681, loss = 0.28350597620010376
In grad_steps = 682, loss = 0.9029174447059631
In grad_steps = 683, loss = 0.19234123826026917
In grad_steps = 684, loss = 0.4637317359447479
In grad_steps = 685, loss = 0.16079255938529968
In grad_steps = 686, loss = 0.20120251178741455
In grad_steps = 687, loss = 0.4134112596511841
In grad_steps = 688, loss = 0.17636916041374207
In grad_steps = 689, loss = 0.35314518213272095
In grad_steps = 690, loss = 0.13825683295726776
In grad_steps = 691, loss = 0.7955350279808044
In grad_steps = 692, loss = 0.0597512423992157
In grad_steps = 693, loss = 0.9136043190956116
In grad_steps = 694, loss = 0.46355903148651123
In grad_steps = 695, loss = 0.12718498706817627
In grad_steps = 696, loss = 0.12378297746181488
In grad_steps = 697, loss = 0.23236887156963348
In grad_steps = 698, loss = 0.13801446557044983
In grad_steps = 699, loss = 0.2229427546262741
In grad_steps = 700, loss = 0.1598663479089737
In grad_steps = 701, loss = 0.8564620614051819
In grad_steps = 702, loss = 0.038292720913887024
In grad_steps = 703, loss = 0.10155186057090759
In grad_steps = 704, loss = 0.0751454308629036
In grad_steps = 705, loss = 1.5354708433151245
In grad_steps = 706, loss = 0.21788132190704346
In grad_steps = 707, loss = 0.18385891616344452
In grad_steps = 708, loss = 0.08030353486537933
In grad_steps = 709, loss = 0.5116521120071411
In grad_steps = 710, loss = 0.13391579687595367
In grad_steps = 711, loss = 0.25682687759399414
In grad_steps = 712, loss = 0.0381670817732811
In grad_steps = 713, loss = 2.9755446910858154
In grad_steps = 714, loss = 0.05679648369550705
In grad_steps = 715, loss = 0.6925296187400818
In grad_steps = 716, loss = 0.18691110610961914
In grad_steps = 717, loss = 0.23958837985992432
In grad_steps = 718, loss = 0.05301176756620407
In grad_steps = 719, loss = 0.39310911297798157
In grad_steps = 720, loss = 2.3408029079437256
In grad_steps = 721, loss = 0.5316534042358398
In grad_steps = 722, loss = 0.22003832459449768
In grad_steps = 723, loss = 0.7194730043411255
In grad_steps = 724, loss = 0.12773825228214264
In grad_steps = 725, loss = 0.22073128819465637
In grad_steps = 726, loss = 0.35008981823921204
In grad_steps = 727, loss = 0.4053741693496704
In grad_steps = 728, loss = 0.3378932476043701
In grad_steps = 729, loss = 0.23739749193191528
In grad_steps = 730, loss = 0.21468813717365265
In grad_steps = 731, loss = 0.2781866788864136
In grad_steps = 732, loss = 0.1927800476551056
In grad_steps = 733, loss = 0.5020753741264343
In grad_steps = 734, loss = 0.34615999460220337
In grad_steps = 735, loss = 0.453685462474823
In grad_steps = 736, loss = 1.6816396713256836
In grad_steps = 737, loss = 0.41172608733177185
In grad_steps = 738, loss = 0.46350374817848206
In grad_steps = 739, loss = 0.1778070628643036
In grad_steps = 740, loss = 0.2563301622867584
In grad_steps = 741, loss = 0.40107661485671997
In grad_steps = 742, loss = 0.3824973702430725
In grad_steps = 743, loss = 0.2130204439163208
In grad_steps = 744, loss = 0.40309926867485046
In grad_steps = 745, loss = 0.02284686267375946
In grad_steps = 746, loss = 0.611921489238739
In grad_steps = 747, loss = 0.24372680485248566
In grad_steps = 748, loss = 0.17529046535491943
In grad_steps = 749, loss = 0.5200588703155518
In grad_steps = 750, loss = 1.2586379051208496
In grad_steps = 751, loss = 0.35621678829193115
In grad_steps = 752, loss = 0.4259253740310669
In grad_steps = 753, loss = 0.14464467763900757
In grad_steps = 754, loss = 0.5491361021995544
In grad_steps = 755, loss = 0.09695855528116226
In grad_steps = 756, loss = 0.08139579743146896
In grad_steps = 757, loss = 0.10269837081432343
In grad_steps = 758, loss = 0.2138843983411789
In grad_steps = 759, loss = 0.5751880407333374
In grad_steps = 760, loss = 0.10186350345611572
In grad_steps = 761, loss = 0.08101361244916916
In grad_steps = 762, loss = 0.10881958156824112
In grad_steps = 763, loss = 0.34877273440361023
In grad_steps = 764, loss = 0.23072949051856995
In grad_steps = 765, loss = 0.05196502059698105
In grad_steps = 766, loss = 0.1344195157289505
In grad_steps = 767, loss = 0.5236550569534302
In grad_steps = 768, loss = 0.012382841669023037
In grad_steps = 769, loss = 0.9410621523857117
In grad_steps = 770, loss = 0.3754202425479889
In grad_steps = 771, loss = 0.3877473473548889
In grad_steps = 772, loss = 0.874030351638794
In grad_steps = 773, loss = 0.44041907787323
In grad_steps = 774, loss = 0.045314714312553406
In grad_steps = 775, loss = 0.04254169017076492
In grad_steps = 776, loss = 0.08098940551280975
In grad_steps = 777, loss = 0.7308309674263
In grad_steps = 778, loss = 0.5643815994262695
In grad_steps = 779, loss = 0.8152830600738525
In grad_steps = 780, loss = 0.32641661167144775
In grad_steps = 781, loss = 0.4465169608592987
In grad_steps = 782, loss = 0.3986656367778778
In grad_steps = 783, loss = 0.40132078528404236
In grad_steps = 784, loss = 0.5991307497024536
In grad_steps = 785, loss = 0.5896097421646118
In grad_steps = 786, loss = 0.516336977481842
In grad_steps = 787, loss = 0.2031041830778122
In grad_steps = 788, loss = 0.3856123387813568
In grad_steps = 789, loss = 0.349552184343338
In grad_steps = 790, loss = 0.8258346915245056
In grad_steps = 791, loss = 0.3956906795501709
In grad_steps = 792, loss = 0.3633802533149719
In grad_steps = 793, loss = 0.21212270855903625
In grad_steps = 794, loss = 0.16013203561306
In grad_steps = 795, loss = 0.43830108642578125
In grad_steps = 796, loss = 0.9421781301498413
In grad_steps = 797, loss = 0.13097377121448517
In grad_steps = 798, loss = 0.17138724029064178
In grad_steps = 799, loss = 0.32085415720939636
In grad_steps = 800, loss = 0.1941995918750763
In grad_steps = 801, loss = 0.10349231213331223
In grad_steps = 802, loss = 0.05986184999346733
In grad_steps = 803, loss = 0.2113630175590515
In grad_steps = 804, loss = 0.057717639952898026
In grad_steps = 805, loss = 0.14069709181785583
In grad_steps = 806, loss = 1.1915380954742432
In grad_steps = 807, loss = 0.27191096544265747
In grad_steps = 808, loss = 0.13092119991779327
In grad_steps = 809, loss = 0.7948324084281921
In grad_steps = 810, loss = 0.09506959468126297
In grad_steps = 811, loss = 0.0349307581782341
In grad_steps = 812, loss = 0.5875335335731506
In grad_steps = 813, loss = 0.07504851371049881
In grad_steps = 814, loss = 0.09594941884279251
In grad_steps = 815, loss = 0.045818910002708435
In grad_steps = 816, loss = 0.056282177567481995
In grad_steps = 817, loss = 0.4577699899673462
In grad_steps = 818, loss = 0.41387221217155457
In grad_steps = 819, loss = 0.8566334843635559
In grad_steps = 820, loss = 0.10224587470293045
In grad_steps = 821, loss = 0.3873157799243927
In grad_steps = 822, loss = 0.9955036640167236
In grad_steps = 823, loss = 0.5067955851554871
In grad_steps = 824, loss = 0.19106769561767578
In grad_steps = 825, loss = 0.1147564947605133
In grad_steps = 826, loss = 0.12228008359670639
In grad_steps = 827, loss = 0.22949454188346863
In grad_steps = 828, loss = 0.4993493854999542
In grad_steps = 829, loss = 0.1545957326889038
In grad_steps = 830, loss = 0.707106351852417
In grad_steps = 831, loss = 0.05125473439693451
In grad_steps = 832, loss = 0.20784421265125275
In grad_steps = 833, loss = 0.6448584198951721
In grad_steps = 834, loss = 0.215696781873703
In grad_steps = 835, loss = 0.27159398794174194
In grad_steps = 836, loss = 0.5141160488128662
In grad_steps = 837, loss = 0.4236353039741516
In grad_steps = 838, loss = 1.2789303064346313
In grad_steps = 839, loss = 0.2411142885684967
In grad_steps = 840, loss = 0.08793644607067108
In grad_steps = 841, loss = 0.12901677191257477
In grad_steps = 842, loss = 0.2700832486152649
In grad_steps = 843, loss = 0.7136238217353821
In grad_steps = 844, loss = 0.544569730758667
In grad_steps = 845, loss = 0.49979397654533386
In grad_steps = 846, loss = 0.5328936576843262
In grad_steps = 847, loss = 0.38447123765945435
In grad_steps = 848, loss = 0.10723009705543518
In grad_steps = 849, loss = 0.1796991378068924
In grad_steps = 850, loss = 0.18617995083332062
In grad_steps = 851, loss = 0.27544406056404114
In grad_steps = 852, loss = 0.5900949835777283
In grad_steps = 853, loss = 1.1459077596664429
In grad_steps = 854, loss = 0.21116472780704498
In grad_steps = 855, loss = 0.3938558101654053
In grad_steps = 856, loss = 0.16226288676261902
In grad_steps = 857, loss = 0.3067127466201782
In grad_steps = 858, loss = 0.4288997948169708
In grad_steps = 859, loss = 0.29792681336402893
In grad_steps = 860, loss = 0.2393072545528412
In grad_steps = 861, loss = 0.8707536458969116
In grad_steps = 862, loss = 0.07882462441921234
In grad_steps = 863, loss = 0.311713844537735
In grad_steps = 864, loss = 0.33277639746665955
In grad_steps = 865, loss = 0.08701787889003754
In grad_steps = 866, loss = 0.4723628759384155
In grad_steps = 867, loss = 0.1103060245513916
In grad_steps = 868, loss = 0.24546512961387634
In grad_steps = 869, loss = 0.11329546570777893
In grad_steps = 870, loss = 0.4753558933734894
In grad_steps = 871, loss = 0.06853248178958893
In grad_steps = 872, loss = 0.3452705144882202
In grad_steps = 873, loss = 0.1975668966770172
In grad_steps = 874, loss = 0.5928319692611694
In grad_steps = 875, loss = 0.368042916059494
In grad_steps = 876, loss = 0.08657325059175491
In grad_steps = 877, loss = 0.043198615312576294
In grad_steps = 878, loss = 0.9152118563652039
In grad_steps = 879, loss = 0.23978647589683533
In grad_steps = 880, loss = 0.19925254583358765
In grad_steps = 881, loss = 0.9126970767974854
In grad_steps = 882, loss = 0.08689282834529877
In grad_steps = 883, loss = 0.7482293248176575
In grad_steps = 884, loss = 0.7614208459854126
In grad_steps = 885, loss = 0.1835792511701584
In grad_steps = 886, loss = 0.33500516414642334
In grad_steps = 887, loss = 0.6646140217781067
In grad_steps = 888, loss = 0.470364511013031
In grad_steps = 889, loss = 0.2518610656261444
In grad_steps = 890, loss = 0.15451468527317047
In grad_steps = 891, loss = 0.7912039756774902
In grad_steps = 892, loss = 0.4426388740539551
In grad_steps = 893, loss = 0.8612344861030579
In grad_steps = 894, loss = 0.10052633285522461
In grad_steps = 895, loss = 0.09466133266687393
In grad_steps = 896, loss = 0.5535805225372314
In grad_steps = 897, loss = 0.19660566747188568
In grad_steps = 898, loss = 0.41078585386276245
In grad_steps = 899, loss = 0.49311602115631104
In grad_steps = 900, loss = 0.5221596360206604
In grad_steps = 901, loss = 0.5064182877540588
In grad_steps = 902, loss = 0.14495056867599487
In grad_steps = 903, loss = 0.6487241387367249
In grad_steps = 904, loss = 0.5346975922584534
In grad_steps = 905, loss = 0.10570494085550308
In grad_steps = 906, loss = 0.6062518358230591
In grad_steps = 907, loss = 0.2699843943119049
In grad_steps = 908, loss = 0.1557239294052124
In grad_steps = 909, loss = 0.805662989616394
In grad_steps = 910, loss = 0.18791095912456512
In grad_steps = 911, loss = 0.47365131974220276
In grad_steps = 912, loss = 0.2564556896686554
In grad_steps = 913, loss = 0.6422709822654724
In grad_steps = 914, loss = 0.4226318299770355
In grad_steps = 915, loss = 0.730778157711029
In grad_steps = 916, loss = 0.20992013812065125
In grad_steps = 917, loss = 0.47255802154541016
In grad_steps = 918, loss = 0.47199803590774536
In grad_steps = 919, loss = 0.145311176776886
In grad_steps = 920, loss = 0.4267754852771759
In grad_steps = 921, loss = 0.7637579441070557
In grad_steps = 922, loss = 0.42685413360595703
In grad_steps = 923, loss = 0.19817492365837097
In grad_steps = 924, loss = 0.14503589272499084
In grad_steps = 925, loss = 0.20749592781066895
In grad_steps = 926, loss = 0.4643510580062866
In grad_steps = 927, loss = 0.051346488296985626
In grad_steps = 928, loss = 0.7370455265045166
In grad_steps = 929, loss = 0.07959887385368347
In grad_steps = 930, loss = 0.7560826539993286
In grad_steps = 931, loss = 0.10485723614692688
In grad_steps = 932, loss = 0.06250762939453125
In grad_steps = 933, loss = 0.47905653715133667
In grad_steps = 934, loss = 0.2716447114944458
In grad_steps = 935, loss = 1.1086106300354004
In grad_steps = 936, loss = 0.45781004428863525
In grad_steps = 937, loss = 0.0884423702955246
In grad_steps = 938, loss = 0.5633671283721924
In grad_steps = 939, loss = 0.26528072357177734
In grad_steps = 940, loss = 0.17036932706832886
In grad_steps = 941, loss = 0.2566312551498413
In grad_steps = 942, loss = 0.2426900416612625
In grad_steps = 943, loss = 1.206315040588379
In grad_steps = 944, loss = 0.5023860931396484
In grad_steps = 945, loss = 0.18071188032627106
In grad_steps = 946, loss = 0.7742279171943665
In grad_steps = 947, loss = 0.36601993441581726
In grad_steps = 948, loss = 0.42429956793785095
In grad_steps = 949, loss = 0.5178467035293579
In grad_steps = 950, loss = 0.6950017809867859
In grad_steps = 951, loss = 1.2073194980621338
In grad_steps = 952, loss = 0.09772699326276779
In grad_steps = 953, loss = 0.40794509649276733
In grad_steps = 954, loss = 0.1931752860546112
In grad_steps = 955, loss = 0.38084548711776733
In grad_steps = 956, loss = 0.25805509090423584
In grad_steps = 957, loss = 0.4222865700721741
In grad_steps = 958, loss = 0.42402511835098267
In grad_steps = 959, loss = 1.2574493885040283
In grad_steps = 960, loss = 0.26730385422706604
In grad_steps = 961, loss = 0.4545668959617615
In grad_steps = 962, loss = 0.1573905050754547
In grad_steps = 963, loss = 0.2372099906206131
In grad_steps = 964, loss = 0.26731961965560913
In grad_steps = 965, loss = 0.31214669346809387
In grad_steps = 966, loss = 0.3429480195045471
In grad_steps = 967, loss = 0.5737205743789673
In grad_steps = 968, loss = 0.49688294529914856
In grad_steps = 969, loss = 0.5472148060798645
In grad_steps = 970, loss = 0.11373917013406754
In grad_steps = 971, loss = 0.4985564351081848
In grad_steps = 972, loss = 0.5192366242408752
In grad_steps = 973, loss = 0.15238073468208313
In grad_steps = 974, loss = 0.33279868960380554
In grad_steps = 975, loss = 0.16221845149993896
In grad_steps = 976, loss = 0.36250248551368713
In grad_steps = 977, loss = 0.21507856249809265
In grad_steps = 978, loss = 0.7870128154754639
In grad_steps = 979, loss = 0.8420172929763794
In grad_steps = 980, loss = 0.8592041730880737
In grad_steps = 981, loss = 0.343069463968277
In grad_steps = 982, loss = 0.5681837797164917
In grad_steps = 983, loss = 0.4179472327232361
In grad_steps = 984, loss = 1.050748586654663
In grad_steps = 985, loss = 0.34763094782829285
In grad_steps = 986, loss = 0.14338576793670654
In grad_steps = 987, loss = 0.7053309082984924
In grad_steps = 988, loss = 0.596129298210144
In grad_steps = 989, loss = 0.7320604920387268
In grad_steps = 990, loss = 0.7677275538444519
In grad_steps = 991, loss = 0.658452570438385
In grad_steps = 992, loss = 0.27821749448776245
In grad_steps = 993, loss = 0.6108209490776062
In grad_steps = 994, loss = 0.43795204162597656
In grad_steps = 995, loss = 0.18554356694221497
In grad_steps = 996, loss = 0.7356023788452148
In grad_steps = 997, loss = 0.23009756207466125
In grad_steps = 998, loss = 0.3990320563316345
In grad_steps = 999, loss = 0.09368180483579636
In grad_steps = 1000, loss = 0.1058194488286972
In grad_steps = 1001, loss = 0.705213189125061
In grad_steps = 1002, loss = 0.24044103920459747
In grad_steps = 1003, loss = 0.23918834328651428
In grad_steps = 1004, loss = 0.5003384947776794
In grad_steps = 1005, loss = 0.7060233950614929
In grad_steps = 1006, loss = 0.17587299644947052
In grad_steps = 1007, loss = 0.40898749232292175
In grad_steps = 1008, loss = 0.2977749705314636
In grad_steps = 1009, loss = 1.005730390548706
In grad_steps = 1010, loss = 0.7354128360748291
In grad_steps = 1011, loss = 0.1277649849653244
In grad_steps = 1012, loss = 0.11653178930282593
In grad_steps = 1013, loss = 0.5230605602264404
In grad_steps = 1014, loss = 0.2572195827960968
In grad_steps = 1015, loss = 0.3421575129032135
In grad_steps = 1016, loss = 0.11258397996425629
In grad_steps = 1017, loss = 0.10113688558340073
In grad_steps = 1018, loss = 0.3310105800628662
In grad_steps = 1019, loss = 0.8085590600967407
In grad_steps = 1020, loss = 1.7134180068969727
In grad_steps = 1021, loss = 0.3978484869003296
In grad_steps = 1022, loss = 0.3961699306964874
In grad_steps = 1023, loss = 0.09709551185369492
In grad_steps = 1024, loss = 0.1946527361869812
In grad_steps = 1025, loss = 0.7231194972991943
In grad_steps = 1026, loss = 0.19764700531959534
In grad_steps = 1027, loss = 0.7326550483703613
In grad_steps = 1028, loss = 0.1549663096666336
In grad_steps = 1029, loss = 0.13710132241249084
In grad_steps = 1030, loss = 0.16076159477233887
In grad_steps = 1031, loss = 0.3340955376625061
In grad_steps = 1032, loss = 0.6139427423477173
In grad_steps = 1033, loss = 0.43696361780166626
In grad_steps = 1034, loss = 0.22536113858222961
In grad_steps = 1035, loss = 0.5348133444786072
In grad_steps = 1036, loss = 0.7279071807861328
In grad_steps = 1037, loss = 0.1784101128578186
In grad_steps = 1038, loss = 0.5149244070053101
In grad_steps = 1039, loss = 0.5678560733795166
In grad_steps = 1040, loss = 0.23442396521568298
In grad_steps = 1041, loss = 0.6869738698005676
In grad_steps = 1042, loss = 0.48129117488861084
In grad_steps = 1043, loss = 0.6221194267272949
In grad_steps = 1044, loss = 0.39468908309936523
In grad_steps = 1045, loss = 0.6470953226089478
In grad_steps = 1046, loss = 0.0933324471116066
In grad_steps = 1047, loss = 0.13360679149627686
In grad_steps = 1048, loss = 0.4048115909099579
In grad_steps = 1049, loss = 0.15722689032554626
In grad_steps = 1050, loss = 1.2712697982788086
In grad_steps = 1051, loss = 0.10510839521884918
In grad_steps = 1052, loss = 0.18582940101623535
In grad_steps = 1053, loss = 0.5999561548233032
In grad_steps = 1054, loss = 0.1014198437333107
In grad_steps = 1055, loss = 0.286449134349823
In grad_steps = 1056, loss = 0.9110679626464844
In grad_steps = 1057, loss = 0.3579825758934021
In grad_steps = 1058, loss = 0.3668784499168396
In grad_steps = 1059, loss = 0.19329063594341278
In grad_steps = 1060, loss = 0.23148752748966217
In grad_steps = 1061, loss = 0.2714366018772125
In grad_steps = 1062, loss = 0.9508576989173889
In grad_steps = 1063, loss = 0.22822892665863037
In grad_steps = 1064, loss = 0.26348885893821716
In grad_steps = 1065, loss = 0.04181855171918869
In grad_steps = 1066, loss = 0.6358574032783508
In grad_steps = 1067, loss = 0.5790837407112122
In grad_steps = 1068, loss = 0.14629559218883514
In grad_steps = 1069, loss = 0.22523260116577148
In grad_steps = 1070, loss = 0.25134748220443726
In grad_steps = 1071, loss = 0.09203287214040756
In grad_steps = 1072, loss = 0.10721969604492188
In grad_steps = 1073, loss = 0.22959265112876892
In grad_steps = 1074, loss = 0.12271230667829514
In grad_steps = 1075, loss = 0.5070765614509583
In grad_steps = 1076, loss = 0.1921616941690445
In grad_steps = 1077, loss = 0.15358257293701172
In grad_steps = 1078, loss = 0.696657657623291
In grad_steps = 1079, loss = 0.11160662770271301
In grad_steps = 1080, loss = 0.09876857697963715
In grad_steps = 1081, loss = 0.914517879486084
In grad_steps = 1082, loss = 0.09446210414171219
In grad_steps = 1083, loss = 0.03513595089316368
In grad_steps = 1084, loss = 0.5879698991775513
In grad_steps = 1085, loss = 1.1431790590286255
In grad_steps = 1086, loss = 0.07792212069034576
In grad_steps = 1087, loss = 0.7556373476982117
In grad_steps = 1088, loss = 0.41523081064224243
In grad_steps = 1089, loss = 0.1323523223400116
In grad_steps = 1090, loss = 0.07047352939844131
In grad_steps = 1091, loss = 0.2527865171432495
In grad_steps = 1092, loss = 0.07066231966018677
In grad_steps = 1093, loss = 0.6395147442817688
In grad_steps = 1094, loss = 0.04911134019494057
In grad_steps = 1095, loss = 0.23639515042304993
In grad_steps = 1096, loss = 0.07839199900627136
In grad_steps = 1097, loss = 0.06066248565912247
In grad_steps = 1098, loss = 0.4610438048839569
In grad_steps = 1099, loss = 0.44239088892936707
In grad_steps = 1100, loss = 0.20308911800384521
In grad_steps = 1101, loss = 0.4345720112323761
In grad_steps = 1102, loss = 0.8911210298538208
In grad_steps = 1103, loss = 0.029773229733109474
In grad_steps = 1104, loss = 1.2824506759643555
In grad_steps = 1105, loss = 0.2693321108818054
In grad_steps = 1106, loss = 0.27922552824020386
In grad_steps = 1107, loss = 0.13653838634490967
In grad_steps = 1108, loss = 0.597943127155304
In grad_steps = 1109, loss = 0.3374592065811157
In grad_steps = 1110, loss = 0.2986304759979248
In grad_steps = 1111, loss = 0.5224597454071045
In grad_steps = 1112, loss = 0.6359878778457642
In grad_steps = 1113, loss = 0.47608354687690735
In grad_steps = 1114, loss = 0.4557856619358063
In grad_steps = 1115, loss = 0.10894538462162018
In grad_steps = 1116, loss = 0.7563306093215942
In grad_steps = 1117, loss = 0.19452545046806335
In grad_steps = 1118, loss = 0.3685474395751953
In grad_steps = 1119, loss = 0.5151022672653198
In grad_steps = 1120, loss = 0.10203735530376434
In grad_steps = 1121, loss = 0.1460680514574051
In grad_steps = 1122, loss = 0.24859808385372162
In grad_steps = 1123, loss = 0.5479512214660645
In grad_steps = 1124, loss = 0.48288679122924805
In grad_steps = 1125, loss = 0.19181057810783386
In grad_steps = 1126, loss = 0.3319515883922577
In grad_steps = 1127, loss = 0.2187061607837677
In grad_steps = 1128, loss = 0.5146362781524658
In grad_steps = 1129, loss = 0.18563605844974518
In grad_steps = 1130, loss = 0.12281689047813416
In grad_steps = 1131, loss = 0.23022344708442688
In grad_steps = 1132, loss = 0.2254289984703064
In grad_steps = 1133, loss = 0.6694707870483398
In grad_steps = 1134, loss = 0.25368911027908325
In grad_steps = 1135, loss = 0.43351003527641296
In grad_steps = 1136, loss = 0.6937680244445801
In grad_steps = 1137, loss = 0.16747966408729553
In grad_steps = 1138, loss = 0.18736892938613892
In grad_steps = 1139, loss = 0.6607815027236938
In grad_steps = 1140, loss = 0.17818096280097961
In grad_steps = 1141, loss = 1.381189227104187
In grad_steps = 1142, loss = 0.22390373051166534
In grad_steps = 1143, loss = 0.6648920774459839
In grad_steps = 1144, loss = 0.010426506400108337
In grad_steps = 1145, loss = 0.7121153473854065
In grad_steps = 1146, loss = 1.3447073698043823
In grad_steps = 1147, loss = 0.9184947609901428
In grad_steps = 1148, loss = 0.23652797937393188
In grad_steps = 1149, loss = 0.0770392045378685
In grad_steps = 1150, loss = 0.529909610748291
In grad_steps = 1151, loss = 0.41517868638038635
In grad_steps = 1152, loss = 0.47153332829475403
In grad_steps = 1153, loss = 0.6833550930023193
In grad_steps = 1154, loss = 0.5618135929107666
In grad_steps = 1155, loss = 0.3681350350379944
In grad_steps = 1156, loss = 0.45816636085510254
In grad_steps = 1157, loss = 0.23256994783878326
In grad_steps = 1158, loss = 0.38851386308670044
In grad_steps = 1159, loss = 0.12305425852537155
In grad_steps = 1160, loss = 0.4032169282436371
In grad_steps = 1161, loss = 0.5609449744224548
In grad_steps = 1162, loss = 0.3847556710243225
In grad_steps = 1163, loss = 0.3408741056919098
In grad_steps = 1164, loss = 0.24965238571166992
In grad_steps = 1165, loss = 0.4458318054676056
In grad_steps = 1166, loss = 1.1429381370544434
In grad_steps = 1167, loss = 0.6617343425750732
In grad_steps = 1168, loss = 0.41673052310943604
In grad_steps = 1169, loss = 0.06342394649982452
In grad_steps = 1170, loss = 0.29032692313194275
In grad_steps = 1171, loss = 0.41302764415740967
In grad_steps = 1172, loss = 0.38872072100639343
In grad_steps = 1173, loss = 0.3747260570526123
In grad_steps = 1174, loss = 0.2806544303894043
In grad_steps = 1175, loss = 0.6170287728309631
In grad_steps = 1176, loss = 0.4604896306991577
In grad_steps = 1177, loss = 0.09497193247079849
In grad_steps = 1178, loss = 0.6628599762916565
In grad_steps = 1179, loss = 0.07734613120555878
In grad_steps = 1180, loss = 0.40134796500205994
In grad_steps = 1181, loss = 0.391959547996521
In grad_steps = 1182, loss = 1.0931036472320557
In grad_steps = 1183, loss = 0.5344028472900391
In grad_steps = 1184, loss = 0.7927474975585938
In grad_steps = 1185, loss = 0.3251648545265198
In grad_steps = 1186, loss = 0.3211282789707184
In grad_steps = 1187, loss = 0.17784279584884644
In grad_steps = 1188, loss = 0.87674480676651
In grad_steps = 1189, loss = 0.24426975846290588
In grad_steps = 1190, loss = 0.14646963775157928
In grad_steps = 1191, loss = 0.2530953884124756
In grad_steps = 1192, loss = 0.23665012419223785
In grad_steps = 1193, loss = 0.15219619870185852
In grad_steps = 1194, loss = 0.2438630759716034
In grad_steps = 1195, loss = 0.6027989387512207
In grad_steps = 1196, loss = 0.15725964307785034
In grad_steps = 1197, loss = 0.6675946116447449
In grad_steps = 1198, loss = 0.09162475168704987
In grad_steps = 1199, loss = 0.8672093749046326
In grad_steps = 1200, loss = 0.18871481716632843
In grad_steps = 1201, loss = 0.08448609709739685
In grad_steps = 1202, loss = 0.40913093090057373
In grad_steps = 1203, loss = 0.049565546214580536
In grad_steps = 1204, loss = 0.06854448467493057
In grad_steps = 1205, loss = 1.3204655647277832
In grad_steps = 1206, loss = 1.4022018909454346
In grad_steps = 1207, loss = 0.08632193505764008
In grad_steps = 1208, loss = 0.2722892165184021
In grad_steps = 1209, loss = 0.5819219946861267
In grad_steps = 1210, loss = 0.47325843572616577
In grad_steps = 1211, loss = 0.49382779002189636
In grad_steps = 1212, loss = 0.06891235709190369
In grad_steps = 1213, loss = 0.6478504538536072
In grad_steps = 1214, loss = 0.3522989749908447
In grad_steps = 1215, loss = 0.44009870290756226
In grad_steps = 1216, loss = 0.16830690205097198
In grad_steps = 1217, loss = 0.08094504475593567
In grad_steps = 1218, loss = 0.5375961661338806
In grad_steps = 1219, loss = 0.19677011668682098
In grad_steps = 1220, loss = 0.3579539358615875
In grad_steps = 1221, loss = 0.4644635021686554
In grad_steps = 1222, loss = 0.40439292788505554
In grad_steps = 1223, loss = 0.3245808780193329
In grad_steps = 1224, loss = 1.0914334058761597
In grad_steps = 1225, loss = 0.10077203810214996
In grad_steps = 1226, loss = 0.1932409107685089
In grad_steps = 1227, loss = 0.21498265862464905
In grad_steps = 1228, loss = 0.24001383781433105
In grad_steps = 1229, loss = 1.2345138788223267
In grad_steps = 1230, loss = 0.2953718900680542
In grad_steps = 1231, loss = 0.10490325093269348
In grad_steps = 1232, loss = 0.05380183830857277
In grad_steps = 1233, loss = 0.5074623823165894
In grad_steps = 1234, loss = 0.2654867172241211
In grad_steps = 1235, loss = 0.3477780818939209
In grad_steps = 1236, loss = 0.514511227607727
In grad_steps = 1237, loss = 0.5866280198097229
In grad_steps = 1238, loss = 0.12080836296081543
In grad_steps = 1239, loss = 0.3263632357120514
In grad_steps = 1240, loss = 0.10207726806402206
In grad_steps = 1241, loss = 0.06782650202512741
In grad_steps = 1242, loss = 0.40478718280792236
In grad_steps = 1243, loss = 0.18729037046432495
In grad_steps = 1244, loss = 0.5957834720611572
In grad_steps = 1245, loss = 0.6528912782669067
In grad_steps = 1246, loss = 0.13055770099163055
In grad_steps = 1247, loss = 0.4853935241699219
In grad_steps = 1248, loss = 0.5234881043434143
In grad_steps = 1249, loss = 0.37297433614730835
In grad_steps = 1250, loss = 0.07887931913137436
In grad_steps = 1251, loss = 0.18683341145515442
In grad_steps = 1252, loss = 0.062265507876873016
In grad_steps = 1253, loss = 0.09076835215091705
In grad_steps = 1254, loss = 0.12595276534557343
In grad_steps = 1255, loss = 1.074930191040039
In grad_steps = 1256, loss = 0.8993205428123474
In grad_steps = 1257, loss = 0.4009793996810913
In grad_steps = 1258, loss = 0.4229292571544647
In grad_steps = 1259, loss = 0.08847121894359589
In grad_steps = 1260, loss = 0.14522917568683624
In grad_steps = 1261, loss = 1.4125901460647583
In grad_steps = 1262, loss = 0.2290087193250656
In grad_steps = 1263, loss = 0.06818528473377228
In grad_steps = 1264, loss = 0.24476021528244019
In grad_steps = 1265, loss = 0.3189029395580292
In grad_steps = 1266, loss = 0.7197097539901733
In grad_steps = 1267, loss = 0.7567300796508789
In grad_steps = 1268, loss = 0.7344498634338379
In grad_steps = 1269, loss = 0.08913924545049667
In grad_steps = 1270, loss = 0.11026984453201294
In grad_steps = 1271, loss = 0.4584524631500244
In grad_steps = 1272, loss = 0.5399920344352722
In grad_steps = 1273, loss = 0.2900152802467346
In grad_steps = 1274, loss = 0.25373944640159607
In grad_steps = 1275, loss = 0.19118720293045044
In grad_steps = 1276, loss = 0.3536146581172943
In grad_steps = 1277, loss = 0.29478880763053894
In grad_steps = 1278, loss = 0.45940738916397095
In grad_steps = 1279, loss = 0.19792279601097107
In grad_steps = 1280, loss = 0.169009730219841
In grad_steps = 1281, loss = 0.45435452461242676
In grad_steps = 1282, loss = 0.5203560590744019
In grad_steps = 1283, loss = 0.2731485962867737
In grad_steps = 1284, loss = 0.17105551064014435
In grad_steps = 1285, loss = 0.21126511693000793
In grad_steps = 1286, loss = 0.3409169018268585
In grad_steps = 1287, loss = 0.08863332122564316
In grad_steps = 1288, loss = 0.4472045600414276
In grad_steps = 1289, loss = 0.10437588393688202
In grad_steps = 1290, loss = 0.06015333905816078
In grad_steps = 1291, loss = 0.34173956513404846
In grad_steps = 1292, loss = 0.8072026968002319
In grad_steps = 1293, loss = 0.10598652064800262
In grad_steps = 1294, loss = 0.3008630871772766
In grad_steps = 1295, loss = 0.1542113572359085
In grad_steps = 1296, loss = 0.1324557662010193
In grad_steps = 1297, loss = 0.09563270956277847
In grad_steps = 1298, loss = 0.34446823596954346
In grad_steps = 1299, loss = 0.9805383086204529
In grad_steps = 1300, loss = 0.08158423006534576
In grad_steps = 1301, loss = 0.1033177599310875
In grad_steps = 1302, loss = 0.030104568228125572
In grad_steps = 1303, loss = 0.7987866401672363
In grad_steps = 1304, loss = 0.0560576356947422
In grad_steps = 1305, loss = 0.8426170945167542
In grad_steps = 1306, loss = 0.8180261254310608
In grad_steps = 1307, loss = 0.4862159490585327
In grad_steps = 1308, loss = 0.9005753397941589
In grad_steps = 1309, loss = 0.29957059025764465
In grad_steps = 1310, loss = 0.11585761606693268
In grad_steps = 1311, loss = 0.18042504787445068
In grad_steps = 1312, loss = 0.15621313452720642
In grad_steps = 1313, loss = 0.10824304819107056
In grad_steps = 1314, loss = 0.07191533595323563
In grad_steps = 1315, loss = 0.15381667017936707
In grad_steps = 1316, loss = 1.0219857692718506
In grad_steps = 1317, loss = 0.6542439460754395
In grad_steps = 1318, loss = 0.2409939020872116
In grad_steps = 1319, loss = 0.38356202840805054
In grad_steps = 1320, loss = 0.5779848098754883
In grad_steps = 1321, loss = 0.1046246737241745
In grad_steps = 1322, loss = 0.6291231513023376
In grad_steps = 1323, loss = 0.3170270621776581
In grad_steps = 1324, loss = 0.07270614057779312
In grad_steps = 1325, loss = 0.13466614484786987
In grad_steps = 1326, loss = 0.17253388464450836
In grad_steps = 1327, loss = 0.6704570651054382
In grad_steps = 1328, loss = 0.07764452695846558
In grad_steps = 1329, loss = 0.6944966912269592
In grad_steps = 1330, loss = 0.693488359451294
In grad_steps = 1331, loss = 0.31461629271507263
In grad_steps = 1332, loss = 0.1023997813463211
In grad_steps = 1333, loss = 0.4491986036300659
In grad_steps = 1334, loss = 0.10210014134645462
In grad_steps = 1335, loss = 0.20895211398601532
In grad_steps = 1336, loss = 0.42108261585235596
In grad_steps = 1337, loss = 0.15837232768535614
In grad_steps = 1338, loss = 0.02200687676668167
In grad_steps = 1339, loss = 0.9312193393707275
In grad_steps = 1340, loss = 0.42850780487060547
In grad_steps = 1341, loss = 0.2341727465391159
In grad_steps = 1342, loss = 0.25557294487953186
In grad_steps = 1343, loss = 0.4299861788749695
In grad_steps = 1344, loss = 0.14965227246284485
In grad_steps = 1345, loss = 1.0678989887237549
In grad_steps = 1346, loss = 0.15918773412704468
In grad_steps = 1347, loss = 0.26746320724487305
In grad_steps = 1348, loss = 0.503087043762207
In grad_steps = 1349, loss = 0.01024568174034357
In grad_steps = 1350, loss = 0.5073389410972595
In grad_steps = 1351, loss = 0.28223368525505066
In grad_steps = 1352, loss = 0.10396303236484528
In grad_steps = 1353, loss = 1.0705034732818604
In grad_steps = 1354, loss = 0.1468450129032135
In grad_steps = 1355, loss = 0.5000219345092773
In grad_steps = 1356, loss = 0.15307627618312836
In grad_steps = 1357, loss = 0.2436409443616867
In grad_steps = 1358, loss = 0.12396789342164993
In grad_steps = 1359, loss = 0.10297684371471405
In grad_steps = 1360, loss = 0.1612028032541275
In grad_steps = 1361, loss = 0.7766746282577515
In grad_steps = 1362, loss = 0.05553325265645981
In grad_steps = 1363, loss = 0.11648181080818176
In grad_steps = 1364, loss = 0.5879644155502319
In grad_steps = 1365, loss = 0.38448888063430786
In grad_steps = 1366, loss = 0.4704676568508148
In grad_steps = 1367, loss = 0.05507771670818329
In grad_steps = 1368, loss = 0.6592158079147339
In grad_steps = 1369, loss = 0.11910694092512131
In grad_steps = 1370, loss = 0.09230262786149979
In grad_steps = 1371, loss = 0.29368147253990173
In grad_steps = 1372, loss = 0.4719630181789398
In grad_steps = 1373, loss = 0.06713143736124039
In grad_steps = 1374, loss = 0.8022287487983704
In grad_steps = 1375, loss = 0.04031040519475937
In grad_steps = 1376, loss = 0.2704614996910095
In grad_steps = 1377, loss = 0.05870944261550903
In grad_steps = 1378, loss = 0.6044188737869263
In grad_steps = 1379, loss = 0.22320160269737244
In grad_steps = 1380, loss = 0.25178465247154236
In grad_steps = 1381, loss = 0.06146597862243652
In grad_steps = 1382, loss = 0.12265840917825699
In grad_steps = 1383, loss = 0.7876837253570557
In grad_steps = 1384, loss = 0.07011048495769501
In grad_steps = 1385, loss = 1.1474508047103882
In grad_steps = 1386, loss = 0.741493821144104
In grad_steps = 1387, loss = 0.21841444075107574
In grad_steps = 1388, loss = 0.0619521327316761
In grad_steps = 1389, loss = 0.07494448870420456
In grad_steps = 1390, loss = 0.03932235389947891
In grad_steps = 1391, loss = 0.660746693611145
In grad_steps = 1392, loss = 0.12947803735733032
In grad_steps = 1393, loss = 0.13713933527469635
In grad_steps = 1394, loss = 0.0894162729382515
In grad_steps = 1395, loss = 0.954620361328125
In grad_steps = 1396, loss = 0.6850059628486633
In grad_steps = 1397, loss = 0.07369805872440338
In grad_steps = 1398, loss = 0.6880555152893066
In grad_steps = 1399, loss = 0.08096228539943695
In grad_steps = 1400, loss = 0.6469823122024536
In grad_steps = 1401, loss = 0.3053555488586426
In grad_steps = 1402, loss = 0.23700954020023346
In grad_steps = 1403, loss = 0.5555676221847534
In grad_steps = 1404, loss = 0.6095579266548157
In grad_steps = 1405, loss = 0.10428877174854279
In grad_steps = 1406, loss = 0.3793104887008667
In grad_steps = 1407, loss = 0.20354953408241272
In grad_steps = 1408, loss = 0.45509758591651917
In grad_steps = 1409, loss = 0.2889663577079773
In grad_steps = 1410, loss = 0.3934658467769623
In grad_steps = 1411, loss = 0.7386125326156616
In grad_steps = 1412, loss = 0.2590898871421814
In grad_steps = 1413, loss = 0.9998647570610046
In grad_steps = 1414, loss = 0.3564256727695465
In grad_steps = 1415, loss = 0.17350134253501892
In grad_steps = 1416, loss = 0.3061360716819763
In grad_steps = 1417, loss = 0.18219155073165894
In grad_steps = 1418, loss = 0.07786574214696884
In grad_steps = 1419, loss = 0.10639815032482147
In grad_steps = 1420, loss = 0.34138914942741394
In grad_steps = 1421, loss = 0.1017015278339386
In grad_steps = 1422, loss = 0.32222750782966614
In grad_steps = 1423, loss = 0.9856618046760559
In grad_steps = 1424, loss = 0.06931660324335098
In grad_steps = 1425, loss = 0.560723602771759
In grad_steps = 1426, loss = 0.09000805020332336
In grad_steps = 1427, loss = 0.3672124445438385
In grad_steps = 1428, loss = 0.5089340209960938
In grad_steps = 1429, loss = 0.3926863968372345
In grad_steps = 1430, loss = 0.14401012659072876
In grad_steps = 1431, loss = 0.671242892742157
In grad_steps = 1432, loss = 0.08194468915462494
In grad_steps = 1433, loss = 0.05662690848112106
In grad_steps = 1434, loss = 0.2957294285297394
In grad_steps = 1435, loss = 0.22366704046726227
In grad_steps = 1436, loss = 0.8205516338348389
In grad_steps = 1437, loss = 0.8150559663772583
In grad_steps = 1438, loss = 0.43027248978614807
In grad_steps = 1439, loss = 0.04434281215071678
In grad_steps = 1440, loss = 0.33105504512786865
In grad_steps = 1441, loss = 1.170263648033142
In grad_steps = 1442, loss = 0.14589691162109375
In grad_steps = 1443, loss = 1.4682073593139648
In grad_steps = 1444, loss = 0.07230497896671295
In grad_steps = 1445, loss = 0.15591572225093842
In grad_steps = 1446, loss = 0.22295230627059937
In grad_steps = 1447, loss = 0.03790997713804245
In grad_steps = 1448, loss = 0.5367782115936279
In grad_steps = 1449, loss = 0.46711933612823486
In grad_steps = 1450, loss = 0.17542050778865814
In grad_steps = 1451, loss = 0.724290132522583
In grad_steps = 1452, loss = 0.12026897072792053
In grad_steps = 1453, loss = 0.7663835287094116
In grad_steps = 1454, loss = 0.13023808598518372
In grad_steps = 1455, loss = 0.8044440746307373
In grad_steps = 1456, loss = 0.32608282566070557
In grad_steps = 1457, loss = 0.2591928243637085
In grad_steps = 1458, loss = 0.2983572483062744
In grad_steps = 1459, loss = 0.18260855972766876
In grad_steps = 1460, loss = 0.09490445256233215
In grad_steps = 1461, loss = 0.5072711110115051
In grad_steps = 1462, loss = 0.16313417255878448
In grad_steps = 1463, loss = 0.6934802532196045
In grad_steps = 1464, loss = 0.43837881088256836
In grad_steps = 1465, loss = 0.5002292394638062
In grad_steps = 1466, loss = 0.13034585118293762
In grad_steps = 1467, loss = 0.32962554693222046
In grad_steps = 1468, loss = 0.0776735469698906
In grad_steps = 1469, loss = 0.2088164985179901
In grad_steps = 1470, loss = 0.18829500675201416
In grad_steps = 1471, loss = 0.07224015146493912
In grad_steps = 1472, loss = 0.6001173853874207
In grad_steps = 1473, loss = 0.06085619330406189
In grad_steps = 1474, loss = 1.0617481470108032
In grad_steps = 1475, loss = 0.6113328337669373
In grad_steps = 1476, loss = 0.33017024397850037
In grad_steps = 1477, loss = 0.21582701802253723
In grad_steps = 1478, loss = 0.6221346259117126
In grad_steps = 1479, loss = 0.22824838757514954
In grad_steps = 1480, loss = 0.11725124716758728
In grad_steps = 1481, loss = 0.3593140244483948
In grad_steps = 1482, loss = 0.10104357451200485
In grad_steps = 1483, loss = 0.4839514195919037
In grad_steps = 1484, loss = 0.3755427896976471
In grad_steps = 1485, loss = 0.2547928988933563
In grad_steps = 1486, loss = 1.1426975727081299
In grad_steps = 1487, loss = 0.1120460107922554
In grad_steps = 1488, loss = 0.7540813684463501
In grad_steps = 1489, loss = 0.4951481223106384
In grad_steps = 1490, loss = 0.7865782976150513
In grad_steps = 1491, loss = 0.49934032559394836
In grad_steps = 1492, loss = 0.07794225215911865
In grad_steps = 1493, loss = 0.47514453530311584
In grad_steps = 1494, loss = 0.5101591944694519
In grad_steps = 1495, loss = 0.16518908739089966
In grad_steps = 1496, loss = 0.07701665908098221
In grad_steps = 1497, loss = 0.292097806930542
In grad_steps = 1498, loss = 0.4031638503074646
In grad_steps = 1499, loss = 0.75962233543396
In grad_steps = 1500, loss = 0.22448354959487915
In grad_steps = 1501, loss = 0.2725811004638672
In grad_steps = 1502, loss = 0.6924667358398438
In grad_steps = 1503, loss = 0.4235580861568451
In grad_steps = 1504, loss = 0.3071925640106201
In grad_steps = 1505, loss = 0.5461004972457886
In grad_steps = 1506, loss = 0.21390411257743835
In grad_steps = 1507, loss = 0.7480257749557495
In grad_steps = 1508, loss = 0.24557507038116455
In grad_steps = 1509, loss = 0.6726096868515015
In grad_steps = 1510, loss = 0.7535823583602905
In grad_steps = 1511, loss = 0.48581618070602417
In grad_steps = 1512, loss = 0.9660822749137878
In grad_steps = 1513, loss = 0.7174111008644104
In grad_steps = 1514, loss = 0.25962141156196594
In grad_steps = 1515, loss = 0.3347192704677582
In grad_steps = 1516, loss = 0.13946430385112762
In grad_steps = 1517, loss = 0.10821864753961563
In grad_steps = 1518, loss = 0.06802287697792053
In grad_steps = 1519, loss = 0.3226170241832733
In grad_steps = 1520, loss = 0.4530564248561859
In grad_steps = 1521, loss = 1.1839133501052856
In grad_steps = 1522, loss = 0.24797332286834717
In grad_steps = 1523, loss = 1.0356547832489014
In grad_steps = 1524, loss = 0.5071110129356384
In grad_steps = 1525, loss = 0.2962803542613983
In grad_steps = 1526, loss = 0.37156248092651367
In grad_steps = 1527, loss = 0.580797016620636
In grad_steps = 1528, loss = 0.7187402248382568
In grad_steps = 1529, loss = 0.11745347082614899
In grad_steps = 1530, loss = 0.535192608833313
In grad_steps = 1531, loss = 0.5925401449203491
In grad_steps = 1532, loss = 0.28303834795951843
In grad_steps = 1533, loss = 0.14907270669937134
In grad_steps = 1534, loss = 0.28386130928993225
In grad_steps = 1535, loss = 0.2711328864097595
In grad_steps = 1536, loss = 0.19334889948368073
In grad_steps = 1537, loss = 0.38879722356796265
In grad_steps = 1538, loss = 0.18212707340717316
In grad_steps = 1539, loss = 0.0875510573387146
In grad_steps = 1540, loss = 0.13629689812660217
In grad_steps = 1541, loss = 0.5787276029586792
In grad_steps = 1542, loss = 0.27474430203437805
In grad_steps = 1543, loss = 0.2930646538734436
In grad_steps = 1544, loss = 0.11020272225141525
In grad_steps = 1545, loss = 0.10691575706005096
In grad_steps = 1546, loss = 0.2803501486778259
In grad_steps = 1547, loss = 1.124835729598999
In grad_steps = 1548, loss = 0.09802958369255066
In grad_steps = 1549, loss = 0.01919429376721382
In grad_steps = 1550, loss = 0.246335968375206
In grad_steps = 1551, loss = 0.5612897276878357
In grad_steps = 1552, loss = 0.07269877195358276
In grad_steps = 1553, loss = 0.06134701892733574
In grad_steps = 1554, loss = 0.4456888437271118
In grad_steps = 1555, loss = 0.11557687819004059
In grad_steps = 1556, loss = 0.8551689982414246
In grad_steps = 1557, loss = 0.11007910966873169
In grad_steps = 1558, loss = 0.5728006362915039
In grad_steps = 1559, loss = 0.5442988872528076
In grad_steps = 1560, loss = 0.16796782612800598
In grad_steps = 1561, loss = 0.357400119304657
In grad_steps = 1562, loss = 0.2292705476284027
In grad_steps = 1563, loss = 0.10820373892784119
In grad_steps = 1564, loss = 1.5175285339355469
In grad_steps = 1565, loss = 0.4893849790096283
In grad_steps = 1566, loss = 0.0885743573307991
In grad_steps = 1567, loss = 0.5214501619338989
In grad_steps = 1568, loss = 0.4640663266181946
In grad_steps = 1569, loss = 0.15387402474880219
In grad_steps = 1570, loss = 0.6678186058998108
In grad_steps = 1571, loss = 0.8312433362007141
In grad_steps = 1572, loss = 0.19168564677238464
In grad_steps = 1573, loss = 0.2996126711368561
In grad_steps = 1574, loss = 0.7563304901123047
In grad_steps = 1575, loss = 0.5893672108650208
In grad_steps = 1576, loss = 0.35890254378318787
In grad_steps = 1577, loss = 0.4216268062591553
In grad_steps = 1578, loss = 0.06393938511610031
In grad_steps = 1579, loss = 0.48742756247520447
In grad_steps = 1580, loss = 0.12806232273578644
In grad_steps = 1581, loss = 0.3378934860229492
In grad_steps = 1582, loss = 0.1771916300058365
In grad_steps = 1583, loss = 0.16134238243103027
In grad_steps = 1584, loss = 0.26361358165740967
In grad_steps = 1585, loss = 0.2545250654220581
In grad_steps = 1586, loss = 0.20577310025691986
In grad_steps = 1587, loss = 0.24713416397571564
In grad_steps = 1588, loss = 0.9393043518066406
In grad_steps = 1589, loss = 0.7792764902114868
In grad_steps = 1590, loss = 0.36108142137527466
In grad_steps = 1591, loss = 0.7697553634643555
In grad_steps = 1592, loss = 0.6863709688186646
In grad_steps = 1593, loss = 0.044990427792072296
In grad_steps = 1594, loss = 0.17941652238368988
In grad_steps = 1595, loss = 0.9423655271530151
In grad_steps = 1596, loss = 0.13865692913532257
In grad_steps = 1597, loss = 0.1389802098274231
In grad_steps = 1598, loss = 0.0953500047326088
In grad_steps = 1599, loss = 0.2379966825246811
In grad_steps = 1600, loss = 0.44261065125465393
In grad_steps = 1601, loss = 0.21982155740261078
In grad_steps = 1602, loss = 0.1838446855545044
In grad_steps = 1603, loss = 0.5571073293685913
In grad_steps = 1604, loss = 0.33413103222846985
In grad_steps = 1605, loss = 0.08903643488883972
In grad_steps = 1606, loss = 0.3419468402862549
In grad_steps = 1607, loss = 0.05285320430994034
In grad_steps = 1608, loss = 0.5513456463813782
In grad_steps = 1609, loss = 0.6519390344619751
In grad_steps = 1610, loss = 0.15279027819633484
In grad_steps = 1611, loss = 0.26921430230140686
In grad_steps = 1612, loss = 0.3353966772556305
In grad_steps = 1613, loss = 0.3166781961917877
In grad_steps = 1614, loss = 0.1088007315993309
In grad_steps = 1615, loss = 0.10681913048028946
In grad_steps = 1616, loss = 0.07962048053741455
In grad_steps = 1617, loss = 0.6168591976165771
In grad_steps = 1618, loss = 0.7680389881134033
In grad_steps = 1619, loss = 0.34064075350761414
In grad_steps = 1620, loss = 0.6768808364868164
In grad_steps = 1621, loss = 0.16298693418502808
In grad_steps = 1622, loss = 1.306014895439148
In grad_steps = 1623, loss = 0.6753110289573669
In grad_steps = 1624, loss = 0.5087867379188538
In grad_steps = 1625, loss = 1.0682016611099243
In grad_steps = 1626, loss = 0.15886357426643372
In grad_steps = 1627, loss = 0.18824465572834015
In grad_steps = 1628, loss = 0.2515324056148529
In grad_steps = 1629, loss = 0.14649701118469238
In grad_steps = 1630, loss = 0.35632577538490295
In grad_steps = 1631, loss = 0.10113401710987091
In grad_steps = 1632, loss = 0.37042900919914246
In grad_steps = 1633, loss = 0.5779860019683838
In grad_steps = 1634, loss = 0.2798709571361542
In grad_steps = 1635, loss = 0.2583606243133545
In grad_steps = 1636, loss = 0.4164503514766693
In grad_steps = 1637, loss = 0.292538046836853
In grad_steps = 1638, loss = 0.13187658786773682
In grad_steps = 1639, loss = 0.16619519889354706
In grad_steps = 1640, loss = 0.34558430314064026
In grad_steps = 1641, loss = 0.27775102853775024
In grad_steps = 1642, loss = 0.06061769649386406
In grad_steps = 1643, loss = 1.014678955078125
In grad_steps = 1644, loss = 0.27183058857917786
In grad_steps = 1645, loss = 0.25577908754348755
In grad_steps = 1646, loss = 0.2922685146331787
In grad_steps = 1647, loss = 0.808954119682312
In grad_steps = 1648, loss = 0.04950959235429764
In grad_steps = 1649, loss = 1.731748104095459
In grad_steps = 1650, loss = 0.25581735372543335
In grad_steps = 1651, loss = 1.0296664237976074
In grad_steps = 1652, loss = 0.09609192609786987
In grad_steps = 1653, loss = 0.6149117946624756
In grad_steps = 1654, loss = 0.6858832836151123
In grad_steps = 1655, loss = 0.6536780595779419
In grad_steps = 1656, loss = 0.16052065789699554
In grad_steps = 1657, loss = 0.30715271830558777
In grad_steps = 1658, loss = 0.22381077706813812
In grad_steps = 1659, loss = 0.09857822209596634
In grad_steps = 1660, loss = 0.5173354148864746
In grad_steps = 1661, loss = 0.23869270086288452
In grad_steps = 1662, loss = 0.3290654718875885
In grad_steps = 1663, loss = 0.2924775779247284
In grad_steps = 1664, loss = 0.3846326172351837
In grad_steps = 1665, loss = 0.42212048172950745
In grad_steps = 1666, loss = 0.25171953439712524
In grad_steps = 1667, loss = 0.2635122239589691
In grad_steps = 1668, loss = 0.36712023615837097
In grad_steps = 1669, loss = 0.564439594745636
In grad_steps = 1670, loss = 0.1694592982530594
In grad_steps = 1671, loss = 0.06433030217885971
In grad_steps = 1672, loss = 0.13707531988620758
In grad_steps = 1673, loss = 0.06196098402142525
In grad_steps = 1674, loss = 0.03952034190297127
In grad_steps = 1675, loss = 0.1717630922794342
In grad_steps = 1676, loss = 0.11828916519880295
In grad_steps = 1677, loss = 0.03882887214422226
In grad_steps = 1678, loss = 0.9934967756271362
In grad_steps = 1679, loss = 1.8339353799819946
In grad_steps = 1680, loss = 0.13771194219589233
In grad_steps = 1681, loss = 0.13273072242736816
In grad_steps = 1682, loss = 0.17456939816474915
In grad_steps = 1683, loss = 0.43679583072662354
In grad_steps = 1684, loss = 0.7152332663536072
In grad_steps = 1685, loss = 0.3830382227897644
In grad_steps = 1686, loss = 0.13280482590198517
In grad_steps = 1687, loss = 0.5769418478012085
In grad_steps = 1688, loss = 0.2749742865562439
In grad_steps = 1689, loss = 1.2323803901672363
In grad_steps = 1690, loss = 0.11061731725931168
In grad_steps = 1691, loss = 0.33337530493736267
In grad_steps = 1692, loss = 0.3250752389431
In grad_steps = 1693, loss = 0.33656278252601624
In grad_steps = 1694, loss = 0.09705847501754761
In grad_steps = 1695, loss = 0.5305115580558777
In grad_steps = 1696, loss = 0.06830737739801407
In grad_steps = 1697, loss = 0.5490020513534546
In grad_steps = 1698, loss = 0.2759200632572174
In grad_steps = 1699, loss = 0.30632415413856506
In grad_steps = 1700, loss = 0.5653983950614929
In grad_steps = 1701, loss = 0.8455424308776855
In grad_steps = 1702, loss = 0.501850962638855
In grad_steps = 1703, loss = 0.22110816836357117
In grad_steps = 1704, loss = 0.07583464682102203
In grad_steps = 1705, loss = 0.2493400275707245
In grad_steps = 1706, loss = 0.1430457979440689
In grad_steps = 1707, loss = 0.4561581611633301
In grad_steps = 1708, loss = 0.03875528648495674
In grad_steps = 1709, loss = 0.10552535951137543
In grad_steps = 1710, loss = 0.938072919845581
In grad_steps = 1711, loss = 0.22870394587516785
In grad_steps = 1712, loss = 0.3427860736846924
In grad_steps = 1713, loss = 0.0651007890701294
In grad_steps = 1714, loss = 0.09205779433250427
In grad_steps = 1715, loss = 0.3028700351715088
In grad_steps = 1716, loss = 0.14748172461986542
In grad_steps = 1717, loss = 0.03686776012182236
In grad_steps = 1718, loss = 1.1751786470413208
In grad_steps = 1719, loss = 0.012812783941626549
In grad_steps = 1720, loss = 0.5115747451782227
In grad_steps = 1721, loss = 0.6233495473861694
In grad_steps = 1722, loss = 0.02846609242260456
In grad_steps = 1723, loss = 0.08284074068069458
In grad_steps = 1724, loss = 0.028878487646579742
In grad_steps = 1725, loss = 0.04000185430049896
In grad_steps = 1726, loss = 0.059054549783468246
In grad_steps = 1727, loss = 0.17484579980373383
In grad_steps = 1728, loss = 0.07074730098247528
In grad_steps = 1729, loss = 0.02689315937459469
In grad_steps = 1730, loss = 0.05399080365896225
In grad_steps = 1731, loss = 0.8847724199295044
In grad_steps = 1732, loss = 0.7683541774749756
In grad_steps = 1733, loss = 0.23622731864452362
In grad_steps = 1734, loss = 0.6546741724014282
In grad_steps = 1735, loss = 0.15109243988990784
In grad_steps = 1736, loss = 0.09852270781993866
In grad_steps = 1737, loss = 0.10585157573223114
In grad_steps = 1738, loss = 0.5516181588172913
In grad_steps = 1739, loss = 0.3900297284126282
In grad_steps = 1740, loss = 0.26511260867118835
In grad_steps = 1741, loss = 0.13995009660720825
In grad_steps = 1742, loss = 0.6214872598648071
In grad_steps = 1743, loss = 0.5789477229118347
In grad_steps = 1744, loss = 0.10748177766799927
In grad_steps = 1745, loss = 0.49151086807250977
In grad_steps = 1746, loss = 0.05929885804653168
In grad_steps = 1747, loss = 0.11732043325901031
In grad_steps = 1748, loss = 0.19238093495368958
In grad_steps = 1749, loss = 1.3660991191864014
In grad_steps = 1750, loss = 0.6582484245300293
In grad_steps = 1751, loss = 0.14854007959365845
In grad_steps = 1752, loss = 0.19970223307609558
In grad_steps = 1753, loss = 0.17729875445365906
In grad_steps = 1754, loss = 0.15169337391853333
In grad_steps = 1755, loss = 0.9476358890533447
In grad_steps = 1756, loss = 0.20238101482391357
In grad_steps = 1757, loss = 0.18817821145057678
In grad_steps = 1758, loss = 0.8675975799560547
In grad_steps = 1759, loss = 0.08868283778429031
In grad_steps = 1760, loss = 0.10781776905059814
In grad_steps = 1761, loss = 0.16809475421905518
In grad_steps = 1762, loss = 0.2996459901332855
In grad_steps = 1763, loss = 0.46311330795288086
In grad_steps = 1764, loss = 0.4651852250099182
In grad_steps = 1765, loss = 0.8674537539482117
In grad_steps = 1766, loss = 0.3262997269630432
In grad_steps = 1767, loss = 0.34522557258605957
In grad_steps = 1768, loss = 0.7289522886276245
In grad_steps = 1769, loss = 0.49742886424064636
In grad_steps = 1770, loss = 0.5394557118415833
In grad_steps = 1771, loss = 0.7190862894058228
In grad_steps = 1772, loss = 0.32878053188323975
In grad_steps = 1773, loss = 0.16057801246643066
In grad_steps = 1774, loss = 0.297701358795166
In grad_steps = 1775, loss = 0.24333727359771729
In grad_steps = 1776, loss = 0.13393746316432953
In grad_steps = 1777, loss = 0.15009811520576477
In grad_steps = 1778, loss = 0.43871843814849854
In grad_steps = 1779, loss = 0.41881173849105835
In grad_steps = 1780, loss = 0.2612416744232178
In grad_steps = 1781, loss = 0.08259578794240952
In grad_steps = 1782, loss = 0.1561996340751648
In grad_steps = 1783, loss = 0.28843510150909424
In grad_steps = 1784, loss = 0.6233150959014893
In grad_steps = 1785, loss = 0.1192995011806488
In grad_steps = 1786, loss = 0.05825958028435707
In grad_steps = 1787, loss = 0.04275457561016083
In grad_steps = 1788, loss = 0.21533575654029846
In grad_steps = 1789, loss = 0.15170621871948242
In grad_steps = 1790, loss = 0.17187519371509552
In grad_steps = 1791, loss = 1.0108991861343384
In grad_steps = 1792, loss = 0.9338695406913757
In grad_steps = 1793, loss = 1.1124449968338013
In grad_steps = 1794, loss = 0.5373221635818481
In grad_steps = 1795, loss = 1.2680156230926514
In grad_steps = 1796, loss = 0.1544836014509201
In grad_steps = 1797, loss = 0.22011393308639526
In grad_steps = 1798, loss = 0.15127834677696228
In grad_steps = 1799, loss = 0.27988871932029724
In grad_steps = 1800, loss = 0.032012201845645905
In grad_steps = 1801, loss = 0.7610597014427185
In grad_steps = 1802, loss = 0.06937001645565033
In grad_steps = 1803, loss = 0.6346381306648254
In grad_steps = 1804, loss = 0.1393360048532486
In grad_steps = 1805, loss = 0.47019264101982117
In grad_steps = 1806, loss = 0.23740680515766144
In grad_steps = 1807, loss = 0.2414621114730835
In grad_steps = 1808, loss = 1.0994197130203247
In grad_steps = 1809, loss = 0.23710593581199646
In grad_steps = 1810, loss = 0.49903368949890137
In grad_steps = 1811, loss = 0.7361104488372803
In grad_steps = 1812, loss = 0.11544303596019745
In grad_steps = 1813, loss = 0.37806224822998047
In grad_steps = 1814, loss = 0.17887768149375916
In grad_steps = 1815, loss = 1.0477440357208252
In grad_steps = 1816, loss = 0.6559790968894958
In grad_steps = 1817, loss = 0.07785440981388092
In grad_steps = 1818, loss = 0.1893911510705948
In grad_steps = 1819, loss = 0.14820846915245056
In grad_steps = 1820, loss = 0.5913963913917542
In grad_steps = 1821, loss = 0.15073925256729126
In grad_steps = 1822, loss = 0.4382500946521759
In grad_steps = 1823, loss = 0.16518080234527588
In grad_steps = 1824, loss = 0.25474801659584045
In grad_steps = 1825, loss = 0.16240084171295166
In grad_steps = 1826, loss = 0.15309764444828033
In grad_steps = 1827, loss = 0.15648898482322693
In grad_steps = 1828, loss = 0.6956430673599243
In grad_steps = 1829, loss = 0.24473147094249725
In grad_steps = 1830, loss = 0.47130948305130005
In grad_steps = 1831, loss = 0.1796611249446869
In grad_steps = 1832, loss = 0.3729989528656006
In grad_steps = 1833, loss = 0.07331334054470062
In grad_steps = 1834, loss = 0.08642589300870895
In grad_steps = 1835, loss = 0.028253020718693733
In grad_steps = 1836, loss = 0.061965249478816986
In grad_steps = 1837, loss = 0.8243753910064697
In grad_steps = 1838, loss = 0.5494226217269897
In grad_steps = 1839, loss = 0.6746002435684204
In grad_steps = 1840, loss = 0.3050815761089325
In grad_steps = 1841, loss = 0.12180114537477493
In grad_steps = 1842, loss = 0.19505281746387482
In grad_steps = 1843, loss = 0.049023620784282684
In grad_steps = 1844, loss = 0.31813424825668335
In grad_steps = 1845, loss = 0.046338316053152084
In grad_steps = 1846, loss = 0.05906584858894348
In grad_steps = 1847, loss = 0.2327442616224289
In grad_steps = 1848, loss = 0.09917865693569183
In grad_steps = 1849, loss = 0.505582332611084
In grad_steps = 1850, loss = 0.06995688378810883
In grad_steps = 1851, loss = 0.08487005531787872
In grad_steps = 1852, loss = 0.09790322929620743
In grad_steps = 1853, loss = 1.9110287427902222
In grad_steps = 1854, loss = 0.08551198989152908
In grad_steps = 1855, loss = 0.11718235909938812
In grad_steps = 1856, loss = 0.2765645980834961
In grad_steps = 1857, loss = 0.6706929802894592
In grad_steps = 1858, loss = 0.06018189340829849
In grad_steps = 1859, loss = 0.09866172075271606
In grad_steps = 1860, loss = 0.16087785363197327
In grad_steps = 1861, loss = 0.558375358581543
In grad_steps = 1862, loss = 0.07040589302778244
In grad_steps = 1863, loss = 1.5972275733947754
In grad_steps = 1864, loss = 0.06168240308761597
In grad_steps = 1865, loss = 0.3060464560985565
In grad_steps = 1866, loss = 0.9369093179702759
In grad_steps = 1867, loss = 0.06137922406196594
In grad_steps = 1868, loss = 0.2017027586698532
In grad_steps = 1869, loss = 0.1370011568069458
In grad_steps = 1870, loss = 0.861624002456665
In grad_steps = 1871, loss = 0.5342569351196289
In grad_steps = 1872, loss = 0.06478224694728851
In grad_steps = 1873, loss = 0.136554554104805
In grad_steps = 1874, loss = 0.6322067975997925
In grad_steps = 1875, loss = 1.1465009450912476
In grad_steps = 1876, loss = 0.5923378467559814
In grad_steps = 1877, loss = 0.4387846887111664
In grad_steps = 1878, loss = 1.5904332399368286
In grad_steps = 1879, loss = 0.0510217547416687
In grad_steps = 1880, loss = 0.615687370300293
In grad_steps = 1881, loss = 0.6099685430526733
In grad_steps = 1882, loss = 0.4632130563259125
In grad_steps = 1883, loss = 0.5768232941627502
In grad_steps = 1884, loss = 0.4326157569885254
In grad_steps = 1885, loss = 0.4776601791381836
In grad_steps = 1886, loss = 0.6872748136520386
In grad_steps = 1887, loss = 1.0013633966445923
In grad_steps = 1888, loss = 0.6892677545547485
In grad_steps = 1889, loss = 0.4444606900215149
In grad_steps = 1890, loss = 0.28830453753471375
In grad_steps = 1891, loss = 0.4883458614349365
In grad_steps = 1892, loss = 0.35685548186302185
In grad_steps = 1893, loss = 0.30239367485046387
In grad_steps = 1894, loss = 0.3249494433403015
In grad_steps = 1895, loss = 0.3027893900871277
In grad_steps = 1896, loss = 0.33154016733169556
In grad_steps = 1897, loss = 0.13167567551136017
In grad_steps = 1898, loss = 0.20325934886932373
In grad_steps = 1899, loss = 0.1851959526538849
In grad_steps = 1900, loss = 0.04968120902776718
In grad_steps = 1901, loss = 0.06022047623991966
In grad_steps = 1902, loss = 0.06303364783525467
In grad_steps = 1903, loss = 0.3379196524620056
In grad_steps = 1904, loss = 0.6683774590492249
In grad_steps = 1905, loss = 1.0892735719680786
In grad_steps = 1906, loss = 0.010485483333468437
In grad_steps = 1907, loss = 0.04421910643577576
In grad_steps = 1908, loss = 0.008559080772101879
In grad_steps = 1909, loss = 0.0955951139330864
In grad_steps = 1910, loss = 0.03034058026969433
In grad_steps = 1911, loss = 0.13605128228664398
In grad_steps = 1912, loss = 0.023318203166127205
In grad_steps = 1913, loss = 0.8150169253349304
In grad_steps = 1914, loss = 0.7593784332275391
In grad_steps = 1915, loss = 0.36665090918540955
In grad_steps = 1916, loss = 0.12046986818313599
In grad_steps = 1917, loss = 0.06927969306707382
In grad_steps = 1918, loss = 0.3728112280368805
In grad_steps = 1919, loss = 0.3024832606315613
In grad_steps = 1920, loss = 0.06120951473712921
In grad_steps = 1921, loss = 0.09481371194124222
In grad_steps = 1922, loss = 0.018773602321743965
In grad_steps = 1923, loss = 0.3521645665168762
In grad_steps = 1924, loss = 1.1548823118209839
In grad_steps = 1925, loss = 0.1123264953494072
In grad_steps = 1926, loss = 0.18982648849487305
In grad_steps = 1927, loss = 0.1133260726928711
In grad_steps = 1928, loss = 0.20444519817829132
In grad_steps = 1929, loss = 0.12953919172286987
In grad_steps = 1930, loss = 0.571898341178894
In grad_steps = 1931, loss = 0.6883927583694458
In grad_steps = 1932, loss = 0.2507237493991852
In grad_steps = 1933, loss = 0.6584455370903015
In grad_steps = 1934, loss = 0.07035563886165619
In grad_steps = 1935, loss = 0.31298011541366577
In grad_steps = 1936, loss = 0.04119453579187393
In grad_steps = 1937, loss = 0.1791759729385376
In grad_steps = 1938, loss = 0.0506625697016716
In grad_steps = 1939, loss = 0.8273208141326904
In grad_steps = 1940, loss = 0.19446341693401337
In grad_steps = 1941, loss = 0.41432613134384155
In grad_steps = 1942, loss = 0.11757530272006989
In grad_steps = 1943, loss = 0.17052631080150604
In grad_steps = 1944, loss = 0.8322274684906006
In grad_steps = 1945, loss = 1.4911905527114868
In grad_steps = 1946, loss = 0.7141415476799011
In grad_steps = 1947, loss = 0.7051379680633545
In grad_steps = 1948, loss = 0.5763792991638184
In grad_steps = 1949, loss = 0.5378749966621399
In grad_steps = 1950, loss = 0.5665087103843689
In grad_steps = 1951, loss = 0.4051107168197632
In grad_steps = 1952, loss = 0.44000381231307983
In grad_steps = 1953, loss = 0.21645070612430573
In grad_steps = 1954, loss = 0.3174377381801605
In grad_steps = 1955, loss = 0.49642205238342285
In grad_steps = 1956, loss = 0.39642098546028137
In grad_steps = 1957, loss = 0.48899734020233154
In grad_steps = 1958, loss = 0.13199904561042786
In grad_steps = 1959, loss = 0.23219314217567444
In grad_steps = 1960, loss = 0.1973980814218521
In grad_steps = 1961, loss = 0.36282211542129517
In grad_steps = 1962, loss = 0.3820416033267975
In grad_steps = 1963, loss = 0.3901723325252533
In grad_steps = 1964, loss = 0.13610321283340454
In grad_steps = 1965, loss = 0.29311782121658325
In grad_steps = 1966, loss = 0.07351610064506531
In grad_steps = 1967, loss = 0.06248856335878372
In grad_steps = 1968, loss = 0.22982363402843475
In grad_steps = 1969, loss = 1.451204538345337
In grad_steps = 1970, loss = 0.5624042749404907
In grad_steps = 1971, loss = 1.3243927955627441
In grad_steps = 1972, loss = 0.410128116607666
In grad_steps = 1973, loss = 0.12724094092845917
In grad_steps = 1974, loss = 0.09873111546039581
In grad_steps = 1975, loss = 0.10237244516611099
In grad_steps = 1976, loss = 0.4227064549922943
In grad_steps = 1977, loss = 0.14427946507930756
In grad_steps = 1978, loss = 0.17502674460411072
In grad_steps = 1979, loss = 0.9049381017684937
In grad_steps = 1980, loss = 0.28960227966308594
In grad_steps = 1981, loss = 0.41045886278152466
In grad_steps = 1982, loss = 0.22905102372169495
In grad_steps = 1983, loss = 0.1136196181178093
In grad_steps = 1984, loss = 0.2691665291786194
In grad_steps = 1985, loss = 0.6857588291168213
In grad_steps = 1986, loss = 0.3256117105484009
In grad_steps = 1987, loss = 0.35423630475997925
In grad_steps = 1988, loss = 1.138689398765564
In grad_steps = 1989, loss = 0.6266561150550842
In grad_steps = 1990, loss = 0.08812887221574783
In grad_steps = 1991, loss = 0.6532313227653503
In grad_steps = 1992, loss = 0.4185131788253784
In grad_steps = 1993, loss = 0.2838421165943146
In grad_steps = 1994, loss = 0.773688018321991
In grad_steps = 1995, loss = 0.22163259983062744
In grad_steps = 1996, loss = 0.41808557510375977
In grad_steps = 1997, loss = 0.30111750960350037
In grad_steps = 1998, loss = 0.13556890189647675
In grad_steps = 1999, loss = 0.23799461126327515
In grad_steps = 2000, loss = 0.16284878551959991
In grad_steps = 2001, loss = 0.9735093116760254
In grad_steps = 2002, loss = 0.2635914087295532
In grad_steps = 2003, loss = 0.16459962725639343
In grad_steps = 2004, loss = 0.24493607878684998
In grad_steps = 2005, loss = 0.18083512783050537
In grad_steps = 2006, loss = 0.18809522688388824
In grad_steps = 2007, loss = 0.1812141388654709
In grad_steps = 2008, loss = 0.24074521660804749
In grad_steps = 2009, loss = 0.03871326893568039
In grad_steps = 2010, loss = 0.20149916410446167
In grad_steps = 2011, loss = 0.2301335632801056
In grad_steps = 2012, loss = 0.6029204726219177
In grad_steps = 2013, loss = 0.3756735026836395
In grad_steps = 2014, loss = 0.10137183964252472
In grad_steps = 2015, loss = 0.175294429063797
In grad_steps = 2016, loss = 0.08553658425807953
In grad_steps = 2017, loss = 0.7650251388549805
In grad_steps = 2018, loss = 0.6813603639602661
In grad_steps = 2019, loss = 0.46646764874458313
In grad_steps = 2020, loss = 0.09986934065818787
In grad_steps = 2021, loss = 0.07176212966442108
In grad_steps = 2022, loss = 0.04460303857922554
In grad_steps = 2023, loss = 0.21201950311660767
In grad_steps = 2024, loss = 0.5776455402374268
In grad_steps = 2025, loss = 0.22513677179813385
In grad_steps = 2026, loss = 0.5512359738349915
In grad_steps = 2027, loss = 0.47701695561408997
In grad_steps = 2028, loss = 0.9627599716186523
In grad_steps = 2029, loss = 0.1246429979801178
In grad_steps = 2030, loss = 0.06820858269929886
In grad_steps = 2031, loss = 0.028570793569087982
In grad_steps = 2032, loss = 0.9979272484779358
In grad_steps = 2033, loss = 0.49401065707206726
In grad_steps = 2034, loss = 0.5359289646148682
In grad_steps = 2035, loss = 0.02287386916577816
In grad_steps = 2036, loss = 0.041628967970609665
In grad_steps = 2037, loss = 0.09767557680606842
In grad_steps = 2038, loss = 0.7042151689529419
In grad_steps = 2039, loss = 0.06286448240280151
In grad_steps = 2040, loss = 0.04911332204937935
In grad_steps = 2041, loss = 0.6067942380905151
In grad_steps = 2042, loss = 0.1536937803030014
In grad_steps = 2043, loss = 0.42426976561546326
In grad_steps = 2044, loss = 0.3868422508239746
In grad_steps = 2045, loss = 0.5545313954353333
In grad_steps = 2046, loss = 0.04370967671275139
In grad_steps = 2047, loss = 0.12984590232372284
In grad_steps = 2048, loss = 0.21643444895744324
In grad_steps = 2049, loss = 0.7544608116149902
In grad_steps = 2050, loss = 0.2745286822319031
In grad_steps = 2051, loss = 0.20387786626815796
In grad_steps = 2052, loss = 0.012008163146674633
Beginning epoch 2
In grad_steps = 2053, loss = 0.3336552381515503
In grad_steps = 2054, loss = 0.07860971987247467
In grad_steps = 2055, loss = 0.17457422614097595
In grad_steps = 2056, loss = 0.4040967524051666
In grad_steps = 2057, loss = 0.11160923540592194
In grad_steps = 2058, loss = 0.04472291097044945
In grad_steps = 2059, loss = 0.1096545085310936
In grad_steps = 2060, loss = 0.07343713939189911
In grad_steps = 2061, loss = 1.0533446073532104
In grad_steps = 2062, loss = 0.40015435218811035
In grad_steps = 2063, loss = 0.8555083870887756
In grad_steps = 2064, loss = 0.04036225005984306
In grad_steps = 2065, loss = 0.38311827182769775
In grad_steps = 2066, loss = 0.056632257997989655
In grad_steps = 2067, loss = 0.29451635479927063
In grad_steps = 2068, loss = 0.5039855241775513
In grad_steps = 2069, loss = 0.3384499251842499
In grad_steps = 2070, loss = 0.45357513427734375
In grad_steps = 2071, loss = 0.19922566413879395
In grad_steps = 2072, loss = 0.6535990238189697
In grad_steps = 2073, loss = 0.06327061355113983
In grad_steps = 2074, loss = 0.6344708204269409
In grad_steps = 2075, loss = 1.1817210912704468
In grad_steps = 2076, loss = 0.1299584060907364
In grad_steps = 2077, loss = 0.01582699455320835
In grad_steps = 2078, loss = 0.5489410758018494
In grad_steps = 2079, loss = 0.3040686249732971
In grad_steps = 2080, loss = 1.0769248008728027
In grad_steps = 2081, loss = 0.12655383348464966
In grad_steps = 2082, loss = 0.1560380905866623
In grad_steps = 2083, loss = 0.5366725325584412
In grad_steps = 2084, loss = 0.5792737007141113
In grad_steps = 2085, loss = 0.3252698481082916
In grad_steps = 2086, loss = 0.6391554474830627
In grad_steps = 2087, loss = 0.18702241778373718
In grad_steps = 2088, loss = 0.2933197617530823
In grad_steps = 2089, loss = 0.3423883020877838
In grad_steps = 2090, loss = 0.17845016717910767
In grad_steps = 2091, loss = 0.2850581109523773
In grad_steps = 2092, loss = 0.2709724009037018
In grad_steps = 2093, loss = 0.4961685538291931
In grad_steps = 2094, loss = 0.11158318817615509
In grad_steps = 2095, loss = 0.2680521309375763
In grad_steps = 2096, loss = 0.19739755988121033
In grad_steps = 2097, loss = 0.24908991158008575
In grad_steps = 2098, loss = 0.34763655066490173
In grad_steps = 2099, loss = 0.9990078210830688
In grad_steps = 2100, loss = 0.5650346875190735
In grad_steps = 2101, loss = 0.11963827908039093
In grad_steps = 2102, loss = 0.08605112880468369
In grad_steps = 2103, loss = 0.4374207854270935
In grad_steps = 2104, loss = 0.07234300673007965
In grad_steps = 2105, loss = 0.12033748626708984
In grad_steps = 2106, loss = 0.025960249826312065
In grad_steps = 2107, loss = 0.029530707746744156
In grad_steps = 2108, loss = 0.13782739639282227
In grad_steps = 2109, loss = 0.45476123690605164
In grad_steps = 2110, loss = 0.3870384097099304
In grad_steps = 2111, loss = 0.10070595145225525
In grad_steps = 2112, loss = 0.9319539070129395
In grad_steps = 2113, loss = 1.5503861904144287
In grad_steps = 2114, loss = 0.3062366843223572
In grad_steps = 2115, loss = 0.22109386324882507
In grad_steps = 2116, loss = 0.14504684507846832
In grad_steps = 2117, loss = 0.10414521396160126
In grad_steps = 2118, loss = 0.18791691958904266
In grad_steps = 2119, loss = 0.519680380821228
In grad_steps = 2120, loss = 0.05255240574479103
In grad_steps = 2121, loss = 0.1998577117919922
In grad_steps = 2122, loss = 0.12929362058639526
In grad_steps = 2123, loss = 0.17057150602340698
In grad_steps = 2124, loss = 0.17316976189613342
In grad_steps = 2125, loss = 0.27413517236709595
In grad_steps = 2126, loss = 0.16881516575813293
In grad_steps = 2127, loss = 0.041714977473020554
In grad_steps = 2128, loss = 0.355201780796051
In grad_steps = 2129, loss = 0.2069309502840042
In grad_steps = 2130, loss = 0.40003713965415955
In grad_steps = 2131, loss = 0.2933964431285858
In grad_steps = 2132, loss = 0.5266671776771545
In grad_steps = 2133, loss = 0.4554751217365265
In grad_steps = 2134, loss = 0.1664828360080719
In grad_steps = 2135, loss = 0.1348160207271576
In grad_steps = 2136, loss = 0.8417903184890747
In grad_steps = 2137, loss = 0.0761682540178299
In grad_steps = 2138, loss = 0.7772724628448486
In grad_steps = 2139, loss = 0.21589522063732147
In grad_steps = 2140, loss = 0.13090023398399353
In grad_steps = 2141, loss = 1.1004576683044434
In grad_steps = 2142, loss = 0.11812043935060501
In grad_steps = 2143, loss = 0.7468212842941284
In grad_steps = 2144, loss = 0.6023116707801819
In grad_steps = 2145, loss = 0.4289509952068329
In grad_steps = 2146, loss = 0.31867021322250366
In grad_steps = 2147, loss = 0.35595154762268066
In grad_steps = 2148, loss = 0.04345837980508804
In grad_steps = 2149, loss = 0.27923452854156494
In grad_steps = 2150, loss = 0.3842048943042755
In grad_steps = 2151, loss = 0.4078405499458313
In grad_steps = 2152, loss = 0.1688677966594696
In grad_steps = 2153, loss = 1.023807406425476
In grad_steps = 2154, loss = 0.1004694253206253
In grad_steps = 2155, loss = 0.6284589767456055
In grad_steps = 2156, loss = 0.2180962860584259
In grad_steps = 2157, loss = 0.3357676863670349
In grad_steps = 2158, loss = 0.19598671793937683
In grad_steps = 2159, loss = 0.20906080305576324
In grad_steps = 2160, loss = 0.11180634051561356
In grad_steps = 2161, loss = 0.4055064022541046
In grad_steps = 2162, loss = 0.059523530304431915
In grad_steps = 2163, loss = 0.20929861068725586
In grad_steps = 2164, loss = 0.02824416756629944
In grad_steps = 2165, loss = 0.03675060346722603
In grad_steps = 2166, loss = 0.4169486463069916
In grad_steps = 2167, loss = 0.07905755192041397
In grad_steps = 2168, loss = 0.8680239319801331
In grad_steps = 2169, loss = 0.15174075961112976
In grad_steps = 2170, loss = 0.09505843371152878
In grad_steps = 2171, loss = 0.08771535009145737
In grad_steps = 2172, loss = 0.10582849383354187
In grad_steps = 2173, loss = 0.3325921893119812
In grad_steps = 2174, loss = 0.14145028591156006
In grad_steps = 2175, loss = 0.019756793975830078
In grad_steps = 2176, loss = 0.29458126425743103
In grad_steps = 2177, loss = 0.20890328288078308
In grad_steps = 2178, loss = 0.4721161127090454
In grad_steps = 2179, loss = 0.052845537662506104
In grad_steps = 2180, loss = 0.6782451868057251
In grad_steps = 2181, loss = 0.30393946170806885
In grad_steps = 2182, loss = 0.2748117446899414
In grad_steps = 2183, loss = 0.08850023150444031
In grad_steps = 2184, loss = 0.0962933599948883
In grad_steps = 2185, loss = 0.6072782278060913
In grad_steps = 2186, loss = 0.4973430633544922
In grad_steps = 2187, loss = 0.07322842627763748
In grad_steps = 2188, loss = 0.18595780432224274
In grad_steps = 2189, loss = 0.04038209095597267
In grad_steps = 2190, loss = 0.17043490707874298
In grad_steps = 2191, loss = 1.2956652641296387
In grad_steps = 2192, loss = 0.05480186268687248
In grad_steps = 2193, loss = 1.0992789268493652
In grad_steps = 2194, loss = 0.023665443062782288
In grad_steps = 2195, loss = 0.20205682516098022
In grad_steps = 2196, loss = 0.034352123737335205
In grad_steps = 2197, loss = 0.5407356023788452
In grad_steps = 2198, loss = 0.16066813468933105
In grad_steps = 2199, loss = 0.24063211679458618
In grad_steps = 2200, loss = 0.1383533775806427
In grad_steps = 2201, loss = 0.08728918433189392
In grad_steps = 2202, loss = 0.08995065838098526
In grad_steps = 2203, loss = 0.45973512530326843
In grad_steps = 2204, loss = 0.04241243004798889
In grad_steps = 2205, loss = 1.0574320554733276
In grad_steps = 2206, loss = 0.16330915689468384
In grad_steps = 2207, loss = 0.20428551733493805
In grad_steps = 2208, loss = 0.058414217084646225
In grad_steps = 2209, loss = 0.09205995500087738
In grad_steps = 2210, loss = 0.04465151205658913
In grad_steps = 2211, loss = 0.09348497539758682
In grad_steps = 2212, loss = 0.14747607707977295
In grad_steps = 2213, loss = 0.07886584848165512
In grad_steps = 2214, loss = 0.1559842824935913
In grad_steps = 2215, loss = 0.1852419376373291
In grad_steps = 2216, loss = 0.07148917019367218
In grad_steps = 2217, loss = 0.24533136188983917
In grad_steps = 2218, loss = 0.0479743666946888
In grad_steps = 2219, loss = 0.014901382848620415
In grad_steps = 2220, loss = 0.022165542468428612
In grad_steps = 2221, loss = 0.5930906534194946
In grad_steps = 2222, loss = 0.018508512526750565
In grad_steps = 2223, loss = 0.021493667736649513
In grad_steps = 2224, loss = 0.4054645299911499
In grad_steps = 2225, loss = 0.0928051769733429
In grad_steps = 2226, loss = 1.3870092630386353
In grad_steps = 2227, loss = 0.0188666470348835
In grad_steps = 2228, loss = 1.3241972923278809
In grad_steps = 2229, loss = 0.013239012099802494
In grad_steps = 2230, loss = 0.017892319709062576
In grad_steps = 2231, loss = 0.2388729602098465
In grad_steps = 2232, loss = 0.1801009625196457
In grad_steps = 2233, loss = 0.08809369802474976
In grad_steps = 2234, loss = 0.09185049682855606
In grad_steps = 2235, loss = 0.10007598251104355
In grad_steps = 2236, loss = 0.9722326397895813
In grad_steps = 2237, loss = 0.07103399187326431
In grad_steps = 2238, loss = 0.5427309274673462
In grad_steps = 2239, loss = 0.04657590389251709
In grad_steps = 2240, loss = 0.5221571326255798
In grad_steps = 2241, loss = 0.1647193729877472
In grad_steps = 2242, loss = 0.1786205768585205
In grad_steps = 2243, loss = 0.11278147995471954
In grad_steps = 2244, loss = 0.2726439833641052
In grad_steps = 2245, loss = 0.16577303409576416
In grad_steps = 2246, loss = 0.6863086223602295
In grad_steps = 2247, loss = 0.06571221351623535
In grad_steps = 2248, loss = 0.13497516512870789
In grad_steps = 2249, loss = 0.07362950593233109
In grad_steps = 2250, loss = 0.6006235480308533
In grad_steps = 2251, loss = 0.12028063088655472
In grad_steps = 2252, loss = 1.0778868198394775
In grad_steps = 2253, loss = 0.039687514305114746
In grad_steps = 2254, loss = 0.3130079209804535
In grad_steps = 2255, loss = 0.055930245667696
In grad_steps = 2256, loss = 0.3627874255180359
In grad_steps = 2257, loss = 0.45169010758399963
In grad_steps = 2258, loss = 0.4671814441680908
In grad_steps = 2259, loss = 0.06840125471353531
In grad_steps = 2260, loss = 0.30197715759277344
In grad_steps = 2261, loss = 0.23747260868549347
In grad_steps = 2262, loss = 0.082506462931633
In grad_steps = 2263, loss = 0.3085232079029083
In grad_steps = 2264, loss = 0.19824951887130737
In grad_steps = 2265, loss = 0.9797772169113159
In grad_steps = 2266, loss = 0.3478069603443146
In grad_steps = 2267, loss = 0.09442031383514404
In grad_steps = 2268, loss = 0.12728843092918396
In grad_steps = 2269, loss = 0.831566333770752
In grad_steps = 2270, loss = 0.7052088975906372
In grad_steps = 2271, loss = 0.6033738851547241
In grad_steps = 2272, loss = 0.3300882875919342
In grad_steps = 2273, loss = 0.39283862709999084
In grad_steps = 2274, loss = 0.7593699097633362
In grad_steps = 2275, loss = 0.42575398087501526
In grad_steps = 2276, loss = 0.2094496190547943
In grad_steps = 2277, loss = 0.1084798127412796
In grad_steps = 2278, loss = 0.12065941840410233
In grad_steps = 2279, loss = 0.38467302918434143
In grad_steps = 2280, loss = 0.1932111233472824
In grad_steps = 2281, loss = 0.26634275913238525
In grad_steps = 2282, loss = 0.23181255161762238
In grad_steps = 2283, loss = 0.15065085887908936
In grad_steps = 2284, loss = 0.5698330998420715
In grad_steps = 2285, loss = 1.0248196125030518
In grad_steps = 2286, loss = 0.11889612674713135
In grad_steps = 2287, loss = 0.8428671360015869
In grad_steps = 2288, loss = 0.3736107051372528
In grad_steps = 2289, loss = 0.5806169509887695
In grad_steps = 2290, loss = 0.1504332423210144
In grad_steps = 2291, loss = 0.09444041550159454
In grad_steps = 2292, loss = 0.16183334589004517
In grad_steps = 2293, loss = 0.10797982662916183
In grad_steps = 2294, loss = 0.3073751628398895
In grad_steps = 2295, loss = 0.16113626956939697
In grad_steps = 2296, loss = 0.16747620701789856
In grad_steps = 2297, loss = 0.15363653004169464
In grad_steps = 2298, loss = 0.15229471027851105
In grad_steps = 2299, loss = 0.09043373912572861
In grad_steps = 2300, loss = 0.1423059105873108
In grad_steps = 2301, loss = 1.153667688369751
In grad_steps = 2302, loss = 0.08533371984958649
In grad_steps = 2303, loss = 0.24307024478912354
In grad_steps = 2304, loss = 0.11931881308555603
In grad_steps = 2305, loss = 0.05574147030711174
In grad_steps = 2306, loss = 0.06484712660312653
In grad_steps = 2307, loss = 0.050554752349853516
In grad_steps = 2308, loss = 0.5618166327476501
In grad_steps = 2309, loss = 1.1155627965927124
In grad_steps = 2310, loss = 0.08278831094503403
In grad_steps = 2311, loss = 0.6835456490516663
In grad_steps = 2312, loss = 0.15312162041664124
In grad_steps = 2313, loss = 0.2895450294017792
In grad_steps = 2314, loss = 1.1458150148391724
In grad_steps = 2315, loss = 0.8275324702262878
In grad_steps = 2316, loss = 0.5047835111618042
In grad_steps = 2317, loss = 0.23328562080860138
In grad_steps = 2318, loss = 0.2647351920604706
In grad_steps = 2319, loss = 0.20427179336547852
In grad_steps = 2320, loss = 0.8654329776763916
In grad_steps = 2321, loss = 0.22378286719322205
In grad_steps = 2322, loss = 0.46160486340522766
In grad_steps = 2323, loss = 0.28847262263298035
In grad_steps = 2324, loss = 0.3592996597290039
In grad_steps = 2325, loss = 0.19859035313129425
In grad_steps = 2326, loss = 0.10338741540908813
In grad_steps = 2327, loss = 0.508399248123169
In grad_steps = 2328, loss = 0.273448646068573
In grad_steps = 2329, loss = 0.22651182115077972
In grad_steps = 2330, loss = 0.10177115350961685
In grad_steps = 2331, loss = 0.42643165588378906
In grad_steps = 2332, loss = 0.11249292641878128
In grad_steps = 2333, loss = 0.2909408211708069
In grad_steps = 2334, loss = 0.07854333519935608
In grad_steps = 2335, loss = 0.5194793939590454
In grad_steps = 2336, loss = 0.532188892364502
In grad_steps = 2337, loss = 0.17185068130493164
In grad_steps = 2338, loss = 0.10678283870220184
In grad_steps = 2339, loss = 0.03642885759472847
In grad_steps = 2340, loss = 0.23236143589019775
In grad_steps = 2341, loss = 0.3264648914337158
In grad_steps = 2342, loss = 0.4341820478439331
In grad_steps = 2343, loss = 0.2294512838125229
In grad_steps = 2344, loss = 0.15397746860980988
In grad_steps = 2345, loss = 0.4226573705673218
In grad_steps = 2346, loss = 0.10449355840682983
In grad_steps = 2347, loss = 0.22787804901599884
In grad_steps = 2348, loss = 0.7917617559432983
In grad_steps = 2349, loss = 1.0300767421722412
In grad_steps = 2350, loss = 0.802648663520813
In grad_steps = 2351, loss = 0.04058045893907547
In grad_steps = 2352, loss = 0.1668773889541626
In grad_steps = 2353, loss = 0.9397621750831604
In grad_steps = 2354, loss = 0.4436882436275482
In grad_steps = 2355, loss = 0.16454333066940308
In grad_steps = 2356, loss = 0.1409984976053238
In grad_steps = 2357, loss = 0.029620882123708725
In grad_steps = 2358, loss = 0.057946156710386276
In grad_steps = 2359, loss = 0.3752272129058838
In grad_steps = 2360, loss = 0.8235799670219421
In grad_steps = 2361, loss = 1.1281828880310059
In grad_steps = 2362, loss = 0.25885796546936035
In grad_steps = 2363, loss = 0.5756509900093079
In grad_steps = 2364, loss = 0.3258121609687805
In grad_steps = 2365, loss = 0.10301057249307632
In grad_steps = 2366, loss = 0.09353052079677582
In grad_steps = 2367, loss = 0.1829012781381607
In grad_steps = 2368, loss = 0.05803990364074707
In grad_steps = 2369, loss = 1.1087911128997803
In grad_steps = 2370, loss = 0.5619730353355408
In grad_steps = 2371, loss = 0.25981080532073975
In grad_steps = 2372, loss = 0.08036849647760391
In grad_steps = 2373, loss = 0.33624565601348877
In grad_steps = 2374, loss = 0.18381187319755554
In grad_steps = 2375, loss = 0.5524896383285522
In grad_steps = 2376, loss = 0.14577801525592804
In grad_steps = 2377, loss = 0.08936681598424911
In grad_steps = 2378, loss = 0.3323424458503723
In grad_steps = 2379, loss = 0.5128223299980164
In grad_steps = 2380, loss = 0.2403344213962555
In grad_steps = 2381, loss = 0.03112482652068138
In grad_steps = 2382, loss = 0.5202680230140686
In grad_steps = 2383, loss = 0.07175741344690323
In grad_steps = 2384, loss = 0.029809661209583282
In grad_steps = 2385, loss = 0.026766959577798843
In grad_steps = 2386, loss = 0.23372159898281097
In grad_steps = 2387, loss = 0.23756754398345947
In grad_steps = 2388, loss = 0.037044707685709
In grad_steps = 2389, loss = 0.051044099032878876
In grad_steps = 2390, loss = 0.023573294281959534
In grad_steps = 2391, loss = 0.039541106671094894
In grad_steps = 2392, loss = 0.2608490586280823
In grad_steps = 2393, loss = 0.022898908704519272
In grad_steps = 2394, loss = 0.7863988280296326
In grad_steps = 2395, loss = 0.02010965719819069
In grad_steps = 2396, loss = 1.1146821975708008
In grad_steps = 2397, loss = 0.13677628338336945
In grad_steps = 2398, loss = 0.6616381406784058
In grad_steps = 2399, loss = 0.1739465296268463
In grad_steps = 2400, loss = 0.31210625171661377
In grad_steps = 2401, loss = 0.35728827118873596
In grad_steps = 2402, loss = 0.07668646425008774
In grad_steps = 2403, loss = 0.031141964718699455
In grad_steps = 2404, loss = 0.1392582654953003
In grad_steps = 2405, loss = 0.1270274668931961
In grad_steps = 2406, loss = 0.8430308699607849
In grad_steps = 2407, loss = 0.13451363146305084
In grad_steps = 2408, loss = 0.19153356552124023
In grad_steps = 2409, loss = 0.2760500907897949
In grad_steps = 2410, loss = 0.457319438457489
In grad_steps = 2411, loss = 0.20954713225364685
In grad_steps = 2412, loss = 0.7478585243225098
In grad_steps = 2413, loss = 0.8296527862548828
In grad_steps = 2414, loss = 0.05771995335817337
In grad_steps = 2415, loss = 0.4058605134487152
In grad_steps = 2416, loss = 1.2084405422210693
In grad_steps = 2417, loss = 0.07978501170873642
In grad_steps = 2418, loss = 0.8001726269721985
In grad_steps = 2419, loss = 0.3533523678779602
In grad_steps = 2420, loss = 0.23291705548763275
In grad_steps = 2421, loss = 0.3723684847354889
In grad_steps = 2422, loss = 0.2767532467842102
In grad_steps = 2423, loss = 0.49282658100128174
In grad_steps = 2424, loss = 0.14576178789138794
In grad_steps = 2425, loss = 0.18083098530769348
In grad_steps = 2426, loss = 0.4253920912742615
In grad_steps = 2427, loss = 0.1259651929140091
In grad_steps = 2428, loss = 0.38593417406082153
In grad_steps = 2429, loss = 0.29677557945251465
In grad_steps = 2430, loss = 0.2677396535873413
In grad_steps = 2431, loss = 0.29036077857017517
In grad_steps = 2432, loss = 0.7128528952598572
In grad_steps = 2433, loss = 0.4179292917251587
In grad_steps = 2434, loss = 0.4308297038078308
In grad_steps = 2435, loss = 0.13705411553382874
In grad_steps = 2436, loss = 0.06713096797466278
In grad_steps = 2437, loss = 0.07837098091840744
In grad_steps = 2438, loss = 0.06749872863292694
In grad_steps = 2439, loss = 0.05747886002063751
In grad_steps = 2440, loss = 0.7478680610656738
In grad_steps = 2441, loss = 0.35248592495918274
In grad_steps = 2442, loss = 0.25622791051864624
In grad_steps = 2443, loss = 0.21128033101558685
In grad_steps = 2444, loss = 0.7867507934570312
In grad_steps = 2445, loss = 0.3272576928138733
In grad_steps = 2446, loss = 0.07840748131275177
In grad_steps = 2447, loss = 0.11572608351707458
In grad_steps = 2448, loss = 1.0674299001693726
In grad_steps = 2449, loss = 1.018943428993225
In grad_steps = 2450, loss = 0.009883756749331951
In grad_steps = 2451, loss = 1.6333086490631104
In grad_steps = 2452, loss = 0.37807393074035645
In grad_steps = 2453, loss = 0.5985082983970642
In grad_steps = 2454, loss = 0.135975182056427
In grad_steps = 2455, loss = 0.2003948986530304
In grad_steps = 2456, loss = 0.14338833093643188
In grad_steps = 2457, loss = 0.08662000298500061
In grad_steps = 2458, loss = 0.1786864995956421
In grad_steps = 2459, loss = 0.47935038805007935
In grad_steps = 2460, loss = 0.05640421807765961
In grad_steps = 2461, loss = 0.16811436414718628
In grad_steps = 2462, loss = 0.22971922159194946
In grad_steps = 2463, loss = 0.6753957271575928
In grad_steps = 2464, loss = 0.3686354160308838
In grad_steps = 2465, loss = 0.14340516924858093
In grad_steps = 2466, loss = 0.6293215751647949
In grad_steps = 2467, loss = 0.18373136222362518
In grad_steps = 2468, loss = 0.453289270401001
In grad_steps = 2469, loss = 1.2284045219421387
In grad_steps = 2470, loss = 0.09983447194099426
In grad_steps = 2471, loss = 0.1593734174966812
In grad_steps = 2472, loss = 0.2621053457260132
In grad_steps = 2473, loss = 0.7165108323097229
In grad_steps = 2474, loss = 0.10376131534576416
In grad_steps = 2475, loss = 0.13102085888385773
In grad_steps = 2476, loss = 0.3318244218826294
In grad_steps = 2477, loss = 0.083403579890728
In grad_steps = 2478, loss = 0.05135006457567215
In grad_steps = 2479, loss = 0.1318942755460739
In grad_steps = 2480, loss = 0.14098142087459564
In grad_steps = 2481, loss = 0.23743274807929993
In grad_steps = 2482, loss = 0.7225250005722046
In grad_steps = 2483, loss = 0.08532749861478806
In grad_steps = 2484, loss = 0.5234186053276062
In grad_steps = 2485, loss = 0.3222823739051819
In grad_steps = 2486, loss = 1.0156868696212769
In grad_steps = 2487, loss = 0.05205199867486954
In grad_steps = 2488, loss = 0.11388564109802246
In grad_steps = 2489, loss = 0.14328429102897644
In grad_steps = 2490, loss = 0.17754912376403809
In grad_steps = 2491, loss = 0.051641810685396194
In grad_steps = 2492, loss = 0.3825796842575073
In grad_steps = 2493, loss = 0.08817271143198013
In grad_steps = 2494, loss = 0.03792492672801018
In grad_steps = 2495, loss = 0.25287702679634094
In grad_steps = 2496, loss = 0.16848835349082947
In grad_steps = 2497, loss = 0.5596203804016113
In grad_steps = 2498, loss = 0.12419123202562332
In grad_steps = 2499, loss = 0.10102536529302597
In grad_steps = 2500, loss = 0.10261902958154678
In grad_steps = 2501, loss = 0.09297330677509308
In grad_steps = 2502, loss = 0.04907923564314842
In grad_steps = 2503, loss = 0.032809067517519
In grad_steps = 2504, loss = 0.03462382033467293
In grad_steps = 2505, loss = 0.08323967456817627
In grad_steps = 2506, loss = 0.019483475014567375
In grad_steps = 2507, loss = 0.06330558657646179
In grad_steps = 2508, loss = 0.058618463575839996
In grad_steps = 2509, loss = 0.6817472577095032
In grad_steps = 2510, loss = 0.010501137934625149
In grad_steps = 2511, loss = 0.339414119720459
In grad_steps = 2512, loss = 0.00504703214392066
In grad_steps = 2513, loss = 0.010296561755239964
In grad_steps = 2514, loss = 0.24716472625732422
In grad_steps = 2515, loss = 0.37112659215927124
In grad_steps = 2516, loss = 0.4393303096294403
In grad_steps = 2517, loss = 0.07189109921455383
In grad_steps = 2518, loss = 0.007104557007551193
In grad_steps = 2519, loss = 0.2568986415863037
In grad_steps = 2520, loss = 0.1950136423110962
In grad_steps = 2521, loss = 0.04827437549829483
In grad_steps = 2522, loss = 1.0058075189590454
In grad_steps = 2523, loss = 0.01087758969515562
In grad_steps = 2524, loss = 0.17587585747241974
In grad_steps = 2525, loss = 0.3584773540496826
In grad_steps = 2526, loss = 0.9170457124710083
In grad_steps = 2527, loss = 0.9004450440406799
In grad_steps = 2528, loss = 0.03602993115782738
In grad_steps = 2529, loss = 0.10143166035413742
In grad_steps = 2530, loss = 0.5140802264213562
In grad_steps = 2531, loss = 0.13937465846538544
In grad_steps = 2532, loss = 0.12774883210659027
In grad_steps = 2533, loss = 0.03057224489748478
In grad_steps = 2534, loss = 0.11847721040248871
In grad_steps = 2535, loss = 0.11439666152000427
In grad_steps = 2536, loss = 0.30626994371414185
In grad_steps = 2537, loss = 0.059681881219148636
In grad_steps = 2538, loss = 0.19736303389072418
In grad_steps = 2539, loss = 0.10392528772354126
In grad_steps = 2540, loss = 0.605950117111206
In grad_steps = 2541, loss = 0.12969562411308289
In grad_steps = 2542, loss = 0.07727822661399841
In grad_steps = 2543, loss = 0.156386137008667
In grad_steps = 2544, loss = 0.12336505949497223
In grad_steps = 2545, loss = 0.09349505603313446
In grad_steps = 2546, loss = 0.08888450264930725
In grad_steps = 2547, loss = 0.27639850974082947
In grad_steps = 2548, loss = 0.07692146301269531
In grad_steps = 2549, loss = 0.4406246542930603
In grad_steps = 2550, loss = 0.5617168545722961
In grad_steps = 2551, loss = 0.2821738123893738
In grad_steps = 2552, loss = 0.10991984605789185
In grad_steps = 2553, loss = 0.04625389352440834
In grad_steps = 2554, loss = 0.029618581756949425
In grad_steps = 2555, loss = 1.5963983535766602
In grad_steps = 2556, loss = 0.09911850094795227
In grad_steps = 2557, loss = 0.2293437272310257
In grad_steps = 2558, loss = 0.015639863908290863
In grad_steps = 2559, loss = 0.16674183309078217
In grad_steps = 2560, loss = 0.09259935468435287
In grad_steps = 2561, loss = 0.3328556418418884
In grad_steps = 2562, loss = 0.8139111995697021
In grad_steps = 2563, loss = 0.06801169365644455
In grad_steps = 2564, loss = 0.20958589017391205
In grad_steps = 2565, loss = 0.07325518131256104
In grad_steps = 2566, loss = 0.035064663738012314
In grad_steps = 2567, loss = 0.08465705066919327
In grad_steps = 2568, loss = 0.04212242737412453
In grad_steps = 2569, loss = 0.054059892892837524
In grad_steps = 2570, loss = 0.07990793138742447
In grad_steps = 2571, loss = 0.09840408712625504
In grad_steps = 2572, loss = 0.03698151558637619
In grad_steps = 2573, loss = 0.6201105713844299
In grad_steps = 2574, loss = 0.1233111098408699
In grad_steps = 2575, loss = 0.6800419688224792
In grad_steps = 2576, loss = 0.6427860260009766
In grad_steps = 2577, loss = 0.09390455484390259
In grad_steps = 2578, loss = 0.3282930850982666
In grad_steps = 2579, loss = 0.3518858551979065
In grad_steps = 2580, loss = 0.5856008529663086
In grad_steps = 2581, loss = 0.4559282660484314
In grad_steps = 2582, loss = 0.05261215567588806
In grad_steps = 2583, loss = 0.06120456010103226
In grad_steps = 2584, loss = 0.047273311764001846
In grad_steps = 2585, loss = 0.8997348546981812
In grad_steps = 2586, loss = 0.7553333640098572
In grad_steps = 2587, loss = 0.0786789134144783
In grad_steps = 2588, loss = 1.702568769454956
In grad_steps = 2589, loss = 0.4985019266605377
In grad_steps = 2590, loss = 0.8572368621826172
In grad_steps = 2591, loss = 0.6115891337394714
In grad_steps = 2592, loss = 0.22832408547401428
In grad_steps = 2593, loss = 0.39109647274017334
In grad_steps = 2594, loss = 0.3883882462978363
In grad_steps = 2595, loss = 0.13592088222503662
In grad_steps = 2596, loss = 0.167545884847641
In grad_steps = 2597, loss = 0.3120247721672058
In grad_steps = 2598, loss = 0.35973578691482544
In grad_steps = 2599, loss = 0.45871472358703613
In grad_steps = 2600, loss = 0.43592265248298645
In grad_steps = 2601, loss = 0.2981640696525574
In grad_steps = 2602, loss = 0.5918955206871033
In grad_steps = 2603, loss = 0.18526963889598846
In grad_steps = 2604, loss = 0.275473952293396
In grad_steps = 2605, loss = 0.24610605835914612
In grad_steps = 2606, loss = 0.4918033182621002
In grad_steps = 2607, loss = 0.06452414393424988
In grad_steps = 2608, loss = 0.14188677072525024
In grad_steps = 2609, loss = 0.4302515685558319
In grad_steps = 2610, loss = 0.3489822745323181
In grad_steps = 2611, loss = 0.08555092662572861
In grad_steps = 2612, loss = 0.22091561555862427
In grad_steps = 2613, loss = 0.15506304800510406
In grad_steps = 2614, loss = 0.10262170433998108
In grad_steps = 2615, loss = 0.16677743196487427
In grad_steps = 2616, loss = 0.07368446886539459
In grad_steps = 2617, loss = 0.27230527997016907
In grad_steps = 2618, loss = 0.21462249755859375
In grad_steps = 2619, loss = 0.2900136709213257
In grad_steps = 2620, loss = 0.8517710566520691
In grad_steps = 2621, loss = 0.16021355986595154
In grad_steps = 2622, loss = 0.16055575013160706
In grad_steps = 2623, loss = 0.10917667299509048
In grad_steps = 2624, loss = 1.3674150705337524
In grad_steps = 2625, loss = 0.025457656010985374
In grad_steps = 2626, loss = 0.05462108924984932
In grad_steps = 2627, loss = 0.3128646910190582
In grad_steps = 2628, loss = 0.06767518818378448
In grad_steps = 2629, loss = 0.6534967422485352
In grad_steps = 2630, loss = 0.19039371609687805
In grad_steps = 2631, loss = 1.1379666328430176
In grad_steps = 2632, loss = 0.05606244504451752
In grad_steps = 2633, loss = 0.1646232306957245
In grad_steps = 2634, loss = 0.07999591529369354
In grad_steps = 2635, loss = 0.7186129093170166
In grad_steps = 2636, loss = 0.14976456761360168
In grad_steps = 2637, loss = 0.10739276558160782
In grad_steps = 2638, loss = 0.5648431181907654
In grad_steps = 2639, loss = 0.15073958039283752
In grad_steps = 2640, loss = 0.11136836558580399
In grad_steps = 2641, loss = 0.08288043737411499
In grad_steps = 2642, loss = 0.189275324344635
In grad_steps = 2643, loss = 0.1809684932231903
In grad_steps = 2644, loss = 0.2949828505516052
In grad_steps = 2645, loss = 0.12338707596063614
In grad_steps = 2646, loss = 0.3700144588947296
In grad_steps = 2647, loss = 0.4633297324180603
In grad_steps = 2648, loss = 0.7741107940673828
In grad_steps = 2649, loss = 0.3040991723537445
In grad_steps = 2650, loss = 0.3791007101535797
In grad_steps = 2651, loss = 0.1897076666355133
In grad_steps = 2652, loss = 0.6456683278083801
In grad_steps = 2653, loss = 0.5113189816474915
In grad_steps = 2654, loss = 0.10117104649543762
In grad_steps = 2655, loss = 0.37996116280555725
In grad_steps = 2656, loss = 0.04732252284884453
In grad_steps = 2657, loss = 0.39213624596595764
In grad_steps = 2658, loss = 0.9767710566520691
In grad_steps = 2659, loss = 0.37411585450172424
In grad_steps = 2660, loss = 0.3406687080860138
In grad_steps = 2661, loss = 0.4919709265232086
In grad_steps = 2662, loss = 0.06321756541728973
In grad_steps = 2663, loss = 0.1815296709537506
In grad_steps = 2664, loss = 0.2579784393310547
In grad_steps = 2665, loss = 0.3303872048854828
In grad_steps = 2666, loss = 0.09088927507400513
In grad_steps = 2667, loss = 0.07577511668205261
In grad_steps = 2668, loss = 0.5004342794418335
In grad_steps = 2669, loss = 0.12076017260551453
In grad_steps = 2670, loss = 0.11968448758125305
In grad_steps = 2671, loss = 0.15674830973148346
In grad_steps = 2672, loss = 0.07400534301996231
In grad_steps = 2673, loss = 0.0849931463599205
In grad_steps = 2674, loss = 0.03306835889816284
In grad_steps = 2675, loss = 0.02249038964509964
In grad_steps = 2676, loss = 0.03129300847649574
In grad_steps = 2677, loss = 0.04033045843243599
In grad_steps = 2678, loss = 0.02032901719212532
In grad_steps = 2679, loss = 0.36007004976272583
In grad_steps = 2680, loss = 0.06103852763772011
In grad_steps = 2681, loss = 0.1311514973640442
In grad_steps = 2682, loss = 0.9725616574287415
In grad_steps = 2683, loss = 0.015178310684859753
In grad_steps = 2684, loss = 0.13997100293636322
In grad_steps = 2685, loss = 0.2377057522535324
In grad_steps = 2686, loss = 0.22609329223632812
In grad_steps = 2687, loss = 0.09282234311103821
In grad_steps = 2688, loss = 0.513799250125885
In grad_steps = 2689, loss = 0.14402048289775848
In grad_steps = 2690, loss = 0.19455285370349884
In grad_steps = 2691, loss = 0.0773310735821724
In grad_steps = 2692, loss = 0.03176797181367874
In grad_steps = 2693, loss = 1.0397557020187378
In grad_steps = 2694, loss = 0.35562068223953247
In grad_steps = 2695, loss = 0.09479289501905441
In grad_steps = 2696, loss = 0.019471006467938423
In grad_steps = 2697, loss = 0.377460241317749
In grad_steps = 2698, loss = 0.13158674538135529
In grad_steps = 2699, loss = 0.06014208868145943
In grad_steps = 2700, loss = 1.0388692617416382
In grad_steps = 2701, loss = 0.04236229136586189
In grad_steps = 2702, loss = 0.23503682017326355
In grad_steps = 2703, loss = 1.2066181898117065
In grad_steps = 2704, loss = 0.058053672313690186
In grad_steps = 2705, loss = 0.1339246928691864
In grad_steps = 2706, loss = 0.07459978759288788
In grad_steps = 2707, loss = 0.9148386716842651
In grad_steps = 2708, loss = 0.4118509292602539
In grad_steps = 2709, loss = 0.4285392463207245
In grad_steps = 2710, loss = 0.5374630689620972
In grad_steps = 2711, loss = 0.08807653933763504
In grad_steps = 2712, loss = 0.129097118973732
In grad_steps = 2713, loss = 0.3609767556190491
In grad_steps = 2714, loss = 0.6210174560546875
In grad_steps = 2715, loss = 0.5845122337341309
In grad_steps = 2716, loss = 0.11192798614501953
In grad_steps = 2717, loss = 1.0254851579666138
In grad_steps = 2718, loss = 0.5629203915596008
In grad_steps = 2719, loss = 0.11446508765220642
In grad_steps = 2720, loss = 0.4660544991493225
In grad_steps = 2721, loss = 0.733296275138855
In grad_steps = 2722, loss = 0.18054074048995972
In grad_steps = 2723, loss = 0.23755067586898804
In grad_steps = 2724, loss = 0.4462238550186157
In grad_steps = 2725, loss = 0.15323033928871155
In grad_steps = 2726, loss = 0.8466330170631409
In grad_steps = 2727, loss = 0.049728378653526306
In grad_steps = 2728, loss = 0.7843432426452637
In grad_steps = 2729, loss = 0.5154942870140076
In grad_steps = 2730, loss = 0.1359182745218277
In grad_steps = 2731, loss = 0.09906838089227676
In grad_steps = 2732, loss = 0.171274334192276
In grad_steps = 2733, loss = 0.327884703874588
In grad_steps = 2734, loss = 0.16544021666049957
In grad_steps = 2735, loss = 0.9643734693527222
In grad_steps = 2736, loss = 0.1495560258626938
In grad_steps = 2737, loss = 0.22873833775520325
In grad_steps = 2738, loss = 0.3215622901916504
In grad_steps = 2739, loss = 0.13832782208919525
In grad_steps = 2740, loss = 0.12634262442588806
In grad_steps = 2741, loss = 0.13596388697624207
In grad_steps = 2742, loss = 0.22428111732006073
In grad_steps = 2743, loss = 0.1310478299856186
In grad_steps = 2744, loss = 0.2734373211860657
In grad_steps = 2745, loss = 0.021816784515976906
In grad_steps = 2746, loss = 1.1222589015960693
In grad_steps = 2747, loss = 0.3520999252796173
In grad_steps = 2748, loss = 0.1115615963935852
In grad_steps = 2749, loss = 0.07565924525260925
In grad_steps = 2750, loss = 0.10117954015731812
In grad_steps = 2751, loss = 0.037532590329647064
In grad_steps = 2752, loss = 0.12775205075740814
In grad_steps = 2753, loss = 0.4247561991214752
In grad_steps = 2754, loss = 0.28761112689971924
In grad_steps = 2755, loss = 0.024489179253578186
In grad_steps = 2756, loss = 0.08656279742717743
In grad_steps = 2757, loss = 0.020301897078752518
In grad_steps = 2758, loss = 1.6927200555801392
In grad_steps = 2759, loss = 0.053094491362571716
In grad_steps = 2760, loss = 0.12167046964168549
In grad_steps = 2761, loss = 0.1558072566986084
In grad_steps = 2762, loss = 0.054969534277915955
In grad_steps = 2763, loss = 0.1520790010690689
In grad_steps = 2764, loss = 0.015400391072034836
In grad_steps = 2765, loss = 0.031950823962688446
In grad_steps = 2766, loss = 1.7444190979003906
In grad_steps = 2767, loss = 0.02173861488699913
In grad_steps = 2768, loss = 1.088431477546692
In grad_steps = 2769, loss = 0.1507250815629959
In grad_steps = 2770, loss = 0.0973496288061142
In grad_steps = 2771, loss = 0.031418025493621826
In grad_steps = 2772, loss = 0.37026122212409973
In grad_steps = 2773, loss = 1.3689926862716675
In grad_steps = 2774, loss = 0.7009896039962769
In grad_steps = 2775, loss = 0.05441989749670029
In grad_steps = 2776, loss = 0.12612223625183105
In grad_steps = 2777, loss = 0.03220423310995102
In grad_steps = 2778, loss = 0.0863240510225296
In grad_steps = 2779, loss = 0.13174223899841309
In grad_steps = 2780, loss = 0.14944908022880554
In grad_steps = 2781, loss = 0.1294649988412857
In grad_steps = 2782, loss = 0.09846072643995285
In grad_steps = 2783, loss = 0.053888749331235886
In grad_steps = 2784, loss = 0.07579932361841202
In grad_steps = 2785, loss = 0.08514470607042313
In grad_steps = 2786, loss = 0.16660183668136597
In grad_steps = 2787, loss = 0.17520812153816223
In grad_steps = 2788, loss = 0.3910019099712372
In grad_steps = 2789, loss = 1.0422296524047852
In grad_steps = 2790, loss = 0.36611688137054443
In grad_steps = 2791, loss = 0.10952705889940262
In grad_steps = 2792, loss = 0.06515660881996155
In grad_steps = 2793, loss = 0.3183368146419525
In grad_steps = 2794, loss = 0.19966690242290497
In grad_steps = 2795, loss = 0.09827417135238647
In grad_steps = 2796, loss = 0.1547374129295349
In grad_steps = 2797, loss = 0.0973154678940773
In grad_steps = 2798, loss = 0.009200800210237503
In grad_steps = 2799, loss = 0.17241249978542328
In grad_steps = 2800, loss = 0.08314777910709381
In grad_steps = 2801, loss = 0.022938856855034828
In grad_steps = 2802, loss = 0.27374082803726196
In grad_steps = 2803, loss = 1.4707773923873901
In grad_steps = 2804, loss = 0.1281229853630066
In grad_steps = 2805, loss = 0.5124375224113464
In grad_steps = 2806, loss = 0.024015162140130997
In grad_steps = 2807, loss = 0.9651823043823242
In grad_steps = 2808, loss = 0.030735157430171967
In grad_steps = 2809, loss = 0.039223503321409225
In grad_steps = 2810, loss = 0.07768401503562927
In grad_steps = 2811, loss = 0.07909458875656128
In grad_steps = 2812, loss = 0.05298740789294243
In grad_steps = 2813, loss = 0.023485608398914337
In grad_steps = 2814, loss = 0.021562419831752777
In grad_steps = 2815, loss = 0.10625185072422028
In grad_steps = 2816, loss = 0.07829813659191132
In grad_steps = 2817, loss = 0.0334106981754303
In grad_steps = 2818, loss = 0.047266583889722824
In grad_steps = 2819, loss = 0.4137645959854126
In grad_steps = 2820, loss = 0.03283361718058586
In grad_steps = 2821, loss = 0.1055389791727066
In grad_steps = 2822, loss = 0.2750818133354187
In grad_steps = 2823, loss = 0.035338759422302246
In grad_steps = 2824, loss = 0.3788200914859772
In grad_steps = 2825, loss = 0.13985146582126617
In grad_steps = 2826, loss = 0.03185456991195679
In grad_steps = 2827, loss = 0.49740540981292725
In grad_steps = 2828, loss = 0.02143198624253273
In grad_steps = 2829, loss = 0.006950469687581062
In grad_steps = 2830, loss = 0.17135311663150787
In grad_steps = 2831, loss = 0.5010381937026978
In grad_steps = 2832, loss = 0.45353275537490845
In grad_steps = 2833, loss = 0.047567591071128845
In grad_steps = 2834, loss = 1.1571351289749146
In grad_steps = 2835, loss = 0.43420517444610596
In grad_steps = 2836, loss = 0.1645660698413849
In grad_steps = 2837, loss = 0.42864012718200684
In grad_steps = 2838, loss = 0.7821890115737915
In grad_steps = 2839, loss = 0.3933626115322113
In grad_steps = 2840, loss = 0.02963506430387497
In grad_steps = 2841, loss = 0.04895012080669403
In grad_steps = 2842, loss = 0.4532283544540405
In grad_steps = 2843, loss = 0.2772294878959656
In grad_steps = 2844, loss = 0.08311928063631058
In grad_steps = 2845, loss = 0.1316550374031067
In grad_steps = 2846, loss = 0.08339162915945053
In grad_steps = 2847, loss = 0.183946430683136
In grad_steps = 2848, loss = 0.0693015307188034
In grad_steps = 2849, loss = 0.5909169316291809
In grad_steps = 2850, loss = 0.1729111522436142
In grad_steps = 2851, loss = 0.07074198126792908
In grad_steps = 2852, loss = 0.10288392007350922
In grad_steps = 2853, loss = 0.11499584466218948
In grad_steps = 2854, loss = 0.03417624905705452
In grad_steps = 2855, loss = 0.04675552248954773
In grad_steps = 2856, loss = 0.05885367840528488
In grad_steps = 2857, loss = 0.019386177882552147
In grad_steps = 2858, loss = 0.05108939856290817
In grad_steps = 2859, loss = 0.8971595764160156
In grad_steps = 2860, loss = 0.09934841841459274
In grad_steps = 2861, loss = 0.06709245592355728
In grad_steps = 2862, loss = 0.5574676394462585
In grad_steps = 2863, loss = 0.012481506913900375
In grad_steps = 2864, loss = 0.015858901664614677
In grad_steps = 2865, loss = 0.0618097297847271
In grad_steps = 2866, loss = 0.07435347139835358
In grad_steps = 2867, loss = 0.05469657480716705
In grad_steps = 2868, loss = 0.02334689348936081
In grad_steps = 2869, loss = 0.008729992434382439
In grad_steps = 2870, loss = 0.15328843891620636
In grad_steps = 2871, loss = 0.020877281203866005
In grad_steps = 2872, loss = 0.7314503192901611
In grad_steps = 2873, loss = 0.010684257373213768
In grad_steps = 2874, loss = 0.5255059003829956
In grad_steps = 2875, loss = 0.19198647141456604
In grad_steps = 2876, loss = 0.08631786704063416
In grad_steps = 2877, loss = 0.013232523575425148
In grad_steps = 2878, loss = 0.1591878980398178
In grad_steps = 2879, loss = 0.0834120362997055
In grad_steps = 2880, loss = 0.07820121943950653
In grad_steps = 2881, loss = 0.25236427783966064
In grad_steps = 2882, loss = 0.0674414113163948
In grad_steps = 2883, loss = 0.3899683356285095
In grad_steps = 2884, loss = 0.05401859059929848
In grad_steps = 2885, loss = 0.19047313928604126
In grad_steps = 2886, loss = 0.5013108253479004
In grad_steps = 2887, loss = 0.5168540477752686
In grad_steps = 2888, loss = 0.12307122349739075
In grad_steps = 2889, loss = 0.554600715637207
In grad_steps = 2890, loss = 0.3419354557991028
In grad_steps = 2891, loss = 0.6099352836608887
In grad_steps = 2892, loss = 0.1476820856332779
In grad_steps = 2893, loss = 0.010318228043615818
In grad_steps = 2894, loss = 0.028609277680516243
In grad_steps = 2895, loss = 0.11677318811416626
In grad_steps = 2896, loss = 1.0696543455123901
In grad_steps = 2897, loss = 0.6922969222068787
In grad_steps = 2898, loss = 0.6328161358833313
In grad_steps = 2899, loss = 0.5544708967208862
In grad_steps = 2900, loss = 0.3447745144367218
In grad_steps = 2901, loss = 0.09939134865999222
In grad_steps = 2902, loss = 0.11638665199279785
In grad_steps = 2903, loss = 0.09625884890556335
In grad_steps = 2904, loss = 0.1716112643480301
In grad_steps = 2905, loss = 0.46058279275894165
In grad_steps = 2906, loss = 0.6462968587875366
In grad_steps = 2907, loss = 0.107198067009449
In grad_steps = 2908, loss = 0.2725057601928711
In grad_steps = 2909, loss = 0.2268102765083313
In grad_steps = 2910, loss = 0.3389241695404053
In grad_steps = 2911, loss = 0.3175099194049835
In grad_steps = 2912, loss = 0.2530696392059326
In grad_steps = 2913, loss = 0.15504109859466553
In grad_steps = 2914, loss = 0.7467778921127319
In grad_steps = 2915, loss = 0.053734079003334045
In grad_steps = 2916, loss = 0.23777559399604797
In grad_steps = 2917, loss = 0.11022623628377914
In grad_steps = 2918, loss = 0.15887992084026337
In grad_steps = 2919, loss = 0.15241093933582306
In grad_steps = 2920, loss = 0.11880524456501007
In grad_steps = 2921, loss = 0.08890228718519211
In grad_steps = 2922, loss = 0.047463804483413696
In grad_steps = 2923, loss = 0.33089450001716614
In grad_steps = 2924, loss = 0.03090478852391243
In grad_steps = 2925, loss = 0.4902266263961792
In grad_steps = 2926, loss = 0.17397503554821014
In grad_steps = 2927, loss = 0.4306028485298157
In grad_steps = 2928, loss = 0.07225993275642395
In grad_steps = 2929, loss = 0.02410523034632206
In grad_steps = 2930, loss = 0.009580951184034348
In grad_steps = 2931, loss = 0.5229283571243286
In grad_steps = 2932, loss = 1.5048484802246094
In grad_steps = 2933, loss = 0.05759167671203613
In grad_steps = 2934, loss = 0.7308696508407593
In grad_steps = 2935, loss = 0.21167059242725372
In grad_steps = 2936, loss = 0.11793068796396255
In grad_steps = 2937, loss = 0.026601165533065796
In grad_steps = 2938, loss = 0.20479737222194672
In grad_steps = 2939, loss = 0.1906018704175949
In grad_steps = 2940, loss = 1.0129026174545288
In grad_steps = 2941, loss = 0.03699634224176407
In grad_steps = 2942, loss = 0.030575569719076157
In grad_steps = 2943, loss = 0.14717811346054077
In grad_steps = 2944, loss = 0.774376630783081
In grad_steps = 2945, loss = 0.4294515550136566
In grad_steps = 2946, loss = 0.8574643731117249
In grad_steps = 2947, loss = 0.09306740760803223
In grad_steps = 2948, loss = 0.031338464468717575
In grad_steps = 2949, loss = 0.118075430393219
In grad_steps = 2950, loss = 0.07855600118637085
In grad_steps = 2951, loss = 0.9269872307777405
In grad_steps = 2952, loss = 0.41692548990249634
In grad_steps = 2953, loss = 0.4730357825756073
In grad_steps = 2954, loss = 0.45131704211235046
In grad_steps = 2955, loss = 0.17291459441184998
In grad_steps = 2956, loss = 0.9965013265609741
In grad_steps = 2957, loss = 0.4740954041481018
In grad_steps = 2958, loss = 0.1487644463777542
In grad_steps = 2959, loss = 0.5336257815361023
In grad_steps = 2960, loss = 0.12337334454059601
In grad_steps = 2961, loss = 0.044491976499557495
In grad_steps = 2962, loss = 0.43382176756858826
In grad_steps = 2963, loss = 0.12524110078811646
In grad_steps = 2964, loss = 0.5581696629524231
In grad_steps = 2965, loss = 0.08408860117197037
In grad_steps = 2966, loss = 0.29195019602775574
In grad_steps = 2967, loss = 0.2581734359264374
In grad_steps = 2968, loss = 1.2973573207855225
In grad_steps = 2969, loss = 0.07241111248731613
In grad_steps = 2970, loss = 0.21982978284358978
In grad_steps = 2971, loss = 0.07295060902833939
In grad_steps = 2972, loss = 0.11814238131046295
In grad_steps = 2973, loss = 0.12374680489301682
In grad_steps = 2974, loss = 0.6415685415267944
In grad_steps = 2975, loss = 0.3632565438747406
In grad_steps = 2976, loss = 0.08784271031618118
In grad_steps = 2977, loss = 0.10550706833600998
In grad_steps = 2978, loss = 0.07575872540473938
In grad_steps = 2979, loss = 0.27451053261756897
In grad_steps = 2980, loss = 0.01980455592274666
In grad_steps = 2981, loss = 0.2600134611129761
In grad_steps = 2982, loss = 0.019176119938492775
In grad_steps = 2983, loss = 0.4309345781803131
In grad_steps = 2984, loss = 0.051155176013708115
In grad_steps = 2985, loss = 0.01783871464431286
In grad_steps = 2986, loss = 0.2218170017004013
In grad_steps = 2987, loss = 0.14005868136882782
In grad_steps = 2988, loss = 0.6125998497009277
In grad_steps = 2989, loss = 0.1935591846704483
In grad_steps = 2990, loss = 0.08764678239822388
In grad_steps = 2991, loss = 0.05809051916003227
In grad_steps = 2992, loss = 0.2327447086572647
In grad_steps = 2993, loss = 0.5235720276832581
In grad_steps = 2994, loss = 0.2059105932712555
In grad_steps = 2995, loss = 0.007654719054698944
In grad_steps = 2996, loss = 1.3431283235549927
In grad_steps = 2997, loss = 0.1942155957221985
In grad_steps = 2998, loss = 0.6976942420005798
In grad_steps = 2999, loss = 0.7199627757072449
In grad_steps = 3000, loss = 0.5511479377746582
In grad_steps = 3001, loss = 0.19707506895065308
In grad_steps = 3002, loss = 0.5277087688446045
In grad_steps = 3003, loss = 0.5735355019569397
In grad_steps = 3004, loss = 1.2527029514312744
In grad_steps = 3005, loss = 0.005641366355121136
In grad_steps = 3006, loss = 0.21423187851905823
In grad_steps = 3007, loss = 0.19745488464832306
In grad_steps = 3008, loss = 0.24918478727340698
In grad_steps = 3009, loss = 0.12119859457015991
In grad_steps = 3010, loss = 0.23634310066699982
In grad_steps = 3011, loss = 0.4709734320640564
In grad_steps = 3012, loss = 0.9256865978240967
In grad_steps = 3013, loss = 0.13362914323806763
In grad_steps = 3014, loss = 0.4530685544013977
In grad_steps = 3015, loss = 0.1639411449432373
In grad_steps = 3016, loss = 0.25987210869789124
In grad_steps = 3017, loss = 0.182241752743721
In grad_steps = 3018, loss = 0.28653430938720703
In grad_steps = 3019, loss = 0.18231898546218872
In grad_steps = 3020, loss = 0.42611533403396606
In grad_steps = 3021, loss = 0.19527645409107208
In grad_steps = 3022, loss = 0.2321368306875229
In grad_steps = 3023, loss = 0.1072763204574585
In grad_steps = 3024, loss = 0.09962362796068192
In grad_steps = 3025, loss = 0.737967312335968
In grad_steps = 3026, loss = 0.11987767368555069
In grad_steps = 3027, loss = 0.06838355958461761
In grad_steps = 3028, loss = 0.1419442594051361
In grad_steps = 3029, loss = 0.07682737708091736
In grad_steps = 3030, loss = 0.12459056079387665
In grad_steps = 3031, loss = 1.0400575399398804
In grad_steps = 3032, loss = 1.2626675367355347
In grad_steps = 3033, loss = 1.058087706565857
In grad_steps = 3034, loss = 0.6412604451179504
In grad_steps = 3035, loss = 0.5130078792572021
In grad_steps = 3036, loss = 0.3481447696685791
In grad_steps = 3037, loss = 0.6060227751731873
In grad_steps = 3038, loss = 0.3710182309150696
In grad_steps = 3039, loss = 0.04067756235599518
In grad_steps = 3040, loss = 0.38766875863075256
In grad_steps = 3041, loss = 0.43243587017059326
In grad_steps = 3042, loss = 0.3176996111869812
In grad_steps = 3043, loss = 0.8687991499900818
In grad_steps = 3044, loss = 1.5293335914611816
In grad_steps = 3045, loss = 0.12996934354305267
In grad_steps = 3046, loss = 0.3072800040245056
In grad_steps = 3047, loss = 0.3599756360054016
In grad_steps = 3048, loss = 0.05209517106413841
In grad_steps = 3049, loss = 0.8514564037322998
In grad_steps = 3050, loss = 0.14467771351337433
In grad_steps = 3051, loss = 0.4819682240486145
In grad_steps = 3052, loss = 0.01661129668354988
In grad_steps = 3053, loss = 0.07822126895189285
In grad_steps = 3054, loss = 0.43332454562187195
In grad_steps = 3055, loss = 0.09303779900074005
In grad_steps = 3056, loss = 0.1944178342819214
In grad_steps = 3057, loss = 0.8227434158325195
In grad_steps = 3058, loss = 0.44243040680885315
In grad_steps = 3059, loss = 0.2547358274459839
In grad_steps = 3060, loss = 0.11844922602176666
In grad_steps = 3061, loss = 0.1267925500869751
In grad_steps = 3062, loss = 1.1566171646118164
In grad_steps = 3063, loss = 0.04490312933921814
In grad_steps = 3064, loss = 0.05372926592826843
In grad_steps = 3065, loss = 0.12980450689792633
In grad_steps = 3066, loss = 0.3810320198535919
In grad_steps = 3067, loss = 0.12208999693393707
In grad_steps = 3068, loss = 0.48446112871170044
In grad_steps = 3069, loss = 0.01616610772907734
In grad_steps = 3070, loss = 0.08007664233446121
In grad_steps = 3071, loss = 0.316224068403244
In grad_steps = 3072, loss = 0.6265732049942017
In grad_steps = 3073, loss = 1.1306043863296509
In grad_steps = 3074, loss = 0.4163327217102051
In grad_steps = 3075, loss = 0.18416689336299896
In grad_steps = 3076, loss = 0.1203361302614212
In grad_steps = 3077, loss = 0.2996678054332733
In grad_steps = 3078, loss = 0.7786601781845093
In grad_steps = 3079, loss = 0.1211894229054451
In grad_steps = 3080, loss = 0.6261059641838074
In grad_steps = 3081, loss = 0.16684192419052124
In grad_steps = 3082, loss = 0.14453303813934326
In grad_steps = 3083, loss = 0.253816157579422
In grad_steps = 3084, loss = 0.43887633085250854
In grad_steps = 3085, loss = 0.6051541566848755
In grad_steps = 3086, loss = 0.20969158411026
In grad_steps = 3087, loss = 0.2700507938861847
In grad_steps = 3088, loss = 0.4761140048503876
In grad_steps = 3089, loss = 0.6208436489105225
In grad_steps = 3090, loss = 0.1730271875858307
In grad_steps = 3091, loss = 0.45069122314453125
In grad_steps = 3092, loss = 0.36055538058280945
In grad_steps = 3093, loss = 0.21053823828697205
In grad_steps = 3094, loss = 0.1454753428697586
In grad_steps = 3095, loss = 0.3401452302932739
In grad_steps = 3096, loss = 0.6848358511924744
In grad_steps = 3097, loss = 0.08576995879411697
In grad_steps = 3098, loss = 1.012321949005127
In grad_steps = 3099, loss = 0.008063026703894138
In grad_steps = 3100, loss = 0.025341324508190155
In grad_steps = 3101, loss = 0.2981792688369751
In grad_steps = 3102, loss = 0.12466290593147278
In grad_steps = 3103, loss = 1.0301439762115479
In grad_steps = 3104, loss = 0.06219997629523277
In grad_steps = 3105, loss = 0.20468485355377197
In grad_steps = 3106, loss = 0.6450711488723755
In grad_steps = 3107, loss = 0.18379555642604828
In grad_steps = 3108, loss = 0.13145799934864044
In grad_steps = 3109, loss = 0.5493006110191345
In grad_steps = 3110, loss = 0.331546425819397
In grad_steps = 3111, loss = 0.3005736172199249
In grad_steps = 3112, loss = 0.3236152231693268
In grad_steps = 3113, loss = 0.37194567918777466
In grad_steps = 3114, loss = 0.31083551049232483
In grad_steps = 3115, loss = 0.6827019453048706
In grad_steps = 3116, loss = 0.10223887860774994
In grad_steps = 3117, loss = 0.11267303675413132
In grad_steps = 3118, loss = 0.02655996009707451
In grad_steps = 3119, loss = 0.5316205024719238
In grad_steps = 3120, loss = 0.3664124011993408
In grad_steps = 3121, loss = 0.0933496281504631
In grad_steps = 3122, loss = 0.16678468883037567
In grad_steps = 3123, loss = 0.3671649098396301
In grad_steps = 3124, loss = 0.08070511370897293
In grad_steps = 3125, loss = 0.055889517068862915
In grad_steps = 3126, loss = 0.17997288703918457
In grad_steps = 3127, loss = 0.12710905075073242
In grad_steps = 3128, loss = 0.438092440366745
In grad_steps = 3129, loss = 0.1049795150756836
In grad_steps = 3130, loss = 0.15630210936069489
In grad_steps = 3131, loss = 0.34113821387290955
In grad_steps = 3132, loss = 0.05721430853009224
In grad_steps = 3133, loss = 0.43033212423324585
In grad_steps = 3134, loss = 0.5779821276664734
In grad_steps = 3135, loss = 0.05395705997943878
In grad_steps = 3136, loss = 0.022953804582357407
In grad_steps = 3137, loss = 0.33765631914138794
In grad_steps = 3138, loss = 1.020368218421936
In grad_steps = 3139, loss = 0.13791508972644806
In grad_steps = 3140, loss = 0.44603490829467773
In grad_steps = 3141, loss = 0.29817497730255127
In grad_steps = 3142, loss = 0.12536557018756866
In grad_steps = 3143, loss = 0.08761673420667648
In grad_steps = 3144, loss = 0.04945945739746094
In grad_steps = 3145, loss = 0.04386623576283455
In grad_steps = 3146, loss = 0.3745053708553314
In grad_steps = 3147, loss = 0.03689725697040558
In grad_steps = 3148, loss = 0.1648840457201004
In grad_steps = 3149, loss = 0.02985612116754055
In grad_steps = 3150, loss = 0.04336147382855415
In grad_steps = 3151, loss = 0.18656817078590393
In grad_steps = 3152, loss = 0.1119016632437706
In grad_steps = 3153, loss = 0.13740620017051697
In grad_steps = 3154, loss = 0.45657724142074585
In grad_steps = 3155, loss = 0.7420642375946045
In grad_steps = 3156, loss = 0.003974421881139278
In grad_steps = 3157, loss = 1.1164947748184204
In grad_steps = 3158, loss = 0.0753236934542656
In grad_steps = 3159, loss = 0.0675530880689621
In grad_steps = 3160, loss = 0.024557702243328094
In grad_steps = 3161, loss = 0.45677778124809265
In grad_steps = 3162, loss = 0.12478992342948914
In grad_steps = 3163, loss = 0.3223508894443512
In grad_steps = 3164, loss = 0.26260823011398315
In grad_steps = 3165, loss = 0.5229570269584656
In grad_steps = 3166, loss = 0.19790926575660706
In grad_steps = 3167, loss = 0.08029818534851074
In grad_steps = 3168, loss = 0.053846318274736404
In grad_steps = 3169, loss = 0.20111629366874695
In grad_steps = 3170, loss = 0.12153218686580658
In grad_steps = 3171, loss = 0.03304365277290344
In grad_steps = 3172, loss = 0.29694926738739014
In grad_steps = 3173, loss = 0.015761958435177803
In grad_steps = 3174, loss = 0.05926883593201637
In grad_steps = 3175, loss = 0.024613700807094574
In grad_steps = 3176, loss = 0.07464911043643951
In grad_steps = 3177, loss = 0.10315113514661789
In grad_steps = 3178, loss = 0.019983094185590744
In grad_steps = 3179, loss = 0.16204681992530823
In grad_steps = 3180, loss = 0.0659816786646843
In grad_steps = 3181, loss = 0.47267675399780273
In grad_steps = 3182, loss = 0.41100218892097473
In grad_steps = 3183, loss = 0.00906645879149437
In grad_steps = 3184, loss = 0.012941212393343449
In grad_steps = 3185, loss = 0.031637225300073624
In grad_steps = 3186, loss = 0.8729626536369324
In grad_steps = 3187, loss = 0.1827448457479477
In grad_steps = 3188, loss = 0.018857629969716072
In grad_steps = 3189, loss = 1.1779600381851196
In grad_steps = 3190, loss = 0.007155774161219597
In grad_steps = 3191, loss = 0.11227259039878845
In grad_steps = 3192, loss = 0.14381621778011322
In grad_steps = 3193, loss = 0.041265055537223816
In grad_steps = 3194, loss = 0.7391765117645264
In grad_steps = 3195, loss = 0.2724124789237976
In grad_steps = 3196, loss = 0.38731127977371216
In grad_steps = 3197, loss = 0.022383274510502815
In grad_steps = 3198, loss = 0.2184101939201355
In grad_steps = 3199, loss = 1.5913054943084717
In grad_steps = 3200, loss = 0.33688074350357056
In grad_steps = 3201, loss = 0.5141441226005554
In grad_steps = 3202, loss = 0.06628555059432983
In grad_steps = 3203, loss = 0.23397336900234222
In grad_steps = 3204, loss = 0.19545914232730865
In grad_steps = 3205, loss = 0.2594984173774719
In grad_steps = 3206, loss = 0.6452906727790833
In grad_steps = 3207, loss = 0.0880124643445015
In grad_steps = 3208, loss = 0.1898304969072342
In grad_steps = 3209, loss = 0.2920030951499939
In grad_steps = 3210, loss = 0.20877787470817566
In grad_steps = 3211, loss = 0.06498904526233673
In grad_steps = 3212, loss = 0.049012355506420135
In grad_steps = 3213, loss = 0.26264289021492004
In grad_steps = 3214, loss = 0.3905615508556366
In grad_steps = 3215, loss = 0.1337244212627411
In grad_steps = 3216, loss = 0.22898058593273163
In grad_steps = 3217, loss = 0.16856783628463745
In grad_steps = 3218, loss = 0.07759230583906174
In grad_steps = 3219, loss = 1.2665345668792725
In grad_steps = 3220, loss = 1.12075674533844
In grad_steps = 3221, loss = 0.2092580944299698
In grad_steps = 3222, loss = 0.0163729190826416
In grad_steps = 3223, loss = 0.0937991738319397
In grad_steps = 3224, loss = 0.16722069680690765
In grad_steps = 3225, loss = 0.20459973812103271
In grad_steps = 3226, loss = 0.3216700255870819
In grad_steps = 3227, loss = 0.1689147800207138
In grad_steps = 3228, loss = 0.6876224279403687
In grad_steps = 3229, loss = 0.18318285048007965
In grad_steps = 3230, loss = 0.03304678201675415
In grad_steps = 3231, loss = 0.5620814561843872
In grad_steps = 3232, loss = 0.017940955236554146
In grad_steps = 3233, loss = 0.0846896767616272
In grad_steps = 3234, loss = 0.08255136013031006
In grad_steps = 3235, loss = 1.2292400598526
In grad_steps = 3236, loss = 0.7819401621818542
In grad_steps = 3237, loss = 0.83150315284729
In grad_steps = 3238, loss = 0.1713763326406479
In grad_steps = 3239, loss = 0.24359549582004547
In grad_steps = 3240, loss = 0.10348594188690186
In grad_steps = 3241, loss = 0.46318763494491577
In grad_steps = 3242, loss = 0.09619757533073425
In grad_steps = 3243, loss = 0.07142774760723114
In grad_steps = 3244, loss = 0.09306463599205017
In grad_steps = 3245, loss = 0.14076252281665802
In grad_steps = 3246, loss = 0.11178848147392273
In grad_steps = 3247, loss = 0.43021851778030396
In grad_steps = 3248, loss = 0.5389924645423889
In grad_steps = 3249, loss = 0.07942020893096924
In grad_steps = 3250, loss = 0.5260486602783203
In grad_steps = 3251, loss = 0.12038509547710419
In grad_steps = 3252, loss = 0.5116914510726929
In grad_steps = 3253, loss = 0.0938488245010376
In grad_steps = 3254, loss = 0.0402018204331398
In grad_steps = 3255, loss = 0.3468666970729828
In grad_steps = 3256, loss = 0.028618264943361282
In grad_steps = 3257, loss = 0.04000376909971237
In grad_steps = 3258, loss = 0.9905828237533569
In grad_steps = 3259, loss = 0.7980293035507202
In grad_steps = 3260, loss = 0.06338842213153839
In grad_steps = 3261, loss = 0.1477958858013153
In grad_steps = 3262, loss = 0.2662116587162018
In grad_steps = 3263, loss = 0.25946325063705444
In grad_steps = 3264, loss = 0.6604794859886169
In grad_steps = 3265, loss = 0.032111819833517075
In grad_steps = 3266, loss = 0.6887139081954956
In grad_steps = 3267, loss = 0.22619038820266724
In grad_steps = 3268, loss = 0.14023543894290924
In grad_steps = 3269, loss = 0.8093044757843018
In grad_steps = 3270, loss = 0.025599980726838112
In grad_steps = 3271, loss = 0.1063585877418518
In grad_steps = 3272, loss = 0.1292879730463028
In grad_steps = 3273, loss = 0.11267133057117462
In grad_steps = 3274, loss = 0.06762547791004181
In grad_steps = 3275, loss = 0.24856141209602356
In grad_steps = 3276, loss = 0.1774706244468689
In grad_steps = 3277, loss = 0.9365319013595581
In grad_steps = 3278, loss = 0.03737012296915054
In grad_steps = 3279, loss = 0.07806862890720367
In grad_steps = 3280, loss = 0.08536785840988159
In grad_steps = 3281, loss = 0.0713641569018364
In grad_steps = 3282, loss = 0.5483740568161011
In grad_steps = 3283, loss = 0.4667659401893616
In grad_steps = 3284, loss = 0.10765683650970459
In grad_steps = 3285, loss = 0.025343839079141617
In grad_steps = 3286, loss = 0.7283461689949036
In grad_steps = 3287, loss = 0.0868469700217247
In grad_steps = 3288, loss = 0.07274138927459717
In grad_steps = 3289, loss = 0.22085073590278625
In grad_steps = 3290, loss = 0.5588407516479492
In grad_steps = 3291, loss = 0.07735010236501694
In grad_steps = 3292, loss = 0.32198792695999146
In grad_steps = 3293, loss = 0.01320938766002655
In grad_steps = 3294, loss = 0.027454858645796776
In grad_steps = 3295, loss = 0.019112393260002136
In grad_steps = 3296, loss = 0.08105331659317017
In grad_steps = 3297, loss = 0.5322168469429016
In grad_steps = 3298, loss = 0.4447064697742462
In grad_steps = 3299, loss = 0.04992344230413437
In grad_steps = 3300, loss = 0.20218321681022644
In grad_steps = 3301, loss = 0.5805978775024414
In grad_steps = 3302, loss = 0.2467086911201477
In grad_steps = 3303, loss = 0.10191122442483902
In grad_steps = 3304, loss = 0.1428956687450409
In grad_steps = 3305, loss = 0.20491312444210052
In grad_steps = 3306, loss = 0.058050163090229034
In grad_steps = 3307, loss = 0.12128077447414398
In grad_steps = 3308, loss = 0.2350737452507019
In grad_steps = 3309, loss = 0.22529816627502441
In grad_steps = 3310, loss = 0.25187838077545166
In grad_steps = 3311, loss = 1.3141816854476929
In grad_steps = 3312, loss = 0.029297437518835068
In grad_steps = 3313, loss = 0.11178731173276901
In grad_steps = 3314, loss = 1.2344565391540527
In grad_steps = 3315, loss = 0.5384398102760315
In grad_steps = 3316, loss = 0.2735966742038727
In grad_steps = 3317, loss = 0.013244039379060268
In grad_steps = 3318, loss = 0.1241292655467987
In grad_steps = 3319, loss = 0.8488837480545044
In grad_steps = 3320, loss = 0.5316005349159241
In grad_steps = 3321, loss = 0.7145819067955017
In grad_steps = 3322, loss = 0.07606661319732666
In grad_steps = 3323, loss = 0.26852598786354065
In grad_steps = 3324, loss = 0.46440768241882324
In grad_steps = 3325, loss = 0.21140125393867493
In grad_steps = 3326, loss = 0.06444870680570602
In grad_steps = 3327, loss = 0.46919509768486023
In grad_steps = 3328, loss = 0.16912835836410522
In grad_steps = 3329, loss = 0.3134952187538147
In grad_steps = 3330, loss = 0.2688477039337158
In grad_steps = 3331, loss = 0.271212637424469
In grad_steps = 3332, loss = 0.08616790175437927
In grad_steps = 3333, loss = 0.07577373087406158
In grad_steps = 3334, loss = 0.4880213141441345
In grad_steps = 3335, loss = 0.2498527467250824
In grad_steps = 3336, loss = 0.5141686797142029
In grad_steps = 3337, loss = 0.23955625295639038
In grad_steps = 3338, loss = 0.2827530801296234
In grad_steps = 3339, loss = 0.10320261120796204
In grad_steps = 3340, loss = 0.10968008637428284
In grad_steps = 3341, loss = 0.07954394072294235
In grad_steps = 3342, loss = 0.06168882176280022
In grad_steps = 3343, loss = 0.04077228531241417
In grad_steps = 3344, loss = 0.03656177967786789
In grad_steps = 3345, loss = 0.4927874207496643
In grad_steps = 3346, loss = 0.244760662317276
In grad_steps = 3347, loss = 0.35104602575302124
In grad_steps = 3348, loss = 0.16836553812026978
In grad_steps = 3349, loss = 0.053294505923986435
In grad_steps = 3350, loss = 0.01797550730407238
In grad_steps = 3351, loss = 0.03136293962597847
In grad_steps = 3352, loss = 0.2535303831100464
In grad_steps = 3353, loss = 0.09746041893959045
In grad_steps = 3354, loss = 0.013981467112898827
In grad_steps = 3355, loss = 0.007969521917402744
In grad_steps = 3356, loss = 0.08395469933748245
In grad_steps = 3357, loss = 0.03776639699935913
In grad_steps = 3358, loss = 0.08160828053951263
In grad_steps = 3359, loss = 0.9316295385360718
In grad_steps = 3360, loss = 0.029431680217385292
In grad_steps = 3361, loss = 0.5867604613304138
In grad_steps = 3362, loss = 0.30063679814338684
In grad_steps = 3363, loss = 0.03357416391372681
In grad_steps = 3364, loss = 0.03223234415054321
In grad_steps = 3365, loss = 0.027684643864631653
In grad_steps = 3366, loss = 0.021832305938005447
In grad_steps = 3367, loss = 0.020027663558721542
In grad_steps = 3368, loss = 0.16395308077335358
In grad_steps = 3369, loss = 0.921910285949707
In grad_steps = 3370, loss = 0.6706655621528625
In grad_steps = 3371, loss = 0.05386943370103836
In grad_steps = 3372, loss = 0.03450064733624458
In grad_steps = 3373, loss = 0.24404604732990265
In grad_steps = 3374, loss = 0.07915044575929642
In grad_steps = 3375, loss = 0.8340999484062195
In grad_steps = 3376, loss = 0.07788845151662827
In grad_steps = 3377, loss = 0.0307978056371212
In grad_steps = 3378, loss = 0.033775582909584045
In grad_steps = 3379, loss = 0.06516922265291214
In grad_steps = 3380, loss = 1.023513913154602
In grad_steps = 3381, loss = 0.03318614512681961
In grad_steps = 3382, loss = 0.8313251733779907
In grad_steps = 3383, loss = 0.556459367275238
In grad_steps = 3384, loss = 0.2172791063785553
In grad_steps = 3385, loss = 0.045631542801856995
In grad_steps = 3386, loss = 0.48329442739486694
In grad_steps = 3387, loss = 0.09245134890079498
In grad_steps = 3388, loss = 0.15440452098846436
In grad_steps = 3389, loss = 0.35854193568229675
In grad_steps = 3390, loss = 0.12495366483926773
In grad_steps = 3391, loss = 0.05994901806116104
In grad_steps = 3392, loss = 0.21504393219947815
In grad_steps = 3393, loss = 0.8164379596710205
In grad_steps = 3394, loss = 0.13762223720550537
In grad_steps = 3395, loss = 0.1645776629447937
In grad_steps = 3396, loss = 0.4935152530670166
In grad_steps = 3397, loss = 0.17170582711696625
In grad_steps = 3398, loss = 0.9077126979827881
In grad_steps = 3399, loss = 0.209099680185318
In grad_steps = 3400, loss = 0.09060515463352203
In grad_steps = 3401, loss = 0.43510541319847107
In grad_steps = 3402, loss = 0.008548160083591938
In grad_steps = 3403, loss = 0.3750147521495819
In grad_steps = 3404, loss = 0.26934072375297546
In grad_steps = 3405, loss = 0.14589320123195648
In grad_steps = 3406, loss = 0.21406541764736176
In grad_steps = 3407, loss = 0.05983641743659973
In grad_steps = 3408, loss = 0.8534258604049683
In grad_steps = 3409, loss = 0.17119385302066803
In grad_steps = 3410, loss = 0.29073983430862427
In grad_steps = 3411, loss = 0.11217035353183746
In grad_steps = 3412, loss = 0.18073275685310364
In grad_steps = 3413, loss = 0.17642106115818024
In grad_steps = 3414, loss = 0.6904790997505188
In grad_steps = 3415, loss = 0.03526970371603966
In grad_steps = 3416, loss = 0.21373698115348816
In grad_steps = 3417, loss = 0.2487965226173401
In grad_steps = 3418, loss = 0.07586030662059784
In grad_steps = 3419, loss = 0.13222849369049072
In grad_steps = 3420, loss = 0.02831377647817135
In grad_steps = 3421, loss = 0.4090687334537506
In grad_steps = 3422, loss = 0.10245424509048462
In grad_steps = 3423, loss = 0.07724043726921082
In grad_steps = 3424, loss = 0.29268357157707214
In grad_steps = 3425, loss = 0.49281010031700134
In grad_steps = 3426, loss = 0.041504260152578354
In grad_steps = 3427, loss = 1.0800937414169312
In grad_steps = 3428, loss = 0.008877010084688663
In grad_steps = 3429, loss = 0.07426765561103821
In grad_steps = 3430, loss = 0.024179741740226746
In grad_steps = 3431, loss = 0.6252709627151489
In grad_steps = 3432, loss = 0.11741191893815994
In grad_steps = 3433, loss = 0.13971374928951263
In grad_steps = 3434, loss = 0.02392551302909851
In grad_steps = 3435, loss = 0.06224437057971954
In grad_steps = 3436, loss = 0.14159871637821198
In grad_steps = 3437, loss = 0.15595248341560364
In grad_steps = 3438, loss = 0.18699675798416138
In grad_steps = 3439, loss = 0.15987925231456757
In grad_steps = 3440, loss = 0.08355008065700531
In grad_steps = 3441, loss = 0.047254495322704315
In grad_steps = 3442, loss = 0.15790623426437378
In grad_steps = 3443, loss = 0.0031936601735651493
In grad_steps = 3444, loss = 0.796877384185791
In grad_steps = 3445, loss = 0.28007614612579346
In grad_steps = 3446, loss = 0.05094924569129944
In grad_steps = 3447, loss = 0.0928279384970665
In grad_steps = 3448, loss = 0.39497309923171997
In grad_steps = 3449, loss = 0.6972653269767761
In grad_steps = 3450, loss = 0.010286069475114346
In grad_steps = 3451, loss = 0.506145715713501
In grad_steps = 3452, loss = 0.040664009749889374
In grad_steps = 3453, loss = 0.7293241024017334
In grad_steps = 3454, loss = 0.12453684955835342
In grad_steps = 3455, loss = 0.11850877106189728
In grad_steps = 3456, loss = 0.5906305313110352
In grad_steps = 3457, loss = 0.23414970934391022
In grad_steps = 3458, loss = 0.012156950309872627
In grad_steps = 3459, loss = 0.3178335130214691
In grad_steps = 3460, loss = 0.43638575077056885
In grad_steps = 3461, loss = 0.39734354615211487
In grad_steps = 3462, loss = 0.101076140999794
In grad_steps = 3463, loss = 0.07195782661437988
In grad_steps = 3464, loss = 0.4469624161720276
In grad_steps = 3465, loss = 0.14559586346149445
In grad_steps = 3466, loss = 1.787597894668579
In grad_steps = 3467, loss = 0.14342191815376282
In grad_steps = 3468, loss = 0.18690527975559235
In grad_steps = 3469, loss = 0.24655179679393768
In grad_steps = 3470, loss = 0.1908726990222931
In grad_steps = 3471, loss = 0.03546281158924103
In grad_steps = 3472, loss = 0.06709159165620804
In grad_steps = 3473, loss = 0.13380952179431915
In grad_steps = 3474, loss = 0.10095591098070145
In grad_steps = 3475, loss = 0.16167084872722626
In grad_steps = 3476, loss = 0.34476613998413086
In grad_steps = 3477, loss = 0.023289307951927185
In grad_steps = 3478, loss = 0.19639451801776886
In grad_steps = 3479, loss = 0.07412905991077423
In grad_steps = 3480, loss = 0.1777624934911728
In grad_steps = 3481, loss = 0.07996556162834167
In grad_steps = 3482, loss = 0.04200957342982292
In grad_steps = 3483, loss = 0.05111100152134895
In grad_steps = 3484, loss = 0.7414529323577881
In grad_steps = 3485, loss = 0.018262337893247604
In grad_steps = 3486, loss = 0.012631744146347046
In grad_steps = 3487, loss = 0.33350175619125366
In grad_steps = 3488, loss = 0.16517692804336548
In grad_steps = 3489, loss = 0.06782306730747223
In grad_steps = 3490, loss = 0.8441349864006042
In grad_steps = 3491, loss = 0.7427231073379517
In grad_steps = 3492, loss = 0.016496583819389343
In grad_steps = 3493, loss = 0.06627338379621506
In grad_steps = 3494, loss = 0.8093729615211487
In grad_steps = 3495, loss = 0.21187566220760345
In grad_steps = 3496, loss = 1.2379488945007324
In grad_steps = 3497, loss = 0.06781669706106186
In grad_steps = 3498, loss = 0.14189007878303528
In grad_steps = 3499, loss = 0.7089781165122986
In grad_steps = 3500, loss = 0.011663447134196758
In grad_steps = 3501, loss = 0.6712910532951355
In grad_steps = 3502, loss = 0.048154134303331375
In grad_steps = 3503, loss = 0.03374427184462547
In grad_steps = 3504, loss = 0.40348684787750244
In grad_steps = 3505, loss = 0.1835012435913086
In grad_steps = 3506, loss = 0.8739933371543884
In grad_steps = 3507, loss = 0.048205479979515076
In grad_steps = 3508, loss = 0.48518994450569153
In grad_steps = 3509, loss = 0.1896418184041977
In grad_steps = 3510, loss = 0.4509866237640381
In grad_steps = 3511, loss = 0.08566537499427795
In grad_steps = 3512, loss = 0.07059810310602188
In grad_steps = 3513, loss = 0.061222758144140244
In grad_steps = 3514, loss = 0.25013267993927
In grad_steps = 3515, loss = 0.059644635766744614
In grad_steps = 3516, loss = 1.0041133165359497
In grad_steps = 3517, loss = 0.14181269705295563
In grad_steps = 3518, loss = 0.2837287187576294
In grad_steps = 3519, loss = 0.07372710108757019
In grad_steps = 3520, loss = 0.19393065571784973
In grad_steps = 3521, loss = 0.09160171449184418
In grad_steps = 3522, loss = 0.1671651005744934
In grad_steps = 3523, loss = 0.16917996108531952
In grad_steps = 3524, loss = 0.08671493083238602
In grad_steps = 3525, loss = 0.26292547583580017
In grad_steps = 3526, loss = 0.08461977541446686
In grad_steps = 3527, loss = 0.8570927381515503
In grad_steps = 3528, loss = 0.1701211780309677
In grad_steps = 3529, loss = 0.11288195848464966
In grad_steps = 3530, loss = 0.08926676213741302
In grad_steps = 3531, loss = 0.1069527342915535
In grad_steps = 3532, loss = 0.717842161655426
In grad_steps = 3533, loss = 0.2816683351993561
In grad_steps = 3534, loss = 0.07142574340105057
In grad_steps = 3535, loss = 0.22310923039913177
In grad_steps = 3536, loss = 0.11012261360883713
In grad_steps = 3537, loss = 0.14804482460021973
In grad_steps = 3538, loss = 0.04980027675628662
In grad_steps = 3539, loss = 0.836629331111908
In grad_steps = 3540, loss = 0.041971705853939056
In grad_steps = 3541, loss = 0.05520879849791527
In grad_steps = 3542, loss = 0.25513195991516113
In grad_steps = 3543, loss = 0.4538126587867737
In grad_steps = 3544, loss = 0.5489962697029114
In grad_steps = 3545, loss = 0.021372171118855476
In grad_steps = 3546, loss = 0.09629857540130615
In grad_steps = 3547, loss = 0.2742975652217865
In grad_steps = 3548, loss = 0.030860353261232376
In grad_steps = 3549, loss = 0.027483584359288216
In grad_steps = 3550, loss = 0.016888901591300964
In grad_steps = 3551, loss = 0.3392435908317566
In grad_steps = 3552, loss = 0.6155133843421936
In grad_steps = 3553, loss = 0.37647175788879395
In grad_steps = 3554, loss = 0.6274184584617615
In grad_steps = 3555, loss = 0.6095741391181946
In grad_steps = 3556, loss = 0.2756529748439789
In grad_steps = 3557, loss = 0.12206847965717316
In grad_steps = 3558, loss = 0.4966602027416229
In grad_steps = 3559, loss = 0.2263454645872116
In grad_steps = 3560, loss = 0.6303343772888184
In grad_steps = 3561, loss = 0.20817838609218597
In grad_steps = 3562, loss = 0.832085907459259
In grad_steps = 3563, loss = 0.9357225298881531
In grad_steps = 3564, loss = 0.35672831535339355
In grad_steps = 3565, loss = 0.8077031970024109
In grad_steps = 3566, loss = 0.3421555757522583
In grad_steps = 3567, loss = 0.15789379179477692
In grad_steps = 3568, loss = 0.2910534143447876
In grad_steps = 3569, loss = 0.13016939163208008
In grad_steps = 3570, loss = 0.11160240322351456
In grad_steps = 3571, loss = 0.1413811445236206
In grad_steps = 3572, loss = 0.26484254002571106
In grad_steps = 3573, loss = 0.16360171139240265
In grad_steps = 3574, loss = 0.9492888450622559
In grad_steps = 3575, loss = 0.04717908799648285
In grad_steps = 3576, loss = 0.5636455416679382
In grad_steps = 3577, loss = 0.29122889041900635
In grad_steps = 3578, loss = 0.42515483498573303
In grad_steps = 3579, loss = 0.314286470413208
In grad_steps = 3580, loss = 0.4675581157207489
In grad_steps = 3581, loss = 0.7286561727523804
In grad_steps = 3582, loss = 0.05842477083206177
In grad_steps = 3583, loss = 0.4847736954689026
In grad_steps = 3584, loss = 0.686266303062439
In grad_steps = 3585, loss = 0.07800020277500153
In grad_steps = 3586, loss = 0.13632287085056305
In grad_steps = 3587, loss = 0.04242100566625595
In grad_steps = 3588, loss = 0.3297309875488281
In grad_steps = 3589, loss = 0.05845136195421219
In grad_steps = 3590, loss = 0.6478250026702881
In grad_steps = 3591, loss = 0.12120509147644043
In grad_steps = 3592, loss = 0.04153744876384735
In grad_steps = 3593, loss = 0.05964091420173645
In grad_steps = 3594, loss = 0.47498053312301636
In grad_steps = 3595, loss = 0.26085734367370605
In grad_steps = 3596, loss = 0.1560789942741394
In grad_steps = 3597, loss = 0.06636369228363037
In grad_steps = 3598, loss = 0.07508248835802078
In grad_steps = 3599, loss = 0.15955473482608795
In grad_steps = 3600, loss = 0.6390818953514099
In grad_steps = 3601, loss = 0.031414370983839035
In grad_steps = 3602, loss = 0.16735105216503143
In grad_steps = 3603, loss = 0.0609646737575531
In grad_steps = 3604, loss = 0.29935741424560547
In grad_steps = 3605, loss = 0.19102442264556885
In grad_steps = 3606, loss = 0.09833107143640518
In grad_steps = 3607, loss = 0.31201767921447754
In grad_steps = 3608, loss = 0.28298723697662354
In grad_steps = 3609, loss = 0.27832886576652527
In grad_steps = 3610, loss = 0.10885858535766602
In grad_steps = 3611, loss = 0.38787391781806946
In grad_steps = 3612, loss = 0.3657582402229309
In grad_steps = 3613, loss = 0.0853186547756195
In grad_steps = 3614, loss = 0.6355109214782715
In grad_steps = 3615, loss = 0.2770094573497772
In grad_steps = 3616, loss = 0.21056079864501953
In grad_steps = 3617, loss = 0.8719053864479065
In grad_steps = 3618, loss = 0.7122355699539185
In grad_steps = 3619, loss = 0.031166620552539825
In grad_steps = 3620, loss = 0.27941393852233887
In grad_steps = 3621, loss = 0.26737263798713684
In grad_steps = 3622, loss = 0.0793028175830841
In grad_steps = 3623, loss = 0.3944898545742035
In grad_steps = 3624, loss = 0.704131007194519
In grad_steps = 3625, loss = 0.15440283715724945
In grad_steps = 3626, loss = 0.16636019945144653
In grad_steps = 3627, loss = 0.315093070268631
In grad_steps = 3628, loss = 0.6424772143363953
In grad_steps = 3629, loss = 0.11552749574184418
In grad_steps = 3630, loss = 0.493335485458374
In grad_steps = 3631, loss = 0.003507074434310198
In grad_steps = 3632, loss = 0.5391052961349487
In grad_steps = 3633, loss = 0.030932528898119926
In grad_steps = 3634, loss = 0.10642927139997482
In grad_steps = 3635, loss = 0.015719033777713776
In grad_steps = 3636, loss = 0.07934696972370148
In grad_steps = 3637, loss = 0.1258840262889862
In grad_steps = 3638, loss = 0.21898600459098816
In grad_steps = 3639, loss = 0.022670891135931015
In grad_steps = 3640, loss = 0.2051071673631668
In grad_steps = 3641, loss = 1.0807846784591675
In grad_steps = 3642, loss = 0.4463356137275696
In grad_steps = 3643, loss = 0.08311858773231506
In grad_steps = 3644, loss = 0.7201814651489258
In grad_steps = 3645, loss = 0.2058262676000595
In grad_steps = 3646, loss = 0.008375708013772964
In grad_steps = 3647, loss = 0.08039108663797379
In grad_steps = 3648, loss = 0.09778246283531189
In grad_steps = 3649, loss = 0.045293472707271576
In grad_steps = 3650, loss = 0.09107546508312225
In grad_steps = 3651, loss = 0.107772596180439
In grad_steps = 3652, loss = 0.32862529158592224
In grad_steps = 3653, loss = 0.5260753631591797
In grad_steps = 3654, loss = 0.05008142441511154
In grad_steps = 3655, loss = 0.15728983283042908
In grad_steps = 3656, loss = 0.6396726369857788
In grad_steps = 3657, loss = 0.07766108959913254
In grad_steps = 3658, loss = 0.1349327266216278
In grad_steps = 3659, loss = 0.359512597322464
In grad_steps = 3660, loss = 0.0283827967941761
In grad_steps = 3661, loss = 0.4568682909011841
In grad_steps = 3662, loss = 0.0987735167145729
In grad_steps = 3663, loss = 0.4300013780593872
In grad_steps = 3664, loss = 0.1712256371974945
In grad_steps = 3665, loss = 0.13705629110336304
In grad_steps = 3666, loss = 0.2566104531288147
In grad_steps = 3667, loss = 0.04111848399043083
In grad_steps = 3668, loss = 0.057660557329654694
In grad_steps = 3669, loss = 0.012327149510383606
In grad_steps = 3670, loss = 0.33389997482299805
In grad_steps = 3671, loss = 0.37169885635375977
In grad_steps = 3672, loss = 0.05528682470321655
In grad_steps = 3673, loss = 0.3764600455760956
In grad_steps = 3674, loss = 0.07763656973838806
In grad_steps = 3675, loss = 1.137306571006775
In grad_steps = 3676, loss = 0.3327345550060272
In grad_steps = 3677, loss = 0.0726683959364891
In grad_steps = 3678, loss = 0.5825421810150146
In grad_steps = 3679, loss = 0.07531615346670151
In grad_steps = 3680, loss = 0.028060322627425194
In grad_steps = 3681, loss = 0.09832712262868881
In grad_steps = 3682, loss = 0.013532962650060654
In grad_steps = 3683, loss = 0.17690454423427582
In grad_steps = 3684, loss = 0.0014598033158108592
In grad_steps = 3685, loss = 0.43047264218330383
In grad_steps = 3686, loss = 0.4255552887916565
In grad_steps = 3687, loss = 0.039852119982242584
In grad_steps = 3688, loss = 0.018121564760804176
In grad_steps = 3689, loss = 0.23567205667495728
In grad_steps = 3690, loss = 0.21280322968959808
In grad_steps = 3691, loss = 0.0317351296544075
In grad_steps = 3692, loss = 0.5909110903739929
In grad_steps = 3693, loss = 0.054654836654663086
In grad_steps = 3694, loss = 0.07052665203809738
In grad_steps = 3695, loss = 0.026111125946044922
In grad_steps = 3696, loss = 0.47983354330062866
In grad_steps = 3697, loss = 0.053490642458200455
In grad_steps = 3698, loss = 0.5023369193077087
In grad_steps = 3699, loss = 0.13642658293247223
In grad_steps = 3700, loss = 0.47890204191207886
In grad_steps = 3701, loss = 0.08497007936239243
In grad_steps = 3702, loss = 1.1699491739273071
In grad_steps = 3703, loss = 0.10398140549659729
In grad_steps = 3704, loss = 0.8230714201927185
In grad_steps = 3705, loss = 0.030582668259739876
In grad_steps = 3706, loss = 0.15317340195178986
In grad_steps = 3707, loss = 0.3707656264305115
In grad_steps = 3708, loss = 0.3977358937263489
In grad_steps = 3709, loss = 0.03314027562737465
In grad_steps = 3710, loss = 0.38216400146484375
In grad_steps = 3711, loss = 0.10032714903354645
In grad_steps = 3712, loss = 0.051761023700237274
In grad_steps = 3713, loss = 0.3238891065120697
In grad_steps = 3714, loss = 0.17301040887832642
In grad_steps = 3715, loss = 0.02008654549717903
In grad_steps = 3716, loss = 0.06435051560401917
In grad_steps = 3717, loss = 0.04936808720231056
In grad_steps = 3718, loss = 0.14220508933067322
In grad_steps = 3719, loss = 0.1218939870595932
In grad_steps = 3720, loss = 0.5449257493019104
In grad_steps = 3721, loss = 0.20048165321350098
In grad_steps = 3722, loss = 0.6371909379959106
In grad_steps = 3723, loss = 0.2803024351596832
In grad_steps = 3724, loss = 0.011423086747527122
In grad_steps = 3725, loss = 0.08522109687328339
In grad_steps = 3726, loss = 0.038823485374450684
In grad_steps = 3727, loss = 0.03058771975338459
In grad_steps = 3728, loss = 0.18623097240924835
In grad_steps = 3729, loss = 0.06383737921714783
In grad_steps = 3730, loss = 0.07238933444023132
In grad_steps = 3731, loss = 0.5007749199867249
In grad_steps = 3732, loss = 1.4009606838226318
In grad_steps = 3733, loss = 0.13568051159381866
In grad_steps = 3734, loss = 0.17963998019695282
In grad_steps = 3735, loss = 0.12680192291736603
In grad_steps = 3736, loss = 0.26624590158462524
In grad_steps = 3737, loss = 0.4652349352836609
In grad_steps = 3738, loss = 0.055146474391222
In grad_steps = 3739, loss = 0.07095689326524734
In grad_steps = 3740, loss = 0.6321285963058472
In grad_steps = 3741, loss = 0.36074137687683105
In grad_steps = 3742, loss = 0.3415547013282776
In grad_steps = 3743, loss = 0.10525023192167282
In grad_steps = 3744, loss = 0.1818769872188568
In grad_steps = 3745, loss = 0.11854775249958038
In grad_steps = 3746, loss = 0.26651331782341003
In grad_steps = 3747, loss = 0.07870180159807205
In grad_steps = 3748, loss = 0.08629024028778076
In grad_steps = 3749, loss = 0.03682837635278702
In grad_steps = 3750, loss = 0.2419339120388031
In grad_steps = 3751, loss = 0.16637200117111206
In grad_steps = 3752, loss = 0.22851498425006866
In grad_steps = 3753, loss = 0.4987257122993469
In grad_steps = 3754, loss = 0.9706438183784485
In grad_steps = 3755, loss = 0.14206604659557343
In grad_steps = 3756, loss = 0.43754127621650696
In grad_steps = 3757, loss = 0.028487056493759155
In grad_steps = 3758, loss = 0.12738348543643951
In grad_steps = 3759, loss = 0.07889258861541748
In grad_steps = 3760, loss = 0.2596055269241333
In grad_steps = 3761, loss = 0.011849470436573029
In grad_steps = 3762, loss = 0.02563929557800293
In grad_steps = 3763, loss = 1.1454272270202637
In grad_steps = 3764, loss = 0.2833769917488098
In grad_steps = 3765, loss = 0.1919548362493515
In grad_steps = 3766, loss = 0.09454604983329773
In grad_steps = 3767, loss = 0.21035347878932953
In grad_steps = 3768, loss = 0.28982943296432495
In grad_steps = 3769, loss = 0.046653542667627335
In grad_steps = 3770, loss = 0.017565809190273285
In grad_steps = 3771, loss = 1.4099254608154297
In grad_steps = 3772, loss = 0.012067138217389584
In grad_steps = 3773, loss = 0.11528992652893066
In grad_steps = 3774, loss = 0.12879040837287903
In grad_steps = 3775, loss = 0.014429960399866104
In grad_steps = 3776, loss = 0.0797099620103836
In grad_steps = 3777, loss = 0.031149744987487793
In grad_steps = 3778, loss = 0.026791078969836235
In grad_steps = 3779, loss = 0.02084190398454666
In grad_steps = 3780, loss = 0.10829886049032211
In grad_steps = 3781, loss = 0.1318758875131607
In grad_steps = 3782, loss = 0.01231699250638485
In grad_steps = 3783, loss = 0.05027264729142189
In grad_steps = 3784, loss = 1.7204833030700684
In grad_steps = 3785, loss = 0.7621232271194458
In grad_steps = 3786, loss = 0.04276682436466217
In grad_steps = 3787, loss = 0.4953796863555908
In grad_steps = 3788, loss = 0.052561596035957336
In grad_steps = 3789, loss = 0.24958109855651855
In grad_steps = 3790, loss = 0.07039296627044678
In grad_steps = 3791, loss = 0.20602776110172272
In grad_steps = 3792, loss = 0.14025111496448517
In grad_steps = 3793, loss = 0.09873142093420029
In grad_steps = 3794, loss = 0.167027547955513
In grad_steps = 3795, loss = 0.07956764101982117
In grad_steps = 3796, loss = 0.630135178565979
In grad_steps = 3797, loss = 0.03648143261671066
In grad_steps = 3798, loss = 0.545046865940094
In grad_steps = 3799, loss = 0.028349345549941063
In grad_steps = 3800, loss = 0.044049374759197235
In grad_steps = 3801, loss = 0.3259177803993225
In grad_steps = 3802, loss = 1.1618207693099976
In grad_steps = 3803, loss = 0.7408074140548706
In grad_steps = 3804, loss = 0.04519785940647125
In grad_steps = 3805, loss = 0.09005777537822723
In grad_steps = 3806, loss = 0.04931750148534775
In grad_steps = 3807, loss = 0.18554875254631042
In grad_steps = 3808, loss = 0.9904356002807617
In grad_steps = 3809, loss = 0.08873500674962997
In grad_steps = 3810, loss = 0.19362981617450714
In grad_steps = 3811, loss = 0.5033801794052124
In grad_steps = 3812, loss = 0.08004169166088104
In grad_steps = 3813, loss = 0.051778484135866165
In grad_steps = 3814, loss = 0.07190719991922379
In grad_steps = 3815, loss = 0.25100642442703247
In grad_steps = 3816, loss = 0.5353106260299683
In grad_steps = 3817, loss = 0.2054099589586258
In grad_steps = 3818, loss = 1.0128651857376099
In grad_steps = 3819, loss = 0.2889738380908966
In grad_steps = 3820, loss = 0.07465312629938126
In grad_steps = 3821, loss = 0.19985860586166382
In grad_steps = 3822, loss = 0.14140497148036957
In grad_steps = 3823, loss = 0.16798743605613708
In grad_steps = 3824, loss = 0.34237855672836304
In grad_steps = 3825, loss = 0.0729077085852623
In grad_steps = 3826, loss = 0.08451741188764572
In grad_steps = 3827, loss = 0.07977883517742157
In grad_steps = 3828, loss = 0.1755632758140564
In grad_steps = 3829, loss = 0.03861024230718613
In grad_steps = 3830, loss = 0.15997639298439026
In grad_steps = 3831, loss = 0.10496331751346588
In grad_steps = 3832, loss = 0.07544049620628357
In grad_steps = 3833, loss = 0.368395060300827
In grad_steps = 3834, loss = 0.03377822786569595
In grad_steps = 3835, loss = 0.15092281997203827
In grad_steps = 3836, loss = 0.14877761900424957
In grad_steps = 3837, loss = 0.47039884328842163
In grad_steps = 3838, loss = 0.12263813614845276
In grad_steps = 3839, loss = 0.017864014953374863
In grad_steps = 3840, loss = 0.04334554448723793
In grad_steps = 3841, loss = 0.019157664850354195
In grad_steps = 3842, loss = 0.04360484704375267
In grad_steps = 3843, loss = 0.7828283309936523
In grad_steps = 3844, loss = 0.27272966504096985
In grad_steps = 3845, loss = 0.08758655935525894
In grad_steps = 3846, loss = 0.20145702362060547
In grad_steps = 3847, loss = 0.09183481335639954
In grad_steps = 3848, loss = 1.8150608539581299
In grad_steps = 3849, loss = 0.046891652047634125
In grad_steps = 3850, loss = 0.12976408004760742
In grad_steps = 3851, loss = 0.21249069273471832
In grad_steps = 3852, loss = 0.3624921441078186
In grad_steps = 3853, loss = 0.008017297834157944
In grad_steps = 3854, loss = 0.3088284432888031
In grad_steps = 3855, loss = 0.008890476077795029
In grad_steps = 3856, loss = 0.9640741944313049
In grad_steps = 3857, loss = 0.01280883140861988
In grad_steps = 3858, loss = 0.09904447197914124
In grad_steps = 3859, loss = 0.03853797912597656
In grad_steps = 3860, loss = 0.5812728404998779
In grad_steps = 3861, loss = 0.7395824790000916
In grad_steps = 3862, loss = 0.5859387516975403
In grad_steps = 3863, loss = 0.17580734193325043
In grad_steps = 3864, loss = 0.6662554144859314
In grad_steps = 3865, loss = 0.03372952342033386
In grad_steps = 3866, loss = 0.5394644141197205
In grad_steps = 3867, loss = 0.08635136485099792
In grad_steps = 3868, loss = 0.6530643105506897
In grad_steps = 3869, loss = 0.38742002844810486
In grad_steps = 3870, loss = 0.037839196622371674
In grad_steps = 3871, loss = 0.07473558187484741
In grad_steps = 3872, loss = 0.0897899717092514
In grad_steps = 3873, loss = 0.3822762370109558
In grad_steps = 3874, loss = 0.09699481725692749
In grad_steps = 3875, loss = 0.1810791790485382
In grad_steps = 3876, loss = 0.06544382125139236
In grad_steps = 3877, loss = 0.19443625211715698
In grad_steps = 3878, loss = 0.3857009708881378
In grad_steps = 3879, loss = 0.31916457414627075
In grad_steps = 3880, loss = 0.1989772617816925
In grad_steps = 3881, loss = 0.7290883660316467
In grad_steps = 3882, loss = 0.10751625150442123
In grad_steps = 3883, loss = 0.8216592669487
In grad_steps = 3884, loss = 0.03727159649133682
In grad_steps = 3885, loss = 0.34102070331573486
In grad_steps = 3886, loss = 0.062448326498270035
In grad_steps = 3887, loss = 0.08844269812107086
In grad_steps = 3888, loss = 0.020757406949996948
In grad_steps = 3889, loss = 0.1409672647714615
In grad_steps = 3890, loss = 0.5754249691963196
In grad_steps = 3891, loss = 0.26356133818626404
In grad_steps = 3892, loss = 0.12583430111408234
In grad_steps = 3893, loss = 0.3379184901714325
In grad_steps = 3894, loss = 0.062474727630615234
In grad_steps = 3895, loss = 0.08321362733840942
In grad_steps = 3896, loss = 0.01714794896543026
In grad_steps = 3897, loss = 0.3456656336784363
In grad_steps = 3898, loss = 0.02308071404695511
In grad_steps = 3899, loss = 0.48328903317451477
In grad_steps = 3900, loss = 0.2337571233510971
In grad_steps = 3901, loss = 0.07022174447774887
In grad_steps = 3902, loss = 0.4848266839981079
In grad_steps = 3903, loss = 0.020220644772052765
In grad_steps = 3904, loss = 0.024418286979198456
In grad_steps = 3905, loss = 0.052700113505125046
In grad_steps = 3906, loss = 1.0218737125396729
In grad_steps = 3907, loss = 0.12230286747217178
In grad_steps = 3908, loss = 0.15631496906280518
In grad_steps = 3909, loss = 0.09110840409994125
In grad_steps = 3910, loss = 0.24783426523208618
In grad_steps = 3911, loss = 0.08754859119653702
In grad_steps = 3912, loss = 0.09383201599121094
In grad_steps = 3913, loss = 0.13944318890571594
In grad_steps = 3914, loss = 0.04211312159895897
In grad_steps = 3915, loss = 0.010186976753175259
In grad_steps = 3916, loss = 0.4096497595310211
In grad_steps = 3917, loss = 0.023050498217344284
In grad_steps = 3918, loss = 0.09795035421848297
In grad_steps = 3919, loss = 1.1731892824172974
In grad_steps = 3920, loss = 0.013591085560619831
In grad_steps = 3921, loss = 0.03777480497956276
In grad_steps = 3922, loss = 0.13029149174690247
In grad_steps = 3923, loss = 1.2511454820632935
In grad_steps = 3924, loss = 0.03292525187134743
In grad_steps = 3925, loss = 0.12877191603183746
In grad_steps = 3926, loss = 0.09572941809892654
In grad_steps = 3927, loss = 0.18800660967826843
In grad_steps = 3928, loss = 1.3004677295684814
In grad_steps = 3929, loss = 0.846401035785675
In grad_steps = 3930, loss = 0.0976097509264946
In grad_steps = 3931, loss = 0.9955090284347534
In grad_steps = 3932, loss = 0.023360345512628555
In grad_steps = 3933, loss = 0.2723703384399414
In grad_steps = 3934, loss = 0.3938291072845459
In grad_steps = 3935, loss = 0.38545626401901245
In grad_steps = 3936, loss = 0.23564353585243225
In grad_steps = 3937, loss = 0.11591766774654388
In grad_steps = 3938, loss = 0.24106094241142273
In grad_steps = 3939, loss = 0.38740894198417664
In grad_steps = 3940, loss = 0.5431476831436157
In grad_steps = 3941, loss = 0.2097647488117218
In grad_steps = 3942, loss = 0.38211238384246826
In grad_steps = 3943, loss = 0.23886385560035706
In grad_steps = 3944, loss = 0.31527891755104065
In grad_steps = 3945, loss = 0.437311053276062
In grad_steps = 3946, loss = 0.24808599054813385
In grad_steps = 3947, loss = 0.25595036149024963
In grad_steps = 3948, loss = 0.06657921522855759
In grad_steps = 3949, loss = 0.203618124127388
In grad_steps = 3950, loss = 0.12920020520687103
In grad_steps = 3951, loss = 0.06325782835483551
In grad_steps = 3952, loss = 0.050331272184848785
In grad_steps = 3953, loss = 0.053821757435798645
In grad_steps = 3954, loss = 0.06951237469911575
In grad_steps = 3955, loss = 0.0914776548743248
In grad_steps = 3956, loss = 0.046331748366355896
In grad_steps = 3957, loss = 0.7156779170036316
In grad_steps = 3958, loss = 0.0859442874789238
In grad_steps = 3959, loss = 0.01843414083123207
In grad_steps = 3960, loss = 0.03434198722243309
In grad_steps = 3961, loss = 0.010959826409816742
In grad_steps = 3962, loss = 0.3042221665382385
In grad_steps = 3963, loss = 0.024093875661492348
In grad_steps = 3964, loss = 0.02144836075603962
In grad_steps = 3965, loss = 0.008912011049687862
In grad_steps = 3966, loss = 0.424249529838562
In grad_steps = 3967, loss = 0.8501332998275757
In grad_steps = 3968, loss = 0.30161169171333313
In grad_steps = 3969, loss = 0.020625099539756775
In grad_steps = 3970, loss = 0.012615930289030075
In grad_steps = 3971, loss = 0.1561221182346344
In grad_steps = 3972, loss = 0.04992982745170593
In grad_steps = 3973, loss = 0.03804329037666321
In grad_steps = 3974, loss = 0.09021684527397156
In grad_steps = 3975, loss = 0.03575228154659271
In grad_steps = 3976, loss = 0.13805073499679565
In grad_steps = 3977, loss = 0.16442662477493286
In grad_steps = 3978, loss = 0.534913182258606
In grad_steps = 3979, loss = 0.008026842027902603
In grad_steps = 3980, loss = 0.05567515268921852
In grad_steps = 3981, loss = 0.3337695300579071
In grad_steps = 3982, loss = 0.04107414186000824
In grad_steps = 3983, loss = 0.7141298055648804
In grad_steps = 3984, loss = 0.8748751282691956
In grad_steps = 3985, loss = 0.014948411844670773
In grad_steps = 3986, loss = 0.13768889009952545
In grad_steps = 3987, loss = 0.03055884689092636
In grad_steps = 3988, loss = 0.015070797875523567
In grad_steps = 3989, loss = 0.02821030095219612
In grad_steps = 3990, loss = 0.03157728165388107
In grad_steps = 3991, loss = 0.052791811525821686
In grad_steps = 3992, loss = 0.6537292003631592
In grad_steps = 3993, loss = 0.08047989755868912
In grad_steps = 3994, loss = 0.030663223937153816
In grad_steps = 3995, loss = 0.4860963523387909
In grad_steps = 3996, loss = 0.057395707815885544
In grad_steps = 3997, loss = 0.4740488827228546
In grad_steps = 3998, loss = 1.317148208618164
In grad_steps = 3999, loss = 0.60639888048172
In grad_steps = 4000, loss = 0.2531293034553528
In grad_steps = 4001, loss = 0.1632731407880783
In grad_steps = 4002, loss = 0.21067433059215546
In grad_steps = 4003, loss = 0.6786131858825684
In grad_steps = 4004, loss = 0.0893731415271759
In grad_steps = 4005, loss = 0.15051257610321045
In grad_steps = 4006, loss = 0.16450943052768707
In grad_steps = 4007, loss = 0.21452488005161285
In grad_steps = 4008, loss = 0.18116925656795502
In grad_steps = 4009, loss = 0.5264426469802856
In grad_steps = 4010, loss = 0.35937607288360596
In grad_steps = 4011, loss = 0.12168748676776886
In grad_steps = 4012, loss = 0.17996425926685333
In grad_steps = 4013, loss = 0.31040334701538086
In grad_steps = 4014, loss = 0.14253553748130798
In grad_steps = 4015, loss = 0.23515500128269196
In grad_steps = 4016, loss = 0.09928277879953384
In grad_steps = 4017, loss = 0.12242385745048523
In grad_steps = 4018, loss = 0.06761571764945984
In grad_steps = 4019, loss = 0.4585960805416107
In grad_steps = 4020, loss = 0.020530635491013527
In grad_steps = 4021, loss = 0.08077263832092285
In grad_steps = 4022, loss = 1.303210973739624
In grad_steps = 4023, loss = 0.21384073793888092
In grad_steps = 4024, loss = 0.6685081124305725
In grad_steps = 4025, loss = 0.1624126136302948
In grad_steps = 4026, loss = 0.043726734817028046
In grad_steps = 4027, loss = 0.04796169325709343
In grad_steps = 4028, loss = 0.03710728883743286
In grad_steps = 4029, loss = 0.2262915074825287
In grad_steps = 4030, loss = 0.11842737346887589
In grad_steps = 4031, loss = 0.20251652598381042
In grad_steps = 4032, loss = 0.3594166040420532
In grad_steps = 4033, loss = 0.12019962072372437
In grad_steps = 4034, loss = 0.2794419229030609
In grad_steps = 4035, loss = 0.18800000846385956
In grad_steps = 4036, loss = 0.03907602280378342
In grad_steps = 4037, loss = 0.06688137352466583
In grad_steps = 4038, loss = 0.5743227601051331
In grad_steps = 4039, loss = 0.28913694620132446
In grad_steps = 4040, loss = 0.02508734166622162
In grad_steps = 4041, loss = 1.7267305850982666
In grad_steps = 4042, loss = 0.9022048115730286
In grad_steps = 4043, loss = 0.04270762950181961
In grad_steps = 4044, loss = 0.2671489119529724
In grad_steps = 4045, loss = 0.05459592118859291
In grad_steps = 4046, loss = 0.1881435215473175
In grad_steps = 4047, loss = 0.666915774345398
In grad_steps = 4048, loss = 0.038261253386735916
In grad_steps = 4049, loss = 0.666547417640686
In grad_steps = 4050, loss = 0.16715821623802185
In grad_steps = 4051, loss = 0.0505349226295948
In grad_steps = 4052, loss = 0.1101202666759491
In grad_steps = 4053, loss = 0.15516920387744904
In grad_steps = 4054, loss = 0.3210791349411011
In grad_steps = 4055, loss = 0.254428505897522
In grad_steps = 4056, loss = 0.06396153569221497
In grad_steps = 4057, loss = 0.09369093179702759
In grad_steps = 4058, loss = 0.0830555409193039
In grad_steps = 4059, loss = 0.14693693816661835
In grad_steps = 4060, loss = 0.03887262940406799
In grad_steps = 4061, loss = 0.08195134997367859
In grad_steps = 4062, loss = 0.7037622332572937
In grad_steps = 4063, loss = 0.3165733218193054
In grad_steps = 4064, loss = 0.40104079246520996
In grad_steps = 4065, loss = 0.3554813861846924
In grad_steps = 4066, loss = 0.2496819645166397
In grad_steps = 4067, loss = 0.06686337292194366
In grad_steps = 4068, loss = 0.07049916684627533
In grad_steps = 4069, loss = 0.0500340461730957
In grad_steps = 4070, loss = 0.25743359327316284
In grad_steps = 4071, loss = 0.35482797026634216
In grad_steps = 4072, loss = 0.12258485704660416
In grad_steps = 4073, loss = 0.033952224999666214
In grad_steps = 4074, loss = 0.04797744378447533
In grad_steps = 4075, loss = 0.06872882694005966
In grad_steps = 4076, loss = 0.048598919063806534
In grad_steps = 4077, loss = 0.052616603672504425
In grad_steps = 4078, loss = 0.041038863360881805
In grad_steps = 4079, loss = 0.03428123518824577
In grad_steps = 4080, loss = 0.07344973087310791
In grad_steps = 4081, loss = 0.10224024206399918
In grad_steps = 4082, loss = 0.1980043649673462
In grad_steps = 4083, loss = 0.17102931439876556
In grad_steps = 4084, loss = 0.02826399728655815
In grad_steps = 4085, loss = 0.3267447352409363
In grad_steps = 4086, loss = 0.028620822355151176
In grad_steps = 4087, loss = 0.024788539856672287
In grad_steps = 4088, loss = 0.007399656344205141
In grad_steps = 4089, loss = 0.5769073963165283
In grad_steps = 4090, loss = 0.3265034854412079
In grad_steps = 4091, loss = 0.8483647108078003
In grad_steps = 4092, loss = 0.013309748843312263
In grad_steps = 4093, loss = 0.006542461924254894
In grad_steps = 4094, loss = 0.32629266381263733
In grad_steps = 4095, loss = 0.007892702706158161
In grad_steps = 4096, loss = 0.05369553714990616
In grad_steps = 4097, loss = 0.39461350440979004
In grad_steps = 4098, loss = 0.020771099254488945
In grad_steps = 4099, loss = 0.006541695445775986
In grad_steps = 4100, loss = 0.03931671008467674
In grad_steps = 4101, loss = 0.10462352633476257
In grad_steps = 4102, loss = 0.5275082588195801
In grad_steps = 4103, loss = 0.01920505426824093
In grad_steps = 4104, loss = 0.47086113691329956
In grad_steps = 4105, loss = 0.006035909987986088
Elapsed time: 2341.3400049209595 seconds for ensemble 1 with 2 epochs
LoRA instance 1 evaluation complete. Data saved to /pscratch/sd/t/tianle/lucid/other_source/SURP_2024/results/tmp_lora_ensemble/Llama3-8B-set-5/test_data_instance_1_seed_10094.npz.
lora instance i = 1 Successfully finished.
Training lora instance 2
DEBUG: Total parameters: 8037076992
DEBUG: Trainable parameters: 6815744
Beginning epoch 1
In grad_steps = 0, loss = 0.7379498481750488
In grad_steps = 1, loss = 0.6387481689453125
In grad_steps = 2, loss = 1.618226170539856
In grad_steps = 3, loss = 0.8022263050079346
In grad_steps = 4, loss = 0.6231769323348999
In grad_steps = 5, loss = 0.8321632146835327
In grad_steps = 6, loss = 0.5277618765830994
In grad_steps = 7, loss = 0.5761269927024841
In grad_steps = 8, loss = 1.1830377578735352
In grad_steps = 9, loss = 0.9756046533584595
In grad_steps = 10, loss = 0.7582152485847473
In grad_steps = 11, loss = 0.7075269222259521
In grad_steps = 12, loss = 0.7455298900604248
In grad_steps = 13, loss = 0.5896276235580444
In grad_steps = 14, loss = 0.9286870956420898
In grad_steps = 15, loss = 0.5364060401916504
In grad_steps = 16, loss = 0.7139226198196411
In grad_steps = 17, loss = 0.693264365196228
In grad_steps = 18, loss = 0.7798033952713013
In grad_steps = 19, loss = 0.74331134557724
In grad_steps = 20, loss = 0.6734253764152527
In grad_steps = 21, loss = 0.6478991508483887
In grad_steps = 22, loss = 0.6752838492393494
In grad_steps = 23, loss = 0.7845897674560547
In grad_steps = 24, loss = 0.6318313479423523
In grad_steps = 25, loss = 0.7573400735855103
In grad_steps = 26, loss = 0.7422193288803101
In grad_steps = 27, loss = 0.7161741852760315
In grad_steps = 28, loss = 0.5685610771179199
In grad_steps = 29, loss = 0.6855762600898743
In grad_steps = 30, loss = 0.8049404621124268
In grad_steps = 31, loss = 0.6257418990135193
In grad_steps = 32, loss = 0.6463626623153687
In grad_steps = 33, loss = 0.6809019446372986
In grad_steps = 34, loss = 0.6315038204193115
In grad_steps = 35, loss = 0.6863924264907837
In grad_steps = 36, loss = 0.6568104028701782
In grad_steps = 37, loss = 0.6852353811264038
In grad_steps = 38, loss = 0.790855884552002
In grad_steps = 39, loss = 0.6891697645187378
In grad_steps = 40, loss = 0.7380018830299377
In grad_steps = 41, loss = 0.6763979196548462
In grad_steps = 42, loss = 0.6353280544281006
In grad_steps = 43, loss = 0.6860235929489136
In grad_steps = 44, loss = 0.5840683579444885
In grad_steps = 45, loss = 0.7195520401000977
In grad_steps = 46, loss = 0.7162681818008423
In grad_steps = 47, loss = 0.6696698069572449
In grad_steps = 48, loss = 0.5205109119415283
In grad_steps = 49, loss = 0.6618040800094604
In grad_steps = 50, loss = 0.7174806594848633
In grad_steps = 51, loss = 0.6746184825897217
In grad_steps = 52, loss = 0.5615668296813965
In grad_steps = 53, loss = 0.7334920763969421
In grad_steps = 54, loss = 0.6396300792694092
In grad_steps = 55, loss = 0.6122590899467468
In grad_steps = 56, loss = 0.691205620765686
In grad_steps = 57, loss = 0.7031270265579224
In grad_steps = 58, loss = 0.7011479139328003
In grad_steps = 59, loss = 0.7211621403694153
In grad_steps = 60, loss = 0.7484151721000671
In grad_steps = 61, loss = 0.6040153503417969
In grad_steps = 62, loss = 0.6392660140991211
In grad_steps = 63, loss = 0.5884954929351807
In grad_steps = 64, loss = 0.47380784153938293
In grad_steps = 65, loss = 0.5112462043762207
In grad_steps = 66, loss = 0.5670480728149414
In grad_steps = 67, loss = 0.37919679284095764
In grad_steps = 68, loss = 0.5047069191932678
In grad_steps = 69, loss = 0.43090081214904785
In grad_steps = 70, loss = 1.13010835647583
In grad_steps = 71, loss = 0.3125617504119873
In grad_steps = 72, loss = 1.2716649770736694
In grad_steps = 73, loss = 0.9318873882293701
In grad_steps = 74, loss = 0.47781461477279663
In grad_steps = 75, loss = 0.7997044324874878
In grad_steps = 76, loss = 0.4058837294578552
In grad_steps = 77, loss = 0.41285550594329834
In grad_steps = 78, loss = 0.5654283761978149
In grad_steps = 79, loss = 0.6182135343551636
In grad_steps = 80, loss = 0.6406550407409668
In grad_steps = 81, loss = 0.2412499189376831
In grad_steps = 82, loss = 0.9767565727233887
In grad_steps = 83, loss = 0.7718086838722229
In grad_steps = 84, loss = 0.7398144602775574
In grad_steps = 85, loss = 0.786310076713562
In grad_steps = 86, loss = 0.8261526226997375
In grad_steps = 87, loss = 0.6218122243881226
In grad_steps = 88, loss = 0.49213850498199463
In grad_steps = 89, loss = 0.6037020683288574
In grad_steps = 90, loss = 0.6545187830924988
In grad_steps = 91, loss = 0.6852116584777832
In grad_steps = 92, loss = 0.6075233221054077
In grad_steps = 93, loss = 0.8250041007995605
In grad_steps = 94, loss = 0.678621232509613
In grad_steps = 95, loss = 0.3485410809516907
In grad_steps = 96, loss = 0.6768506169319153
In grad_steps = 97, loss = 0.7467926144599915
In grad_steps = 98, loss = 0.7601937055587769
In grad_steps = 99, loss = 0.5317139625549316
In grad_steps = 100, loss = 0.8598712682723999
In grad_steps = 101, loss = 0.979602575302124
In grad_steps = 102, loss = 0.8198711276054382
In grad_steps = 103, loss = 0.3764825761318207
In grad_steps = 104, loss = 0.5660125017166138
In grad_steps = 105, loss = 0.6093391180038452
In grad_steps = 106, loss = 0.534419596195221
In grad_steps = 107, loss = 0.6447197198867798
In grad_steps = 108, loss = 0.5423089861869812
In grad_steps = 109, loss = 0.5138483047485352
In grad_steps = 110, loss = 0.8793582916259766
In grad_steps = 111, loss = 0.3724016547203064
In grad_steps = 112, loss = 0.2994017004966736
In grad_steps = 113, loss = 0.5671766996383667
In grad_steps = 114, loss = 0.24508744478225708
In grad_steps = 115, loss = 1.021736741065979
In grad_steps = 116, loss = 0.37774837017059326
In grad_steps = 117, loss = 0.47250354290008545
In grad_steps = 118, loss = 0.5328871011734009
In grad_steps = 119, loss = 0.7083361148834229
In grad_steps = 120, loss = 0.3095436692237854
In grad_steps = 121, loss = 0.36742669343948364
In grad_steps = 122, loss = 0.3611665964126587
In grad_steps = 123, loss = 0.8626528382301331
In grad_steps = 124, loss = 0.6896375417709351
In grad_steps = 125, loss = 0.7642097473144531
In grad_steps = 126, loss = 0.1427096128463745
In grad_steps = 127, loss = 0.6091698408126831
In grad_steps = 128, loss = 0.5380462408065796
In grad_steps = 129, loss = 0.48082977533340454
In grad_steps = 130, loss = 1.065608024597168
In grad_steps = 131, loss = 0.6635955572128296
In grad_steps = 132, loss = 0.5990715026855469
In grad_steps = 133, loss = 0.5115343332290649
In grad_steps = 134, loss = 0.6576178669929504
In grad_steps = 135, loss = 0.6096324324607849
In grad_steps = 136, loss = 0.5041779279708862
In grad_steps = 137, loss = 0.31122326850891113
In grad_steps = 138, loss = 0.5073577165603638
In grad_steps = 139, loss = 0.31287986040115356
In grad_steps = 140, loss = 0.7791053652763367
In grad_steps = 141, loss = 0.41013649106025696
In grad_steps = 142, loss = 0.4428790807723999
In grad_steps = 143, loss = 0.20310711860656738
In grad_steps = 144, loss = 0.6872509121894836
In grad_steps = 145, loss = 0.31030598282814026
In grad_steps = 146, loss = 0.6955246925354004
In grad_steps = 147, loss = 0.245193213224411
In grad_steps = 148, loss = 0.25440046191215515
In grad_steps = 149, loss = 0.368254691362381
In grad_steps = 150, loss = 0.905478835105896
In grad_steps = 151, loss = 0.5209782719612122
In grad_steps = 152, loss = 1.4386898279190063
In grad_steps = 153, loss = 0.9794310331344604
In grad_steps = 154, loss = 0.7763593196868896
In grad_steps = 155, loss = 0.2009754180908203
In grad_steps = 156, loss = 0.4469637870788574
In grad_steps = 157, loss = 0.15881392359733582
In grad_steps = 158, loss = 0.26623010635375977
In grad_steps = 159, loss = 0.5818158984184265
In grad_steps = 160, loss = 0.21976368129253387
In grad_steps = 161, loss = 0.7849951982498169
In grad_steps = 162, loss = 0.5370681285858154
In grad_steps = 163, loss = 0.3467992842197418
In grad_steps = 164, loss = 0.7644854784011841
In grad_steps = 165, loss = 0.2332528531551361
In grad_steps = 166, loss = 0.17954778671264648
In grad_steps = 167, loss = 0.6698849201202393
In grad_steps = 168, loss = 0.6298503279685974
In grad_steps = 169, loss = 0.1540844589471817
In grad_steps = 170, loss = 0.26022934913635254
In grad_steps = 171, loss = 0.4700028896331787
In grad_steps = 172, loss = 0.3313141167163849
In grad_steps = 173, loss = 0.1451457440853119
In grad_steps = 174, loss = 0.13909068703651428
In grad_steps = 175, loss = 0.665002703666687
In grad_steps = 176, loss = 0.07871696352958679
In grad_steps = 177, loss = 0.22580799460411072
In grad_steps = 178, loss = 0.6303916573524475
In grad_steps = 179, loss = 1.2217484712600708
In grad_steps = 180, loss = 0.2372642457485199
In grad_steps = 181, loss = 0.48256006836891174
In grad_steps = 182, loss = 0.6947145462036133
In grad_steps = 183, loss = 0.3408830761909485
In grad_steps = 184, loss = 0.5728135108947754
In grad_steps = 185, loss = 0.49842700362205505
In grad_steps = 186, loss = 0.3887609541416168
In grad_steps = 187, loss = 0.45419827103614807
In grad_steps = 188, loss = 0.575111985206604
In grad_steps = 189, loss = 0.3347468674182892
In grad_steps = 190, loss = 0.17346897721290588
In grad_steps = 191, loss = 0.3055627942085266
In grad_steps = 192, loss = 0.6099489331245422
In grad_steps = 193, loss = 0.5121673345565796
In grad_steps = 194, loss = 0.9805293083190918
In grad_steps = 195, loss = 0.6630454063415527
In grad_steps = 196, loss = 0.6204985976219177
In grad_steps = 197, loss = 0.5424355268478394
In grad_steps = 198, loss = 0.49702414870262146
In grad_steps = 199, loss = 1.0918128490447998
In grad_steps = 200, loss = 0.2649756968021393
In grad_steps = 201, loss = 0.77447909116745
In grad_steps = 202, loss = 0.537839949131012
In grad_steps = 203, loss = 0.3254614472389221
In grad_steps = 204, loss = 0.53512042760849
In grad_steps = 205, loss = 0.5658679008483887
In grad_steps = 206, loss = 0.2713564336299896
In grad_steps = 207, loss = 0.396287202835083
In grad_steps = 208, loss = 0.3041805326938629
In grad_steps = 209, loss = 0.15386587381362915
In grad_steps = 210, loss = 0.20163801312446594
In grad_steps = 211, loss = 1.035509467124939
In grad_steps = 212, loss = 0.39684242010116577
In grad_steps = 213, loss = 0.3485763967037201
In grad_steps = 214, loss = 0.15351301431655884
In grad_steps = 215, loss = 0.18884339928627014
In grad_steps = 216, loss = 0.5158993005752563
In grad_steps = 217, loss = 0.48783665895462036
In grad_steps = 218, loss = 1.852250099182129
In grad_steps = 219, loss = 0.5186495780944824
In grad_steps = 220, loss = 0.33110010623931885
In grad_steps = 221, loss = 0.9574028849601746
In grad_steps = 222, loss = 0.5943062901496887
In grad_steps = 223, loss = 0.5263007283210754
In grad_steps = 224, loss = 0.2015211135149002
In grad_steps = 225, loss = 0.08937472850084305
In grad_steps = 226, loss = 0.7173212170600891
In grad_steps = 227, loss = 0.4753042459487915
In grad_steps = 228, loss = 0.4478486180305481
In grad_steps = 229, loss = 0.3220442533493042
In grad_steps = 230, loss = 0.32266688346862793
In grad_steps = 231, loss = 0.7474095225334167
In grad_steps = 232, loss = 1.0106121301651
In grad_steps = 233, loss = 0.26954081654548645
In grad_steps = 234, loss = 0.6448736786842346
In grad_steps = 235, loss = 0.6760556101799011
In grad_steps = 236, loss = 0.5798329710960388
In grad_steps = 237, loss = 0.2218082994222641
In grad_steps = 238, loss = 0.13454723358154297
In grad_steps = 239, loss = 0.2883846163749695
In grad_steps = 240, loss = 0.1853192299604416
In grad_steps = 241, loss = 0.5931583642959595
In grad_steps = 242, loss = 0.2991790473461151
In grad_steps = 243, loss = 0.3191828429698944
In grad_steps = 244, loss = 1.1348743438720703
In grad_steps = 245, loss = 0.2470044195652008
In grad_steps = 246, loss = 0.19361911714076996
In grad_steps = 247, loss = 0.44561001658439636
In grad_steps = 248, loss = 1.2461180686950684
In grad_steps = 249, loss = 0.18124644458293915
In grad_steps = 250, loss = 0.5936884880065918
In grad_steps = 251, loss = 0.9625958204269409
In grad_steps = 252, loss = 0.17017047107219696
In grad_steps = 253, loss = 0.5854663252830505
In grad_steps = 254, loss = 0.545897364616394
In grad_steps = 255, loss = 0.5520545244216919
In grad_steps = 256, loss = 0.9328219890594482
In grad_steps = 257, loss = 0.6539218425750732
In grad_steps = 258, loss = 0.8503850698471069
In grad_steps = 259, loss = 0.6646938323974609
In grad_steps = 260, loss = 0.6875221729278564
In grad_steps = 261, loss = 1.0101205110549927
In grad_steps = 262, loss = 0.4980310797691345
In grad_steps = 263, loss = 0.6134385466575623
In grad_steps = 264, loss = 0.469303160905838
In grad_steps = 265, loss = 0.6643590927124023
In grad_steps = 266, loss = 0.9494427442550659
In grad_steps = 267, loss = 0.9057167172431946
In grad_steps = 268, loss = 0.56172776222229
In grad_steps = 269, loss = 0.32262998819351196
In grad_steps = 270, loss = 0.5028031468391418
In grad_steps = 271, loss = 0.6515892148017883
In grad_steps = 272, loss = 0.46132588386535645
In grad_steps = 273, loss = 0.5011133551597595
In grad_steps = 274, loss = 0.4075866639614105
In grad_steps = 275, loss = 0.5639656782150269
In grad_steps = 276, loss = 0.5226168632507324
In grad_steps = 277, loss = 0.3798240125179291
In grad_steps = 278, loss = 0.4779896140098572
In grad_steps = 279, loss = 0.6753024458885193
In grad_steps = 280, loss = 0.5743255615234375
In grad_steps = 281, loss = 0.4696906805038452
In grad_steps = 282, loss = 0.5292045474052429
In grad_steps = 283, loss = 0.5486205816268921
In grad_steps = 284, loss = 0.5265271067619324
In grad_steps = 285, loss = 0.3000292181968689
In grad_steps = 286, loss = 0.18466350436210632
In grad_steps = 287, loss = 1.0874983072280884
In grad_steps = 288, loss = 1.208488941192627
In grad_steps = 289, loss = 0.41852477192878723
In grad_steps = 290, loss = 0.4102138876914978
In grad_steps = 291, loss = 1.1145000457763672
In grad_steps = 292, loss = 0.5826094746589661
In grad_steps = 293, loss = 0.793945848941803
In grad_steps = 294, loss = 0.4166206419467926
In grad_steps = 295, loss = 0.7945558428764343
In grad_steps = 296, loss = 0.4260826110839844
In grad_steps = 297, loss = 0.37469127774238586
In grad_steps = 298, loss = 0.4106268882751465
In grad_steps = 299, loss = 0.5067586898803711
In grad_steps = 300, loss = 0.5903324484825134
In grad_steps = 301, loss = 0.574704647064209
In grad_steps = 302, loss = 0.4009133577346802
In grad_steps = 303, loss = 0.5085954070091248
In grad_steps = 304, loss = 0.18234476447105408
In grad_steps = 305, loss = 0.2625581920146942
In grad_steps = 306, loss = 0.6303199529647827
In grad_steps = 307, loss = 0.411484032869339
In grad_steps = 308, loss = 0.648858368396759
In grad_steps = 309, loss = 0.45622414350509644
In grad_steps = 310, loss = 0.19470764696598053
In grad_steps = 311, loss = 1.4924685955047607
In grad_steps = 312, loss = 0.2019522786140442
In grad_steps = 313, loss = 0.23136869072914124
In grad_steps = 314, loss = 0.5481931567192078
In grad_steps = 315, loss = 0.10614056140184402
In grad_steps = 316, loss = 1.278826117515564
In grad_steps = 317, loss = 0.449773371219635
In grad_steps = 318, loss = 0.33179640769958496
In grad_steps = 319, loss = 0.13201358914375305
In grad_steps = 320, loss = 0.6012380123138428
In grad_steps = 321, loss = 0.18927037715911865
In grad_steps = 322, loss = 0.9776886701583862
In grad_steps = 323, loss = 0.397754967212677
In grad_steps = 324, loss = 0.41272225975990295
In grad_steps = 325, loss = 0.23172785341739655
In grad_steps = 326, loss = 0.39849191904067993
In grad_steps = 327, loss = 0.29514822363853455
In grad_steps = 328, loss = 0.2599700689315796
In grad_steps = 329, loss = 0.9751393795013428
In grad_steps = 330, loss = 0.21952012181282043
In grad_steps = 331, loss = 0.09250673651695251
In grad_steps = 332, loss = 0.12497436255216599
In grad_steps = 333, loss = 0.18946528434753418
In grad_steps = 334, loss = 0.5182499885559082
In grad_steps = 335, loss = 0.27443334460258484
In grad_steps = 336, loss = 0.6359381675720215
In grad_steps = 337, loss = 0.18737317621707916
In grad_steps = 338, loss = 0.09933287650346756
In grad_steps = 339, loss = 0.4938518702983856
In grad_steps = 340, loss = 0.09547571837902069
In grad_steps = 341, loss = 0.7734176516532898
In grad_steps = 342, loss = 0.20016613602638245
In grad_steps = 343, loss = 0.372783899307251
In grad_steps = 344, loss = 0.9571043252944946
In grad_steps = 345, loss = 0.6435731053352356
In grad_steps = 346, loss = 0.3469164967536926
In grad_steps = 347, loss = 0.5203562378883362
In grad_steps = 348, loss = 0.7426948547363281
In grad_steps = 349, loss = 0.9538629055023193
In grad_steps = 350, loss = 0.178585946559906
In grad_steps = 351, loss = 1.3142764568328857
In grad_steps = 352, loss = 0.7506421804428101
In grad_steps = 353, loss = 0.6129058003425598
In grad_steps = 354, loss = 0.7813237905502319
In grad_steps = 355, loss = 0.7607883810997009
In grad_steps = 356, loss = 0.32910940051078796
In grad_steps = 357, loss = 0.2722334861755371
In grad_steps = 358, loss = 0.852211594581604
In grad_steps = 359, loss = 0.519514262676239
In grad_steps = 360, loss = 0.896137535572052
In grad_steps = 361, loss = 0.773337721824646
In grad_steps = 362, loss = 0.6755855083465576
In grad_steps = 363, loss = 0.9598007798194885
In grad_steps = 364, loss = 0.3314161002635956
In grad_steps = 365, loss = 0.7138515710830688
In grad_steps = 366, loss = 0.7917751669883728
In grad_steps = 367, loss = 0.5904319286346436
In grad_steps = 368, loss = 0.49194109439849854
In grad_steps = 369, loss = 0.4019754230976105
In grad_steps = 370, loss = 0.5553857684135437
In grad_steps = 371, loss = 0.48831096291542053
In grad_steps = 372, loss = 0.43351152539253235
In grad_steps = 373, loss = 0.5506480932235718
In grad_steps = 374, loss = 0.35510748624801636
In grad_steps = 375, loss = 0.5019720792770386
In grad_steps = 376, loss = 0.41792958974838257
In grad_steps = 377, loss = 0.3302476406097412
In grad_steps = 378, loss = 0.28899434208869934
In grad_steps = 379, loss = 0.17517012357711792
In grad_steps = 380, loss = 0.6210318803787231
In grad_steps = 381, loss = 0.8413376212120056
In grad_steps = 382, loss = 0.37096112966537476
In grad_steps = 383, loss = 0.2977924346923828
In grad_steps = 384, loss = 0.08771482855081558
In grad_steps = 385, loss = 0.03748372942209244
In grad_steps = 386, loss = 0.18440121412277222
In grad_steps = 387, loss = 1.6589583158493042
In grad_steps = 388, loss = 0.7472978234291077
In grad_steps = 389, loss = 0.35800379514694214
In grad_steps = 390, loss = 0.2524484694004059
In grad_steps = 391, loss = 0.32334259152412415
In grad_steps = 392, loss = 0.3887123465538025
In grad_steps = 393, loss = 0.5468312501907349
In grad_steps = 394, loss = 0.26808249950408936
In grad_steps = 395, loss = 1.1334302425384521
In grad_steps = 396, loss = 0.7152128219604492
In grad_steps = 397, loss = 0.09759503602981567
In grad_steps = 398, loss = 0.7783518433570862
In grad_steps = 399, loss = 0.19582059979438782
In grad_steps = 400, loss = 1.108036756515503
In grad_steps = 401, loss = 0.27035725116729736
In grad_steps = 402, loss = 0.22519364953041077
In grad_steps = 403, loss = 0.6750317215919495
In grad_steps = 404, loss = 0.26902610063552856
In grad_steps = 405, loss = 0.36470457911491394
In grad_steps = 406, loss = 0.8111430406570435
In grad_steps = 407, loss = 0.30654266476631165
In grad_steps = 408, loss = 0.785873532295227
In grad_steps = 409, loss = 0.5426225662231445
In grad_steps = 410, loss = 0.3483891487121582
In grad_steps = 411, loss = 0.27273309230804443
In grad_steps = 412, loss = 0.7755911350250244
In grad_steps = 413, loss = 0.8906543254852295
In grad_steps = 414, loss = 1.1462006568908691
In grad_steps = 415, loss = 0.5728896856307983
In grad_steps = 416, loss = 1.051010012626648
In grad_steps = 417, loss = 0.2954792380332947
In grad_steps = 418, loss = 0.2120845913887024
In grad_steps = 419, loss = 0.26837778091430664
In grad_steps = 420, loss = 0.3609869182109833
In grad_steps = 421, loss = 0.1553841084241867
In grad_steps = 422, loss = 0.435028076171875
In grad_steps = 423, loss = 0.7623041272163391
In grad_steps = 424, loss = 0.5434271097183228
In grad_steps = 425, loss = 0.15131227672100067
In grad_steps = 426, loss = 0.1221967339515686
In grad_steps = 427, loss = 0.3400179147720337
In grad_steps = 428, loss = 0.22968241572380066
In grad_steps = 429, loss = 0.8443955183029175
In grad_steps = 430, loss = 0.1352934092283249
In grad_steps = 431, loss = 1.206039309501648
In grad_steps = 432, loss = 0.6456889510154724
In grad_steps = 433, loss = 1.2272671461105347
In grad_steps = 434, loss = 0.24501708149909973
In grad_steps = 435, loss = 0.17310801148414612
In grad_steps = 436, loss = 0.19320857524871826
In grad_steps = 437, loss = 0.3714793622493744
In grad_steps = 438, loss = 0.21892863512039185
In grad_steps = 439, loss = 0.15878194570541382
In grad_steps = 440, loss = 0.10482221841812134
In grad_steps = 441, loss = 0.08532120287418365
In grad_steps = 442, loss = 0.28812143206596375
In grad_steps = 443, loss = 0.3605380952358246
In grad_steps = 444, loss = 0.9252199530601501
In grad_steps = 445, loss = 0.1577056348323822
In grad_steps = 446, loss = 0.16542461514472961
In grad_steps = 447, loss = 0.18001557886600494
In grad_steps = 448, loss = 0.23571471869945526
In grad_steps = 449, loss = 0.10240956395864487
In grad_steps = 450, loss = 0.13135170936584473
In grad_steps = 451, loss = 0.2609799802303314
In grad_steps = 452, loss = 0.1612832397222519
In grad_steps = 453, loss = 0.10144150257110596
In grad_steps = 454, loss = 0.26395705342292786
In grad_steps = 455, loss = 0.43878400325775146
In grad_steps = 456, loss = 1.0770721435546875
In grad_steps = 457, loss = 0.019655246287584305
In grad_steps = 458, loss = 0.12289050966501236
In grad_steps = 459, loss = 0.10771315544843674
In grad_steps = 460, loss = 0.031222384423017502
In grad_steps = 461, loss = 0.8797523379325867
In grad_steps = 462, loss = 0.262767493724823
In grad_steps = 463, loss = 0.2839461863040924
In grad_steps = 464, loss = 0.5510155558586121
In grad_steps = 465, loss = 0.02193738892674446
In grad_steps = 466, loss = 0.3491605520248413
In grad_steps = 467, loss = 0.3795449137687683
In grad_steps = 468, loss = 0.40968409180641174
In grad_steps = 469, loss = 1.7939143180847168
In grad_steps = 470, loss = 0.14527684450149536
In grad_steps = 471, loss = 0.9271469116210938
In grad_steps = 472, loss = 0.5483442544937134
In grad_steps = 473, loss = 0.7536006569862366
In grad_steps = 474, loss = 0.31311100721359253
In grad_steps = 475, loss = 0.11355321854352951
In grad_steps = 476, loss = 0.13766337931156158
In grad_steps = 477, loss = 0.7271585464477539
In grad_steps = 478, loss = 0.21645180881023407
In grad_steps = 479, loss = 0.23059357702732086
In grad_steps = 480, loss = 0.169223815202713
In grad_steps = 481, loss = 0.22494640946388245
In grad_steps = 482, loss = 0.3227435350418091
In grad_steps = 483, loss = 0.9424187541007996
In grad_steps = 484, loss = 0.2256399393081665
In grad_steps = 485, loss = 0.46739572286605835
In grad_steps = 486, loss = 0.23729895055294037
In grad_steps = 487, loss = 0.43506065011024475
In grad_steps = 488, loss = 0.2057865858078003
In grad_steps = 489, loss = 0.3332083225250244
In grad_steps = 490, loss = 0.17086569964885712
In grad_steps = 491, loss = 0.1330793797969818
In grad_steps = 492, loss = 0.24796062707901
In grad_steps = 493, loss = 0.5190752148628235
In grad_steps = 494, loss = 0.3038330376148224
In grad_steps = 495, loss = 0.2179393172264099
In grad_steps = 496, loss = 0.6733267903327942
In grad_steps = 497, loss = 0.7204992175102234
In grad_steps = 498, loss = 0.3836662769317627
In grad_steps = 499, loss = 0.6057536005973816
In grad_steps = 500, loss = 0.05530594289302826
In grad_steps = 501, loss = 0.05532270297408104
In grad_steps = 502, loss = 1.2250351905822754
In grad_steps = 503, loss = 0.0890045315027237
In grad_steps = 504, loss = 0.6829690933227539
In grad_steps = 505, loss = 0.051511310040950775
In grad_steps = 506, loss = 0.35574841499328613
In grad_steps = 507, loss = 0.12170930206775665
In grad_steps = 508, loss = 0.5419286489486694
In grad_steps = 509, loss = 0.7505549192428589
In grad_steps = 510, loss = 0.5336611866950989
In grad_steps = 511, loss = 0.26071539521217346
In grad_steps = 512, loss = 0.3820106089115143
In grad_steps = 513, loss = 0.15394257009029388
In grad_steps = 514, loss = 0.40329885482788086
In grad_steps = 515, loss = 0.10551758110523224
In grad_steps = 516, loss = 0.19309642910957336
In grad_steps = 517, loss = 0.3763413727283478
In grad_steps = 518, loss = 0.14023956656455994
In grad_steps = 519, loss = 0.19967645406723022
In grad_steps = 520, loss = 0.6561643481254578
In grad_steps = 521, loss = 0.40041613578796387
In grad_steps = 522, loss = 0.5092442035675049
In grad_steps = 523, loss = 0.7024757862091064
In grad_steps = 524, loss = 0.6529356241226196
In grad_steps = 525, loss = 0.5108127593994141
In grad_steps = 526, loss = 0.5780074596405029
In grad_steps = 527, loss = 1.014305591583252
In grad_steps = 528, loss = 0.5232987999916077
In grad_steps = 529, loss = 0.3528319299221039
In grad_steps = 530, loss = 0.11126493662595749
In grad_steps = 531, loss = 0.13205599784851074
In grad_steps = 532, loss = 0.6584957838058472
In grad_steps = 533, loss = 0.5939505100250244
In grad_steps = 534, loss = 0.3292608857154846
In grad_steps = 535, loss = 1.4004039764404297
In grad_steps = 536, loss = 0.4784504175186157
In grad_steps = 537, loss = 0.530519962310791
In grad_steps = 538, loss = 0.6555871963500977
In grad_steps = 539, loss = 0.6271407604217529
In grad_steps = 540, loss = 0.23578178882598877
In grad_steps = 541, loss = 0.3512439429759979
In grad_steps = 542, loss = 0.3458276093006134
In grad_steps = 543, loss = 0.3476201593875885
In grad_steps = 544, loss = 0.637622594833374
In grad_steps = 545, loss = 0.680511474609375
In grad_steps = 546, loss = 0.44593194127082825
In grad_steps = 547, loss = 0.5602396726608276
In grad_steps = 548, loss = 0.18066754937171936
In grad_steps = 549, loss = 0.6371915340423584
In grad_steps = 550, loss = 0.22684861719608307
In grad_steps = 551, loss = 0.2732037901878357
In grad_steps = 552, loss = 0.41063717007637024
In grad_steps = 553, loss = 0.3893697261810303
In grad_steps = 554, loss = 0.2995663583278656
In grad_steps = 555, loss = 0.17224746942520142
In grad_steps = 556, loss = 0.5664287805557251
In grad_steps = 557, loss = 0.6403278708457947
In grad_steps = 558, loss = 0.2670508623123169
In grad_steps = 559, loss = 0.4341621696949005
In grad_steps = 560, loss = 0.31757691502571106
In grad_steps = 561, loss = 0.2398906946182251
In grad_steps = 562, loss = 0.19250574707984924
In grad_steps = 563, loss = 0.47350406646728516
In grad_steps = 564, loss = 0.24077832698822021
In grad_steps = 565, loss = 0.2082444727420807
In grad_steps = 566, loss = 0.3652845323085785
In grad_steps = 567, loss = 1.313401460647583
In grad_steps = 568, loss = 0.3620772957801819
In grad_steps = 569, loss = 0.30624139308929443
In grad_steps = 570, loss = 0.6476254463195801
In grad_steps = 571, loss = 0.6333622336387634
In grad_steps = 572, loss = 0.07749900221824646
In grad_steps = 573, loss = 0.09024977684020996
In grad_steps = 574, loss = 0.19487571716308594
In grad_steps = 575, loss = 0.06951246410608292
In grad_steps = 576, loss = 0.454456627368927
In grad_steps = 577, loss = 0.07812126725912094
In grad_steps = 578, loss = 1.6757978200912476
In grad_steps = 579, loss = 0.2894747853279114
In grad_steps = 580, loss = 0.42364275455474854
In grad_steps = 581, loss = 0.7536700367927551
In grad_steps = 582, loss = 0.7456192970275879
In grad_steps = 583, loss = 0.261346697807312
In grad_steps = 584, loss = 0.3383111357688904
In grad_steps = 585, loss = 0.6549598574638367
In grad_steps = 586, loss = 0.22217002511024475
In grad_steps = 587, loss = 0.2516095042228699
In grad_steps = 588, loss = 0.3618624210357666
In grad_steps = 589, loss = 0.3414697051048279
In grad_steps = 590, loss = 0.16298510134220123
In grad_steps = 591, loss = 0.2327403575181961
In grad_steps = 592, loss = 0.3760494887828827
In grad_steps = 593, loss = 0.2343789041042328
In grad_steps = 594, loss = 0.3268548846244812
In grad_steps = 595, loss = 0.7236203551292419
In grad_steps = 596, loss = 0.6855672001838684
In grad_steps = 597, loss = 0.3747745752334595
In grad_steps = 598, loss = 0.2704360783100128
In grad_steps = 599, loss = 0.7485980987548828
In grad_steps = 600, loss = 1.1040215492248535
In grad_steps = 601, loss = 0.30390310287475586
In grad_steps = 602, loss = 0.5998013019561768
In grad_steps = 603, loss = 0.29620489478111267
In grad_steps = 604, loss = 0.7824788093566895
In grad_steps = 605, loss = 1.2162669897079468
In grad_steps = 606, loss = 0.19732408225536346
In grad_steps = 607, loss = 0.19233861565589905
In grad_steps = 608, loss = 0.46829524636268616
In grad_steps = 609, loss = 0.15866436064243317
In grad_steps = 610, loss = 0.188601553440094
In grad_steps = 611, loss = 0.26562732458114624
In grad_steps = 612, loss = 0.27676594257354736
In grad_steps = 613, loss = 0.17037495970726013
In grad_steps = 614, loss = 0.22192946076393127
In grad_steps = 615, loss = 0.8519439697265625
In grad_steps = 616, loss = 0.30095410346984863
In grad_steps = 617, loss = 0.23799842596054077
In grad_steps = 618, loss = 0.4118596017360687
In grad_steps = 619, loss = 0.14877693355083466
In grad_steps = 620, loss = 0.2749757170677185
In grad_steps = 621, loss = 0.223832905292511
In grad_steps = 622, loss = 0.08437374234199524
In grad_steps = 623, loss = 0.1304456740617752
In grad_steps = 624, loss = 0.06098967418074608
In grad_steps = 625, loss = 0.09526780247688293
In grad_steps = 626, loss = 0.1660553514957428
In grad_steps = 627, loss = 0.297576904296875
In grad_steps = 628, loss = 0.2830283045768738
In grad_steps = 629, loss = 0.6171448826789856
In grad_steps = 630, loss = 0.36533403396606445
In grad_steps = 631, loss = 0.6294742822647095
In grad_steps = 632, loss = 0.4151785671710968
In grad_steps = 633, loss = 0.5563002824783325
In grad_steps = 634, loss = 0.2792491316795349
In grad_steps = 635, loss = 1.102247953414917
In grad_steps = 636, loss = 0.31732508540153503
In grad_steps = 637, loss = 0.4072636365890503
In grad_steps = 638, loss = 0.15146838128566742
In grad_steps = 639, loss = 1.0305665731430054
In grad_steps = 640, loss = 1.2364606857299805
In grad_steps = 641, loss = 0.5870805978775024
In grad_steps = 642, loss = 0.17118529975414276
In grad_steps = 643, loss = 0.09287875890731812
In grad_steps = 644, loss = 0.646130383014679
In grad_steps = 645, loss = 0.6535858511924744
In grad_steps = 646, loss = 0.638768196105957
In grad_steps = 647, loss = 0.52283775806427
In grad_steps = 648, loss = 0.16621723771095276
In grad_steps = 649, loss = 0.37617170810699463
In grad_steps = 650, loss = 0.535338819026947
In grad_steps = 651, loss = 0.5894755721092224
In grad_steps = 652, loss = 0.3792535662651062
In grad_steps = 653, loss = 0.35605311393737793
In grad_steps = 654, loss = 0.6505560874938965
In grad_steps = 655, loss = 0.534690260887146
In grad_steps = 656, loss = 0.41541817784309387
In grad_steps = 657, loss = 0.7189939618110657
In grad_steps = 658, loss = 0.3036152720451355
In grad_steps = 659, loss = 0.5583541393280029
In grad_steps = 660, loss = 0.8500203490257263
In grad_steps = 661, loss = 0.8876253366470337
In grad_steps = 662, loss = 0.5494154691696167
In grad_steps = 663, loss = 0.24581769108772278
In grad_steps = 664, loss = 0.8870917558670044
In grad_steps = 665, loss = 0.6487212777137756
In grad_steps = 666, loss = 0.17685584723949432
In grad_steps = 667, loss = 0.4858025312423706
In grad_steps = 668, loss = 0.6018728017807007
In grad_steps = 669, loss = 0.3171412944793701
In grad_steps = 670, loss = 0.3629981279373169
In grad_steps = 671, loss = 0.612423300743103
In grad_steps = 672, loss = 0.511559247970581
In grad_steps = 673, loss = 0.7117074131965637
In grad_steps = 674, loss = 0.21404191851615906
In grad_steps = 675, loss = 0.9889373779296875
In grad_steps = 676, loss = 0.6486824750900269
In grad_steps = 677, loss = 0.2659294605255127
In grad_steps = 678, loss = 0.24809373915195465
In grad_steps = 679, loss = 0.29499417543411255
In grad_steps = 680, loss = 0.36589211225509644
In grad_steps = 681, loss = 0.2903900742530823
In grad_steps = 682, loss = 0.8742110133171082
In grad_steps = 683, loss = 0.22229242324829102
In grad_steps = 684, loss = 0.4406111240386963
In grad_steps = 685, loss = 0.20940330624580383
In grad_steps = 686, loss = 0.23881469666957855
In grad_steps = 687, loss = 0.3871154189109802
In grad_steps = 688, loss = 0.16727547347545624
In grad_steps = 689, loss = 0.3994191884994507
In grad_steps = 690, loss = 0.1370886117219925
In grad_steps = 691, loss = 0.8513931035995483
In grad_steps = 692, loss = 0.061281438916921616
In grad_steps = 693, loss = 0.9506536722183228
In grad_steps = 694, loss = 0.4946728050708771
In grad_steps = 695, loss = 0.12829717993736267
In grad_steps = 696, loss = 0.17279823124408722
In grad_steps = 697, loss = 0.20944735407829285
In grad_steps = 698, loss = 0.1946839690208435
In grad_steps = 699, loss = 0.24222838878631592
In grad_steps = 700, loss = 0.16081811487674713
In grad_steps = 701, loss = 0.9828923940658569
In grad_steps = 702, loss = 0.04238858073949814
In grad_steps = 703, loss = 0.1152421236038208
In grad_steps = 704, loss = 0.0913480669260025
In grad_steps = 705, loss = 1.5735841989517212
In grad_steps = 706, loss = 0.24295809864997864
In grad_steps = 707, loss = 0.2256111055612564
In grad_steps = 708, loss = 0.09144462645053864
In grad_steps = 709, loss = 0.5600510835647583
In grad_steps = 710, loss = 0.1256786584854126
In grad_steps = 711, loss = 0.3152098059654236
In grad_steps = 712, loss = 0.07079196721315384
In grad_steps = 713, loss = 2.98675537109375
In grad_steps = 714, loss = 0.06855174899101257
In grad_steps = 715, loss = 0.7317193150520325
In grad_steps = 716, loss = 0.3852631151676178
In grad_steps = 717, loss = 0.3956766426563263
In grad_steps = 718, loss = 0.07773435860872269
In grad_steps = 719, loss = 0.32377445697784424
In grad_steps = 720, loss = 1.9583258628845215
In grad_steps = 721, loss = 0.5095818638801575
In grad_steps = 722, loss = 0.2686370015144348
In grad_steps = 723, loss = 0.7662837505340576
In grad_steps = 724, loss = 0.13425131142139435
In grad_steps = 725, loss = 0.23161792755126953
In grad_steps = 726, loss = 0.3141689598560333
In grad_steps = 727, loss = 0.3730437159538269
In grad_steps = 728, loss = 0.3350088596343994
In grad_steps = 729, loss = 0.24467377364635468
In grad_steps = 730, loss = 0.20147612690925598
In grad_steps = 731, loss = 0.26539286971092224
In grad_steps = 732, loss = 0.2069103866815567
In grad_steps = 733, loss = 0.5281537771224976
In grad_steps = 734, loss = 0.40023118257522583
In grad_steps = 735, loss = 0.4670044779777527
In grad_steps = 736, loss = 1.6793416738510132
In grad_steps = 737, loss = 0.37774381041526794
In grad_steps = 738, loss = 0.47283920645713806
In grad_steps = 739, loss = 0.18147847056388855
In grad_steps = 740, loss = 0.3489053547382355
In grad_steps = 741, loss = 0.4027813673019409
In grad_steps = 742, loss = 0.4157487452030182
In grad_steps = 743, loss = 0.23572862148284912
In grad_steps = 744, loss = 0.3128117322921753
In grad_steps = 745, loss = 0.027256764471530914
In grad_steps = 746, loss = 0.47720974683761597
In grad_steps = 747, loss = 0.23584555089473724
In grad_steps = 748, loss = 0.20218154788017273
In grad_steps = 749, loss = 0.33036574721336365
In grad_steps = 750, loss = 1.2360098361968994
In grad_steps = 751, loss = 0.6792848110198975
In grad_steps = 752, loss = 0.35178565979003906
In grad_steps = 753, loss = 0.10819890350103378
In grad_steps = 754, loss = 0.47190743684768677
In grad_steps = 755, loss = 0.11706460267305374
In grad_steps = 756, loss = 0.07703091949224472
In grad_steps = 757, loss = 0.08490538597106934
In grad_steps = 758, loss = 0.2825133800506592
In grad_steps = 759, loss = 0.650906503200531
In grad_steps = 760, loss = 0.15462112426757812
In grad_steps = 761, loss = 0.04226820170879364
In grad_steps = 762, loss = 0.2157370150089264
In grad_steps = 763, loss = 0.4461110532283783
In grad_steps = 764, loss = 0.4042029082775116
In grad_steps = 765, loss = 0.06036565452814102
In grad_steps = 766, loss = 0.14647707343101501
In grad_steps = 767, loss = 0.5011396408081055
In grad_steps = 768, loss = 0.02802126109600067
In grad_steps = 769, loss = 1.002573847770691
In grad_steps = 770, loss = 0.9901629090309143
In grad_steps = 771, loss = 0.7718775272369385
In grad_steps = 772, loss = 0.6544370651245117
In grad_steps = 773, loss = 0.18806801736354828
In grad_steps = 774, loss = 0.13782939314842224
In grad_steps = 775, loss = 0.07455489039421082
In grad_steps = 776, loss = 0.10832658410072327
In grad_steps = 777, loss = 0.7110618352890015
In grad_steps = 778, loss = 0.5647134780883789
In grad_steps = 779, loss = 0.6518646478652954
In grad_steps = 780, loss = 0.2393355518579483
In grad_steps = 781, loss = 0.49508073925971985
In grad_steps = 782, loss = 0.43380263447761536
In grad_steps = 783, loss = 0.5040168762207031
In grad_steps = 784, loss = 0.8823250532150269
In grad_steps = 785, loss = 0.7078404426574707
In grad_steps = 786, loss = 0.4281473159790039
In grad_steps = 787, loss = 0.4167487919330597
In grad_steps = 788, loss = 0.3108307421207428
In grad_steps = 789, loss = 0.3796047270298004
In grad_steps = 790, loss = 0.5962622165679932
In grad_steps = 791, loss = 0.28289881348609924
In grad_steps = 792, loss = 0.3233112692832947
In grad_steps = 793, loss = 0.22127166390419006
In grad_steps = 794, loss = 0.22037802636623383
In grad_steps = 795, loss = 0.2598217725753784
In grad_steps = 796, loss = 0.7538316249847412
In grad_steps = 797, loss = 0.10574624687433243
In grad_steps = 798, loss = 0.21803782880306244
In grad_steps = 799, loss = 0.26208165287971497
In grad_steps = 800, loss = 0.1331641674041748
In grad_steps = 801, loss = 0.09910833835601807
In grad_steps = 802, loss = 0.06880858540534973
In grad_steps = 803, loss = 0.25342661142349243
In grad_steps = 804, loss = 0.0227358415722847
In grad_steps = 805, loss = 0.11241322010755539
In grad_steps = 806, loss = 1.034498929977417
In grad_steps = 807, loss = 0.3951007127761841
In grad_steps = 808, loss = 0.23215819895267487
In grad_steps = 809, loss = 0.5771329998970032
In grad_steps = 810, loss = 0.40414679050445557
In grad_steps = 811, loss = 0.019015934318304062
In grad_steps = 812, loss = 0.7716673016548157
In grad_steps = 813, loss = 0.05532585456967354
In grad_steps = 814, loss = 0.05113239586353302
In grad_steps = 815, loss = 0.03714824467897415
In grad_steps = 816, loss = 0.12026510387659073
In grad_steps = 817, loss = 1.1602102518081665
In grad_steps = 818, loss = 1.2310389280319214
In grad_steps = 819, loss = 0.9346588850021362
In grad_steps = 820, loss = 0.1296004205942154
In grad_steps = 821, loss = 0.4962238073348999
In grad_steps = 822, loss = 0.7091556787490845
In grad_steps = 823, loss = 0.5378017425537109
In grad_steps = 824, loss = 0.275871604681015
In grad_steps = 825, loss = 0.18401558697223663
In grad_steps = 826, loss = 0.2600672245025635
In grad_steps = 827, loss = 0.46914011240005493
In grad_steps = 828, loss = 0.40406352281570435
In grad_steps = 829, loss = 0.4320695698261261
In grad_steps = 830, loss = 0.49260348081588745
In grad_steps = 831, loss = 0.11597871780395508
In grad_steps = 832, loss = 0.1896669864654541
In grad_steps = 833, loss = 0.6197766065597534
In grad_steps = 834, loss = 0.26237767934799194
In grad_steps = 835, loss = 0.19968026876449585
In grad_steps = 836, loss = 0.3979252874851227
In grad_steps = 837, loss = 0.4020063877105713
In grad_steps = 838, loss = 1.5113847255706787
In grad_steps = 839, loss = 0.26764875650405884
In grad_steps = 840, loss = 0.10004140436649323
In grad_steps = 841, loss = 0.05725394934415817
In grad_steps = 842, loss = 0.2635643780231476
In grad_steps = 843, loss = 0.9907802939414978
In grad_steps = 844, loss = 0.6088884472846985
In grad_steps = 845, loss = 0.4259008467197418
In grad_steps = 846, loss = 0.7186057567596436
In grad_steps = 847, loss = 0.4519437253475189
In grad_steps = 848, loss = 0.1310388594865799
In grad_steps = 849, loss = 0.24865467846393585
In grad_steps = 850, loss = 0.32165032625198364
In grad_steps = 851, loss = 0.14059793949127197
In grad_steps = 852, loss = 0.4939066767692566
In grad_steps = 853, loss = 0.8937016129493713
In grad_steps = 854, loss = 0.14595292508602142
In grad_steps = 855, loss = 0.4528855085372925
In grad_steps = 856, loss = 0.12548446655273438
In grad_steps = 857, loss = 0.2834838032722473
In grad_steps = 858, loss = 0.5402677655220032
In grad_steps = 859, loss = 0.2321978062391281
In grad_steps = 860, loss = 0.28101712465286255
In grad_steps = 861, loss = 0.7126431465148926
In grad_steps = 862, loss = 0.07180003821849823
In grad_steps = 863, loss = 0.31138932704925537
In grad_steps = 864, loss = 0.6936215162277222
In grad_steps = 865, loss = 0.16565658152103424
In grad_steps = 866, loss = 0.39899370074272156
In grad_steps = 867, loss = 0.12490875273942947
In grad_steps = 868, loss = 0.12004613876342773
In grad_steps = 869, loss = 0.1459718495607376
In grad_steps = 870, loss = 0.6454333662986755
In grad_steps = 871, loss = 0.05523836985230446
In grad_steps = 872, loss = 0.48262447118759155
In grad_steps = 873, loss = 0.361644446849823
In grad_steps = 874, loss = 0.5404846668243408
In grad_steps = 875, loss = 0.3730439245700836
In grad_steps = 876, loss = 0.16554003953933716
In grad_steps = 877, loss = 0.11424876004457474
In grad_steps = 878, loss = 0.9270462989807129
In grad_steps = 879, loss = 0.2747015655040741
In grad_steps = 880, loss = 0.17662329971790314
In grad_steps = 881, loss = 0.7068473100662231
In grad_steps = 882, loss = 0.15572436153888702
In grad_steps = 883, loss = 0.60115647315979
In grad_steps = 884, loss = 0.5390688180923462
In grad_steps = 885, loss = 0.1187049150466919
In grad_steps = 886, loss = 0.27532899379730225
In grad_steps = 887, loss = 0.49315014481544495
In grad_steps = 888, loss = 1.202372670173645
In grad_steps = 889, loss = 0.32158204913139343
In grad_steps = 890, loss = 0.11787042021751404
In grad_steps = 891, loss = 0.7633957862854004
In grad_steps = 892, loss = 0.5121685862541199
In grad_steps = 893, loss = 0.4927341938018799
In grad_steps = 894, loss = 0.09739872068166733
In grad_steps = 895, loss = 0.35646507143974304
In grad_steps = 896, loss = 0.6392510533332825
In grad_steps = 897, loss = 0.23515446484088898
In grad_steps = 898, loss = 0.7459268569946289
In grad_steps = 899, loss = 0.5773923397064209
In grad_steps = 900, loss = 0.5779443383216858
In grad_steps = 901, loss = 0.5409233570098877
In grad_steps = 902, loss = 0.1682179570198059
In grad_steps = 903, loss = 0.6440064907073975
In grad_steps = 904, loss = 0.45994314551353455
In grad_steps = 905, loss = 0.13044272363185883
In grad_steps = 906, loss = 0.6186748743057251
In grad_steps = 907, loss = 0.5042121410369873
In grad_steps = 908, loss = 0.23068997263908386
In grad_steps = 909, loss = 0.6539376378059387
In grad_steps = 910, loss = 0.23662069439888
In grad_steps = 911, loss = 0.42462271451950073
In grad_steps = 912, loss = 0.2144482284784317
In grad_steps = 913, loss = 0.3988494873046875
In grad_steps = 914, loss = 0.5170945525169373
In grad_steps = 915, loss = 0.8078937530517578
In grad_steps = 916, loss = 0.2253076285123825
In grad_steps = 917, loss = 0.591694712638855
In grad_steps = 918, loss = 0.41512981057167053
In grad_steps = 919, loss = 0.18870602548122406
In grad_steps = 920, loss = 0.3472617566585541
In grad_steps = 921, loss = 0.929565966129303
In grad_steps = 922, loss = 0.43811240792274475
In grad_steps = 923, loss = 0.19155016541481018
In grad_steps = 924, loss = 0.17265820503234863
In grad_steps = 925, loss = 0.1832401156425476
In grad_steps = 926, loss = 0.5705727338790894
In grad_steps = 927, loss = 0.09069302678108215
In grad_steps = 928, loss = 0.5841694474220276
In grad_steps = 929, loss = 0.12372884899377823
In grad_steps = 930, loss = 0.6015209555625916
In grad_steps = 931, loss = 0.10024987161159515
In grad_steps = 932, loss = 0.06139532849192619
In grad_steps = 933, loss = 0.6009647250175476
In grad_steps = 934, loss = 0.17000322043895721
In grad_steps = 935, loss = 0.8874223232269287
In grad_steps = 936, loss = 0.4055398106575012
In grad_steps = 937, loss = 0.09322461485862732
In grad_steps = 938, loss = 0.3701324164867401
In grad_steps = 939, loss = 0.20737484097480774
In grad_steps = 940, loss = 0.15732863545417786
In grad_steps = 941, loss = 0.20076721906661987
In grad_steps = 942, loss = 0.28572168946266174
In grad_steps = 943, loss = 1.396421194076538
In grad_steps = 944, loss = 0.622602641582489
In grad_steps = 945, loss = 0.22526195645332336
In grad_steps = 946, loss = 0.8112479448318481
In grad_steps = 947, loss = 0.33296167850494385
In grad_steps = 948, loss = 0.4666314423084259
In grad_steps = 949, loss = 0.5387886762619019
In grad_steps = 950, loss = 0.6079204082489014
In grad_steps = 951, loss = 1.0147838592529297
In grad_steps = 952, loss = 0.09044571220874786
In grad_steps = 953, loss = 0.3885272145271301
In grad_steps = 954, loss = 0.19147014617919922
In grad_steps = 955, loss = 0.2731199860572815
In grad_steps = 956, loss = 0.38822007179260254
In grad_steps = 957, loss = 0.44506245851516724
In grad_steps = 958, loss = 0.48147886991500854
In grad_steps = 959, loss = 1.1232675313949585
In grad_steps = 960, loss = 0.17724059522151947
In grad_steps = 961, loss = 0.4136684238910675
In grad_steps = 962, loss = 0.16262324154376984
In grad_steps = 963, loss = 0.25246840715408325
In grad_steps = 964, loss = 0.2889399230480194
In grad_steps = 965, loss = 0.23819370567798615
In grad_steps = 966, loss = 0.3712848722934723
In grad_steps = 967, loss = 0.8206457495689392
In grad_steps = 968, loss = 0.6410269141197205
In grad_steps = 969, loss = 0.5552799105644226
In grad_steps = 970, loss = 0.14774082601070404
In grad_steps = 971, loss = 0.3980325758457184
In grad_steps = 972, loss = 0.6239420175552368
In grad_steps = 973, loss = 0.1952355057001114
In grad_steps = 974, loss = 0.4082614779472351
In grad_steps = 975, loss = 0.20867560803890228
In grad_steps = 976, loss = 0.4042506814002991
In grad_steps = 977, loss = 0.3834148943424225
In grad_steps = 978, loss = 0.9071630835533142
In grad_steps = 979, loss = 0.7050307393074036
In grad_steps = 980, loss = 0.7447972297668457
In grad_steps = 981, loss = 0.42909348011016846
In grad_steps = 982, loss = 0.6215516328811646
In grad_steps = 983, loss = 0.48024454712867737
In grad_steps = 984, loss = 0.6339948773384094
In grad_steps = 985, loss = 0.35882461071014404
In grad_steps = 986, loss = 0.1672477275133133
In grad_steps = 987, loss = 0.7479890584945679
In grad_steps = 988, loss = 0.5788366198539734
In grad_steps = 989, loss = 0.7241555452346802
In grad_steps = 990, loss = 0.7257018685340881
In grad_steps = 991, loss = 0.7084499001502991
In grad_steps = 992, loss = 0.22978320717811584
In grad_steps = 993, loss = 0.6089870929718018
In grad_steps = 994, loss = 0.5338810086250305
In grad_steps = 995, loss = 0.25436878204345703
In grad_steps = 996, loss = 0.5680466890335083
In grad_steps = 997, loss = 0.2033061534166336
In grad_steps = 998, loss = 0.4467868506908417
In grad_steps = 999, loss = 0.07853351533412933
In grad_steps = 1000, loss = 0.11925292760133743
In grad_steps = 1001, loss = 0.5771875977516174
In grad_steps = 1002, loss = 0.28166866302490234
In grad_steps = 1003, loss = 0.2544389069080353
In grad_steps = 1004, loss = 0.3928603529930115
In grad_steps = 1005, loss = 0.8188108205795288
In grad_steps = 1006, loss = 0.2221374362707138
In grad_steps = 1007, loss = 0.3824237585067749
In grad_steps = 1008, loss = 0.3038731813430786
In grad_steps = 1009, loss = 0.9931650161743164
In grad_steps = 1010, loss = 0.33341270685195923
In grad_steps = 1011, loss = 0.1074676588177681
In grad_steps = 1012, loss = 0.17180995643138885
In grad_steps = 1013, loss = 0.550581157207489
In grad_steps = 1014, loss = 0.2943466901779175
In grad_steps = 1015, loss = 0.4479007124900818
In grad_steps = 1016, loss = 0.07645625621080399
In grad_steps = 1017, loss = 0.09838324040174484
In grad_steps = 1018, loss = 0.3040043115615845
In grad_steps = 1019, loss = 0.7445236444473267
In grad_steps = 1020, loss = 1.6954197883605957
In grad_steps = 1021, loss = 0.3754000663757324
In grad_steps = 1022, loss = 0.2265186607837677
In grad_steps = 1023, loss = 0.10954344272613525
In grad_steps = 1024, loss = 0.15121415257453918
In grad_steps = 1025, loss = 0.790262758731842
In grad_steps = 1026, loss = 0.20566217601299286
In grad_steps = 1027, loss = 0.7837069034576416
In grad_steps = 1028, loss = 0.14037789404392242
In grad_steps = 1029, loss = 0.19363227486610413
In grad_steps = 1030, loss = 0.16783764958381653
In grad_steps = 1031, loss = 0.4908950924873352
In grad_steps = 1032, loss = 0.5014176964759827
In grad_steps = 1033, loss = 0.315096914768219
In grad_steps = 1034, loss = 0.18835587799549103
In grad_steps = 1035, loss = 0.6172188520431519
In grad_steps = 1036, loss = 0.7222503423690796
In grad_steps = 1037, loss = 0.21270906925201416
In grad_steps = 1038, loss = 0.4217071533203125
In grad_steps = 1039, loss = 0.6591973304748535
In grad_steps = 1040, loss = 0.30735665559768677
In grad_steps = 1041, loss = 0.6846931576728821
In grad_steps = 1042, loss = 0.5624204277992249
In grad_steps = 1043, loss = 0.7209519743919373
In grad_steps = 1044, loss = 0.47271907329559326
In grad_steps = 1045, loss = 0.5788447856903076
In grad_steps = 1046, loss = 0.08934278786182404
In grad_steps = 1047, loss = 0.14554059505462646
In grad_steps = 1048, loss = 0.3935517370700836
In grad_steps = 1049, loss = 0.22747834026813507
In grad_steps = 1050, loss = 1.356644630432129
In grad_steps = 1051, loss = 0.11967785656452179
In grad_steps = 1052, loss = 0.16814231872558594
In grad_steps = 1053, loss = 0.7193268537521362
In grad_steps = 1054, loss = 0.17609430849552155
In grad_steps = 1055, loss = 0.4231565594673157
In grad_steps = 1056, loss = 0.9922760725021362
In grad_steps = 1057, loss = 0.577207088470459
In grad_steps = 1058, loss = 0.31502479314804077
In grad_steps = 1059, loss = 0.2593350112438202
In grad_steps = 1060, loss = 0.4171113967895508
In grad_steps = 1061, loss = 0.2965511679649353
In grad_steps = 1062, loss = 0.7901027798652649
In grad_steps = 1063, loss = 0.24597527086734772
In grad_steps = 1064, loss = 0.3550099730491638
In grad_steps = 1065, loss = 0.07629567384719849
In grad_steps = 1066, loss = 0.5329211950302124
In grad_steps = 1067, loss = 0.5555356740951538
In grad_steps = 1068, loss = 0.1538224220275879
In grad_steps = 1069, loss = 0.2670309841632843
In grad_steps = 1070, loss = 0.3234383165836334
In grad_steps = 1071, loss = 0.142872154712677
In grad_steps = 1072, loss = 0.1724100410938263
In grad_steps = 1073, loss = 0.3424953818321228
In grad_steps = 1074, loss = 0.15746812522411346
In grad_steps = 1075, loss = 0.43516916036605835
In grad_steps = 1076, loss = 0.22431589663028717
In grad_steps = 1077, loss = 0.18870016932487488
In grad_steps = 1078, loss = 0.6786031723022461
In grad_steps = 1079, loss = 0.10636347532272339
In grad_steps = 1080, loss = 0.24141575396060944
In grad_steps = 1081, loss = 0.7319878935813904
In grad_steps = 1082, loss = 0.1084836795926094
In grad_steps = 1083, loss = 0.0370589941740036
In grad_steps = 1084, loss = 0.5749614834785461
In grad_steps = 1085, loss = 1.3703303337097168
In grad_steps = 1086, loss = 0.0779002457857132
In grad_steps = 1087, loss = 0.8053237199783325
In grad_steps = 1088, loss = 0.35414984822273254
In grad_steps = 1089, loss = 0.13170397281646729
In grad_steps = 1090, loss = 0.06617657095193863
In grad_steps = 1091, loss = 0.3805656135082245
In grad_steps = 1092, loss = 0.04905688762664795
In grad_steps = 1093, loss = 0.7967749834060669
In grad_steps = 1094, loss = 0.06266144663095474
In grad_steps = 1095, loss = 0.38631758093833923
In grad_steps = 1096, loss = 0.09821061789989471
In grad_steps = 1097, loss = 0.11497314274311066
In grad_steps = 1098, loss = 0.30739825963974
In grad_steps = 1099, loss = 0.48281770944595337
In grad_steps = 1100, loss = 0.3308321535587311
In grad_steps = 1101, loss = 0.446577787399292
In grad_steps = 1102, loss = 0.7642998695373535
In grad_steps = 1103, loss = 0.0406951904296875
In grad_steps = 1104, loss = 1.1835460662841797
In grad_steps = 1105, loss = 0.3812256157398224
In grad_steps = 1106, loss = 0.23325075209140778
In grad_steps = 1107, loss = 0.204068124294281
In grad_steps = 1108, loss = 0.6034746766090393
In grad_steps = 1109, loss = 0.29807591438293457
In grad_steps = 1110, loss = 0.46509236097335815
In grad_steps = 1111, loss = 0.3467271029949188
In grad_steps = 1112, loss = 0.6874271035194397
In grad_steps = 1113, loss = 0.41315746307373047
In grad_steps = 1114, loss = 0.2703009247779846
In grad_steps = 1115, loss = 0.11671071499586105
In grad_steps = 1116, loss = 0.8319739103317261
In grad_steps = 1117, loss = 0.2763133943080902
In grad_steps = 1118, loss = 0.24213923513889313
In grad_steps = 1119, loss = 0.581880509853363
In grad_steps = 1120, loss = 0.09105830639600754
In grad_steps = 1121, loss = 0.1388782560825348
In grad_steps = 1122, loss = 0.3059918284416199
In grad_steps = 1123, loss = 0.4825744926929474
In grad_steps = 1124, loss = 0.38855332136154175
In grad_steps = 1125, loss = 0.19321796298027039
In grad_steps = 1126, loss = 0.17228004336357117
In grad_steps = 1127, loss = 0.16310110688209534
In grad_steps = 1128, loss = 0.6029872894287109
In grad_steps = 1129, loss = 0.22206631302833557
In grad_steps = 1130, loss = 0.09461702406406403
In grad_steps = 1131, loss = 0.16658750176429749
In grad_steps = 1132, loss = 0.24188855290412903
In grad_steps = 1133, loss = 0.6905972957611084
In grad_steps = 1134, loss = 0.259351909160614
In grad_steps = 1135, loss = 0.388744592666626
In grad_steps = 1136, loss = 0.8620082139968872
In grad_steps = 1137, loss = 0.3792583644390106
In grad_steps = 1138, loss = 0.3163295388221741
In grad_steps = 1139, loss = 0.699749231338501
In grad_steps = 1140, loss = 0.2195369154214859
In grad_steps = 1141, loss = 1.1901973485946655
In grad_steps = 1142, loss = 0.2606704533100128
In grad_steps = 1143, loss = 0.5937479138374329
In grad_steps = 1144, loss = 0.024630649015307426
In grad_steps = 1145, loss = 0.459414541721344
In grad_steps = 1146, loss = 1.5195649862289429
In grad_steps = 1147, loss = 0.5057177543640137
In grad_steps = 1148, loss = 0.25162309408187866
In grad_steps = 1149, loss = 0.08428779989480972
In grad_steps = 1150, loss = 0.5593268275260925
In grad_steps = 1151, loss = 0.4055333435535431
In grad_steps = 1152, loss = 0.5080410242080688
In grad_steps = 1153, loss = 0.6614812016487122
In grad_steps = 1154, loss = 0.46060535311698914
In grad_steps = 1155, loss = 0.3419881761074066
In grad_steps = 1156, loss = 0.39464476704597473
In grad_steps = 1157, loss = 0.23377391695976257
In grad_steps = 1158, loss = 0.3880310654640198
In grad_steps = 1159, loss = 0.16690540313720703
In grad_steps = 1160, loss = 0.31107041239738464
In grad_steps = 1161, loss = 0.7176593542098999
In grad_steps = 1162, loss = 0.31104761362075806
In grad_steps = 1163, loss = 0.29436036944389343
In grad_steps = 1164, loss = 0.19502940773963928
In grad_steps = 1165, loss = 0.27453485131263733
In grad_steps = 1166, loss = 1.4927563667297363
In grad_steps = 1167, loss = 0.7563230991363525
In grad_steps = 1168, loss = 0.30130136013031006
In grad_steps = 1169, loss = 0.08199915289878845
In grad_steps = 1170, loss = 0.30620619654655457
In grad_steps = 1171, loss = 0.3300390839576721
In grad_steps = 1172, loss = 0.44231539964675903
In grad_steps = 1173, loss = 0.3771823048591614
In grad_steps = 1174, loss = 0.2555829882621765
In grad_steps = 1175, loss = 0.6258445978164673
In grad_steps = 1176, loss = 0.5283949375152588
In grad_steps = 1177, loss = 0.08217238634824753
In grad_steps = 1178, loss = 0.6734839677810669
In grad_steps = 1179, loss = 0.07902771234512329
In grad_steps = 1180, loss = 0.5211504101753235
In grad_steps = 1181, loss = 0.45596179366111755
In grad_steps = 1182, loss = 1.114855408668518
In grad_steps = 1183, loss = 0.5223764777183533
In grad_steps = 1184, loss = 0.828360378742218
In grad_steps = 1185, loss = 0.37189310789108276
In grad_steps = 1186, loss = 0.228306844830513
In grad_steps = 1187, loss = 0.2252412736415863
In grad_steps = 1188, loss = 0.7151180505752563
In grad_steps = 1189, loss = 0.20290550589561462
In grad_steps = 1190, loss = 0.17713865637779236
In grad_steps = 1191, loss = 0.19856137037277222
In grad_steps = 1192, loss = 0.20620271563529968
In grad_steps = 1193, loss = 0.1416395902633667
In grad_steps = 1194, loss = 0.23655986785888672
In grad_steps = 1195, loss = 0.6470223069190979
In grad_steps = 1196, loss = 0.13444477319717407
In grad_steps = 1197, loss = 0.6723877191543579
In grad_steps = 1198, loss = 0.17776459455490112
In grad_steps = 1199, loss = 0.9335590600967407
In grad_steps = 1200, loss = 0.15153434872627258
In grad_steps = 1201, loss = 0.08372668921947479
In grad_steps = 1202, loss = 0.5790285468101501
In grad_steps = 1203, loss = 0.06527769565582275
In grad_steps = 1204, loss = 0.10357848554849625
In grad_steps = 1205, loss = 1.172167181968689
In grad_steps = 1206, loss = 1.1543291807174683
In grad_steps = 1207, loss = 0.1419275403022766
In grad_steps = 1208, loss = 0.17913801968097687
In grad_steps = 1209, loss = 0.6002089381217957
In grad_steps = 1210, loss = 0.42438948154449463
In grad_steps = 1211, loss = 0.4720938503742218
In grad_steps = 1212, loss = 0.1083531528711319
In grad_steps = 1213, loss = 0.6783530116081238
In grad_steps = 1214, loss = 0.3552384376525879
In grad_steps = 1215, loss = 0.41650068759918213
In grad_steps = 1216, loss = 0.1640402227640152
In grad_steps = 1217, loss = 0.05343889072537422
In grad_steps = 1218, loss = 0.681971549987793
In grad_steps = 1219, loss = 0.1491222381591797
In grad_steps = 1220, loss = 0.5270246267318726
In grad_steps = 1221, loss = 0.5024333000183105
In grad_steps = 1222, loss = 0.46877819299697876
In grad_steps = 1223, loss = 0.4453052878379822
In grad_steps = 1224, loss = 1.1615709066390991
In grad_steps = 1225, loss = 0.18981216847896576
In grad_steps = 1226, loss = 0.3428892493247986
In grad_steps = 1227, loss = 0.2924617826938629
In grad_steps = 1228, loss = 0.1626053750514984
In grad_steps = 1229, loss = 1.0172290802001953
In grad_steps = 1230, loss = 0.31358587741851807
In grad_steps = 1231, loss = 0.09707874059677124
In grad_steps = 1232, loss = 0.05526931211352348
In grad_steps = 1233, loss = 0.4663066267967224
In grad_steps = 1234, loss = 0.30706092715263367
In grad_steps = 1235, loss = 0.31860795617103577
In grad_steps = 1236, loss = 0.6785308122634888
In grad_steps = 1237, loss = 0.4417894184589386
In grad_steps = 1238, loss = 0.1985875368118286
In grad_steps = 1239, loss = 0.37345054745674133
In grad_steps = 1240, loss = 0.2211637943983078
In grad_steps = 1241, loss = 0.08664774894714355
In grad_steps = 1242, loss = 0.40481796860694885
In grad_steps = 1243, loss = 0.2549102306365967
In grad_steps = 1244, loss = 0.689800500869751
In grad_steps = 1245, loss = 0.7033140063285828
In grad_steps = 1246, loss = 0.1352234184741974
In grad_steps = 1247, loss = 0.3139519989490509
In grad_steps = 1248, loss = 0.6368942260742188
In grad_steps = 1249, loss = 0.4065120816230774
In grad_steps = 1250, loss = 0.09690612554550171
In grad_steps = 1251, loss = 0.26070091128349304
In grad_steps = 1252, loss = 0.06681706756353378
In grad_steps = 1253, loss = 0.13731390237808228
In grad_steps = 1254, loss = 0.2812183201313019
In grad_steps = 1255, loss = 0.7110003232955933
In grad_steps = 1256, loss = 0.5223962664604187
In grad_steps = 1257, loss = 0.33373749256134033
In grad_steps = 1258, loss = 0.6092137694358826
In grad_steps = 1259, loss = 0.12799276411533356
In grad_steps = 1260, loss = 0.2033431977033615
In grad_steps = 1261, loss = 1.3768291473388672
In grad_steps = 1262, loss = 0.41057679057121277
In grad_steps = 1263, loss = 0.05680616572499275
In grad_steps = 1264, loss = 0.26666581630706787
In grad_steps = 1265, loss = 0.2966896891593933
In grad_steps = 1266, loss = 0.7248963713645935
In grad_steps = 1267, loss = 0.7787725925445557
In grad_steps = 1268, loss = 0.6136553883552551
In grad_steps = 1269, loss = 0.09521470218896866
In grad_steps = 1270, loss = 0.09936033189296722
In grad_steps = 1271, loss = 0.6105157136917114
In grad_steps = 1272, loss = 0.4504154622554779
In grad_steps = 1273, loss = 0.1755692958831787
In grad_steps = 1274, loss = 0.47322168946266174
In grad_steps = 1275, loss = 0.19832979142665863
In grad_steps = 1276, loss = 0.36300063133239746
In grad_steps = 1277, loss = 0.19627845287322998
In grad_steps = 1278, loss = 0.27877888083457947
In grad_steps = 1279, loss = 0.17217549681663513
In grad_steps = 1280, loss = 0.14511743187904358
In grad_steps = 1281, loss = 0.4367806017398834
In grad_steps = 1282, loss = 0.5479120016098022
In grad_steps = 1283, loss = 0.6153384447097778
In grad_steps = 1284, loss = 0.17058846354484558
In grad_steps = 1285, loss = 0.26445868611335754
In grad_steps = 1286, loss = 0.25689229369163513
In grad_steps = 1287, loss = 0.11368772387504578
In grad_steps = 1288, loss = 0.3374324440956116
In grad_steps = 1289, loss = 0.0865379348397255
In grad_steps = 1290, loss = 0.051111940294504166
In grad_steps = 1291, loss = 0.23545397818088531
In grad_steps = 1292, loss = 0.6998870968818665
In grad_steps = 1293, loss = 0.09589771181344986
In grad_steps = 1294, loss = 0.4820129871368408
In grad_steps = 1295, loss = 0.2438955456018448
In grad_steps = 1296, loss = 0.10227183252573013
In grad_steps = 1297, loss = 0.07692389190196991
In grad_steps = 1298, loss = 0.14145678281784058
In grad_steps = 1299, loss = 1.1626384258270264
In grad_steps = 1300, loss = 0.133311465382576
In grad_steps = 1301, loss = 0.043740589171648026
In grad_steps = 1302, loss = 0.029018770903348923
In grad_steps = 1303, loss = 0.5852692723274231
In grad_steps = 1304, loss = 0.02585894800722599
In grad_steps = 1305, loss = 0.806684672832489
In grad_steps = 1306, loss = 0.855419933795929
In grad_steps = 1307, loss = 0.33144131302833557
In grad_steps = 1308, loss = 0.9144576787948608
In grad_steps = 1309, loss = 0.35212674736976624
In grad_steps = 1310, loss = 0.14445443451404572
In grad_steps = 1311, loss = 0.1419447958469391
In grad_steps = 1312, loss = 0.10495439171791077
In grad_steps = 1313, loss = 0.093854159116745
In grad_steps = 1314, loss = 0.07387571036815643
In grad_steps = 1315, loss = 0.1741023063659668
In grad_steps = 1316, loss = 1.2588083744049072
In grad_steps = 1317, loss = 0.9573807716369629
In grad_steps = 1318, loss = 0.3138795793056488
In grad_steps = 1319, loss = 0.2030879110097885
In grad_steps = 1320, loss = 0.31143152713775635
In grad_steps = 1321, loss = 0.1461653709411621
In grad_steps = 1322, loss = 0.6483834385871887
In grad_steps = 1323, loss = 0.4154471457004547
In grad_steps = 1324, loss = 0.07013295590877533
In grad_steps = 1325, loss = 0.10769904404878616
In grad_steps = 1326, loss = 0.1570914387702942
In grad_steps = 1327, loss = 0.7628123164176941
In grad_steps = 1328, loss = 0.06808045506477356
In grad_steps = 1329, loss = 0.7487964034080505
In grad_steps = 1330, loss = 0.7125085592269897
In grad_steps = 1331, loss = 0.32358768582344055
In grad_steps = 1332, loss = 0.0909641906619072
In grad_steps = 1333, loss = 0.5953950881958008
In grad_steps = 1334, loss = 0.24929170310497284
In grad_steps = 1335, loss = 0.4267987012863159
In grad_steps = 1336, loss = 0.4097278118133545
In grad_steps = 1337, loss = 0.2942740321159363
In grad_steps = 1338, loss = 0.04352535307407379
In grad_steps = 1339, loss = 0.6551016569137573
In grad_steps = 1340, loss = 0.424660325050354
In grad_steps = 1341, loss = 0.2021418660879135
In grad_steps = 1342, loss = 0.26448503136634827
In grad_steps = 1343, loss = 0.542025625705719
In grad_steps = 1344, loss = 0.2182646095752716
In grad_steps = 1345, loss = 0.8301575183868408
In grad_steps = 1346, loss = 0.2983827590942383
In grad_steps = 1347, loss = 0.5591339468955994
In grad_steps = 1348, loss = 0.3450184464454651
In grad_steps = 1349, loss = 0.01198794785887003
In grad_steps = 1350, loss = 0.5624017119407654
In grad_steps = 1351, loss = 0.3750762343406677
In grad_steps = 1352, loss = 0.15925204753875732
In grad_steps = 1353, loss = 0.9855066537857056
In grad_steps = 1354, loss = 0.13590478897094727
In grad_steps = 1355, loss = 0.615607500076294
In grad_steps = 1356, loss = 0.1754741668701172
In grad_steps = 1357, loss = 0.384856253862381
In grad_steps = 1358, loss = 0.11759689450263977
In grad_steps = 1359, loss = 0.13758805394172668
In grad_steps = 1360, loss = 0.11528147757053375
In grad_steps = 1361, loss = 0.8965622186660767
In grad_steps = 1362, loss = 0.07147738337516785
In grad_steps = 1363, loss = 0.34913793206214905
In grad_steps = 1364, loss = 0.26181983947753906
In grad_steps = 1365, loss = 0.5322004556655884
In grad_steps = 1366, loss = 0.22545397281646729
In grad_steps = 1367, loss = 0.051951076835393906
In grad_steps = 1368, loss = 0.6628546714782715
In grad_steps = 1369, loss = 0.16007007658481598
In grad_steps = 1370, loss = 0.0864407941699028
In grad_steps = 1371, loss = 0.3766777217388153
In grad_steps = 1372, loss = 0.5914016962051392
In grad_steps = 1373, loss = 0.06026046350598335
In grad_steps = 1374, loss = 0.8490062952041626
In grad_steps = 1375, loss = 0.040572404861450195
In grad_steps = 1376, loss = 0.4494791626930237
In grad_steps = 1377, loss = 0.07372117042541504
In grad_steps = 1378, loss = 0.5822628140449524
In grad_steps = 1379, loss = 0.2203311324119568
In grad_steps = 1380, loss = 0.33210548758506775
In grad_steps = 1381, loss = 0.08003857731819153
In grad_steps = 1382, loss = 0.1544184386730194
In grad_steps = 1383, loss = 1.0514938831329346
In grad_steps = 1384, loss = 0.13997039198875427
In grad_steps = 1385, loss = 0.9524028301239014
In grad_steps = 1386, loss = 0.4013717472553253
In grad_steps = 1387, loss = 0.15832853317260742
In grad_steps = 1388, loss = 0.06315650045871735
In grad_steps = 1389, loss = 0.07788620889186859
In grad_steps = 1390, loss = 0.044484034180641174
In grad_steps = 1391, loss = 0.66239333152771
In grad_steps = 1392, loss = 0.14995209872722626
In grad_steps = 1393, loss = 0.1308412253856659
In grad_steps = 1394, loss = 0.09898383915424347
In grad_steps = 1395, loss = 0.8710352778434753
In grad_steps = 1396, loss = 0.6612408757209778
In grad_steps = 1397, loss = 0.05530508607625961
In grad_steps = 1398, loss = 0.6104399561882019
In grad_steps = 1399, loss = 0.07566767930984497
In grad_steps = 1400, loss = 0.6624870300292969
In grad_steps = 1401, loss = 0.2386922389268875
In grad_steps = 1402, loss = 0.20764553546905518
In grad_steps = 1403, loss = 0.5450006723403931
In grad_steps = 1404, loss = 0.7895514369010925
In grad_steps = 1405, loss = 0.10034321248531342
In grad_steps = 1406, loss = 0.4095352292060852
In grad_steps = 1407, loss = 0.20731264352798462
In grad_steps = 1408, loss = 0.4363553822040558
In grad_steps = 1409, loss = 0.260579913854599
In grad_steps = 1410, loss = 0.33243101835250854
In grad_steps = 1411, loss = 0.5090823173522949
In grad_steps = 1412, loss = 0.16064637899398804
In grad_steps = 1413, loss = 1.1274133920669556
In grad_steps = 1414, loss = 0.3678787648677826
In grad_steps = 1415, loss = 0.13535751402378082
In grad_steps = 1416, loss = 0.3798412084579468
In grad_steps = 1417, loss = 0.3234490752220154
In grad_steps = 1418, loss = 0.06350687891244888
In grad_steps = 1419, loss = 0.08607842028141022
In grad_steps = 1420, loss = 0.2722306549549103
In grad_steps = 1421, loss = 0.09672781825065613
In grad_steps = 1422, loss = 0.4860879182815552
In grad_steps = 1423, loss = 1.0387476682662964
In grad_steps = 1424, loss = 0.07700370252132416
In grad_steps = 1425, loss = 0.6342507600784302
In grad_steps = 1426, loss = 0.08318512886762619
In grad_steps = 1427, loss = 0.31152498722076416
In grad_steps = 1428, loss = 0.5608454942703247
In grad_steps = 1429, loss = 0.5008139610290527
In grad_steps = 1430, loss = 0.09576688706874847
In grad_steps = 1431, loss = 0.7298492193222046
In grad_steps = 1432, loss = 0.07307931780815125
In grad_steps = 1433, loss = 0.07649517059326172
In grad_steps = 1434, loss = 0.32421860098838806
In grad_steps = 1435, loss = 0.5046246647834778
In grad_steps = 1436, loss = 0.2961371839046478
In grad_steps = 1437, loss = 0.7924357056617737
In grad_steps = 1438, loss = 0.29204678535461426
In grad_steps = 1439, loss = 0.06717709451913834
In grad_steps = 1440, loss = 0.5435698628425598
In grad_steps = 1441, loss = 0.854577362537384
In grad_steps = 1442, loss = 0.3718876838684082
In grad_steps = 1443, loss = 1.219521403312683
In grad_steps = 1444, loss = 0.08361772447824478
In grad_steps = 1445, loss = 0.2778758406639099
In grad_steps = 1446, loss = 0.29109737277030945
In grad_steps = 1447, loss = 0.040336377918720245
In grad_steps = 1448, loss = 0.44447028636932373
In grad_steps = 1449, loss = 0.4160933494567871
In grad_steps = 1450, loss = 0.2870049476623535
In grad_steps = 1451, loss = 0.7048211693763733
In grad_steps = 1452, loss = 0.10018926858901978
In grad_steps = 1453, loss = 0.9372318983078003
In grad_steps = 1454, loss = 0.15577729046344757
In grad_steps = 1455, loss = 0.5782531499862671
In grad_steps = 1456, loss = 0.502389132976532
In grad_steps = 1457, loss = 0.3463025391101837
In grad_steps = 1458, loss = 0.28032732009887695
In grad_steps = 1459, loss = 0.26700854301452637
In grad_steps = 1460, loss = 0.10905042290687561
In grad_steps = 1461, loss = 0.5173078179359436
In grad_steps = 1462, loss = 0.14521469175815582
In grad_steps = 1463, loss = 0.8531103730201721
In grad_steps = 1464, loss = 0.5170153379440308
In grad_steps = 1465, loss = 0.35109516978263855
In grad_steps = 1466, loss = 0.1655789315700531
In grad_steps = 1467, loss = 0.40871864557266235
In grad_steps = 1468, loss = 0.11798515170812607
In grad_steps = 1469, loss = 0.3782806098461151
In grad_steps = 1470, loss = 0.3039146065711975
In grad_steps = 1471, loss = 0.15672749280929565
In grad_steps = 1472, loss = 0.3984980583190918
In grad_steps = 1473, loss = 0.09351861476898193
In grad_steps = 1474, loss = 0.9952024221420288
In grad_steps = 1475, loss = 0.41920968890190125
In grad_steps = 1476, loss = 0.39132797718048096
In grad_steps = 1477, loss = 0.29711395502090454
In grad_steps = 1478, loss = 0.4365765154361725
In grad_steps = 1479, loss = 0.27244818210601807
In grad_steps = 1480, loss = 0.16084551811218262
In grad_steps = 1481, loss = 0.23277948796749115
In grad_steps = 1482, loss = 0.12968741357326508
In grad_steps = 1483, loss = 0.6389023661613464
In grad_steps = 1484, loss = 0.344673752784729
In grad_steps = 1485, loss = 0.2850711941719055
In grad_steps = 1486, loss = 1.1170088052749634
In grad_steps = 1487, loss = 0.13430406153202057
In grad_steps = 1488, loss = 0.9297047853469849
In grad_steps = 1489, loss = 0.40958520770072937
In grad_steps = 1490, loss = 0.7837496995925903
In grad_steps = 1491, loss = 0.547831118106842
In grad_steps = 1492, loss = 0.07464195787906647
In grad_steps = 1493, loss = 0.4939286410808563
In grad_steps = 1494, loss = 0.4027306139469147
In grad_steps = 1495, loss = 0.13367122411727905
In grad_steps = 1496, loss = 0.08483969420194626
In grad_steps = 1497, loss = 0.36245739459991455
In grad_steps = 1498, loss = 0.4543045163154602
In grad_steps = 1499, loss = 0.7019530534744263
In grad_steps = 1500, loss = 0.22522664070129395
In grad_steps = 1501, loss = 0.33744704723358154
In grad_steps = 1502, loss = 0.6481813788414001
In grad_steps = 1503, loss = 0.32442808151245117
In grad_steps = 1504, loss = 0.2741893529891968
In grad_steps = 1505, loss = 0.5673065781593323
In grad_steps = 1506, loss = 0.24547922611236572
In grad_steps = 1507, loss = 0.666825532913208
In grad_steps = 1508, loss = 0.1947842240333557
In grad_steps = 1509, loss = 0.8324437141418457
In grad_steps = 1510, loss = 0.727623462677002
In grad_steps = 1511, loss = 0.7356312274932861
In grad_steps = 1512, loss = 1.111175537109375
In grad_steps = 1513, loss = 0.6325333118438721
In grad_steps = 1514, loss = 0.274438738822937
In grad_steps = 1515, loss = 0.32045871019363403
In grad_steps = 1516, loss = 0.12820500135421753
In grad_steps = 1517, loss = 0.08040190488100052
In grad_steps = 1518, loss = 0.07911163568496704
In grad_steps = 1519, loss = 0.3938988447189331
In grad_steps = 1520, loss = 0.5758952498435974
In grad_steps = 1521, loss = 1.2735893726348877
In grad_steps = 1522, loss = 0.15995152294635773
In grad_steps = 1523, loss = 1.0787298679351807
In grad_steps = 1524, loss = 0.6013274788856506
In grad_steps = 1525, loss = 0.3913950026035309
In grad_steps = 1526, loss = 0.46173492074012756
In grad_steps = 1527, loss = 0.5952491164207458
In grad_steps = 1528, loss = 0.7311121821403503
In grad_steps = 1529, loss = 0.08382654190063477
In grad_steps = 1530, loss = 0.6298983097076416
In grad_steps = 1531, loss = 0.5710026621818542
In grad_steps = 1532, loss = 0.5377343893051147
In grad_steps = 1533, loss = 0.27752685546875
In grad_steps = 1534, loss = 0.3157396614551544
In grad_steps = 1535, loss = 0.28497475385665894
In grad_steps = 1536, loss = 0.1503298580646515
In grad_steps = 1537, loss = 0.4482167363166809
In grad_steps = 1538, loss = 0.20626425743103027
In grad_steps = 1539, loss = 0.12389514595270157
In grad_steps = 1540, loss = 0.1791190356016159
In grad_steps = 1541, loss = 0.3895370066165924
In grad_steps = 1542, loss = 0.2247142493724823
In grad_steps = 1543, loss = 0.19352087378501892
In grad_steps = 1544, loss = 0.10087021440267563
In grad_steps = 1545, loss = 0.15949665009975433
In grad_steps = 1546, loss = 0.12275148928165436
In grad_steps = 1547, loss = 1.0836129188537598
In grad_steps = 1548, loss = 0.03635457903146744
In grad_steps = 1549, loss = 0.021819189190864563
In grad_steps = 1550, loss = 0.157915860414505
In grad_steps = 1551, loss = 0.8762101531028748
In grad_steps = 1552, loss = 0.11131573468446732
In grad_steps = 1553, loss = 0.057305753231048584
In grad_steps = 1554, loss = 0.3240429162979126
In grad_steps = 1555, loss = 0.19348642230033875
In grad_steps = 1556, loss = 1.097033977508545
In grad_steps = 1557, loss = 0.2987973093986511
In grad_steps = 1558, loss = 0.4221424162387848
In grad_steps = 1559, loss = 0.5765259861946106
In grad_steps = 1560, loss = 0.1958463490009308
In grad_steps = 1561, loss = 0.46540534496307373
In grad_steps = 1562, loss = 0.3198678493499756
In grad_steps = 1563, loss = 0.11979793012142181
In grad_steps = 1564, loss = 1.4661003351211548
In grad_steps = 1565, loss = 0.5500178337097168
In grad_steps = 1566, loss = 0.08379753679037094
In grad_steps = 1567, loss = 0.4434550106525421
In grad_steps = 1568, loss = 0.31764549016952515
In grad_steps = 1569, loss = 0.2080831527709961
In grad_steps = 1570, loss = 0.6445106267929077
In grad_steps = 1571, loss = 0.8664834499359131
In grad_steps = 1572, loss = 0.22617453336715698
In grad_steps = 1573, loss = 0.2548651695251465
In grad_steps = 1574, loss = 0.6720592975616455
In grad_steps = 1575, loss = 0.6586822271347046
In grad_steps = 1576, loss = 0.4452248513698578
In grad_steps = 1577, loss = 0.4776501953601837
In grad_steps = 1578, loss = 0.07995527982711792
In grad_steps = 1579, loss = 0.5472710728645325
In grad_steps = 1580, loss = 0.15263311564922333
In grad_steps = 1581, loss = 0.40368780493736267
In grad_steps = 1582, loss = 0.2467518448829651
In grad_steps = 1583, loss = 0.1716802418231964
In grad_steps = 1584, loss = 0.24562568962574005
In grad_steps = 1585, loss = 0.2571898102760315
In grad_steps = 1586, loss = 0.19060418009757996
In grad_steps = 1587, loss = 0.23075738549232483
In grad_steps = 1588, loss = 1.0072349309921265
In grad_steps = 1589, loss = 0.6780679225921631
In grad_steps = 1590, loss = 0.3002249002456665
In grad_steps = 1591, loss = 0.8138341307640076
In grad_steps = 1592, loss = 0.6365031599998474
In grad_steps = 1593, loss = 0.04273722320795059
In grad_steps = 1594, loss = 0.1726437211036682
In grad_steps = 1595, loss = 0.8075353503227234
In grad_steps = 1596, loss = 0.12990733981132507
In grad_steps = 1597, loss = 0.21490168571472168
In grad_steps = 1598, loss = 0.0836566761136055
In grad_steps = 1599, loss = 0.20588535070419312
In grad_steps = 1600, loss = 0.34044501185417175
In grad_steps = 1601, loss = 0.23146718740463257
In grad_steps = 1602, loss = 0.21661974489688873
In grad_steps = 1603, loss = 0.588822603225708
In grad_steps = 1604, loss = 0.39678335189819336
In grad_steps = 1605, loss = 0.08326613157987595
In grad_steps = 1606, loss = 0.28827691078186035
In grad_steps = 1607, loss = 0.04947609454393387
In grad_steps = 1608, loss = 0.5499542951583862
In grad_steps = 1609, loss = 0.5543452501296997
In grad_steps = 1610, loss = 0.22421957552433014
In grad_steps = 1611, loss = 0.1936163306236267
In grad_steps = 1612, loss = 0.38670065999031067
In grad_steps = 1613, loss = 0.3391980528831482
In grad_steps = 1614, loss = 0.126506969332695
In grad_steps = 1615, loss = 0.0802806168794632
In grad_steps = 1616, loss = 0.07948487251996994
In grad_steps = 1617, loss = 0.593947172164917
In grad_steps = 1618, loss = 0.818906307220459
In grad_steps = 1619, loss = 0.19359645247459412
In grad_steps = 1620, loss = 0.43475884199142456
In grad_steps = 1621, loss = 0.26821526885032654
In grad_steps = 1622, loss = 1.2905080318450928
In grad_steps = 1623, loss = 0.891440749168396
In grad_steps = 1624, loss = 0.40769097208976746
In grad_steps = 1625, loss = 1.0572898387908936
In grad_steps = 1626, loss = 0.22824044525623322
In grad_steps = 1627, loss = 0.18637457489967346
In grad_steps = 1628, loss = 0.2674906551837921
In grad_steps = 1629, loss = 0.14809457957744598
In grad_steps = 1630, loss = 0.33654189109802246
In grad_steps = 1631, loss = 0.11201493442058563
In grad_steps = 1632, loss = 0.36794671416282654
In grad_steps = 1633, loss = 0.5637388229370117
In grad_steps = 1634, loss = 0.22154149413108826
In grad_steps = 1635, loss = 0.21944591403007507
In grad_steps = 1636, loss = 0.3753903806209564
In grad_steps = 1637, loss = 0.2766962945461273
In grad_steps = 1638, loss = 0.20405752956867218
In grad_steps = 1639, loss = 0.1754070222377777
In grad_steps = 1640, loss = 0.3393823206424713
In grad_steps = 1641, loss = 0.4094652831554413
In grad_steps = 1642, loss = 0.050719115883111954
In grad_steps = 1643, loss = 1.1191211938858032
In grad_steps = 1644, loss = 0.2618348002433777
In grad_steps = 1645, loss = 0.31568610668182373
In grad_steps = 1646, loss = 0.21959421038627625
In grad_steps = 1647, loss = 0.6432223320007324
In grad_steps = 1648, loss = 0.06610526144504547
In grad_steps = 1649, loss = 1.607747197151184
In grad_steps = 1650, loss = 0.33624646067619324
In grad_steps = 1651, loss = 0.9135192036628723
In grad_steps = 1652, loss = 0.06887703388929367
In grad_steps = 1653, loss = 0.36113065481185913
In grad_steps = 1654, loss = 0.5474632382392883
In grad_steps = 1655, loss = 0.6644885540008545
In grad_steps = 1656, loss = 0.27817502617836
In grad_steps = 1657, loss = 0.4584695100784302
In grad_steps = 1658, loss = 0.1389675736427307
In grad_steps = 1659, loss = 0.0907658189535141
In grad_steps = 1660, loss = 0.5705336928367615
In grad_steps = 1661, loss = 0.23082512617111206
In grad_steps = 1662, loss = 0.47093141078948975
In grad_steps = 1663, loss = 0.24970577657222748
In grad_steps = 1664, loss = 0.4393647015094757
In grad_steps = 1665, loss = 0.2957695722579956
In grad_steps = 1666, loss = 0.2677246928215027
In grad_steps = 1667, loss = 0.2439831793308258
In grad_steps = 1668, loss = 0.25648683309555054
In grad_steps = 1669, loss = 0.6504840850830078
In grad_steps = 1670, loss = 0.11924494802951813
In grad_steps = 1671, loss = 0.08209052681922913
In grad_steps = 1672, loss = 0.125900000333786
In grad_steps = 1673, loss = 0.06519336253404617
In grad_steps = 1674, loss = 0.04935791343450546
In grad_steps = 1675, loss = 0.1986159235239029
In grad_steps = 1676, loss = 0.12705262005329132
In grad_steps = 1677, loss = 0.1325739026069641
In grad_steps = 1678, loss = 0.8210837841033936
In grad_steps = 1679, loss = 1.4075658321380615
In grad_steps = 1680, loss = 0.12546080350875854
In grad_steps = 1681, loss = 0.127616286277771
In grad_steps = 1682, loss = 0.21096619963645935
In grad_steps = 1683, loss = 0.38998734951019287
In grad_steps = 1684, loss = 0.72870272397995
In grad_steps = 1685, loss = 0.20883576571941376
In grad_steps = 1686, loss = 0.07284734398126602
In grad_steps = 1687, loss = 1.0589635372161865
In grad_steps = 1688, loss = 0.27561211585998535
In grad_steps = 1689, loss = 0.8895840048789978
In grad_steps = 1690, loss = 0.20867504179477692
In grad_steps = 1691, loss = 0.45345598459243774
In grad_steps = 1692, loss = 0.23399187624454498
In grad_steps = 1693, loss = 0.2803109288215637
In grad_steps = 1694, loss = 0.08157851547002792
In grad_steps = 1695, loss = 0.6463024616241455
In grad_steps = 1696, loss = 0.05106190964579582
In grad_steps = 1697, loss = 0.4843231737613678
In grad_steps = 1698, loss = 0.45468869805336
In grad_steps = 1699, loss = 0.22013172507286072
In grad_steps = 1700, loss = 0.45330172777175903
In grad_steps = 1701, loss = 0.7973707914352417
In grad_steps = 1702, loss = 0.4844713807106018
In grad_steps = 1703, loss = 0.24024544656276703
In grad_steps = 1704, loss = 0.057243987917900085
In grad_steps = 1705, loss = 0.4354962706565857
In grad_steps = 1706, loss = 0.11601647734642029
In grad_steps = 1707, loss = 0.5871837735176086
In grad_steps = 1708, loss = 0.041216298937797546
In grad_steps = 1709, loss = 0.14224028587341309
In grad_steps = 1710, loss = 1.0698928833007812
In grad_steps = 1711, loss = 0.29073306918144226
In grad_steps = 1712, loss = 0.35037708282470703
In grad_steps = 1713, loss = 0.05845848470926285
In grad_steps = 1714, loss = 0.12680670619010925
In grad_steps = 1715, loss = 0.29347479343414307
In grad_steps = 1716, loss = 0.15503469109535217
In grad_steps = 1717, loss = 0.033790215849876404
In grad_steps = 1718, loss = 1.0072897672653198
In grad_steps = 1719, loss = 0.0211009681224823
In grad_steps = 1720, loss = 0.3695392906665802
In grad_steps = 1721, loss = 0.6491595506668091
In grad_steps = 1722, loss = 0.042865294963121414
In grad_steps = 1723, loss = 0.09694663435220718
In grad_steps = 1724, loss = 0.04114688187837601
In grad_steps = 1725, loss = 0.040387701243162155
In grad_steps = 1726, loss = 0.047449175268411636
In grad_steps = 1727, loss = 0.3337215483188629
In grad_steps = 1728, loss = 0.26557210087776184
In grad_steps = 1729, loss = 0.03598380833864212
In grad_steps = 1730, loss = 0.07686060667037964
In grad_steps = 1731, loss = 0.9689204692840576
In grad_steps = 1732, loss = 1.042480707168579
In grad_steps = 1733, loss = 0.3298146724700928
In grad_steps = 1734, loss = 0.7498692274093628
In grad_steps = 1735, loss = 0.09445112198591232
In grad_steps = 1736, loss = 0.1784675121307373
In grad_steps = 1737, loss = 0.17393435537815094
In grad_steps = 1738, loss = 0.7500401735305786
In grad_steps = 1739, loss = 0.3962833881378174
In grad_steps = 1740, loss = 0.38175973296165466
In grad_steps = 1741, loss = 0.15339869260787964
In grad_steps = 1742, loss = 0.46052974462509155
In grad_steps = 1743, loss = 0.5385079383850098
In grad_steps = 1744, loss = 0.056114308536052704
In grad_steps = 1745, loss = 0.46017763018608093
In grad_steps = 1746, loss = 0.18308261036872864
In grad_steps = 1747, loss = 0.2527533769607544
In grad_steps = 1748, loss = 0.19651371240615845
In grad_steps = 1749, loss = 1.1908742189407349
In grad_steps = 1750, loss = 0.6084846258163452
In grad_steps = 1751, loss = 0.17088815569877625
In grad_steps = 1752, loss = 0.4782417416572571
In grad_steps = 1753, loss = 0.15624625980854034
In grad_steps = 1754, loss = 0.13828083872795105
In grad_steps = 1755, loss = 0.9168412089347839
In grad_steps = 1756, loss = 0.537792980670929
In grad_steps = 1757, loss = 0.17053140699863434
In grad_steps = 1758, loss = 1.2294952869415283
In grad_steps = 1759, loss = 0.06635748594999313
In grad_steps = 1760, loss = 0.16085410118103027
In grad_steps = 1761, loss = 0.4900236129760742
In grad_steps = 1762, loss = 0.5023950338363647
In grad_steps = 1763, loss = 0.5673038959503174
In grad_steps = 1764, loss = 0.32488390803337097
In grad_steps = 1765, loss = 0.7980048656463623
In grad_steps = 1766, loss = 0.2779391407966614
In grad_steps = 1767, loss = 0.2303697019815445
In grad_steps = 1768, loss = 0.5229380130767822
In grad_steps = 1769, loss = 0.4879183769226074
In grad_steps = 1770, loss = 0.33196619153022766
In grad_steps = 1771, loss = 0.5771968364715576
In grad_steps = 1772, loss = 0.28936338424682617
In grad_steps = 1773, loss = 0.28351891040802
In grad_steps = 1774, loss = 0.4363892376422882
In grad_steps = 1775, loss = 0.1660144329071045
In grad_steps = 1776, loss = 0.1899447739124298
In grad_steps = 1777, loss = 0.13479360938072205
In grad_steps = 1778, loss = 0.47359874844551086
In grad_steps = 1779, loss = 0.34619438648223877
In grad_steps = 1780, loss = 0.2438046634197235
In grad_steps = 1781, loss = 0.20519515872001648
In grad_steps = 1782, loss = 0.1537913680076599
In grad_steps = 1783, loss = 0.3414260447025299
In grad_steps = 1784, loss = 0.4675581455230713
In grad_steps = 1785, loss = 0.11738792806863785
In grad_steps = 1786, loss = 0.03651904687285423
In grad_steps = 1787, loss = 0.050099242478609085
In grad_steps = 1788, loss = 0.0839637815952301
In grad_steps = 1789, loss = 0.20029453933238983
In grad_steps = 1790, loss = 0.19585344195365906
In grad_steps = 1791, loss = 0.5708325505256653
In grad_steps = 1792, loss = 0.5885130167007446
In grad_steps = 1793, loss = 0.8579601645469666
In grad_steps = 1794, loss = 0.595824658870697
In grad_steps = 1795, loss = 1.5965933799743652
In grad_steps = 1796, loss = 0.1193089410662651
In grad_steps = 1797, loss = 0.33882004022598267
In grad_steps = 1798, loss = 0.1846415102481842
In grad_steps = 1799, loss = 0.3316696286201477
In grad_steps = 1800, loss = 0.009173931553959846
In grad_steps = 1801, loss = 0.4140201807022095
In grad_steps = 1802, loss = 0.03738752752542496
In grad_steps = 1803, loss = 0.8922838568687439
In grad_steps = 1804, loss = 0.08122628927230835
In grad_steps = 1805, loss = 0.44302764534950256
In grad_steps = 1806, loss = 0.2745437026023865
In grad_steps = 1807, loss = 0.23531679809093475
In grad_steps = 1808, loss = 1.2225730419158936
In grad_steps = 1809, loss = 0.1651342511177063
In grad_steps = 1810, loss = 0.5823793411254883
In grad_steps = 1811, loss = 0.7316910624504089
In grad_steps = 1812, loss = 0.12115141749382019
In grad_steps = 1813, loss = 0.4370877146720886
In grad_steps = 1814, loss = 0.2005976438522339
In grad_steps = 1815, loss = 1.067542314529419
In grad_steps = 1816, loss = 0.6559440493583679
In grad_steps = 1817, loss = 0.10233582556247711
In grad_steps = 1818, loss = 0.21033445000648499
In grad_steps = 1819, loss = 0.15743760764598846
In grad_steps = 1820, loss = 0.5895851850509644
In grad_steps = 1821, loss = 0.19990316033363342
In grad_steps = 1822, loss = 0.516655683517456
In grad_steps = 1823, loss = 0.16921699047088623
In grad_steps = 1824, loss = 0.34768053889274597
In grad_steps = 1825, loss = 0.2292543351650238
In grad_steps = 1826, loss = 0.18051683902740479
In grad_steps = 1827, loss = 0.20659714937210083
In grad_steps = 1828, loss = 0.6496777534484863
In grad_steps = 1829, loss = 0.166281595826149
In grad_steps = 1830, loss = 0.9075536727905273
In grad_steps = 1831, loss = 0.20368137955665588
In grad_steps = 1832, loss = 0.5279675126075745
In grad_steps = 1833, loss = 0.10527246445417404
In grad_steps = 1834, loss = 0.12368756532669067
In grad_steps = 1835, loss = 0.04872361570596695
In grad_steps = 1836, loss = 0.08560143411159515
In grad_steps = 1837, loss = 0.7985364198684692
In grad_steps = 1838, loss = 0.5162023305892944
In grad_steps = 1839, loss = 0.5615379214286804
In grad_steps = 1840, loss = 0.20361438393592834
In grad_steps = 1841, loss = 0.15230177342891693
In grad_steps = 1842, loss = 0.15477754175662994
In grad_steps = 1843, loss = 0.07057303190231323
In grad_steps = 1844, loss = 0.22347676753997803
In grad_steps = 1845, loss = 0.05644943192601204
In grad_steps = 1846, loss = 0.11830051243305206
In grad_steps = 1847, loss = 0.23774166405200958
In grad_steps = 1848, loss = 0.22458666563034058
In grad_steps = 1849, loss = 0.5969246625900269
In grad_steps = 1850, loss = 0.05243096873164177
In grad_steps = 1851, loss = 0.05384493246674538
In grad_steps = 1852, loss = 0.104135662317276
In grad_steps = 1853, loss = 1.203818678855896
In grad_steps = 1854, loss = 0.19804900884628296
In grad_steps = 1855, loss = 0.19842131435871124
In grad_steps = 1856, loss = 0.15192602574825287
In grad_steps = 1857, loss = 0.6208800077438354
In grad_steps = 1858, loss = 0.03128533810377121
In grad_steps = 1859, loss = 0.14318975806236267
In grad_steps = 1860, loss = 0.4385320246219635
In grad_steps = 1861, loss = 0.7308390736579895
In grad_steps = 1862, loss = 0.06401631981134415
In grad_steps = 1863, loss = 2.066318988800049
In grad_steps = 1864, loss = 0.04361620917916298
In grad_steps = 1865, loss = 0.2668267786502838
In grad_steps = 1866, loss = 0.9674860239028931
In grad_steps = 1867, loss = 0.049778226763010025
In grad_steps = 1868, loss = 0.28453993797302246
In grad_steps = 1869, loss = 0.10527084767818451
In grad_steps = 1870, loss = 0.8669530749320984
In grad_steps = 1871, loss = 0.9356625080108643
In grad_steps = 1872, loss = 0.062218695878982544
In grad_steps = 1873, loss = 0.10408416390419006
In grad_steps = 1874, loss = 0.5119023323059082
In grad_steps = 1875, loss = 1.2079732418060303
In grad_steps = 1876, loss = 0.6851071715354919
In grad_steps = 1877, loss = 0.5761770606040955
In grad_steps = 1878, loss = 1.4904212951660156
In grad_steps = 1879, loss = 0.04754699021577835
In grad_steps = 1880, loss = 0.49851900339126587
In grad_steps = 1881, loss = 0.42782869935035706
In grad_steps = 1882, loss = 0.4907177686691284
In grad_steps = 1883, loss = 0.6126123666763306
In grad_steps = 1884, loss = 0.35245728492736816
In grad_steps = 1885, loss = 0.43893930315971375
In grad_steps = 1886, loss = 0.6632707715034485
In grad_steps = 1887, loss = 0.8926580548286438
In grad_steps = 1888, loss = 0.6157907247543335
In grad_steps = 1889, loss = 0.43653690814971924
In grad_steps = 1890, loss = 0.27673089504241943
In grad_steps = 1891, loss = 0.49435222148895264
In grad_steps = 1892, loss = 0.4273478090763092
In grad_steps = 1893, loss = 0.27831417322158813
In grad_steps = 1894, loss = 0.3770483434200287
In grad_steps = 1895, loss = 0.34283119440078735
In grad_steps = 1896, loss = 0.36042147874832153
In grad_steps = 1897, loss = 0.20384341478347778
In grad_steps = 1898, loss = 0.3131946921348572
In grad_steps = 1899, loss = 0.21702703833580017
In grad_steps = 1900, loss = 0.12203818559646606
In grad_steps = 1901, loss = 0.1407761126756668
In grad_steps = 1902, loss = 0.12189751863479614
In grad_steps = 1903, loss = 0.46246033906936646
In grad_steps = 1904, loss = 0.6715604066848755
In grad_steps = 1905, loss = 0.9493510127067566
In grad_steps = 1906, loss = 0.04069353640079498
In grad_steps = 1907, loss = 0.15996694564819336
In grad_steps = 1908, loss = 0.02263898029923439
In grad_steps = 1909, loss = 0.34143784642219543
In grad_steps = 1910, loss = 0.05076592043042183
In grad_steps = 1911, loss = 0.19173789024353027
In grad_steps = 1912, loss = 0.03232581168413162
In grad_steps = 1913, loss = 0.84730464220047
In grad_steps = 1914, loss = 0.7346018552780151
In grad_steps = 1915, loss = 0.44591525197029114
In grad_steps = 1916, loss = 0.11465246975421906
In grad_steps = 1917, loss = 0.0725853443145752
In grad_steps = 1918, loss = 0.43826764822006226
In grad_steps = 1919, loss = 0.1656108945608139
In grad_steps = 1920, loss = 0.11537614464759827
In grad_steps = 1921, loss = 0.17494171857833862
In grad_steps = 1922, loss = 0.028469722718000412
In grad_steps = 1923, loss = 0.21826568245887756
In grad_steps = 1924, loss = 1.030012845993042
In grad_steps = 1925, loss = 0.13707280158996582
In grad_steps = 1926, loss = 0.051807060837745667
In grad_steps = 1927, loss = 0.12618856132030487
In grad_steps = 1928, loss = 0.13030046224594116
In grad_steps = 1929, loss = 0.08695237338542938
In grad_steps = 1930, loss = 0.7504392862319946
In grad_steps = 1931, loss = 0.7499023079872131
In grad_steps = 1932, loss = 0.20330554246902466
In grad_steps = 1933, loss = 0.6395156383514404
In grad_steps = 1934, loss = 0.060492560267448425
In grad_steps = 1935, loss = 0.22183723747730255
In grad_steps = 1936, loss = 0.05738459527492523
In grad_steps = 1937, loss = 0.16522330045700073
In grad_steps = 1938, loss = 0.06598416715860367
In grad_steps = 1939, loss = 0.845612645149231
In grad_steps = 1940, loss = 0.3221615254878998
In grad_steps = 1941, loss = 0.1505136638879776
In grad_steps = 1942, loss = 0.07160084694623947
In grad_steps = 1943, loss = 0.19266700744628906
In grad_steps = 1944, loss = 0.9914616346359253
In grad_steps = 1945, loss = 1.6285102367401123
In grad_steps = 1946, loss = 0.6533523797988892
In grad_steps = 1947, loss = 0.7350882887840271
In grad_steps = 1948, loss = 0.5351245403289795
In grad_steps = 1949, loss = 0.23281127214431763
In grad_steps = 1950, loss = 0.5024293065071106
In grad_steps = 1951, loss = 0.47548097372055054
In grad_steps = 1952, loss = 0.6488485336303711
In grad_steps = 1953, loss = 0.18852347135543823
In grad_steps = 1954, loss = 0.350485622882843
In grad_steps = 1955, loss = 0.4885598421096802
In grad_steps = 1956, loss = 0.4700998365879059
In grad_steps = 1957, loss = 0.5732935667037964
In grad_steps = 1958, loss = 0.1623230278491974
In grad_steps = 1959, loss = 0.2765294909477234
In grad_steps = 1960, loss = 0.20867764949798584
In grad_steps = 1961, loss = 0.43860185146331787
In grad_steps = 1962, loss = 0.4903508424758911
In grad_steps = 1963, loss = 0.4573749303817749
In grad_steps = 1964, loss = 0.18378572165966034
In grad_steps = 1965, loss = 0.37372541427612305
In grad_steps = 1966, loss = 0.22153353691101074
In grad_steps = 1967, loss = 0.21688489615917206
In grad_steps = 1968, loss = 0.32476839423179626
In grad_steps = 1969, loss = 0.9532140493392944
In grad_steps = 1970, loss = 0.8553017377853394
In grad_steps = 1971, loss = 1.1595895290374756
In grad_steps = 1972, loss = 0.25397926568984985
In grad_steps = 1973, loss = 0.10387216508388519
In grad_steps = 1974, loss = 0.24439691007137299
In grad_steps = 1975, loss = 0.078093521296978
In grad_steps = 1976, loss = 0.35200658440589905
In grad_steps = 1977, loss = 0.2755284309387207
In grad_steps = 1978, loss = 0.22665759921073914
In grad_steps = 1979, loss = 0.7087585926055908
In grad_steps = 1980, loss = 0.23462343215942383
In grad_steps = 1981, loss = 0.30135321617126465
In grad_steps = 1982, loss = 0.39764511585235596
In grad_steps = 1983, loss = 0.10438309609889984
In grad_steps = 1984, loss = 0.20748071372509003
In grad_steps = 1985, loss = 0.6708840131759644
In grad_steps = 1986, loss = 0.2507798373699188
In grad_steps = 1987, loss = 0.3437056541442871
In grad_steps = 1988, loss = 0.9687387943267822
In grad_steps = 1989, loss = 0.433441698551178
In grad_steps = 1990, loss = 0.07129472494125366
In grad_steps = 1991, loss = 0.6192275285720825
In grad_steps = 1992, loss = 0.2448263317346573
In grad_steps = 1993, loss = 0.29210278391838074
In grad_steps = 1994, loss = 0.6656442880630493
In grad_steps = 1995, loss = 0.17189554870128632
In grad_steps = 1996, loss = 0.47134995460510254
In grad_steps = 1997, loss = 0.23626983165740967
In grad_steps = 1998, loss = 0.09381261467933655
In grad_steps = 1999, loss = 0.1475500613451004
In grad_steps = 2000, loss = 0.12100738286972046
In grad_steps = 2001, loss = 0.7717918157577515
In grad_steps = 2002, loss = 0.21134299039840698
In grad_steps = 2003, loss = 0.2040313184261322
In grad_steps = 2004, loss = 0.1288512945175171
In grad_steps = 2005, loss = 0.18925271928310394
In grad_steps = 2006, loss = 0.18665483593940735
In grad_steps = 2007, loss = 0.07223213464021683
In grad_steps = 2008, loss = 0.1739053726196289
In grad_steps = 2009, loss = 0.04416249319911003
In grad_steps = 2010, loss = 0.1455880105495453
In grad_steps = 2011, loss = 0.1507035195827484
In grad_steps = 2012, loss = 0.5104017853736877
In grad_steps = 2013, loss = 0.41261792182922363
In grad_steps = 2014, loss = 0.029513996094465256
In grad_steps = 2015, loss = 0.07845956832170486
In grad_steps = 2016, loss = 0.11330606788396835
In grad_steps = 2017, loss = 0.7680152654647827
In grad_steps = 2018, loss = 0.5401755571365356
In grad_steps = 2019, loss = 0.24471700191497803
In grad_steps = 2020, loss = 0.07299089431762695
In grad_steps = 2021, loss = 0.10582363605499268
In grad_steps = 2022, loss = 0.014717860147356987
In grad_steps = 2023, loss = 0.2031368911266327
In grad_steps = 2024, loss = 0.8108404874801636
In grad_steps = 2025, loss = 0.13389766216278076
In grad_steps = 2026, loss = 0.5837128162384033
In grad_steps = 2027, loss = 0.23229822516441345
In grad_steps = 2028, loss = 0.37731295824050903
In grad_steps = 2029, loss = 0.05859413743019104
In grad_steps = 2030, loss = 0.07141367346048355
In grad_steps = 2031, loss = 0.011121805757284164
In grad_steps = 2032, loss = 1.0824519395828247
In grad_steps = 2033, loss = 0.7567840218544006
In grad_steps = 2034, loss = 0.7083445191383362
In grad_steps = 2035, loss = 0.025125326588749886
In grad_steps = 2036, loss = 0.044613488018512726
In grad_steps = 2037, loss = 0.07524669170379639
In grad_steps = 2038, loss = 0.7221688032150269
In grad_steps = 2039, loss = 0.08034586161375046
In grad_steps = 2040, loss = 0.04743051156401634
In grad_steps = 2041, loss = 0.5267996191978455
In grad_steps = 2042, loss = 0.10404364764690399
In grad_steps = 2043, loss = 0.17704221606254578
In grad_steps = 2044, loss = 0.7207024693489075
In grad_steps = 2045, loss = 0.524312436580658
In grad_steps = 2046, loss = 0.05850037559866905
In grad_steps = 2047, loss = 0.21831399202346802
In grad_steps = 2048, loss = 0.27797168493270874
In grad_steps = 2049, loss = 0.7385203242301941
In grad_steps = 2050, loss = 0.4380377531051636
In grad_steps = 2051, loss = 0.20925253629684448
In grad_steps = 2052, loss = 0.010235674679279327
Beginning epoch 2
In grad_steps = 2053, loss = 0.33258670568466187
In grad_steps = 2054, loss = 0.09811080247163773
In grad_steps = 2055, loss = 0.5895882248878479
In grad_steps = 2056, loss = 0.41001543402671814
In grad_steps = 2057, loss = 0.18322668969631195
In grad_steps = 2058, loss = 0.05344761535525322
In grad_steps = 2059, loss = 0.14068232476711273
In grad_steps = 2060, loss = 0.12186314910650253
In grad_steps = 2061, loss = 0.838505208492279
In grad_steps = 2062, loss = 0.6016058921813965
In grad_steps = 2063, loss = 0.7743313312530518
In grad_steps = 2064, loss = 0.08988625556230545
In grad_steps = 2065, loss = 0.42808207869529724
In grad_steps = 2066, loss = 0.07600844651460648
In grad_steps = 2067, loss = 0.2746005654335022
In grad_steps = 2068, loss = 0.6614081859588623
In grad_steps = 2069, loss = 0.12251833081245422
In grad_steps = 2070, loss = 0.3370952904224396
In grad_steps = 2071, loss = 0.4520512819290161
In grad_steps = 2072, loss = 0.39674273133277893
In grad_steps = 2073, loss = 0.047684408724308014
In grad_steps = 2074, loss = 0.5599882006645203
In grad_steps = 2075, loss = 0.9752651453018188
In grad_steps = 2076, loss = 0.12005463242530823
In grad_steps = 2077, loss = 0.009861867874860764
In grad_steps = 2078, loss = 0.3820740580558777
In grad_steps = 2079, loss = 0.29900676012039185
In grad_steps = 2080, loss = 1.094150185585022
In grad_steps = 2081, loss = 0.11507910490036011
In grad_steps = 2082, loss = 0.11707285046577454
In grad_steps = 2083, loss = 0.6016199588775635
In grad_steps = 2084, loss = 0.4522934854030609
In grad_steps = 2085, loss = 0.24780528247356415
In grad_steps = 2086, loss = 0.6657370328903198
In grad_steps = 2087, loss = 0.23858720064163208
In grad_steps = 2088, loss = 0.28673243522644043
In grad_steps = 2089, loss = 0.2460951805114746
In grad_steps = 2090, loss = 0.20564506947994232
In grad_steps = 2091, loss = 0.35329562425613403
In grad_steps = 2092, loss = 0.27165305614471436
In grad_steps = 2093, loss = 0.44348782300949097
In grad_steps = 2094, loss = 0.12177258729934692
In grad_steps = 2095, loss = 0.24283181130886078
In grad_steps = 2096, loss = 0.13677147030830383
In grad_steps = 2097, loss = 0.3662322163581848
In grad_steps = 2098, loss = 0.2729876637458801
In grad_steps = 2099, loss = 0.7101967930793762
In grad_steps = 2100, loss = 0.6208911538124084
In grad_steps = 2101, loss = 0.13348069787025452
In grad_steps = 2102, loss = 0.07727013528347015
In grad_steps = 2103, loss = 0.4463817775249481
In grad_steps = 2104, loss = 0.05716712772846222
In grad_steps = 2105, loss = 0.08650708198547363
In grad_steps = 2106, loss = 0.017929736524820328
In grad_steps = 2107, loss = 0.031518906354904175
In grad_steps = 2108, loss = 0.1812743842601776
In grad_steps = 2109, loss = 0.5475343465805054
In grad_steps = 2110, loss = 0.3568792939186096
In grad_steps = 2111, loss = 0.07483226805925369
In grad_steps = 2112, loss = 0.9179045557975769
In grad_steps = 2113, loss = 1.6659094095230103
In grad_steps = 2114, loss = 0.0757671594619751
In grad_steps = 2115, loss = 0.30320367217063904
In grad_steps = 2116, loss = 0.15493401885032654
In grad_steps = 2117, loss = 0.11532610654830933
In grad_steps = 2118, loss = 0.13775457441806793
In grad_steps = 2119, loss = 0.606468141078949
In grad_steps = 2120, loss = 0.05332549288868904
In grad_steps = 2121, loss = 0.31134799122810364
In grad_steps = 2122, loss = 0.19655659794807434
In grad_steps = 2123, loss = 0.2651497721672058
In grad_steps = 2124, loss = 0.12042605876922607
In grad_steps = 2125, loss = 0.38272711634635925
In grad_steps = 2126, loss = 0.261414110660553
In grad_steps = 2127, loss = 0.052056096494197845
In grad_steps = 2128, loss = 0.7620949745178223
In grad_steps = 2129, loss = 0.10353805124759674
In grad_steps = 2130, loss = 0.3706039786338806
In grad_steps = 2131, loss = 0.15690457820892334
In grad_steps = 2132, loss = 0.5201002359390259
In grad_steps = 2133, loss = 0.5532739162445068
In grad_steps = 2134, loss = 0.20959721505641937
In grad_steps = 2135, loss = 0.1426149606704712
In grad_steps = 2136, loss = 0.4953644573688507
In grad_steps = 2137, loss = 0.13850107789039612
In grad_steps = 2138, loss = 0.34186017513275146
In grad_steps = 2139, loss = 0.14657744765281677
In grad_steps = 2140, loss = 0.08648326992988586
In grad_steps = 2141, loss = 0.8525654077529907
In grad_steps = 2142, loss = 0.08951088786125183
In grad_steps = 2143, loss = 0.6344964504241943
In grad_steps = 2144, loss = 0.49673017859458923
In grad_steps = 2145, loss = 0.4152275025844574
In grad_steps = 2146, loss = 0.4922447204589844
In grad_steps = 2147, loss = 0.2668836712837219
In grad_steps = 2148, loss = 0.05469594895839691
In grad_steps = 2149, loss = 0.14887696504592896
In grad_steps = 2150, loss = 0.3688470423221588
In grad_steps = 2151, loss = 0.3612439036369324
In grad_steps = 2152, loss = 0.19649863243103027
In grad_steps = 2153, loss = 0.8440477252006531
In grad_steps = 2154, loss = 0.06815977394580841
In grad_steps = 2155, loss = 0.5519945621490479
In grad_steps = 2156, loss = 0.062147799879312515
In grad_steps = 2157, loss = 0.38357552886009216
In grad_steps = 2158, loss = 0.1491488367319107
In grad_steps = 2159, loss = 0.08359799534082413
In grad_steps = 2160, loss = 0.1265849769115448
In grad_steps = 2161, loss = 0.16742844879627228
In grad_steps = 2162, loss = 0.039044756442308426
In grad_steps = 2163, loss = 0.11065920442342758
In grad_steps = 2164, loss = 0.026938781142234802
In grad_steps = 2165, loss = 0.0472138412296772
In grad_steps = 2166, loss = 0.44994914531707764
In grad_steps = 2167, loss = 0.06756722182035446
In grad_steps = 2168, loss = 0.7272575497627258
In grad_steps = 2169, loss = 0.073365218937397
In grad_steps = 2170, loss = 0.06219784542918205
In grad_steps = 2171, loss = 0.14756819605827332
In grad_steps = 2172, loss = 0.2643231153488159
In grad_steps = 2173, loss = 0.2240036278963089
In grad_steps = 2174, loss = 0.06331995874643326
In grad_steps = 2175, loss = 0.042991891503334045
In grad_steps = 2176, loss = 0.28345444798469543
In grad_steps = 2177, loss = 0.32077550888061523
In grad_steps = 2178, loss = 0.38117891550064087
In grad_steps = 2179, loss = 0.028609735891222954
In grad_steps = 2180, loss = 0.7680151462554932
In grad_steps = 2181, loss = 0.06426740437746048
In grad_steps = 2182, loss = 0.07769200950860977
In grad_steps = 2183, loss = 0.1208345890045166
In grad_steps = 2184, loss = 0.2502474784851074
In grad_steps = 2185, loss = 0.4537816643714905
In grad_steps = 2186, loss = 0.17343613505363464
In grad_steps = 2187, loss = 0.16685494780540466
In grad_steps = 2188, loss = 0.3157850503921509
In grad_steps = 2189, loss = 0.041200101375579834
In grad_steps = 2190, loss = 0.01459703128784895
In grad_steps = 2191, loss = 1.327359914779663
In grad_steps = 2192, loss = 0.022484101355075836
In grad_steps = 2193, loss = 1.2772551774978638
In grad_steps = 2194, loss = 0.01806093379855156
In grad_steps = 2195, loss = 0.09983587265014648
In grad_steps = 2196, loss = 0.03747374936938286
In grad_steps = 2197, loss = 0.7760021090507507
In grad_steps = 2198, loss = 0.19413571059703827
In grad_steps = 2199, loss = 0.21279163658618927
In grad_steps = 2200, loss = 0.12921781837940216
In grad_steps = 2201, loss = 0.11414763331413269
In grad_steps = 2202, loss = 0.0789988562464714
In grad_steps = 2203, loss = 0.568730354309082
In grad_steps = 2204, loss = 0.2891542911529541
In grad_steps = 2205, loss = 1.1151151657104492
In grad_steps = 2206, loss = 0.15513285994529724
In grad_steps = 2207, loss = 0.35078540444374084
In grad_steps = 2208, loss = 0.08389133214950562
In grad_steps = 2209, loss = 0.09704161435365677
In grad_steps = 2210, loss = 0.08292906731367111
In grad_steps = 2211, loss = 0.3465680480003357
In grad_steps = 2212, loss = 0.1708066165447235
In grad_steps = 2213, loss = 0.11017370223999023
In grad_steps = 2214, loss = 0.5618706345558167
In grad_steps = 2215, loss = 0.33517444133758545
In grad_steps = 2216, loss = 0.13657993078231812
In grad_steps = 2217, loss = 0.15967583656311035
In grad_steps = 2218, loss = 0.09567370265722275
In grad_steps = 2219, loss = 0.041572559624910355
In grad_steps = 2220, loss = 0.05008267983794212
In grad_steps = 2221, loss = 0.43273743987083435
In grad_steps = 2222, loss = 0.16141416132450104
In grad_steps = 2223, loss = 0.20899748802185059
In grad_steps = 2224, loss = 0.2746513783931732
In grad_steps = 2225, loss = 0.16599535942077637
In grad_steps = 2226, loss = 1.1135375499725342
In grad_steps = 2227, loss = 0.024903573095798492
In grad_steps = 2228, loss = 0.9240476489067078
In grad_steps = 2229, loss = 0.02062436193227768
In grad_steps = 2230, loss = 0.01967102475464344
In grad_steps = 2231, loss = 0.2750982940196991
In grad_steps = 2232, loss = 0.513471782207489
In grad_steps = 2233, loss = 0.052102088928222656
In grad_steps = 2234, loss = 0.09112691134214401
In grad_steps = 2235, loss = 0.15487529337406158
In grad_steps = 2236, loss = 0.8293973803520203
In grad_steps = 2237, loss = 0.09053666144609451
In grad_steps = 2238, loss = 0.4524744749069214
In grad_steps = 2239, loss = 0.04131424054503441
In grad_steps = 2240, loss = 0.3493741452693939
In grad_steps = 2241, loss = 0.11288328468799591
In grad_steps = 2242, loss = 0.26308509707450867
In grad_steps = 2243, loss = 0.10998355597257614
In grad_steps = 2244, loss = 0.7667213082313538
In grad_steps = 2245, loss = 0.15705527365207672
In grad_steps = 2246, loss = 0.44306227564811707
In grad_steps = 2247, loss = 0.1299806535243988
In grad_steps = 2248, loss = 0.18412983417510986
In grad_steps = 2249, loss = 0.07159986346960068
In grad_steps = 2250, loss = 0.43368861079216003
In grad_steps = 2251, loss = 0.29499295353889465
In grad_steps = 2252, loss = 0.9963259696960449
In grad_steps = 2253, loss = 0.05504279583692551
In grad_steps = 2254, loss = 0.27529361844062805
In grad_steps = 2255, loss = 0.07060844451189041
In grad_steps = 2256, loss = 0.4248543977737427
In grad_steps = 2257, loss = 0.23796947300434113
In grad_steps = 2258, loss = 0.24364235997200012
In grad_steps = 2259, loss = 0.08935524523258209
In grad_steps = 2260, loss = 0.4254748225212097
In grad_steps = 2261, loss = 0.19895778596401215
In grad_steps = 2262, loss = 0.09693408012390137
In grad_steps = 2263, loss = 0.3721843659877777
In grad_steps = 2264, loss = 0.282030314207077
In grad_steps = 2265, loss = 0.6611212491989136
In grad_steps = 2266, loss = 0.23933301866054535
In grad_steps = 2267, loss = 0.05731368809938431
In grad_steps = 2268, loss = 0.10204784572124481
In grad_steps = 2269, loss = 0.776394784450531
In grad_steps = 2270, loss = 0.9641945958137512
In grad_steps = 2271, loss = 0.6513776183128357
In grad_steps = 2272, loss = 0.28922006487846375
In grad_steps = 2273, loss = 0.27391842007637024
In grad_steps = 2274, loss = 0.5759173631668091
In grad_steps = 2275, loss = 0.3315804600715637
In grad_steps = 2276, loss = 0.8260393142700195
In grad_steps = 2277, loss = 0.20458628237247467
In grad_steps = 2278, loss = 0.2513687014579773
In grad_steps = 2279, loss = 0.25498953461647034
In grad_steps = 2280, loss = 0.11616112291812897
In grad_steps = 2281, loss = 0.19413867592811584
In grad_steps = 2282, loss = 0.15526778995990753
In grad_steps = 2283, loss = 0.15827004611492157
In grad_steps = 2284, loss = 0.43021735548973083
In grad_steps = 2285, loss = 1.0607272386550903
In grad_steps = 2286, loss = 0.08019423484802246
In grad_steps = 2287, loss = 1.0748834609985352
In grad_steps = 2288, loss = 0.6160790920257568
In grad_steps = 2289, loss = 0.4529365599155426
In grad_steps = 2290, loss = 0.14111313223838806
In grad_steps = 2291, loss = 0.09531009942293167
In grad_steps = 2292, loss = 0.14074590802192688
In grad_steps = 2293, loss = 0.09550931304693222
In grad_steps = 2294, loss = 0.3740333318710327
In grad_steps = 2295, loss = 0.1525983065366745
In grad_steps = 2296, loss = 0.24849969148635864
In grad_steps = 2297, loss = 0.3330898880958557
In grad_steps = 2298, loss = 0.2543461322784424
In grad_steps = 2299, loss = 0.07414240390062332
In grad_steps = 2300, loss = 0.19189682602882385
In grad_steps = 2301, loss = 1.1756809949874878
In grad_steps = 2302, loss = 0.13256694376468658
In grad_steps = 2303, loss = 0.18477395176887512
In grad_steps = 2304, loss = 0.2684209644794464
In grad_steps = 2305, loss = 0.053817134350538254
In grad_steps = 2306, loss = 0.09293629229068756
In grad_steps = 2307, loss = 0.1180541142821312
In grad_steps = 2308, loss = 0.566071629524231
In grad_steps = 2309, loss = 1.0531134605407715
In grad_steps = 2310, loss = 0.19075357913970947
In grad_steps = 2311, loss = 0.6485592722892761
In grad_steps = 2312, loss = 0.13735707104206085
In grad_steps = 2313, loss = 0.39567095041275024
In grad_steps = 2314, loss = 0.9242124557495117
In grad_steps = 2315, loss = 0.810322642326355
In grad_steps = 2316, loss = 0.4111739993095398
In grad_steps = 2317, loss = 0.23998719453811646
In grad_steps = 2318, loss = 0.2577355206012726
In grad_steps = 2319, loss = 0.2717011272907257
In grad_steps = 2320, loss = 0.9811569452285767
In grad_steps = 2321, loss = 0.300234854221344
In grad_steps = 2322, loss = 0.38265714049339294
In grad_steps = 2323, loss = 0.1398053616285324
In grad_steps = 2324, loss = 0.5722333788871765
In grad_steps = 2325, loss = 0.20070646703243256
In grad_steps = 2326, loss = 0.11068303138017654
In grad_steps = 2327, loss = 0.34731316566467285
In grad_steps = 2328, loss = 0.30964505672454834
In grad_steps = 2329, loss = 0.2708473801612854
In grad_steps = 2330, loss = 0.12362018972635269
In grad_steps = 2331, loss = 0.34931468963623047
In grad_steps = 2332, loss = 0.13573244214057922
In grad_steps = 2333, loss = 0.4527284801006317
In grad_steps = 2334, loss = 0.18582847714424133
In grad_steps = 2335, loss = 0.5703484416007996
In grad_steps = 2336, loss = 0.4393474757671356
In grad_steps = 2337, loss = 0.13814343512058258
In grad_steps = 2338, loss = 0.09868646413087845
In grad_steps = 2339, loss = 0.03382585197687149
In grad_steps = 2340, loss = 0.39187514781951904
In grad_steps = 2341, loss = 0.37426650524139404
In grad_steps = 2342, loss = 0.23091082274913788
In grad_steps = 2343, loss = 0.1438097059726715
In grad_steps = 2344, loss = 0.3751852512359619
In grad_steps = 2345, loss = 0.2075159102678299
In grad_steps = 2346, loss = 0.22995923459529877
In grad_steps = 2347, loss = 0.1721457690000534
In grad_steps = 2348, loss = 0.5394551753997803
In grad_steps = 2349, loss = 0.8398947715759277
In grad_steps = 2350, loss = 0.597621738910675
In grad_steps = 2351, loss = 0.01544711273163557
In grad_steps = 2352, loss = 0.02164033241569996
In grad_steps = 2353, loss = 0.8832688927650452
In grad_steps = 2354, loss = 0.13969451189041138
In grad_steps = 2355, loss = 0.7641866207122803
In grad_steps = 2356, loss = 0.1311662197113037
In grad_steps = 2357, loss = 0.015747476369142532
In grad_steps = 2358, loss = 0.035857874900102615
In grad_steps = 2359, loss = 0.4238259494304657
In grad_steps = 2360, loss = 0.8790302276611328
In grad_steps = 2361, loss = 0.960337221622467
In grad_steps = 2362, loss = 0.18530882894992828
In grad_steps = 2363, loss = 0.4344955086708069
In grad_steps = 2364, loss = 0.38190585374832153
In grad_steps = 2365, loss = 0.11523046344518661
In grad_steps = 2366, loss = 0.18244540691375732
In grad_steps = 2367, loss = 0.13820376992225647
In grad_steps = 2368, loss = 0.014383865520358086
In grad_steps = 2369, loss = 1.4969631433486938
In grad_steps = 2370, loss = 0.3208758533000946
In grad_steps = 2371, loss = 0.20955775678157806
In grad_steps = 2372, loss = 0.07417792081832886
In grad_steps = 2373, loss = 0.25814008712768555
In grad_steps = 2374, loss = 0.20000191032886505
In grad_steps = 2375, loss = 0.48920148611068726
In grad_steps = 2376, loss = 0.2432233989238739
In grad_steps = 2377, loss = 0.038277678191661835
In grad_steps = 2378, loss = 0.31703922152519226
In grad_steps = 2379, loss = 0.3453298807144165
In grad_steps = 2380, loss = 0.23955322802066803
In grad_steps = 2381, loss = 0.019954094663262367
In grad_steps = 2382, loss = 0.8274844884872437
In grad_steps = 2383, loss = 0.259806364774704
In grad_steps = 2384, loss = 0.02565290965139866
In grad_steps = 2385, loss = 0.019579632207751274
In grad_steps = 2386, loss = 0.33642512559890747
In grad_steps = 2387, loss = 0.4312572479248047
In grad_steps = 2388, loss = 0.04926184192299843
In grad_steps = 2389, loss = 0.22271797060966492
In grad_steps = 2390, loss = 0.022145066410303116
In grad_steps = 2391, loss = 0.05339157208800316
In grad_steps = 2392, loss = 0.29026177525520325
In grad_steps = 2393, loss = 0.024713920429348946
In grad_steps = 2394, loss = 0.7458692789077759
In grad_steps = 2395, loss = 0.05155579000711441
In grad_steps = 2396, loss = 0.9956948161125183
In grad_steps = 2397, loss = 0.16125497221946716
In grad_steps = 2398, loss = 0.7520335912704468
In grad_steps = 2399, loss = 0.09952406585216522
In grad_steps = 2400, loss = 0.12929540872573853
In grad_steps = 2401, loss = 0.2117963582277298
In grad_steps = 2402, loss = 0.05097907409071922
In grad_steps = 2403, loss = 0.04444120079278946
In grad_steps = 2404, loss = 0.1130460873246193
In grad_steps = 2405, loss = 0.10518743097782135
In grad_steps = 2406, loss = 0.8839448094367981
In grad_steps = 2407, loss = 0.07156530022621155
In grad_steps = 2408, loss = 0.31224462389945984
In grad_steps = 2409, loss = 0.43553704023361206
In grad_steps = 2410, loss = 0.5747422575950623
In grad_steps = 2411, loss = 0.1738913506269455
In grad_steps = 2412, loss = 0.7139007449150085
In grad_steps = 2413, loss = 0.510053813457489
In grad_steps = 2414, loss = 0.07853429764509201
In grad_steps = 2415, loss = 0.4443286657333374
In grad_steps = 2416, loss = 0.9892821311950684
In grad_steps = 2417, loss = 0.06854324787855148
In grad_steps = 2418, loss = 0.7499021291732788
In grad_steps = 2419, loss = 0.3863465487957001
In grad_steps = 2420, loss = 0.44141507148742676
In grad_steps = 2421, loss = 0.22744503617286682
In grad_steps = 2422, loss = 0.3576648533344269
In grad_steps = 2423, loss = 0.6330633163452148
In grad_steps = 2424, loss = 0.1352415829896927
In grad_steps = 2425, loss = 0.20851737260818481
In grad_steps = 2426, loss = 0.3113064169883728
In grad_steps = 2427, loss = 0.1535271406173706
In grad_steps = 2428, loss = 0.4608747363090515
In grad_steps = 2429, loss = 0.27172333002090454
In grad_steps = 2430, loss = 0.19105030596256256
In grad_steps = 2431, loss = 0.17062050104141235
In grad_steps = 2432, loss = 0.14270268380641937
In grad_steps = 2433, loss = 0.511082649230957
In grad_steps = 2434, loss = 0.421980082988739
In grad_steps = 2435, loss = 0.15348273515701294
In grad_steps = 2436, loss = 0.05952974408864975
In grad_steps = 2437, loss = 0.05746559798717499
In grad_steps = 2438, loss = 0.03504134714603424
In grad_steps = 2439, loss = 0.0350203737616539
In grad_steps = 2440, loss = 0.7387338876724243
In grad_steps = 2441, loss = 0.2581600546836853
In grad_steps = 2442, loss = 0.16502094268798828
In grad_steps = 2443, loss = 0.037470899522304535
In grad_steps = 2444, loss = 0.7928314208984375
In grad_steps = 2445, loss = 0.27612027525901794
In grad_steps = 2446, loss = 0.07289572060108185
In grad_steps = 2447, loss = 0.11692652106285095
In grad_steps = 2448, loss = 1.2765171527862549
In grad_steps = 2449, loss = 1.2178972959518433
In grad_steps = 2450, loss = 0.003815713804215193
In grad_steps = 2451, loss = 1.3726540803909302
In grad_steps = 2452, loss = 0.17491208016872406
In grad_steps = 2453, loss = 0.8793463706970215
In grad_steps = 2454, loss = 0.12380345165729523
In grad_steps = 2455, loss = 0.20820289850234985
In grad_steps = 2456, loss = 0.24354760348796844
In grad_steps = 2457, loss = 0.06127581000328064
In grad_steps = 2458, loss = 0.12015213072299957
In grad_steps = 2459, loss = 0.34190094470977783
In grad_steps = 2460, loss = 0.05905976891517639
In grad_steps = 2461, loss = 0.21937337517738342
In grad_steps = 2462, loss = 0.45213815569877625
In grad_steps = 2463, loss = 0.7485784292221069
In grad_steps = 2464, loss = 0.23605361580848694
In grad_steps = 2465, loss = 0.21317052841186523
In grad_steps = 2466, loss = 0.5171166062355042
In grad_steps = 2467, loss = 0.23744431138038635
In grad_steps = 2468, loss = 0.7199510335922241
In grad_steps = 2469, loss = 1.004083275794983
In grad_steps = 2470, loss = 0.08085603266954422
In grad_steps = 2471, loss = 0.2811204791069031
In grad_steps = 2472, loss = 0.2889437675476074
In grad_steps = 2473, loss = 0.8779283165931702
In grad_steps = 2474, loss = 0.1069275289773941
In grad_steps = 2475, loss = 0.10105947405099869
In grad_steps = 2476, loss = 0.3171805739402771
In grad_steps = 2477, loss = 0.07787856459617615
In grad_steps = 2478, loss = 0.04899995028972626
In grad_steps = 2479, loss = 0.1214752346277237
In grad_steps = 2480, loss = 0.21076203882694244
In grad_steps = 2481, loss = 0.30139774084091187
In grad_steps = 2482, loss = 0.49360227584838867
In grad_steps = 2483, loss = 0.09921593964099884
In grad_steps = 2484, loss = 0.5201681852340698
In grad_steps = 2485, loss = 0.22873185575008392
In grad_steps = 2486, loss = 0.5199068784713745
In grad_steps = 2487, loss = 0.05644374340772629
In grad_steps = 2488, loss = 0.08991372585296631
In grad_steps = 2489, loss = 0.04273372143507004
In grad_steps = 2490, loss = 0.11954744905233383
In grad_steps = 2491, loss = 0.037306685000658035
In grad_steps = 2492, loss = 0.2751765847206116
In grad_steps = 2493, loss = 0.057376641780138016
In grad_steps = 2494, loss = 0.03457126393914223
In grad_steps = 2495, loss = 0.22718951106071472
In grad_steps = 2496, loss = 0.13689793646335602
In grad_steps = 2497, loss = 0.6137646436691284
In grad_steps = 2498, loss = 0.04595573991537094
In grad_steps = 2499, loss = 0.13917875289916992
In grad_steps = 2500, loss = 0.05434686318039894
In grad_steps = 2501, loss = 0.034766845405101776
In grad_steps = 2502, loss = 0.03543072193861008
In grad_steps = 2503, loss = 0.01465548574924469
In grad_steps = 2504, loss = 0.025357836857438087
In grad_steps = 2505, loss = 0.11710044741630554
In grad_steps = 2506, loss = 0.009047459810972214
In grad_steps = 2507, loss = 0.06680551171302795
In grad_steps = 2508, loss = 0.0382121317088604
In grad_steps = 2509, loss = 0.8768731355667114
In grad_steps = 2510, loss = 0.004060500301420689
In grad_steps = 2511, loss = 0.1481868326663971
In grad_steps = 2512, loss = 0.0015820072731003165
In grad_steps = 2513, loss = 0.004536984022706747
In grad_steps = 2514, loss = 0.03858085349202156
In grad_steps = 2515, loss = 0.4370805025100708
In grad_steps = 2516, loss = 0.12506383657455444
In grad_steps = 2517, loss = 0.024962428957223892
In grad_steps = 2518, loss = 0.006401818245649338
In grad_steps = 2519, loss = 0.3305063843727112
In grad_steps = 2520, loss = 0.5797206163406372
In grad_steps = 2521, loss = 0.049363430589437485
In grad_steps = 2522, loss = 1.425248146057129
In grad_steps = 2523, loss = 0.025555912405252457
In grad_steps = 2524, loss = 0.13430698215961456
In grad_steps = 2525, loss = 0.34183230996131897
In grad_steps = 2526, loss = 0.6244774460792542
In grad_steps = 2527, loss = 1.126325249671936
In grad_steps = 2528, loss = 0.059986043721437454
In grad_steps = 2529, loss = 0.12311023473739624
In grad_steps = 2530, loss = 0.4536745250225067
In grad_steps = 2531, loss = 0.16347844898700714
In grad_steps = 2532, loss = 0.25599542260169983
In grad_steps = 2533, loss = 0.14057184755802155
In grad_steps = 2534, loss = 0.07876977324485779
In grad_steps = 2535, loss = 0.09596338123083115
In grad_steps = 2536, loss = 0.2745252847671509
In grad_steps = 2537, loss = 0.050818271934986115
In grad_steps = 2538, loss = 0.14575548470020294
In grad_steps = 2539, loss = 0.1486719846725464
In grad_steps = 2540, loss = 0.7366605401039124
In grad_steps = 2541, loss = 0.1297653764486313
In grad_steps = 2542, loss = 0.07280685752630234
In grad_steps = 2543, loss = 0.1095244288444519
In grad_steps = 2544, loss = 0.12800443172454834
In grad_steps = 2545, loss = 0.14569057524204254
In grad_steps = 2546, loss = 0.07208991795778275
In grad_steps = 2547, loss = 0.21469636261463165
In grad_steps = 2548, loss = 0.4044697880744934
In grad_steps = 2549, loss = 0.7315908670425415
In grad_steps = 2550, loss = 0.8791871070861816
In grad_steps = 2551, loss = 0.2962617874145508
In grad_steps = 2552, loss = 0.04511772841215134
In grad_steps = 2553, loss = 0.06726144999265671
In grad_steps = 2554, loss = 0.029298268258571625
In grad_steps = 2555, loss = 1.2683663368225098
In grad_steps = 2556, loss = 0.14701056480407715
In grad_steps = 2557, loss = 0.6091166734695435
In grad_steps = 2558, loss = 0.02177937887609005
In grad_steps = 2559, loss = 0.18109798431396484
In grad_steps = 2560, loss = 0.15467539429664612
In grad_steps = 2561, loss = 0.23410791158676147
In grad_steps = 2562, loss = 0.8292014598846436
In grad_steps = 2563, loss = 0.09399403631687164
In grad_steps = 2564, loss = 0.17179244756698608
In grad_steps = 2565, loss = 0.11954627931118011
In grad_steps = 2566, loss = 0.053711771965026855
In grad_steps = 2567, loss = 0.14377613365650177
In grad_steps = 2568, loss = 0.0784764438867569
In grad_steps = 2569, loss = 0.06751884520053864
In grad_steps = 2570, loss = 0.14623460173606873
In grad_steps = 2571, loss = 0.10546597838401794
In grad_steps = 2572, loss = 0.06348104774951935
In grad_steps = 2573, loss = 0.5199941992759705
In grad_steps = 2574, loss = 0.3117974102497101
In grad_steps = 2575, loss = 0.6536091566085815
In grad_steps = 2576, loss = 0.6795322895050049
In grad_steps = 2577, loss = 0.10967452824115753
In grad_steps = 2578, loss = 0.22647593915462494
In grad_steps = 2579, loss = 0.2800675630569458
In grad_steps = 2580, loss = 0.28230020403862
In grad_steps = 2581, loss = 0.7174612283706665
In grad_steps = 2582, loss = 0.03455568477511406
In grad_steps = 2583, loss = 0.06160099804401398
In grad_steps = 2584, loss = 0.033239178359508514
In grad_steps = 2585, loss = 1.21160089969635
In grad_steps = 2586, loss = 0.9352426528930664
In grad_steps = 2587, loss = 0.055332884192466736
In grad_steps = 2588, loss = 1.7438256740570068
In grad_steps = 2589, loss = 0.11042645573616028
In grad_steps = 2590, loss = 0.4558546543121338
In grad_steps = 2591, loss = 0.7457537055015564
In grad_steps = 2592, loss = 0.26999059319496155
In grad_steps = 2593, loss = 0.18628081679344177
In grad_steps = 2594, loss = 0.3460659682750702
In grad_steps = 2595, loss = 0.136257141828537
In grad_steps = 2596, loss = 0.14379741251468658
In grad_steps = 2597, loss = 0.3043100833892822
In grad_steps = 2598, loss = 0.25758153200149536
In grad_steps = 2599, loss = 0.4617033004760742
In grad_steps = 2600, loss = 0.38437992334365845
In grad_steps = 2601, loss = 0.41223785281181335
In grad_steps = 2602, loss = 0.5955632925033569
In grad_steps = 2603, loss = 0.38572704792022705
In grad_steps = 2604, loss = 0.31819120049476624
In grad_steps = 2605, loss = 0.220754012465477
In grad_steps = 2606, loss = 0.21981944143772125
In grad_steps = 2607, loss = 0.08222289383411407
In grad_steps = 2608, loss = 0.12216390669345856
In grad_steps = 2609, loss = 0.5642936825752258
In grad_steps = 2610, loss = 0.4410477578639984
In grad_steps = 2611, loss = 0.06442251801490784
In grad_steps = 2612, loss = 0.26013562083244324
In grad_steps = 2613, loss = 0.12742121517658234
In grad_steps = 2614, loss = 0.14234569668769836
In grad_steps = 2615, loss = 0.0947803258895874
In grad_steps = 2616, loss = 0.09914129972457886
In grad_steps = 2617, loss = 0.2836356461048126
In grad_steps = 2618, loss = 0.23521433770656586
In grad_steps = 2619, loss = 0.09315607696771622
In grad_steps = 2620, loss = 0.7728962302207947
In grad_steps = 2621, loss = 0.12240983545780182
In grad_steps = 2622, loss = 0.12484726309776306
In grad_steps = 2623, loss = 0.309724897146225
In grad_steps = 2624, loss = 0.6337643265724182
In grad_steps = 2625, loss = 0.027388950809836388
In grad_steps = 2626, loss = 0.025241924449801445
In grad_steps = 2627, loss = 0.1604793667793274
In grad_steps = 2628, loss = 0.06238254904747009
In grad_steps = 2629, loss = 0.9182018637657166
In grad_steps = 2630, loss = 0.545328676700592
In grad_steps = 2631, loss = 1.0900988578796387
In grad_steps = 2632, loss = 0.06282764673233032
In grad_steps = 2633, loss = 0.2317039966583252
In grad_steps = 2634, loss = 0.05388079956173897
In grad_steps = 2635, loss = 0.7898377180099487
In grad_steps = 2636, loss = 0.15964911878108978
In grad_steps = 2637, loss = 0.08986559510231018
In grad_steps = 2638, loss = 0.49236083030700684
In grad_steps = 2639, loss = 0.21220223605632782
In grad_steps = 2640, loss = 0.13096646964550018
In grad_steps = 2641, loss = 0.22953730821609497
In grad_steps = 2642, loss = 0.1568537950515747
In grad_steps = 2643, loss = 0.2071755826473236
In grad_steps = 2644, loss = 0.33850812911987305
In grad_steps = 2645, loss = 0.17784276604652405
In grad_steps = 2646, loss = 0.655036449432373
In grad_steps = 2647, loss = 0.39971432089805603
In grad_steps = 2648, loss = 0.6816138625144958
In grad_steps = 2649, loss = 0.3951752781867981
In grad_steps = 2650, loss = 0.37116697430610657
In grad_steps = 2651, loss = 0.13826420903205872
In grad_steps = 2652, loss = 0.6795355081558228
In grad_steps = 2653, loss = 0.41914114356040955
In grad_steps = 2654, loss = 0.12479090690612793
In grad_steps = 2655, loss = 0.4766722023487091
In grad_steps = 2656, loss = 0.04159065708518028
In grad_steps = 2657, loss = 0.5510697364807129
In grad_steps = 2658, loss = 1.3467153310775757
In grad_steps = 2659, loss = 0.4350874423980713
In grad_steps = 2660, loss = 0.19363142549991608
In grad_steps = 2661, loss = 0.4707590937614441
In grad_steps = 2662, loss = 0.09791862219572067
In grad_steps = 2663, loss = 0.33928370475769043
In grad_steps = 2664, loss = 0.2842285931110382
In grad_steps = 2665, loss = 0.44190818071365356
In grad_steps = 2666, loss = 0.16397957503795624
In grad_steps = 2667, loss = 0.11539527028799057
In grad_steps = 2668, loss = 0.47339820861816406
In grad_steps = 2669, loss = 0.12564444541931152
In grad_steps = 2670, loss = 0.15786275267601013
In grad_steps = 2671, loss = 0.25774458050727844
In grad_steps = 2672, loss = 0.07093057781457901
In grad_steps = 2673, loss = 0.10274969041347504
In grad_steps = 2674, loss = 0.04978224262595177
In grad_steps = 2675, loss = 0.049005135893821716
In grad_steps = 2676, loss = 0.12578348815441132
In grad_steps = 2677, loss = 0.024898812174797058
In grad_steps = 2678, loss = 0.031936973333358765
In grad_steps = 2679, loss = 0.2980846166610718
In grad_steps = 2680, loss = 0.04559553042054176
In grad_steps = 2681, loss = 0.09816572070121765
In grad_steps = 2682, loss = 0.6941727995872498
In grad_steps = 2683, loss = 0.02810278907418251
In grad_steps = 2684, loss = 0.08808918297290802
In grad_steps = 2685, loss = 0.20973309874534607
In grad_steps = 2686, loss = 0.17326614260673523
In grad_steps = 2687, loss = 0.09979385882616043
In grad_steps = 2688, loss = 0.3183578848838806
In grad_steps = 2689, loss = 0.0671200379729271
In grad_steps = 2690, loss = 0.5552486777305603
In grad_steps = 2691, loss = 0.04897715523838997
In grad_steps = 2692, loss = 0.2715536653995514
In grad_steps = 2693, loss = 1.3148421049118042
In grad_steps = 2694, loss = 0.4152756929397583
In grad_steps = 2695, loss = 0.07536827772855759
In grad_steps = 2696, loss = 0.017529010772705078
In grad_steps = 2697, loss = 0.19774174690246582
In grad_steps = 2698, loss = 0.15182437002658844
In grad_steps = 2699, loss = 0.026419654488563538
In grad_steps = 2700, loss = 0.9881581664085388
In grad_steps = 2701, loss = 0.03541148081421852
In grad_steps = 2702, loss = 0.2267145961523056
In grad_steps = 2703, loss = 1.0734156370162964
In grad_steps = 2704, loss = 0.16701997816562653
In grad_steps = 2705, loss = 0.21057343482971191
In grad_steps = 2706, loss = 0.06001559644937515
In grad_steps = 2707, loss = 0.3652050793170929
In grad_steps = 2708, loss = 0.26537007093429565
In grad_steps = 2709, loss = 0.48253491520881653
In grad_steps = 2710, loss = 0.6616653203964233
In grad_steps = 2711, loss = 0.13167715072631836
In grad_steps = 2712, loss = 0.1461765468120575
In grad_steps = 2713, loss = 0.9153003096580505
In grad_steps = 2714, loss = 0.6003817319869995
In grad_steps = 2715, loss = 0.4289548993110657
In grad_steps = 2716, loss = 0.06833459436893463
In grad_steps = 2717, loss = 0.7383493781089783
In grad_steps = 2718, loss = 0.5699582695960999
In grad_steps = 2719, loss = 0.11173291504383087
In grad_steps = 2720, loss = 0.4572659134864807
In grad_steps = 2721, loss = 0.4867915213108063
In grad_steps = 2722, loss = 0.15115638077259064
In grad_steps = 2723, loss = 0.27348896861076355
In grad_steps = 2724, loss = 0.5314069390296936
In grad_steps = 2725, loss = 0.10091312229633331
In grad_steps = 2726, loss = 0.7567556500434875
In grad_steps = 2727, loss = 0.035829298198223114
In grad_steps = 2728, loss = 0.8915172219276428
In grad_steps = 2729, loss = 0.4901856482028961
In grad_steps = 2730, loss = 0.0870559960603714
In grad_steps = 2731, loss = 0.09534866362810135
In grad_steps = 2732, loss = 0.13700172305107117
In grad_steps = 2733, loss = 0.39558595418930054
In grad_steps = 2734, loss = 0.123520128428936
In grad_steps = 2735, loss = 1.0038859844207764
In grad_steps = 2736, loss = 0.07561264932155609
In grad_steps = 2737, loss = 0.2520817518234253
In grad_steps = 2738, loss = 0.18894052505493164
In grad_steps = 2739, loss = 0.1378708779811859
In grad_steps = 2740, loss = 0.0816812515258789
In grad_steps = 2741, loss = 0.11294116079807281
In grad_steps = 2742, loss = 0.23549340665340424
In grad_steps = 2743, loss = 0.12417872250080109
In grad_steps = 2744, loss = 0.1789512038230896
In grad_steps = 2745, loss = 0.021614085882902145
In grad_steps = 2746, loss = 1.019982933998108
In grad_steps = 2747, loss = 0.43951553106307983
In grad_steps = 2748, loss = 0.1420222669839859
In grad_steps = 2749, loss = 0.05498948693275452
In grad_steps = 2750, loss = 0.09157329797744751
In grad_steps = 2751, loss = 0.056773822754621506
In grad_steps = 2752, loss = 0.06718491017818451
In grad_steps = 2753, loss = 0.4697348475456238
In grad_steps = 2754, loss = 0.39642471075057983
In grad_steps = 2755, loss = 0.025350818410515785
In grad_steps = 2756, loss = 0.0737064778804779
In grad_steps = 2757, loss = 0.06402119249105453
In grad_steps = 2758, loss = 1.1065661907196045
In grad_steps = 2759, loss = 0.07707693427801132
In grad_steps = 2760, loss = 0.042758941650390625
In grad_steps = 2761, loss = 0.10121744126081467
In grad_steps = 2762, loss = 0.1752167046070099
In grad_steps = 2763, loss = 0.09469158202409744
In grad_steps = 2764, loss = 0.022569186985492706
In grad_steps = 2765, loss = 0.09912997484207153
In grad_steps = 2766, loss = 1.4357601404190063
In grad_steps = 2767, loss = 0.0548059307038784
In grad_steps = 2768, loss = 0.6748798489570618
In grad_steps = 2769, loss = 0.04566577449440956
In grad_steps = 2770, loss = 0.10888507217168808
In grad_steps = 2771, loss = 0.03917032480239868
In grad_steps = 2772, loss = 0.31169939041137695
In grad_steps = 2773, loss = 1.659517765045166
In grad_steps = 2774, loss = 0.6209471225738525
In grad_steps = 2775, loss = 0.0610482357442379
In grad_steps = 2776, loss = 0.14956781268119812
In grad_steps = 2777, loss = 0.022112682461738586
In grad_steps = 2778, loss = 0.1600077599287033
In grad_steps = 2779, loss = 0.11322873830795288
In grad_steps = 2780, loss = 0.15071779489517212
In grad_steps = 2781, loss = 0.09323626756668091
In grad_steps = 2782, loss = 0.09864632785320282
In grad_steps = 2783, loss = 0.03892096132040024
In grad_steps = 2784, loss = 0.08941218256950378
In grad_steps = 2785, loss = 0.08315867185592651
In grad_steps = 2786, loss = 0.0760318860411644
In grad_steps = 2787, loss = 0.14960454404354095
In grad_steps = 2788, loss = 0.3497224748134613
In grad_steps = 2789, loss = 0.991065263748169
In grad_steps = 2790, loss = 0.45652762055397034
In grad_steps = 2791, loss = 0.17523065209388733
In grad_steps = 2792, loss = 0.06126411259174347
In grad_steps = 2793, loss = 0.15235987305641174
In grad_steps = 2794, loss = 0.15321117639541626
In grad_steps = 2795, loss = 0.2225218415260315
In grad_steps = 2796, loss = 0.20373177528381348
In grad_steps = 2797, loss = 0.11041735857725143
In grad_steps = 2798, loss = 0.01360427588224411
In grad_steps = 2799, loss = 0.09501496702432632
In grad_steps = 2800, loss = 0.09000849723815918
In grad_steps = 2801, loss = 0.031127842143177986
In grad_steps = 2802, loss = 0.13905256986618042
In grad_steps = 2803, loss = 1.0738085508346558
In grad_steps = 2804, loss = 0.13098812103271484
In grad_steps = 2805, loss = 0.45186275243759155
In grad_steps = 2806, loss = 0.017464395612478256
In grad_steps = 2807, loss = 0.8398146629333496
In grad_steps = 2808, loss = 0.020298074930906296
In grad_steps = 2809, loss = 0.03675924241542816
In grad_steps = 2810, loss = 0.05561656132340431
In grad_steps = 2811, loss = 0.07062371075153351
In grad_steps = 2812, loss = 0.031768057495355606
In grad_steps = 2813, loss = 0.018943222239613533
In grad_steps = 2814, loss = 0.05398249998688698
In grad_steps = 2815, loss = 0.14035466313362122
In grad_steps = 2816, loss = 0.10849496722221375
In grad_steps = 2817, loss = 0.01457106601446867
In grad_steps = 2818, loss = 0.22532261908054352
In grad_steps = 2819, loss = 0.374184250831604
In grad_steps = 2820, loss = 0.020865334197878838
In grad_steps = 2821, loss = 0.4117949604988098
In grad_steps = 2822, loss = 0.11334486305713654
In grad_steps = 2823, loss = 0.047757118940353394
In grad_steps = 2824, loss = 0.33186206221580505
In grad_steps = 2825, loss = 0.6023300290107727
In grad_steps = 2826, loss = 0.05760490149259567
In grad_steps = 2827, loss = 0.03657006472349167
In grad_steps = 2828, loss = 0.018553562462329865
In grad_steps = 2829, loss = 0.04242268204689026
In grad_steps = 2830, loss = 0.4608114957809448
In grad_steps = 2831, loss = 0.5892202258110046
In grad_steps = 2832, loss = 0.08076313138008118
In grad_steps = 2833, loss = 0.08131866157054901
In grad_steps = 2834, loss = 1.0217692852020264
In grad_steps = 2835, loss = 0.7068163156509399
In grad_steps = 2836, loss = 0.36336085200309753
In grad_steps = 2837, loss = 0.1865670382976532
In grad_steps = 2838, loss = 0.6952864527702332
In grad_steps = 2839, loss = 0.4815027117729187
In grad_steps = 2840, loss = 0.06995002925395966
In grad_steps = 2841, loss = 0.09430092573165894
In grad_steps = 2842, loss = 0.2178889513015747
In grad_steps = 2843, loss = 0.2056114226579666
In grad_steps = 2844, loss = 0.12498008459806442
In grad_steps = 2845, loss = 0.16151955723762512
In grad_steps = 2846, loss = 0.13269923627376556
In grad_steps = 2847, loss = 0.1140228882431984
In grad_steps = 2848, loss = 0.11497686803340912
In grad_steps = 2849, loss = 0.538340151309967
In grad_steps = 2850, loss = 0.03586861863732338
In grad_steps = 2851, loss = 0.1671733409166336
In grad_steps = 2852, loss = 0.08920574188232422
In grad_steps = 2853, loss = 0.10162469744682312
In grad_steps = 2854, loss = 0.02299080416560173
In grad_steps = 2855, loss = 0.018999051302671432
In grad_steps = 2856, loss = 0.02786770649254322
In grad_steps = 2857, loss = 0.012287329882383347
In grad_steps = 2858, loss = 0.09742572158575058
In grad_steps = 2859, loss = 1.1299662590026855
In grad_steps = 2860, loss = 0.0395454540848732
In grad_steps = 2861, loss = 0.07026646286249161
In grad_steps = 2862, loss = 0.1697053760290146
In grad_steps = 2863, loss = 0.00857292115688324
In grad_steps = 2864, loss = 0.00972627941519022
In grad_steps = 2865, loss = 0.0200443547219038
In grad_steps = 2866, loss = 0.02656463161110878
In grad_steps = 2867, loss = 0.10967643558979034
In grad_steps = 2868, loss = 0.014466914348304272
In grad_steps = 2869, loss = 0.012131856754422188
In grad_steps = 2870, loss = 0.07617563009262085
In grad_steps = 2871, loss = 0.03139470890164375
In grad_steps = 2872, loss = 0.11113930493593216
In grad_steps = 2873, loss = 0.012200241908431053
In grad_steps = 2874, loss = 0.7559970617294312
In grad_steps = 2875, loss = 0.07071837037801743
In grad_steps = 2876, loss = 0.28273260593414307
In grad_steps = 2877, loss = 0.012849328108131886
In grad_steps = 2878, loss = 0.21170593798160553
In grad_steps = 2879, loss = 0.02165006287395954
In grad_steps = 2880, loss = 0.0398409441113472
In grad_steps = 2881, loss = 0.4281265437602997
In grad_steps = 2882, loss = 0.018239673227071762
In grad_steps = 2883, loss = 0.7937732338905334
In grad_steps = 2884, loss = 0.007114505395293236
In grad_steps = 2885, loss = 0.1467267870903015
In grad_steps = 2886, loss = 0.3562621474266052
In grad_steps = 2887, loss = 0.8499183058738708
In grad_steps = 2888, loss = 0.42732512950897217
In grad_steps = 2889, loss = 0.4164539575576782
In grad_steps = 2890, loss = 0.16211757063865662
In grad_steps = 2891, loss = 0.7402216196060181
In grad_steps = 2892, loss = 0.09340604394674301
In grad_steps = 2893, loss = 0.034391626715660095
In grad_steps = 2894, loss = 0.026628518477082253
In grad_steps = 2895, loss = 0.36004722118377686
In grad_steps = 2896, loss = 0.926070511341095
In grad_steps = 2897, loss = 0.7820647954940796
In grad_steps = 2898, loss = 0.46312248706817627
In grad_steps = 2899, loss = 0.5741217136383057
In grad_steps = 2900, loss = 0.415061354637146
In grad_steps = 2901, loss = 0.2328266203403473
In grad_steps = 2902, loss = 0.1670236736536026
In grad_steps = 2903, loss = 0.14241451025009155
In grad_steps = 2904, loss = 0.11831575632095337
In grad_steps = 2905, loss = 0.2709212005138397
In grad_steps = 2906, loss = 0.7081474661827087
In grad_steps = 2907, loss = 0.16348370909690857
In grad_steps = 2908, loss = 0.2978713810443878
In grad_steps = 2909, loss = 0.14325320720672607
In grad_steps = 2910, loss = 0.33804047107696533
In grad_steps = 2911, loss = 0.24186821281909943
In grad_steps = 2912, loss = 0.2606178820133209
In grad_steps = 2913, loss = 0.2029261589050293
In grad_steps = 2914, loss = 0.8172157406806946
In grad_steps = 2915, loss = 0.11240941286087036
In grad_steps = 2916, loss = 0.2624639868736267
In grad_steps = 2917, loss = 0.05813740938901901
In grad_steps = 2918, loss = 0.26374852657318115
In grad_steps = 2919, loss = 0.08775962889194489
In grad_steps = 2920, loss = 0.10765309631824493
In grad_steps = 2921, loss = 0.13012908399105072
In grad_steps = 2922, loss = 0.19591650366783142
In grad_steps = 2923, loss = 0.5694466829299927
In grad_steps = 2924, loss = 0.018696341663599014
In grad_steps = 2925, loss = 0.6657655239105225
In grad_steps = 2926, loss = 0.08030888438224792
In grad_steps = 2927, loss = 0.5014777183532715
In grad_steps = 2928, loss = 0.03600580617785454
In grad_steps = 2929, loss = 0.02429889515042305
In grad_steps = 2930, loss = 0.022431477904319763
In grad_steps = 2931, loss = 0.9925592541694641
In grad_steps = 2932, loss = 1.101286768913269
In grad_steps = 2933, loss = 0.01573149301111698
In grad_steps = 2934, loss = 0.6566855907440186
In grad_steps = 2935, loss = 0.05729762837290764
In grad_steps = 2936, loss = 0.5273903012275696
In grad_steps = 2937, loss = 0.3651432394981384
In grad_steps = 2938, loss = 0.4350089728832245
In grad_steps = 2939, loss = 0.5581847429275513
In grad_steps = 2940, loss = 0.7822192907333374
In grad_steps = 2941, loss = 0.09932524710893631
In grad_steps = 2942, loss = 0.15016821026802063
In grad_steps = 2943, loss = 0.17970740795135498
In grad_steps = 2944, loss = 0.5752890706062317
In grad_steps = 2945, loss = 0.3846314251422882
In grad_steps = 2946, loss = 0.3630146384239197
In grad_steps = 2947, loss = 0.14414836466312408
In grad_steps = 2948, loss = 0.17832738161087036
In grad_steps = 2949, loss = 0.17554783821105957
In grad_steps = 2950, loss = 0.23419904708862305
In grad_steps = 2951, loss = 0.649241030216217
In grad_steps = 2952, loss = 0.39835530519485474
In grad_steps = 2953, loss = 0.4951516091823578
In grad_steps = 2954, loss = 0.35942161083221436
In grad_steps = 2955, loss = 0.10781392455101013
In grad_steps = 2956, loss = 0.7612720727920532
In grad_steps = 2957, loss = 0.4775090515613556
In grad_steps = 2958, loss = 0.15637770295143127
In grad_steps = 2959, loss = 0.3863867521286011
In grad_steps = 2960, loss = 0.10817276686429977
In grad_steps = 2961, loss = 0.045050181448459625
In grad_steps = 2962, loss = 0.2687603831291199
In grad_steps = 2963, loss = 0.15532375872135162
In grad_steps = 2964, loss = 0.32003024220466614
In grad_steps = 2965, loss = 0.02691791206598282
In grad_steps = 2966, loss = 0.2033013105392456
In grad_steps = 2967, loss = 0.47150880098342896
In grad_steps = 2968, loss = 0.8074936866760254
In grad_steps = 2969, loss = 0.08688852936029434
In grad_steps = 2970, loss = 0.06751453876495361
In grad_steps = 2971, loss = 0.07045182585716248
In grad_steps = 2972, loss = 0.3206694722175598
In grad_steps = 2973, loss = 0.42129385471343994
In grad_steps = 2974, loss = 0.9710894823074341
In grad_steps = 2975, loss = 0.335306316614151
In grad_steps = 2976, loss = 0.11180170625448227
In grad_steps = 2977, loss = 0.06795285642147064
In grad_steps = 2978, loss = 0.07195167243480682
In grad_steps = 2979, loss = 0.22091858088970184
In grad_steps = 2980, loss = 0.03206073120236397
In grad_steps = 2981, loss = 0.19340239465236664
In grad_steps = 2982, loss = 0.02326083928346634
In grad_steps = 2983, loss = 0.5920864343643188
In grad_steps = 2984, loss = 0.12346340715885162
In grad_steps = 2985, loss = 0.2190876305103302
In grad_steps = 2986, loss = 0.3716425895690918
In grad_steps = 2987, loss = 0.1588745415210724
In grad_steps = 2988, loss = 0.9359472990036011
In grad_steps = 2989, loss = 0.1952190399169922
In grad_steps = 2990, loss = 0.2003294974565506
In grad_steps = 2991, loss = 0.11246616393327713
In grad_steps = 2992, loss = 0.1192660927772522
In grad_steps = 2993, loss = 0.13379088044166565
In grad_steps = 2994, loss = 0.2221733182668686
In grad_steps = 2995, loss = 0.023975439369678497
In grad_steps = 2996, loss = 1.063509225845337
In grad_steps = 2997, loss = 0.16952799260616302
In grad_steps = 2998, loss = 0.1466631442308426
In grad_steps = 2999, loss = 0.09739437699317932
In grad_steps = 3000, loss = 0.3121652603149414
In grad_steps = 3001, loss = 0.06838646531105042
In grad_steps = 3002, loss = 0.51518315076828
In grad_steps = 3003, loss = 0.3832155764102936
In grad_steps = 3004, loss = 0.9509003162384033
In grad_steps = 3005, loss = 0.0164442490786314
In grad_steps = 3006, loss = 0.08585314452648163
In grad_steps = 3007, loss = 0.040983036160469055
In grad_steps = 3008, loss = 0.23130112886428833
In grad_steps = 3009, loss = 0.029693905264139175
In grad_steps = 3010, loss = 0.25174614787101746
In grad_steps = 3011, loss = 0.945979654788971
In grad_steps = 3012, loss = 1.018643856048584
In grad_steps = 3013, loss = 0.031313274055719376
In grad_steps = 3014, loss = 0.440073162317276
In grad_steps = 3015, loss = 0.08550660312175751
In grad_steps = 3016, loss = 0.15441547334194183
In grad_steps = 3017, loss = 0.16134321689605713
In grad_steps = 3018, loss = 0.3289743959903717
In grad_steps = 3019, loss = 0.3395933508872986
In grad_steps = 3020, loss = 0.3814393877983093
In grad_steps = 3021, loss = 0.09606530517339706
In grad_steps = 3022, loss = 0.37250369787216187
In grad_steps = 3023, loss = 0.03994472324848175
In grad_steps = 3024, loss = 0.0795891210436821
In grad_steps = 3025, loss = 0.44741585850715637
In grad_steps = 3026, loss = 0.1294526755809784
In grad_steps = 3027, loss = 0.26868534088134766
In grad_steps = 3028, loss = 0.06725439429283142
In grad_steps = 3029, loss = 0.15984609723091125
In grad_steps = 3030, loss = 0.1520429253578186
In grad_steps = 3031, loss = 0.9457210302352905
In grad_steps = 3032, loss = 1.0816655158996582
In grad_steps = 3033, loss = 0.9645040035247803
In grad_steps = 3034, loss = 0.866848886013031
In grad_steps = 3035, loss = 0.8657873868942261
In grad_steps = 3036, loss = 0.6192721724510193
In grad_steps = 3037, loss = 0.37759730219841003
In grad_steps = 3038, loss = 0.44482240080833435
In grad_steps = 3039, loss = 0.04304984211921692
In grad_steps = 3040, loss = 0.6048186421394348
In grad_steps = 3041, loss = 0.24646204710006714
In grad_steps = 3042, loss = 0.3402962386608124
In grad_steps = 3043, loss = 0.7290290594100952
In grad_steps = 3044, loss = 0.9732748866081238
In grad_steps = 3045, loss = 0.2231936901807785
In grad_steps = 3046, loss = 0.3639729619026184
In grad_steps = 3047, loss = 0.496695876121521
In grad_steps = 3048, loss = 0.12921859323978424
In grad_steps = 3049, loss = 0.5553607940673828
In grad_steps = 3050, loss = 0.07916881144046783
In grad_steps = 3051, loss = 0.29299628734588623
In grad_steps = 3052, loss = 0.019167473539710045
In grad_steps = 3053, loss = 0.04299735277891159
In grad_steps = 3054, loss = 0.3441539406776428
In grad_steps = 3055, loss = 0.07119165360927582
In grad_steps = 3056, loss = 0.07100460678339005
In grad_steps = 3057, loss = 0.39604052901268005
In grad_steps = 3058, loss = 0.3784055709838867
In grad_steps = 3059, loss = 0.04190536588430405
In grad_steps = 3060, loss = 0.01715759187936783
In grad_steps = 3061, loss = 0.09935133904218674
In grad_steps = 3062, loss = 1.1221438646316528
In grad_steps = 3063, loss = 0.00853186659514904
In grad_steps = 3064, loss = 0.013311163522303104
In grad_steps = 3065, loss = 0.4195787310600281
In grad_steps = 3066, loss = 0.17296156287193298
In grad_steps = 3067, loss = 0.10622313618659973
In grad_steps = 3068, loss = 0.5741814970970154
In grad_steps = 3069, loss = 0.0016412532422691584
In grad_steps = 3070, loss = 0.13773763179779053
In grad_steps = 3071, loss = 0.06719401478767395
In grad_steps = 3072, loss = 0.756269633769989
In grad_steps = 3073, loss = 2.294729232788086
In grad_steps = 3074, loss = 0.24225957691669464
In grad_steps = 3075, loss = 0.18680819869041443
In grad_steps = 3076, loss = 0.09679986536502838
In grad_steps = 3077, loss = 0.2483745515346527
In grad_steps = 3078, loss = 0.9693998098373413
In grad_steps = 3079, loss = 0.16461911797523499
In grad_steps = 3080, loss = 0.7022312879562378
In grad_steps = 3081, loss = 0.16971738636493683
In grad_steps = 3082, loss = 0.1031089723110199
In grad_steps = 3083, loss = 0.47491905093193054
In grad_steps = 3084, loss = 0.6884289383888245
In grad_steps = 3085, loss = 0.6868515610694885
In grad_steps = 3086, loss = 0.1313532143831253
In grad_steps = 3087, loss = 0.1643642634153366
In grad_steps = 3088, loss = 0.5347508788108826
In grad_steps = 3089, loss = 0.7983608841896057
In grad_steps = 3090, loss = 0.13426682353019714
In grad_steps = 3091, loss = 0.34774908423423767
In grad_steps = 3092, loss = 0.27344104647636414
In grad_steps = 3093, loss = 0.4113151729106903
In grad_steps = 3094, loss = 0.3066847324371338
In grad_steps = 3095, loss = 0.2228894978761673
In grad_steps = 3096, loss = 0.5729888081550598
In grad_steps = 3097, loss = 0.10466595739126205
In grad_steps = 3098, loss = 0.42469993233680725
In grad_steps = 3099, loss = 0.034428201615810394
In grad_steps = 3100, loss = 0.04862569645047188
In grad_steps = 3101, loss = 0.22467796504497528
In grad_steps = 3102, loss = 0.3835048973560333
In grad_steps = 3103, loss = 1.0799076557159424
In grad_steps = 3104, loss = 0.035691410303115845
In grad_steps = 3105, loss = 0.11817839741706848
In grad_steps = 3106, loss = 0.754206657409668
In grad_steps = 3107, loss = 0.04610633850097656
In grad_steps = 3108, loss = 0.05585499107837677
In grad_steps = 3109, loss = 0.6937867403030396
In grad_steps = 3110, loss = 0.5792410373687744
In grad_steps = 3111, loss = 0.2413577437400818
In grad_steps = 3112, loss = 0.14887431263923645
In grad_steps = 3113, loss = 1.1803468465805054
In grad_steps = 3114, loss = 0.11528664827346802
In grad_steps = 3115, loss = 0.20041897892951965
In grad_steps = 3116, loss = 0.0951952189207077
In grad_steps = 3117, loss = 0.10291033983230591
In grad_steps = 3118, loss = 0.04324262589216232
In grad_steps = 3119, loss = 0.4674910008907318
In grad_steps = 3120, loss = 0.530731737613678
In grad_steps = 3121, loss = 0.0859031230211258
In grad_steps = 3122, loss = 0.20792213082313538
In grad_steps = 3123, loss = 0.3425253629684448
In grad_steps = 3124, loss = 0.15767930448055267
In grad_steps = 3125, loss = 0.16641420125961304
In grad_steps = 3126, loss = 0.15552392601966858
In grad_steps = 3127, loss = 0.1521664261817932
In grad_steps = 3128, loss = 0.264495313167572
In grad_steps = 3129, loss = 0.324746698141098
In grad_steps = 3130, loss = 0.14461296796798706
In grad_steps = 3131, loss = 0.2991224527359009
In grad_steps = 3132, loss = 0.09854832291603088
In grad_steps = 3133, loss = 0.19292637705802917
In grad_steps = 3134, loss = 0.402210533618927
In grad_steps = 3135, loss = 0.1080017015337944
In grad_steps = 3136, loss = 0.020577795803546906
In grad_steps = 3137, loss = 0.3984009325504303
In grad_steps = 3138, loss = 0.8152474164962769
In grad_steps = 3139, loss = 0.07826074957847595
In grad_steps = 3140, loss = 0.4910719692707062
In grad_steps = 3141, loss = 0.45145183801651
In grad_steps = 3142, loss = 0.07689335197210312
In grad_steps = 3143, loss = 0.14601671695709229
In grad_steps = 3144, loss = 0.056889258325099945
In grad_steps = 3145, loss = 0.03373867645859718
In grad_steps = 3146, loss = 0.15130911767482758
In grad_steps = 3147, loss = 0.03324102982878685
In grad_steps = 3148, loss = 0.2123132348060608
In grad_steps = 3149, loss = 0.028352633118629456
In grad_steps = 3150, loss = 0.016319947317242622
In grad_steps = 3151, loss = 0.4746256470680237
In grad_steps = 3152, loss = 0.09795300662517548
In grad_steps = 3153, loss = 0.0718943327665329
In grad_steps = 3154, loss = 0.5525433421134949
In grad_steps = 3155, loss = 0.6981143355369568
In grad_steps = 3156, loss = 0.008378728292882442
In grad_steps = 3157, loss = 0.6255488395690918
In grad_steps = 3158, loss = 0.10592952370643616
In grad_steps = 3159, loss = 0.06616144627332687
In grad_steps = 3160, loss = 0.07270775735378265
In grad_steps = 3161, loss = 0.3948066234588623
In grad_steps = 3162, loss = 0.03791441395878792
In grad_steps = 3163, loss = 0.20124538242816925
In grad_steps = 3164, loss = 0.2368314564228058
In grad_steps = 3165, loss = 1.3751866817474365
In grad_steps = 3166, loss = 0.41175559163093567
In grad_steps = 3167, loss = 0.037517987191677094
In grad_steps = 3168, loss = 0.06176629662513733
In grad_steps = 3169, loss = 0.37616854906082153
In grad_steps = 3170, loss = 0.04212404787540436
In grad_steps = 3171, loss = 0.17834003269672394
In grad_steps = 3172, loss = 0.910156786441803
In grad_steps = 3173, loss = 0.01554325595498085
In grad_steps = 3174, loss = 0.05077991634607315
In grad_steps = 3175, loss = 0.05452007055282593
In grad_steps = 3176, loss = 0.1694938987493515
In grad_steps = 3177, loss = 0.12407508492469788
In grad_steps = 3178, loss = 0.11469249427318573
In grad_steps = 3179, loss = 0.26748567819595337
In grad_steps = 3180, loss = 0.12706100940704346
In grad_steps = 3181, loss = 0.24662697315216064
In grad_steps = 3182, loss = 0.08447764813899994
In grad_steps = 3183, loss = 0.02618701383471489
In grad_steps = 3184, loss = 0.07478426396846771
In grad_steps = 3185, loss = 0.07319118082523346
In grad_steps = 3186, loss = 0.9302864074707031
In grad_steps = 3187, loss = 0.0516417920589447
In grad_steps = 3188, loss = 0.05745849758386612
In grad_steps = 3189, loss = 0.8249443173408508
In grad_steps = 3190, loss = 0.013177474029362202
In grad_steps = 3191, loss = 0.2309737503528595
In grad_steps = 3192, loss = 0.12972530722618103
In grad_steps = 3193, loss = 0.0529458150267601
In grad_steps = 3194, loss = 0.9300949573516846
In grad_steps = 3195, loss = 0.09920862317085266
In grad_steps = 3196, loss = 0.24557165801525116
In grad_steps = 3197, loss = 0.02124485746026039
In grad_steps = 3198, loss = 0.10114195197820663
In grad_steps = 3199, loss = 1.7487437725067139
In grad_steps = 3200, loss = 0.06415271013975143
In grad_steps = 3201, loss = 0.241679385304451
In grad_steps = 3202, loss = 0.04518616572022438
In grad_steps = 3203, loss = 0.4629386365413666
In grad_steps = 3204, loss = 0.10766196250915527
In grad_steps = 3205, loss = 0.4381503462791443
In grad_steps = 3206, loss = 0.770341157913208
In grad_steps = 3207, loss = 0.17374593019485474
In grad_steps = 3208, loss = 0.19127869606018066
In grad_steps = 3209, loss = 0.182664692401886
In grad_steps = 3210, loss = 0.1823345422744751
In grad_steps = 3211, loss = 0.10342758893966675
In grad_steps = 3212, loss = 0.08019101619720459
In grad_steps = 3213, loss = 0.13733385503292084
In grad_steps = 3214, loss = 0.3609274923801422
In grad_steps = 3215, loss = 0.13796615600585938
In grad_steps = 3216, loss = 0.47476524114608765
In grad_steps = 3217, loss = 0.09990185499191284
In grad_steps = 3218, loss = 0.09969784319400787
In grad_steps = 3219, loss = 0.892706036567688
In grad_steps = 3220, loss = 0.4843516945838928
In grad_steps = 3221, loss = 0.11600816249847412
In grad_steps = 3222, loss = 0.023671209812164307
In grad_steps = 3223, loss = 0.06276144832372665
In grad_steps = 3224, loss = 0.21482449769973755
In grad_steps = 3225, loss = 0.5165921449661255
In grad_steps = 3226, loss = 0.5839414596557617
In grad_steps = 3227, loss = 0.16405999660491943
In grad_steps = 3228, loss = 0.34269019961357117
In grad_steps = 3229, loss = 0.07628358155488968
In grad_steps = 3230, loss = 0.03516045957803726
In grad_steps = 3231, loss = 0.5270114541053772
In grad_steps = 3232, loss = 0.02008580043911934
In grad_steps = 3233, loss = 0.058170124888420105
In grad_steps = 3234, loss = 0.11501311510801315
In grad_steps = 3235, loss = 1.0217350721359253
In grad_steps = 3236, loss = 0.6476648449897766
In grad_steps = 3237, loss = 0.9237686395645142
In grad_steps = 3238, loss = 0.32064852118492126
In grad_steps = 3239, loss = 0.25318193435668945
In grad_steps = 3240, loss = 0.09424727410078049
In grad_steps = 3241, loss = 0.4289748966693878
In grad_steps = 3242, loss = 0.07302381098270416
In grad_steps = 3243, loss = 0.07138295471668243
In grad_steps = 3244, loss = 0.07086293399333954
In grad_steps = 3245, loss = 0.1270217001438141
In grad_steps = 3246, loss = 0.11709069460630417
In grad_steps = 3247, loss = 0.2167220413684845
In grad_steps = 3248, loss = 0.6910973787307739
In grad_steps = 3249, loss = 0.1332600712776184
In grad_steps = 3250, loss = 0.6709956526756287
In grad_steps = 3251, loss = 0.14667628705501556
In grad_steps = 3252, loss = 0.9713459014892578
In grad_steps = 3253, loss = 0.10594790428876877
In grad_steps = 3254, loss = 0.046353891491889954
In grad_steps = 3255, loss = 0.27270403504371643
In grad_steps = 3256, loss = 0.0819288045167923
In grad_steps = 3257, loss = 0.06673478335142136
In grad_steps = 3258, loss = 0.9063980579376221
In grad_steps = 3259, loss = 0.830786406993866
In grad_steps = 3260, loss = 0.11734537035226822
In grad_steps = 3261, loss = 0.055194467306137085
In grad_steps = 3262, loss = 0.31947532296180725
In grad_steps = 3263, loss = 0.22890470921993256
In grad_steps = 3264, loss = 0.39351513981819153
In grad_steps = 3265, loss = 0.026313578709959984
In grad_steps = 3266, loss = 0.6689310669898987
In grad_steps = 3267, loss = 0.3603711724281311
In grad_steps = 3268, loss = 0.10252645611763
In grad_steps = 3269, loss = 1.1666322946548462
In grad_steps = 3270, loss = 0.02504659816622734
In grad_steps = 3271, loss = 0.0822857990860939
In grad_steps = 3272, loss = 0.07829906046390533
In grad_steps = 3273, loss = 0.09112843871116638
In grad_steps = 3274, loss = 0.16473466157913208
In grad_steps = 3275, loss = 0.15534354746341705
In grad_steps = 3276, loss = 0.21505951881408691
In grad_steps = 3277, loss = 1.1422349214553833
In grad_steps = 3278, loss = 0.037036698311567307
In grad_steps = 3279, loss = 0.06511221826076508
In grad_steps = 3280, loss = 0.07930294424295425
In grad_steps = 3281, loss = 0.18904682993888855
In grad_steps = 3282, loss = 0.40147095918655396
In grad_steps = 3283, loss = 0.19517557322978973
In grad_steps = 3284, loss = 0.07273153215646744
In grad_steps = 3285, loss = 0.01751192845404148
In grad_steps = 3286, loss = 0.5039985179901123
In grad_steps = 3287, loss = 0.07296271622180939
In grad_steps = 3288, loss = 0.06505556404590607
In grad_steps = 3289, loss = 0.3238518238067627
In grad_steps = 3290, loss = 0.36770063638687134
In grad_steps = 3291, loss = 0.04211266711354256
In grad_steps = 3292, loss = 0.39243972301483154
In grad_steps = 3293, loss = 0.022325923666357994
In grad_steps = 3294, loss = 0.050350289791822433
In grad_steps = 3295, loss = 0.023121006786823273
In grad_steps = 3296, loss = 0.09686705470085144
In grad_steps = 3297, loss = 0.6585572957992554
In grad_steps = 3298, loss = 0.7512235641479492
In grad_steps = 3299, loss = 0.055328719317913055
In grad_steps = 3300, loss = 0.23073339462280273
In grad_steps = 3301, loss = 0.480225145816803
In grad_steps = 3302, loss = 0.4952336549758911
In grad_steps = 3303, loss = 0.06635144352912903
In grad_steps = 3304, loss = 0.252063512802124
In grad_steps = 3305, loss = 0.15834318101406097
In grad_steps = 3306, loss = 0.14273405075073242
In grad_steps = 3307, loss = 0.05652203783392906
In grad_steps = 3308, loss = 0.14777809381484985
In grad_steps = 3309, loss = 1.108405351638794
In grad_steps = 3310, loss = 0.4491238594055176
In grad_steps = 3311, loss = 1.2333016395568848
In grad_steps = 3312, loss = 0.06395190954208374
In grad_steps = 3313, loss = 0.20462355017662048
In grad_steps = 3314, loss = 1.3944015502929688
In grad_steps = 3315, loss = 0.7334568500518799
In grad_steps = 3316, loss = 0.07761590927839279
In grad_steps = 3317, loss = 0.047868918627500534
In grad_steps = 3318, loss = 0.14116346836090088
In grad_steps = 3319, loss = 0.37925487756729126
In grad_steps = 3320, loss = 0.5339972376823425
In grad_steps = 3321, loss = 0.13600341975688934
In grad_steps = 3322, loss = 0.07258503139019012
In grad_steps = 3323, loss = 0.04077538102865219
In grad_steps = 3324, loss = 0.23337937891483307
In grad_steps = 3325, loss = 0.20189693570137024
In grad_steps = 3326, loss = 0.07440463453531265
In grad_steps = 3327, loss = 0.20761950314044952
In grad_steps = 3328, loss = 0.08369079232215881
In grad_steps = 3329, loss = 0.33325326442718506
In grad_steps = 3330, loss = 0.1162252426147461
In grad_steps = 3331, loss = 0.19927558302879333
In grad_steps = 3332, loss = 0.12388807535171509
In grad_steps = 3333, loss = 0.05111221969127655
In grad_steps = 3334, loss = 0.13574554026126862
In grad_steps = 3335, loss = 0.19642479717731476
In grad_steps = 3336, loss = 0.2233511507511139
In grad_steps = 3337, loss = 0.14547106623649597
In grad_steps = 3338, loss = 0.5135866403579712
In grad_steps = 3339, loss = 0.06422945111989975
In grad_steps = 3340, loss = 0.022318445146083832
In grad_steps = 3341, loss = 0.47782230377197266
In grad_steps = 3342, loss = 0.030857548117637634
In grad_steps = 3343, loss = 0.011263277381658554
In grad_steps = 3344, loss = 0.043811023235321045
In grad_steps = 3345, loss = 0.19903889298439026
In grad_steps = 3346, loss = 0.35731765627861023
In grad_steps = 3347, loss = 0.29440465569496155
In grad_steps = 3348, loss = 0.061074089258909225
In grad_steps = 3349, loss = 0.17142927646636963
In grad_steps = 3350, loss = 0.010907121002674103
In grad_steps = 3351, loss = 0.037318505346775055
In grad_steps = 3352, loss = 0.1021086722612381
In grad_steps = 3353, loss = 0.12541228532791138
In grad_steps = 3354, loss = 0.023276912048459053
In grad_steps = 3355, loss = 0.009511854499578476
In grad_steps = 3356, loss = 0.03065445087850094
In grad_steps = 3357, loss = 0.43714630603790283
In grad_steps = 3358, loss = 0.595061182975769
In grad_steps = 3359, loss = 1.0316503047943115
In grad_steps = 3360, loss = 0.024151917546987534
In grad_steps = 3361, loss = 1.0207995176315308
In grad_steps = 3362, loss = 0.04353322088718414
In grad_steps = 3363, loss = 0.05159568414092064
In grad_steps = 3364, loss = 0.037762999534606934
In grad_steps = 3365, loss = 0.02697259932756424
In grad_steps = 3366, loss = 0.028432205319404602
In grad_steps = 3367, loss = 0.036902815103530884
In grad_steps = 3368, loss = 0.0707005113363266
In grad_steps = 3369, loss = 0.7267502546310425
In grad_steps = 3370, loss = 1.1306798458099365
In grad_steps = 3371, loss = 0.05239221826195717
In grad_steps = 3372, loss = 0.48594677448272705
In grad_steps = 3373, loss = 0.15391361713409424
In grad_steps = 3374, loss = 0.07974265515804291
In grad_steps = 3375, loss = 0.6128884553909302
In grad_steps = 3376, loss = 0.1971084177494049
In grad_steps = 3377, loss = 0.041898127645254135
In grad_steps = 3378, loss = 0.17461559176445007
In grad_steps = 3379, loss = 0.19283851981163025
In grad_steps = 3380, loss = 0.5915459990501404
In grad_steps = 3381, loss = 0.04528907686471939
In grad_steps = 3382, loss = 0.5865514278411865
In grad_steps = 3383, loss = 0.6460150480270386
In grad_steps = 3384, loss = 0.17742502689361572
In grad_steps = 3385, loss = 0.06746488809585571
In grad_steps = 3386, loss = 0.23855262994766235
In grad_steps = 3387, loss = 0.08159719407558441
In grad_steps = 3388, loss = 0.16129601001739502
In grad_steps = 3389, loss = 0.24775266647338867
In grad_steps = 3390, loss = 0.16618256270885468
In grad_steps = 3391, loss = 0.01560765691101551
In grad_steps = 3392, loss = 0.7567653059959412
In grad_steps = 3393, loss = 0.46885719895362854
In grad_steps = 3394, loss = 0.11471834778785706
In grad_steps = 3395, loss = 0.1517784595489502
In grad_steps = 3396, loss = 0.27639034390449524
In grad_steps = 3397, loss = 0.10271237790584564
In grad_steps = 3398, loss = 1.3592886924743652
In grad_steps = 3399, loss = 0.1526937633752823
In grad_steps = 3400, loss = 0.026375001296401024
In grad_steps = 3401, loss = 0.4679115414619446
In grad_steps = 3402, loss = 0.003934712614864111
In grad_steps = 3403, loss = 0.3156614601612091
In grad_steps = 3404, loss = 0.262570321559906
In grad_steps = 3405, loss = 0.15496066212654114
In grad_steps = 3406, loss = 0.1660955399274826
In grad_steps = 3407, loss = 0.048124756664037704
In grad_steps = 3408, loss = 0.4749157726764679
In grad_steps = 3409, loss = 0.07090982794761658
In grad_steps = 3410, loss = 0.23498855531215668
In grad_steps = 3411, loss = 0.05306755751371384
In grad_steps = 3412, loss = 0.13986122608184814
In grad_steps = 3413, loss = 0.1090037003159523
In grad_steps = 3414, loss = 0.8741307258605957
In grad_steps = 3415, loss = 0.033585574477910995
In grad_steps = 3416, loss = 0.058203279972076416
In grad_steps = 3417, loss = 0.11946847289800644
In grad_steps = 3418, loss = 0.02497677132487297
In grad_steps = 3419, loss = 0.09627626091241837
In grad_steps = 3420, loss = 0.023771267384290695
In grad_steps = 3421, loss = 0.2131383866071701
In grad_steps = 3422, loss = 0.087044857442379
In grad_steps = 3423, loss = 0.026736430823802948
In grad_steps = 3424, loss = 0.19125965237617493
In grad_steps = 3425, loss = 0.5169030427932739
In grad_steps = 3426, loss = 0.022175131365656853
In grad_steps = 3427, loss = 1.4751853942871094
In grad_steps = 3428, loss = 0.010836416855454445
In grad_steps = 3429, loss = 0.39883339405059814
In grad_steps = 3430, loss = 0.026772500947117805
In grad_steps = 3431, loss = 0.7919767498970032
In grad_steps = 3432, loss = 0.11853434145450592
In grad_steps = 3433, loss = 0.07266455888748169
In grad_steps = 3434, loss = 0.036303456872701645
In grad_steps = 3435, loss = 0.09691862016916275
In grad_steps = 3436, loss = 0.5184697508811951
In grad_steps = 3437, loss = 0.09923819452524185
In grad_steps = 3438, loss = 0.26748985052108765
In grad_steps = 3439, loss = 0.25140663981437683
In grad_steps = 3440, loss = 0.36469894647598267
In grad_steps = 3441, loss = 0.06694594770669937
In grad_steps = 3442, loss = 0.05523723363876343
In grad_steps = 3443, loss = 0.017041809856891632
In grad_steps = 3444, loss = 0.6466096043586731
In grad_steps = 3445, loss = 0.16849945485591888
In grad_steps = 3446, loss = 0.22867514193058014
In grad_steps = 3447, loss = 0.06535190343856812
In grad_steps = 3448, loss = 0.8637881875038147
In grad_steps = 3449, loss = 0.6572868227958679
In grad_steps = 3450, loss = 0.02510334737598896
In grad_steps = 3451, loss = 0.5962480306625366
In grad_steps = 3452, loss = 0.0496370866894722
In grad_steps = 3453, loss = 0.6221011281013489
In grad_steps = 3454, loss = 0.08238733559846878
In grad_steps = 3455, loss = 0.46578988432884216
In grad_steps = 3456, loss = 0.5322761535644531
In grad_steps = 3457, loss = 0.2302294373512268
In grad_steps = 3458, loss = 0.03570261225104332
In grad_steps = 3459, loss = 0.4215492010116577
In grad_steps = 3460, loss = 0.3986159563064575
In grad_steps = 3461, loss = 0.39856672286987305
In grad_steps = 3462, loss = 0.20165064930915833
In grad_steps = 3463, loss = 0.16310733556747437
In grad_steps = 3464, loss = 0.6600943207740784
In grad_steps = 3465, loss = 0.22537702322006226
In grad_steps = 3466, loss = 0.9333515763282776
In grad_steps = 3467, loss = 0.22492259740829468
In grad_steps = 3468, loss = 0.19326592981815338
In grad_steps = 3469, loss = 0.27703213691711426
In grad_steps = 3470, loss = 0.5277632474899292
In grad_steps = 3471, loss = 0.03876201808452606
In grad_steps = 3472, loss = 0.06674876064062119
In grad_steps = 3473, loss = 0.163677379488945
In grad_steps = 3474, loss = 0.2163287103176117
In grad_steps = 3475, loss = 0.19569596648216248
In grad_steps = 3476, loss = 0.7900682687759399
In grad_steps = 3477, loss = 0.07632152736186981
In grad_steps = 3478, loss = 0.3145056962966919
In grad_steps = 3479, loss = 0.05547071248292923
In grad_steps = 3480, loss = 0.18939198553562164
In grad_steps = 3481, loss = 0.12276358157396317
In grad_steps = 3482, loss = 0.05209735780954361
In grad_steps = 3483, loss = 0.04276859387755394
In grad_steps = 3484, loss = 0.7698536515235901
In grad_steps = 3485, loss = 0.018645653501152992
In grad_steps = 3486, loss = 0.027409791946411133
In grad_steps = 3487, loss = 0.173085555434227
In grad_steps = 3488, loss = 0.15150222182273865
In grad_steps = 3489, loss = 0.07211564481258392
In grad_steps = 3490, loss = 1.1998026371002197
In grad_steps = 3491, loss = 0.24240192770957947
In grad_steps = 3492, loss = 0.019913271069526672
In grad_steps = 3493, loss = 0.0971275195479393
In grad_steps = 3494, loss = 0.5255309343338013
In grad_steps = 3495, loss = 0.12716582417488098
In grad_steps = 3496, loss = 1.1533252000808716
In grad_steps = 3497, loss = 0.09057290852069855
In grad_steps = 3498, loss = 0.1764102280139923
In grad_steps = 3499, loss = 0.35472750663757324
In grad_steps = 3500, loss = 0.008129024878144264
In grad_steps = 3501, loss = 0.39295220375061035
In grad_steps = 3502, loss = 0.05420103296637535
In grad_steps = 3503, loss = 0.0315370187163353
In grad_steps = 3504, loss = 0.18362128734588623
In grad_steps = 3505, loss = 0.12846410274505615
In grad_steps = 3506, loss = 0.9558753967285156
In grad_steps = 3507, loss = 0.06507544219493866
In grad_steps = 3508, loss = 0.4723730683326721
In grad_steps = 3509, loss = 0.2461700588464737
In grad_steps = 3510, loss = 0.1362709254026413
In grad_steps = 3511, loss = 0.143609881401062
In grad_steps = 3512, loss = 0.08983749896287918
In grad_steps = 3513, loss = 0.041038550436496735
In grad_steps = 3514, loss = 0.1104942262172699
In grad_steps = 3515, loss = 0.49562570452690125
In grad_steps = 3516, loss = 1.2291080951690674
In grad_steps = 3517, loss = 0.06288264691829681
In grad_steps = 3518, loss = 0.2879737615585327
In grad_steps = 3519, loss = 0.04731656610965729
In grad_steps = 3520, loss = 0.13485832512378693
In grad_steps = 3521, loss = 0.03596948832273483
In grad_steps = 3522, loss = 0.0779053345322609
In grad_steps = 3523, loss = 0.15703554451465607
In grad_steps = 3524, loss = 0.04624197259545326
In grad_steps = 3525, loss = 0.4960780143737793
In grad_steps = 3526, loss = 0.052331872284412384
In grad_steps = 3527, loss = 0.7447410821914673
In grad_steps = 3528, loss = 0.08741040527820587
In grad_steps = 3529, loss = 0.28658559918403625
In grad_steps = 3530, loss = 0.24733823537826538
In grad_steps = 3531, loss = 0.2879417836666107
In grad_steps = 3532, loss = 0.16388154029846191
In grad_steps = 3533, loss = 0.14633703231811523
In grad_steps = 3534, loss = 0.05641372501850128
In grad_steps = 3535, loss = 0.07226955145597458
In grad_steps = 3536, loss = 0.4314531981945038
In grad_steps = 3537, loss = 0.1526820808649063
In grad_steps = 3538, loss = 0.14274275302886963
In grad_steps = 3539, loss = 1.2612264156341553
In grad_steps = 3540, loss = 0.07556487619876862
In grad_steps = 3541, loss = 0.1655901074409485
In grad_steps = 3542, loss = 0.2928367257118225
In grad_steps = 3543, loss = 0.08533503115177155
In grad_steps = 3544, loss = 0.36975303292274475
In grad_steps = 3545, loss = 0.023948146030306816
In grad_steps = 3546, loss = 0.146566241979599
In grad_steps = 3547, loss = 0.4834708869457245
In grad_steps = 3548, loss = 0.059208452701568604
In grad_steps = 3549, loss = 0.024485085159540176
In grad_steps = 3550, loss = 0.03018980473279953
In grad_steps = 3551, loss = 0.3536205291748047
In grad_steps = 3552, loss = 0.6682027578353882
In grad_steps = 3553, loss = 0.07995382696390152
In grad_steps = 3554, loss = 0.362170547246933
In grad_steps = 3555, loss = 0.33551251888275146
In grad_steps = 3556, loss = 0.36544501781463623
In grad_steps = 3557, loss = 0.2035468965768814
In grad_steps = 3558, loss = 0.6294481754302979
In grad_steps = 3559, loss = 0.15808476507663727
In grad_steps = 3560, loss = 0.4016202390193939
In grad_steps = 3561, loss = 0.13186736404895782
In grad_steps = 3562, loss = 0.8529067039489746
In grad_steps = 3563, loss = 0.4107728600502014
In grad_steps = 3564, loss = 0.5567156076431274
In grad_steps = 3565, loss = 1.3337091207504272
In grad_steps = 3566, loss = 0.3596908450126648
In grad_steps = 3567, loss = 0.18077264726161957
In grad_steps = 3568, loss = 0.18455897271633148
In grad_steps = 3569, loss = 0.10696335136890411
In grad_steps = 3570, loss = 0.06302304565906525
In grad_steps = 3571, loss = 0.031679827719926834
In grad_steps = 3572, loss = 0.0385810025036335
In grad_steps = 3573, loss = 0.11866138130426407
In grad_steps = 3574, loss = 1.2738815546035767
In grad_steps = 3575, loss = 0.03976544365286827
In grad_steps = 3576, loss = 0.6742299795150757
In grad_steps = 3577, loss = 0.21622970700263977
In grad_steps = 3578, loss = 0.78748619556427
In grad_steps = 3579, loss = 0.49452388286590576
In grad_steps = 3580, loss = 0.7316506505012512
In grad_steps = 3581, loss = 0.7409771680831909
In grad_steps = 3582, loss = 0.15101942420005798
In grad_steps = 3583, loss = 0.5323454737663269
In grad_steps = 3584, loss = 0.5393790602684021
In grad_steps = 3585, loss = 0.14073725044727325
In grad_steps = 3586, loss = 0.0885603278875351
In grad_steps = 3587, loss = 0.11633633077144623
In grad_steps = 3588, loss = 0.24277378618717194
In grad_steps = 3589, loss = 0.09413106739521027
In grad_steps = 3590, loss = 0.7141289710998535
In grad_steps = 3591, loss = 0.149615079164505
In grad_steps = 3592, loss = 0.04928123950958252
In grad_steps = 3593, loss = 0.12394924461841583
In grad_steps = 3594, loss = 0.33611056208610535
In grad_steps = 3595, loss = 0.42650294303894043
In grad_steps = 3596, loss = 0.19983413815498352
In grad_steps = 3597, loss = 0.09973733127117157
In grad_steps = 3598, loss = 0.06462545692920685
In grad_steps = 3599, loss = 0.22045113146305084
In grad_steps = 3600, loss = 0.966274082660675
In grad_steps = 3601, loss = 0.0672113373875618
In grad_steps = 3602, loss = 0.025512520223855972
In grad_steps = 3603, loss = 0.09949647635221481
In grad_steps = 3604, loss = 0.38527876138687134
In grad_steps = 3605, loss = 0.05926617980003357
In grad_steps = 3606, loss = 0.041165612637996674
In grad_steps = 3607, loss = 0.3722957968711853
In grad_steps = 3608, loss = 0.14420267939567566
In grad_steps = 3609, loss = 0.6409220099449158
In grad_steps = 3610, loss = 0.08929546177387238
In grad_steps = 3611, loss = 0.23365014791488647
In grad_steps = 3612, loss = 0.449186235666275
In grad_steps = 3613, loss = 0.06369923055171967
In grad_steps = 3614, loss = 0.2690383493900299
In grad_steps = 3615, loss = 0.4842650592327118
In grad_steps = 3616, loss = 0.13406644761562347
In grad_steps = 3617, loss = 0.7331382036209106
In grad_steps = 3618, loss = 0.6308292746543884
In grad_steps = 3619, loss = 0.021500051021575928
In grad_steps = 3620, loss = 0.2019728571176529
In grad_steps = 3621, loss = 0.10243341326713562
In grad_steps = 3622, loss = 0.037201814353466034
In grad_steps = 3623, loss = 0.44549858570098877
In grad_steps = 3624, loss = 1.2831073999404907
In grad_steps = 3625, loss = 0.14662820100784302
In grad_steps = 3626, loss = 0.14464186131954193
In grad_steps = 3627, loss = 0.35634368658065796
In grad_steps = 3628, loss = 1.1078524589538574
In grad_steps = 3629, loss = 0.03691874444484711
In grad_steps = 3630, loss = 0.4719691872596741
In grad_steps = 3631, loss = 0.011346163228154182
In grad_steps = 3632, loss = 0.44689232110977173
In grad_steps = 3633, loss = 0.0866909846663475
In grad_steps = 3634, loss = 0.1130061149597168
In grad_steps = 3635, loss = 0.02308332547545433
In grad_steps = 3636, loss = 0.11606688797473907
In grad_steps = 3637, loss = 0.0854523628950119
In grad_steps = 3638, loss = 0.15952524542808533
In grad_steps = 3639, loss = 0.049524012953042984
In grad_steps = 3640, loss = 0.15799064934253693
In grad_steps = 3641, loss = 1.082871675491333
In grad_steps = 3642, loss = 0.6239818334579468
In grad_steps = 3643, loss = 0.06507817655801773
In grad_steps = 3644, loss = 0.7655808925628662
In grad_steps = 3645, loss = 0.10297346860170364
In grad_steps = 3646, loss = 0.018147025257349014
In grad_steps = 3647, loss = 0.12943986058235168
In grad_steps = 3648, loss = 0.5508720874786377
In grad_steps = 3649, loss = 0.04913237690925598
In grad_steps = 3650, loss = 0.07126102596521378
In grad_steps = 3651, loss = 0.16518770158290863
In grad_steps = 3652, loss = 0.1211867555975914
In grad_steps = 3653, loss = 0.3859083354473114
In grad_steps = 3654, loss = 0.05226483196020126
In grad_steps = 3655, loss = 0.09696507453918457
In grad_steps = 3656, loss = 0.4230622947216034
In grad_steps = 3657, loss = 0.14198361337184906
In grad_steps = 3658, loss = 0.14330551028251648
In grad_steps = 3659, loss = 0.2547002136707306
In grad_steps = 3660, loss = 0.046048298478126526
In grad_steps = 3661, loss = 0.4583815038204193
In grad_steps = 3662, loss = 0.19970199465751648
In grad_steps = 3663, loss = 0.5135780572891235
In grad_steps = 3664, loss = 0.24271927773952484
In grad_steps = 3665, loss = 0.044349975883960724
In grad_steps = 3666, loss = 0.16164910793304443
In grad_steps = 3667, loss = 0.02056053839623928
In grad_steps = 3668, loss = 0.033848173916339874
In grad_steps = 3669, loss = 0.049720630049705505
In grad_steps = 3670, loss = 0.044192418456077576
In grad_steps = 3671, loss = 0.8120139241218567
In grad_steps = 3672, loss = 0.06176145002245903
In grad_steps = 3673, loss = 0.5944700241088867
In grad_steps = 3674, loss = 0.09988337755203247
In grad_steps = 3675, loss = 1.1997030973434448
In grad_steps = 3676, loss = 0.26283368468284607
In grad_steps = 3677, loss = 0.04438697174191475
In grad_steps = 3678, loss = 0.6579614281654358
In grad_steps = 3679, loss = 0.07645947486162186
In grad_steps = 3680, loss = 0.024707332253456116
In grad_steps = 3681, loss = 0.06131988391280174
In grad_steps = 3682, loss = 0.02753141149878502
In grad_steps = 3683, loss = 0.20314644277095795
In grad_steps = 3684, loss = 0.006870703771710396
In grad_steps = 3685, loss = 0.18612854182720184
In grad_steps = 3686, loss = 0.4897657334804535
In grad_steps = 3687, loss = 0.07451771199703217
In grad_steps = 3688, loss = 0.0936596691608429
In grad_steps = 3689, loss = 0.45090481638908386
In grad_steps = 3690, loss = 0.14461393654346466
In grad_steps = 3691, loss = 0.0239889957010746
In grad_steps = 3692, loss = 0.5113438963890076
In grad_steps = 3693, loss = 0.01875240169465542
In grad_steps = 3694, loss = 0.07607562839984894
In grad_steps = 3695, loss = 0.023157088086009026
In grad_steps = 3696, loss = 0.37136197090148926
In grad_steps = 3697, loss = 0.05658116936683655
In grad_steps = 3698, loss = 0.5063786506652832
In grad_steps = 3699, loss = 0.1808687001466751
In grad_steps = 3700, loss = 0.5640555620193481
In grad_steps = 3701, loss = 0.0817829892039299
In grad_steps = 3702, loss = 1.149070143699646
In grad_steps = 3703, loss = 0.09572813659906387
In grad_steps = 3704, loss = 0.9181380271911621
In grad_steps = 3705, loss = 0.3107559382915497
In grad_steps = 3706, loss = 0.18504001200199127
In grad_steps = 3707, loss = 0.47154751420021057
In grad_steps = 3708, loss = 0.5233046412467957
In grad_steps = 3709, loss = 0.12556590139865875
In grad_steps = 3710, loss = 0.28659960627555847
In grad_steps = 3711, loss = 0.11918662488460541
In grad_steps = 3712, loss = 0.043226778507232666
In grad_steps = 3713, loss = 0.4361266791820526
In grad_steps = 3714, loss = 0.2785069942474365
In grad_steps = 3715, loss = 0.040666572749614716
In grad_steps = 3716, loss = 0.20907649397850037
In grad_steps = 3717, loss = 0.10301589965820312
In grad_steps = 3718, loss = 0.2161191701889038
In grad_steps = 3719, loss = 0.1580866426229477
In grad_steps = 3720, loss = 0.07357661426067352
In grad_steps = 3721, loss = 0.15304289758205414
In grad_steps = 3722, loss = 0.6101607084274292
In grad_steps = 3723, loss = 0.5243809819221497
In grad_steps = 3724, loss = 0.015997523441910744
In grad_steps = 3725, loss = 0.11942273378372192
In grad_steps = 3726, loss = 0.05061538890004158
In grad_steps = 3727, loss = 0.04321315884590149
In grad_steps = 3728, loss = 0.032015811651945114
In grad_steps = 3729, loss = 0.2782658338546753
In grad_steps = 3730, loss = 0.05387427285313606
In grad_steps = 3731, loss = 0.23826731741428375
In grad_steps = 3732, loss = 1.3306986093521118
In grad_steps = 3733, loss = 0.09660685807466507
In grad_steps = 3734, loss = 0.0832047164440155
In grad_steps = 3735, loss = 0.061103783547878265
In grad_steps = 3736, loss = 0.055043987929821014
In grad_steps = 3737, loss = 0.7781705856323242
In grad_steps = 3738, loss = 0.046238746494054794
In grad_steps = 3739, loss = 0.02525933086872101
In grad_steps = 3740, loss = 0.7689419388771057
In grad_steps = 3741, loss = 0.1769658476114273
In grad_steps = 3742, loss = 0.23499853909015656
In grad_steps = 3743, loss = 0.06988254189491272
In grad_steps = 3744, loss = 0.25159889459609985
In grad_steps = 3745, loss = 0.09152106195688248
In grad_steps = 3746, loss = 0.47520849108695984
In grad_steps = 3747, loss = 0.039925605058670044
In grad_steps = 3748, loss = 0.08725113421678543
In grad_steps = 3749, loss = 0.015468530356884003
In grad_steps = 3750, loss = 0.33403289318084717
In grad_steps = 3751, loss = 0.1136164516210556
In grad_steps = 3752, loss = 0.22269532084465027
In grad_steps = 3753, loss = 0.49711018800735474
In grad_steps = 3754, loss = 0.9691407084465027
In grad_steps = 3755, loss = 0.06379968672990799
In grad_steps = 3756, loss = 0.7301130294799805
In grad_steps = 3757, loss = 0.059924814850091934
In grad_steps = 3758, loss = 0.0728054940700531
In grad_steps = 3759, loss = 0.3879392743110657
In grad_steps = 3760, loss = 1.0250413417816162
In grad_steps = 3761, loss = 0.027790863066911697
In grad_steps = 3762, loss = 0.06888419389724731
In grad_steps = 3763, loss = 0.8814476728439331
In grad_steps = 3764, loss = 0.28414902091026306
In grad_steps = 3765, loss = 0.10976741462945938
In grad_steps = 3766, loss = 0.13929082453250885
In grad_steps = 3767, loss = 0.2015790343284607
In grad_steps = 3768, loss = 0.43917590379714966
In grad_steps = 3769, loss = 0.15760456025600433
In grad_steps = 3770, loss = 0.03273283317685127
In grad_steps = 3771, loss = 1.0180131196975708
In grad_steps = 3772, loss = 0.03034789115190506
In grad_steps = 3773, loss = 0.12766271829605103
In grad_steps = 3774, loss = 0.3628641664981842
In grad_steps = 3775, loss = 0.04563922435045242
In grad_steps = 3776, loss = 0.09959337115287781
In grad_steps = 3777, loss = 0.06183311343193054
In grad_steps = 3778, loss = 0.0462016761302948
In grad_steps = 3779, loss = 0.04647604376077652
In grad_steps = 3780, loss = 0.13914135098457336
In grad_steps = 3781, loss = 0.12103859335184097
In grad_steps = 3782, loss = 0.02758006379008293
In grad_steps = 3783, loss = 0.08400007337331772
In grad_steps = 3784, loss = 0.9385138750076294
In grad_steps = 3785, loss = 0.5596444010734558
In grad_steps = 3786, loss = 0.3932926058769226
In grad_steps = 3787, loss = 0.8550646901130676
In grad_steps = 3788, loss = 0.09491986036300659
In grad_steps = 3789, loss = 0.2632477879524231
In grad_steps = 3790, loss = 0.17441263794898987
In grad_steps = 3791, loss = 0.4194473624229431
In grad_steps = 3792, loss = 0.183294415473938
In grad_steps = 3793, loss = 0.26323437690734863
In grad_steps = 3794, loss = 0.11079469323158264
In grad_steps = 3795, loss = 0.11902356147766113
In grad_steps = 3796, loss = 0.40813136100769043
In grad_steps = 3797, loss = 0.09683913737535477
In grad_steps = 3798, loss = 0.4365790784358978
In grad_steps = 3799, loss = 0.041591450572013855
In grad_steps = 3800, loss = 0.08123445510864258
In grad_steps = 3801, loss = 0.0692998617887497
In grad_steps = 3802, loss = 1.1118719577789307
In grad_steps = 3803, loss = 0.5795187950134277
In grad_steps = 3804, loss = 0.05841178074479103
In grad_steps = 3805, loss = 0.13854657113552094
In grad_steps = 3806, loss = 0.08408795297145844
In grad_steps = 3807, loss = 0.22421765327453613
In grad_steps = 3808, loss = 0.6965023875236511
In grad_steps = 3809, loss = 0.12531276047229767
In grad_steps = 3810, loss = 0.2262924760580063
In grad_steps = 3811, loss = 1.0380752086639404
In grad_steps = 3812, loss = 0.055156998336315155
In grad_steps = 3813, loss = 0.18446263670921326
In grad_steps = 3814, loss = 0.18267910182476044
In grad_steps = 3815, loss = 0.20061597228050232
In grad_steps = 3816, loss = 0.5035807490348816
In grad_steps = 3817, loss = 0.19246508181095123
In grad_steps = 3818, loss = 0.966059684753418
In grad_steps = 3819, loss = 0.13177800178527832
In grad_steps = 3820, loss = 0.1557435691356659
In grad_steps = 3821, loss = 0.4032386243343353
In grad_steps = 3822, loss = 0.21414823830127716
In grad_steps = 3823, loss = 0.2694949209690094
In grad_steps = 3824, loss = 0.615004301071167
In grad_steps = 3825, loss = 0.07943715900182724
In grad_steps = 3826, loss = 0.15842407941818237
In grad_steps = 3827, loss = 0.4790184795856476
In grad_steps = 3828, loss = 0.10932953655719757
In grad_steps = 3829, loss = 0.06830047070980072
In grad_steps = 3830, loss = 0.1040838286280632
In grad_steps = 3831, loss = 0.3479493260383606
In grad_steps = 3832, loss = 0.12737353146076202
In grad_steps = 3833, loss = 0.13146133720874786
In grad_steps = 3834, loss = 0.06070258468389511
In grad_steps = 3835, loss = 0.1418962925672531
In grad_steps = 3836, loss = 0.14650218188762665
In grad_steps = 3837, loss = 0.6126060485839844
In grad_steps = 3838, loss = 0.11719583719968796
In grad_steps = 3839, loss = 0.03469858318567276
In grad_steps = 3840, loss = 0.018770866096019745
In grad_steps = 3841, loss = 0.0793769434094429
In grad_steps = 3842, loss = 0.03373098373413086
In grad_steps = 3843, loss = 0.2703053951263428
In grad_steps = 3844, loss = 0.18238024413585663
In grad_steps = 3845, loss = 0.41499829292297363
In grad_steps = 3846, loss = 0.2996493875980377
In grad_steps = 3847, loss = 0.042515985667705536
In grad_steps = 3848, loss = 1.3964715003967285
In grad_steps = 3849, loss = 0.0271703340113163
In grad_steps = 3850, loss = 0.10998977720737457
In grad_steps = 3851, loss = 0.3493894934654236
In grad_steps = 3852, loss = 0.05537395551800728
In grad_steps = 3853, loss = 0.01771564781665802
In grad_steps = 3854, loss = 0.03893468901515007
In grad_steps = 3855, loss = 0.010051466524600983
In grad_steps = 3856, loss = 0.04034384712576866
In grad_steps = 3857, loss = 0.00922808051109314
In grad_steps = 3858, loss = 0.38427579402923584
In grad_steps = 3859, loss = 0.1006295382976532
In grad_steps = 3860, loss = 0.1413799375295639
In grad_steps = 3861, loss = 0.8660246133804321
In grad_steps = 3862, loss = 0.1128276139497757
In grad_steps = 3863, loss = 0.11471476405858994
In grad_steps = 3864, loss = 0.7541887760162354
In grad_steps = 3865, loss = 0.24736221134662628
In grad_steps = 3866, loss = 1.4211013317108154
In grad_steps = 3867, loss = 0.486537367105484
In grad_steps = 3868, loss = 1.069373607635498
In grad_steps = 3869, loss = 0.37000519037246704
In grad_steps = 3870, loss = 0.028913484886288643
In grad_steps = 3871, loss = 0.05294659361243248
In grad_steps = 3872, loss = 0.16277587413787842
In grad_steps = 3873, loss = 0.2352534532546997
In grad_steps = 3874, loss = 0.07951128482818604
In grad_steps = 3875, loss = 0.17148545384407043
In grad_steps = 3876, loss = 0.14675791561603546
In grad_steps = 3877, loss = 0.15568648278713226
In grad_steps = 3878, loss = 0.30922022461891174
In grad_steps = 3879, loss = 0.09144344180822372
In grad_steps = 3880, loss = 0.24388638138771057
In grad_steps = 3881, loss = 0.7171575427055359
In grad_steps = 3882, loss = 0.0669587031006813
In grad_steps = 3883, loss = 0.7198421955108643
In grad_steps = 3884, loss = 0.05480213090777397
In grad_steps = 3885, loss = 0.20872844755649567
In grad_steps = 3886, loss = 0.055207617580890656
In grad_steps = 3887, loss = 0.028499078005552292
In grad_steps = 3888, loss = 0.03644677624106407
In grad_steps = 3889, loss = 0.036957643926143646
In grad_steps = 3890, loss = 0.615757167339325
In grad_steps = 3891, loss = 0.3140406608581543
In grad_steps = 3892, loss = 0.06994212418794632
In grad_steps = 3893, loss = 0.05642608180642128
In grad_steps = 3894, loss = 0.1912693977355957
In grad_steps = 3895, loss = 0.0905742198228836
In grad_steps = 3896, loss = 0.02146241068840027
In grad_steps = 3897, loss = 0.12721934914588928
In grad_steps = 3898, loss = 0.025176456198096275
In grad_steps = 3899, loss = 0.23513492941856384
In grad_steps = 3900, loss = 0.05494336038827896
In grad_steps = 3901, loss = 0.05321710556745529
In grad_steps = 3902, loss = 0.08588286489248276
In grad_steps = 3903, loss = 0.017788317054510117
In grad_steps = 3904, loss = 0.026365824043750763
In grad_steps = 3905, loss = 0.0733531191945076
In grad_steps = 3906, loss = 0.8595007658004761
In grad_steps = 3907, loss = 0.11986475437879562
In grad_steps = 3908, loss = 0.06880828738212585
In grad_steps = 3909, loss = 0.10140849649906158
In grad_steps = 3910, loss = 0.11703004688024521
In grad_steps = 3911, loss = 0.013026785105466843
In grad_steps = 3912, loss = 0.04914896935224533
In grad_steps = 3913, loss = 0.02612282708287239
In grad_steps = 3914, loss = 0.0020416369661688805
In grad_steps = 3915, loss = 0.0074515207670629025
In grad_steps = 3916, loss = 0.49957478046417236
In grad_steps = 3917, loss = 0.0029359760228544474
In grad_steps = 3918, loss = 0.13362450897693634
In grad_steps = 3919, loss = 1.5621874332427979
In grad_steps = 3920, loss = 0.003694867715239525
In grad_steps = 3921, loss = 0.007841994054615498
In grad_steps = 3922, loss = 0.6820387244224548
In grad_steps = 3923, loss = 1.513635277748108
In grad_steps = 3924, loss = 0.15899856388568878
In grad_steps = 3925, loss = 0.11240899562835693
In grad_steps = 3926, loss = 0.14052347838878632
In grad_steps = 3927, loss = 0.09599753469228745
In grad_steps = 3928, loss = 1.6333434581756592
In grad_steps = 3929, loss = 0.44342419505119324
In grad_steps = 3930, loss = 0.16073647141456604
In grad_steps = 3931, loss = 1.2625975608825684
In grad_steps = 3932, loss = 0.026998911052942276
In grad_steps = 3933, loss = 0.5677688121795654
In grad_steps = 3934, loss = 0.3676133453845978
In grad_steps = 3935, loss = 0.3122941553592682
In grad_steps = 3936, loss = 0.39729437232017517
In grad_steps = 3937, loss = 0.09984222054481506
In grad_steps = 3938, loss = 0.14775480329990387
In grad_steps = 3939, loss = 0.3035973608493805
In grad_steps = 3940, loss = 0.937614917755127
In grad_steps = 3941, loss = 0.5909857153892517
In grad_steps = 3942, loss = 0.5584713220596313
In grad_steps = 3943, loss = 0.6026233434677124
In grad_steps = 3944, loss = 0.1711997538805008
In grad_steps = 3945, loss = 0.3117445707321167
In grad_steps = 3946, loss = 0.12408231198787689
In grad_steps = 3947, loss = 0.1691550314426422
In grad_steps = 3948, loss = 0.12729179859161377
In grad_steps = 3949, loss = 0.6096969246864319
In grad_steps = 3950, loss = 0.1257438212633133
In grad_steps = 3951, loss = 0.12077035009860992
In grad_steps = 3952, loss = 0.06901410222053528
In grad_steps = 3953, loss = 0.11128468811511993
In grad_steps = 3954, loss = 0.06277064979076385
In grad_steps = 3955, loss = 0.13230212032794952
In grad_steps = 3956, loss = 0.15048590302467346
In grad_steps = 3957, loss = 0.8101091384887695
In grad_steps = 3958, loss = 0.26818859577178955
In grad_steps = 3959, loss = 0.019606541842222214
In grad_steps = 3960, loss = 0.09029990434646606
In grad_steps = 3961, loss = 0.011492221616208553
In grad_steps = 3962, loss = 0.08443771302700043
In grad_steps = 3963, loss = 0.0302968081086874
In grad_steps = 3964, loss = 0.02148069068789482
In grad_steps = 3965, loss = 0.008117175661027431
In grad_steps = 3966, loss = 0.9759631156921387
In grad_steps = 3967, loss = 0.9430190324783325
In grad_steps = 3968, loss = 0.31781667470932007
In grad_steps = 3969, loss = 0.04922827333211899
In grad_steps = 3970, loss = 0.012215218506753445
In grad_steps = 3971, loss = 0.3419833779335022
In grad_steps = 3972, loss = 0.06473729014396667
In grad_steps = 3973, loss = 0.13763102889060974
In grad_steps = 3974, loss = 0.11219362169504166
In grad_steps = 3975, loss = 0.11508268117904663
In grad_steps = 3976, loss = 0.10141018778085709
In grad_steps = 3977, loss = 0.08661466836929321
In grad_steps = 3978, loss = 0.044333357363939285
In grad_steps = 3979, loss = 0.028612738475203514
In grad_steps = 3980, loss = 0.041481535881757736
In grad_steps = 3981, loss = 0.4239500164985657
In grad_steps = 3982, loss = 0.0495506189763546
In grad_steps = 3983, loss = 0.5014007687568665
In grad_steps = 3984, loss = 0.8777154684066772
In grad_steps = 3985, loss = 0.02755807526409626
In grad_steps = 3986, loss = 0.6723244786262512
In grad_steps = 3987, loss = 0.0487334206700325
In grad_steps = 3988, loss = 0.045021601021289825
In grad_steps = 3989, loss = 0.1296580284833908
In grad_steps = 3990, loss = 0.1959431767463684
In grad_steps = 3991, loss = 0.45183420181274414
In grad_steps = 3992, loss = 0.4998708963394165
In grad_steps = 3993, loss = 0.18787145614624023
In grad_steps = 3994, loss = 0.15517503023147583
In grad_steps = 3995, loss = 0.018515322357416153
In grad_steps = 3996, loss = 0.08140813559293747
In grad_steps = 3997, loss = 0.6025552153587341
In grad_steps = 3998, loss = 1.461655616760254
In grad_steps = 3999, loss = 0.6363258361816406
In grad_steps = 4000, loss = 0.32240980863571167
In grad_steps = 4001, loss = 0.35128363966941833
In grad_steps = 4002, loss = 0.2882678806781769
In grad_steps = 4003, loss = 0.15100254118442535
In grad_steps = 4004, loss = 0.32956811785697937
In grad_steps = 4005, loss = 0.25802624225616455
In grad_steps = 4006, loss = 0.1286764293909073
In grad_steps = 4007, loss = 0.14372430741786957
In grad_steps = 4008, loss = 0.22698339819908142
In grad_steps = 4009, loss = 0.46842265129089355
In grad_steps = 4010, loss = 0.18262819945812225
In grad_steps = 4011, loss = 0.09615880250930786
In grad_steps = 4012, loss = 0.4682110548019409
In grad_steps = 4013, loss = 0.07135436683893204
In grad_steps = 4014, loss = 0.2255324274301529
In grad_steps = 4015, loss = 0.17784985899925232
In grad_steps = 4016, loss = 0.21033374965190887
In grad_steps = 4017, loss = 0.15175661444664001
In grad_steps = 4018, loss = 0.18146035075187683
In grad_steps = 4019, loss = 0.012659097090363503
In grad_steps = 4020, loss = 0.007480152882635593
In grad_steps = 4021, loss = 0.19851365685462952
In grad_steps = 4022, loss = 0.7644969820976257
In grad_steps = 4023, loss = 0.08305186778306961
In grad_steps = 4024, loss = 1.0447946786880493
In grad_steps = 4025, loss = 0.0014444910921156406
In grad_steps = 4026, loss = 0.018783733248710632
In grad_steps = 4027, loss = 0.067580446600914
In grad_steps = 4028, loss = 0.49116653203964233
In grad_steps = 4029, loss = 0.34117501974105835
In grad_steps = 4030, loss = 0.06884440034627914
In grad_steps = 4031, loss = 0.5093450546264648
In grad_steps = 4032, loss = 0.44637298583984375
In grad_steps = 4033, loss = 0.20550012588500977
In grad_steps = 4034, loss = 0.17665843665599823
In grad_steps = 4035, loss = 0.7365007996559143
In grad_steps = 4036, loss = 0.052163757383823395
In grad_steps = 4037, loss = 0.0881652683019638
In grad_steps = 4038, loss = 0.6936456561088562
In grad_steps = 4039, loss = 0.24177950620651245
In grad_steps = 4040, loss = 0.18902575969696045
In grad_steps = 4041, loss = 0.9405021071434021
In grad_steps = 4042, loss = 0.5942867994308472
In grad_steps = 4043, loss = 0.038643255829811096
In grad_steps = 4044, loss = 0.20283296704292297
In grad_steps = 4045, loss = 0.2465074211359024
In grad_steps = 4046, loss = 0.4072559177875519
In grad_steps = 4047, loss = 0.5302308797836304
In grad_steps = 4048, loss = 0.09639114886522293
In grad_steps = 4049, loss = 0.21414370834827423
In grad_steps = 4050, loss = 0.1677863895893097
In grad_steps = 4051, loss = 0.0885353609919548
In grad_steps = 4052, loss = 0.18251588940620422
In grad_steps = 4053, loss = 0.14252857863903046
In grad_steps = 4054, loss = 0.15758585929870605
In grad_steps = 4055, loss = 0.23897187411785126
In grad_steps = 4056, loss = 0.1370677947998047
In grad_steps = 4057, loss = 0.10194693505764008
In grad_steps = 4058, loss = 0.13451939821243286
In grad_steps = 4059, loss = 0.17902526259422302
In grad_steps = 4060, loss = 0.07222364842891693
In grad_steps = 4061, loss = 0.1703670471906662
In grad_steps = 4062, loss = 0.1727822870016098
In grad_steps = 4063, loss = 0.41847100853919983
In grad_steps = 4064, loss = 0.07232338190078735
In grad_steps = 4065, loss = 0.16816404461860657
In grad_steps = 4066, loss = 0.17539364099502563
In grad_steps = 4067, loss = 0.005793606862425804
In grad_steps = 4068, loss = 0.04787575080990791
In grad_steps = 4069, loss = 0.17597094178199768
In grad_steps = 4070, loss = 0.33125755190849304
In grad_steps = 4071, loss = 0.2504098415374756
In grad_steps = 4072, loss = 0.12035416066646576
In grad_steps = 4073, loss = 0.12784630060195923
In grad_steps = 4074, loss = 0.008476098999381065
In grad_steps = 4075, loss = 0.38600510358810425
In grad_steps = 4076, loss = 0.012316304259002209
In grad_steps = 4077, loss = 0.027063388377428055
In grad_steps = 4078, loss = 0.17486730217933655
In grad_steps = 4079, loss = 0.08719100058078766
In grad_steps = 4080, loss = 0.6662206649780273
In grad_steps = 4081, loss = 0.07386701554059982
In grad_steps = 4082, loss = 0.05908972769975662
In grad_steps = 4083, loss = 0.6781406402587891
In grad_steps = 4084, loss = 1.0982898473739624
In grad_steps = 4085, loss = 0.5713522434234619
In grad_steps = 4086, loss = 0.012736024335026741
In grad_steps = 4087, loss = 0.0066430941224098206
In grad_steps = 4088, loss = 0.011972791515290737
In grad_steps = 4089, loss = 0.06009414419531822
In grad_steps = 4090, loss = 0.4145890772342682
In grad_steps = 4091, loss = 1.1185344457626343
In grad_steps = 4092, loss = 0.09752771258354187
In grad_steps = 4093, loss = 0.01453990675508976
In grad_steps = 4094, loss = 0.2407037615776062
In grad_steps = 4095, loss = 0.03628353029489517
In grad_steps = 4096, loss = 0.24568289518356323
In grad_steps = 4097, loss = 0.27599257230758667
In grad_steps = 4098, loss = 0.8358206152915955
In grad_steps = 4099, loss = 0.02467363514006138
In grad_steps = 4100, loss = 0.21628934144973755
In grad_steps = 4101, loss = 0.2945972681045532
In grad_steps = 4102, loss = 0.16874702274799347
In grad_steps = 4103, loss = 0.13268917798995972
In grad_steps = 4104, loss = 0.15515539050102234
In grad_steps = 4105, loss = 0.006737846881151199
Elapsed time: 2337.6951031684875 seconds for ensemble 2 with 2 epochs
LoRA instance 2 evaluation complete. Data saved to /pscratch/sd/t/tianle/lucid/other_source/SURP_2024/results/tmp_lora_ensemble/Llama3-8B-set-5/test_data_instance_2_seed_20187.npz.
lora instance i = 2 Successfully finished.
Training lora instance 3
DEBUG: Total parameters: 8037076992
DEBUG: Trainable parameters: 6815744
Beginning epoch 1
In grad_steps = 0, loss = 0.7379498481750488
In grad_steps = 1, loss = 0.6386909484863281
In grad_steps = 2, loss = 1.6107289791107178
In grad_steps = 3, loss = 0.8041878938674927
In grad_steps = 4, loss = 0.6280970573425293
In grad_steps = 5, loss = 0.8087180852890015
In grad_steps = 6, loss = 0.531046450138092
In grad_steps = 7, loss = 0.567368745803833
In grad_steps = 8, loss = 1.1940616369247437
In grad_steps = 9, loss = 0.9800740480422974
In grad_steps = 10, loss = 0.7672512531280518
In grad_steps = 11, loss = 0.7045134902000427
In grad_steps = 12, loss = 0.7459672689437866
In grad_steps = 13, loss = 0.5891300439834595
In grad_steps = 14, loss = 0.9336453676223755
In grad_steps = 15, loss = 0.5350539684295654
In grad_steps = 16, loss = 0.705736517906189
In grad_steps = 17, loss = 0.689083456993103
In grad_steps = 18, loss = 0.7748807668685913
In grad_steps = 19, loss = 0.7419096231460571
In grad_steps = 20, loss = 0.6657254695892334
In grad_steps = 21, loss = 0.6474626064300537
In grad_steps = 22, loss = 0.6752490997314453
In grad_steps = 23, loss = 0.7908754348754883
In grad_steps = 24, loss = 0.632861852645874
In grad_steps = 25, loss = 0.7517157793045044
In grad_steps = 26, loss = 0.7355046272277832
In grad_steps = 27, loss = 0.7228298783302307
In grad_steps = 28, loss = 0.5632225275039673
In grad_steps = 29, loss = 0.6828648447990417
In grad_steps = 30, loss = 0.8139315843582153
In grad_steps = 31, loss = 0.6275255680084229
In grad_steps = 32, loss = 0.6404135227203369
In grad_steps = 33, loss = 0.6758298873901367
In grad_steps = 34, loss = 0.6254864931106567
In grad_steps = 35, loss = 0.678949236869812
In grad_steps = 36, loss = 0.651870608329773
In grad_steps = 37, loss = 0.6812261343002319
In grad_steps = 38, loss = 0.7973669767379761
In grad_steps = 39, loss = 0.6716660261154175
In grad_steps = 40, loss = 0.7429591417312622
In grad_steps = 41, loss = 0.6828173398971558
In grad_steps = 42, loss = 0.616933286190033
In grad_steps = 43, loss = 0.6680678725242615
In grad_steps = 44, loss = 0.5839701294898987
In grad_steps = 45, loss = 0.7248517274856567
In grad_steps = 46, loss = 0.7199629545211792
In grad_steps = 47, loss = 0.6545534729957581
In grad_steps = 48, loss = 0.5197612047195435
In grad_steps = 49, loss = 0.6512683033943176
In grad_steps = 50, loss = 0.7239612936973572
In grad_steps = 51, loss = 0.6649965643882751
In grad_steps = 52, loss = 0.5564221739768982
In grad_steps = 53, loss = 0.7119817137718201
In grad_steps = 54, loss = 0.6071959733963013
In grad_steps = 55, loss = 0.6079609990119934
In grad_steps = 56, loss = 0.6864219903945923
In grad_steps = 57, loss = 0.697344183921814
In grad_steps = 58, loss = 0.707038402557373
In grad_steps = 59, loss = 0.7241252660751343
In grad_steps = 60, loss = 0.7071664929389954
In grad_steps = 61, loss = 0.5718879699707031
In grad_steps = 62, loss = 0.6983728408813477
In grad_steps = 63, loss = 0.5600423812866211
In grad_steps = 64, loss = 0.40820372104644775
In grad_steps = 65, loss = 0.43023747205734253
In grad_steps = 66, loss = 0.5714153051376343
In grad_steps = 67, loss = 0.3563231825828552
In grad_steps = 68, loss = 0.4375549554824829
In grad_steps = 69, loss = 0.31059059500694275
In grad_steps = 70, loss = 1.2357749938964844
In grad_steps = 71, loss = 0.34433630108833313
In grad_steps = 72, loss = 1.6937779188156128
In grad_steps = 73, loss = 1.326948642730713
In grad_steps = 74, loss = 0.3134685754776001
In grad_steps = 75, loss = 1.053633689880371
In grad_steps = 76, loss = 0.38246625661849976
In grad_steps = 77, loss = 0.3673507571220398
In grad_steps = 78, loss = 0.5518700480461121
In grad_steps = 79, loss = 0.6088503003120422
In grad_steps = 80, loss = 0.6203291416168213
In grad_steps = 81, loss = 0.21422633528709412
In grad_steps = 82, loss = 1.0203803777694702
In grad_steps = 83, loss = 0.7706384658813477
In grad_steps = 84, loss = 0.7384029030799866
In grad_steps = 85, loss = 0.7694770097732544
In grad_steps = 86, loss = 0.8155115246772766
In grad_steps = 87, loss = 0.6032692790031433
In grad_steps = 88, loss = 0.4719778299331665
In grad_steps = 89, loss = 0.6498243808746338
In grad_steps = 90, loss = 0.6831395626068115
In grad_steps = 91, loss = 0.7040261030197144
In grad_steps = 92, loss = 0.6290719509124756
In grad_steps = 93, loss = 0.8719539642333984
In grad_steps = 94, loss = 0.700592041015625
In grad_steps = 95, loss = 0.3728005290031433
In grad_steps = 96, loss = 0.6846633553504944
In grad_steps = 97, loss = 0.6729023456573486
In grad_steps = 98, loss = 0.704753577709198
In grad_steps = 99, loss = 0.49926769733428955
In grad_steps = 100, loss = 0.9365618228912354
In grad_steps = 101, loss = 1.0428447723388672
In grad_steps = 102, loss = 0.8903844952583313
In grad_steps = 103, loss = 0.3210030794143677
In grad_steps = 104, loss = 0.5686824321746826
In grad_steps = 105, loss = 0.6022152900695801
In grad_steps = 106, loss = 0.5351237058639526
In grad_steps = 107, loss = 0.6228656768798828
In grad_steps = 108, loss = 0.49348366260528564
In grad_steps = 109, loss = 0.5349657535552979
In grad_steps = 110, loss = 0.9804061651229858
In grad_steps = 111, loss = 0.3678259253501892
In grad_steps = 112, loss = 0.3012135624885559
In grad_steps = 113, loss = 0.5810785293579102
In grad_steps = 114, loss = 0.24634118378162384
In grad_steps = 115, loss = 1.0588793754577637
In grad_steps = 116, loss = 0.41958022117614746
In grad_steps = 117, loss = 0.4803309440612793
In grad_steps = 118, loss = 0.5248312950134277
In grad_steps = 119, loss = 0.6343671083450317
In grad_steps = 120, loss = 0.3394401967525482
In grad_steps = 121, loss = 0.37687891721725464
In grad_steps = 122, loss = 0.3859136402606964
In grad_steps = 123, loss = 0.8763324022293091
In grad_steps = 124, loss = 0.695961594581604
In grad_steps = 125, loss = 0.7311681509017944
In grad_steps = 126, loss = 0.2008824646472931
In grad_steps = 127, loss = 0.5414706468582153
In grad_steps = 128, loss = 0.4536518156528473
In grad_steps = 129, loss = 0.5543903708457947
In grad_steps = 130, loss = 1.123680591583252
In grad_steps = 131, loss = 0.735059916973114
In grad_steps = 132, loss = 0.5327509045600891
In grad_steps = 133, loss = 0.4845157861709595
In grad_steps = 134, loss = 0.5644258260726929
In grad_steps = 135, loss = 0.6178210377693176
In grad_steps = 136, loss = 0.42642340064048767
In grad_steps = 137, loss = 0.3084288239479065
In grad_steps = 138, loss = 0.5092985033988953
In grad_steps = 139, loss = 0.29296180605888367
In grad_steps = 140, loss = 0.8125301599502563
In grad_steps = 141, loss = 0.3640434741973877
In grad_steps = 142, loss = 0.46521565318107605
In grad_steps = 143, loss = 0.2122030109167099
In grad_steps = 144, loss = 0.648618757724762
In grad_steps = 145, loss = 0.26505452394485474
In grad_steps = 146, loss = 0.6622406840324402
In grad_steps = 147, loss = 0.250624418258667
In grad_steps = 148, loss = 0.25959494709968567
In grad_steps = 149, loss = 0.3855065703392029
In grad_steps = 150, loss = 0.9416985511779785
In grad_steps = 151, loss = 0.5628063678741455
In grad_steps = 152, loss = 1.4308207035064697
In grad_steps = 153, loss = 0.8650482892990112
In grad_steps = 154, loss = 0.5988332033157349
In grad_steps = 155, loss = 0.20624490082263947
In grad_steps = 156, loss = 0.49510177969932556
In grad_steps = 157, loss = 0.15220780670642853
In grad_steps = 158, loss = 0.217869833111763
In grad_steps = 159, loss = 0.5452308654785156
In grad_steps = 160, loss = 0.21403072774410248
In grad_steps = 161, loss = 0.7945896983146667
In grad_steps = 162, loss = 0.5274873971939087
In grad_steps = 163, loss = 0.30051371455192566
In grad_steps = 164, loss = 0.7862293720245361
In grad_steps = 165, loss = 0.2398666888475418
In grad_steps = 166, loss = 0.16019882261753082
In grad_steps = 167, loss = 0.6624430418014526
In grad_steps = 168, loss = 0.5978364944458008
In grad_steps = 169, loss = 0.13128527998924255
In grad_steps = 170, loss = 0.21830105781555176
In grad_steps = 171, loss = 0.4693281948566437
In grad_steps = 172, loss = 0.3350837826728821
In grad_steps = 173, loss = 0.1336783766746521
In grad_steps = 174, loss = 0.12229315936565399
In grad_steps = 175, loss = 0.6904884576797485
In grad_steps = 176, loss = 0.06706087291240692
In grad_steps = 177, loss = 0.16473469138145447
In grad_steps = 178, loss = 0.7359093427658081
In grad_steps = 179, loss = 1.1454746723175049
In grad_steps = 180, loss = 0.19627411663532257
In grad_steps = 181, loss = 0.3917520046234131
In grad_steps = 182, loss = 0.6635432839393616
In grad_steps = 183, loss = 0.285222589969635
In grad_steps = 184, loss = 0.5681367516517639
In grad_steps = 185, loss = 0.5364086627960205
In grad_steps = 186, loss = 0.41977083683013916
In grad_steps = 187, loss = 0.4495261609554291
In grad_steps = 188, loss = 0.5592169165611267
In grad_steps = 189, loss = 0.287921279668808
In grad_steps = 190, loss = 0.16101175546646118
In grad_steps = 191, loss = 0.3125514090061188
In grad_steps = 192, loss = 0.5177401900291443
In grad_steps = 193, loss = 0.49811452627182007
In grad_steps = 194, loss = 1.0044668912887573
In grad_steps = 195, loss = 0.6372382640838623
In grad_steps = 196, loss = 0.605972170829773
In grad_steps = 197, loss = 0.6151163578033447
In grad_steps = 198, loss = 0.5723282694816589
In grad_steps = 199, loss = 1.2543432712554932
In grad_steps = 200, loss = 0.2761884331703186
In grad_steps = 201, loss = 0.7882949113845825
In grad_steps = 202, loss = 0.47061190009117126
In grad_steps = 203, loss = 0.34366267919540405
In grad_steps = 204, loss = 0.5253143310546875
In grad_steps = 205, loss = 0.545066237449646
In grad_steps = 206, loss = 0.2862321436405182
In grad_steps = 207, loss = 0.40837791562080383
In grad_steps = 208, loss = 0.3495253622531891
In grad_steps = 209, loss = 0.15610459446907043
In grad_steps = 210, loss = 0.23093678057193756
In grad_steps = 211, loss = 0.8887550830841064
In grad_steps = 212, loss = 0.45106834173202515
In grad_steps = 213, loss = 0.3391852378845215
In grad_steps = 214, loss = 0.20202501118183136
In grad_steps = 215, loss = 0.26169174909591675
In grad_steps = 216, loss = 0.5160335898399353
In grad_steps = 217, loss = 0.44154754281044006
In grad_steps = 218, loss = 1.6188786029815674
In grad_steps = 219, loss = 0.5273703336715698
In grad_steps = 220, loss = 0.36426568031311035
In grad_steps = 221, loss = 0.9545332193374634
In grad_steps = 222, loss = 0.4841799736022949
In grad_steps = 223, loss = 0.5494756698608398
In grad_steps = 224, loss = 0.17860856652259827
In grad_steps = 225, loss = 0.06978084146976471
In grad_steps = 226, loss = 0.6794279217720032
In grad_steps = 227, loss = 0.48604506254196167
In grad_steps = 228, loss = 0.4721437692642212
In grad_steps = 229, loss = 0.3347534239292145
In grad_steps = 230, loss = 0.3194037079811096
In grad_steps = 231, loss = 0.7558423280715942
In grad_steps = 232, loss = 0.9574015736579895
In grad_steps = 233, loss = 0.20988884568214417
In grad_steps = 234, loss = 0.628238320350647
In grad_steps = 235, loss = 0.7256330251693726
In grad_steps = 236, loss = 0.5799526572227478
In grad_steps = 237, loss = 0.19530539214611053
In grad_steps = 238, loss = 0.17495252192020416
In grad_steps = 239, loss = 0.258652001619339
In grad_steps = 240, loss = 0.17581835389137268
In grad_steps = 241, loss = 0.6770750284194946
In grad_steps = 242, loss = 0.3312388062477112
In grad_steps = 243, loss = 0.29255807399749756
In grad_steps = 244, loss = 0.9219903945922852
In grad_steps = 245, loss = 0.23459309339523315
In grad_steps = 246, loss = 0.21103617548942566
In grad_steps = 247, loss = 0.516377329826355
In grad_steps = 248, loss = 1.135514736175537
In grad_steps = 249, loss = 0.14519260823726654
In grad_steps = 250, loss = 0.7502850890159607
In grad_steps = 251, loss = 0.8105915188789368
In grad_steps = 252, loss = 0.1851075142621994
In grad_steps = 253, loss = 0.5455726981163025
In grad_steps = 254, loss = 0.5133728981018066
In grad_steps = 255, loss = 0.5162779688835144
In grad_steps = 256, loss = 1.0190668106079102
In grad_steps = 257, loss = 0.6150546073913574
In grad_steps = 258, loss = 0.9610151052474976
In grad_steps = 259, loss = 0.7342802882194519
In grad_steps = 260, loss = 0.7089272737503052
In grad_steps = 261, loss = 1.146349310874939
In grad_steps = 262, loss = 0.5947946310043335
In grad_steps = 263, loss = 0.5771346092224121
In grad_steps = 264, loss = 0.46837353706359863
In grad_steps = 265, loss = 0.6141151189804077
In grad_steps = 266, loss = 0.9025405645370483
In grad_steps = 267, loss = 0.8903588652610779
In grad_steps = 268, loss = 0.5526022911071777
In grad_steps = 269, loss = 0.36213189363479614
In grad_steps = 270, loss = 0.5029821991920471
In grad_steps = 271, loss = 0.6280484199523926
In grad_steps = 272, loss = 0.5336236357688904
In grad_steps = 273, loss = 0.5008721351623535
In grad_steps = 274, loss = 0.4712587594985962
In grad_steps = 275, loss = 0.5878759026527405
In grad_steps = 276, loss = 0.5370818972587585
In grad_steps = 277, loss = 0.46659791469573975
In grad_steps = 278, loss = 0.49504712224006653
In grad_steps = 279, loss = 0.567453145980835
In grad_steps = 280, loss = 0.5401164293289185
In grad_steps = 281, loss = 0.48425889015197754
In grad_steps = 282, loss = 0.6308816075325012
In grad_steps = 283, loss = 0.4824918806552887
In grad_steps = 284, loss = 0.4653931260108948
In grad_steps = 285, loss = 0.3889939486980438
In grad_steps = 286, loss = 0.27676814794540405
In grad_steps = 287, loss = 0.8600901365280151
In grad_steps = 288, loss = 0.9081417918205261
In grad_steps = 289, loss = 0.493685781955719
In grad_steps = 290, loss = 0.3827652335166931
In grad_steps = 291, loss = 1.1281615495681763
In grad_steps = 292, loss = 0.6336389780044556
In grad_steps = 293, loss = 0.8255466222763062
In grad_steps = 294, loss = 0.3629651665687561
In grad_steps = 295, loss = 0.9349300265312195
In grad_steps = 296, loss = 0.36496615409851074
In grad_steps = 297, loss = 0.3533281981945038
In grad_steps = 298, loss = 0.3602263629436493
In grad_steps = 299, loss = 0.5426837801933289
In grad_steps = 300, loss = 0.66029953956604
In grad_steps = 301, loss = 0.5821638703346252
In grad_steps = 302, loss = 0.3931747078895569
In grad_steps = 303, loss = 0.5686361789703369
In grad_steps = 304, loss = 0.14113301038742065
In grad_steps = 305, loss = 0.224492147564888
In grad_steps = 306, loss = 0.6211822032928467
In grad_steps = 307, loss = 0.37179988622665405
In grad_steps = 308, loss = 0.6511325836181641
In grad_steps = 309, loss = 0.5123859643936157
In grad_steps = 310, loss = 0.2248014658689499
In grad_steps = 311, loss = 1.516353726387024
In grad_steps = 312, loss = 0.23044614493846893
In grad_steps = 313, loss = 0.25437819957733154
In grad_steps = 314, loss = 0.5115559697151184
In grad_steps = 315, loss = 0.11650783568620682
In grad_steps = 316, loss = 1.1823103427886963
In grad_steps = 317, loss = 0.4097633957862854
In grad_steps = 318, loss = 0.3469875156879425
In grad_steps = 319, loss = 0.1549021154642105
In grad_steps = 320, loss = 0.5834302306175232
In grad_steps = 321, loss = 0.18909306824207306
In grad_steps = 322, loss = 0.9776208996772766
In grad_steps = 323, loss = 0.3929138779640198
In grad_steps = 324, loss = 0.3902469873428345
In grad_steps = 325, loss = 0.23725149035453796
In grad_steps = 326, loss = 0.4593353867530823
In grad_steps = 327, loss = 0.28091955184936523
In grad_steps = 328, loss = 0.2572755217552185
In grad_steps = 329, loss = 0.9085906744003296
In grad_steps = 330, loss = 0.23791082203388214
In grad_steps = 331, loss = 0.08583559840917587
In grad_steps = 332, loss = 0.11554858833551407
In grad_steps = 333, loss = 0.16927722096443176
In grad_steps = 334, loss = 0.447701096534729
In grad_steps = 335, loss = 0.31033679842948914
In grad_steps = 336, loss = 0.7015871405601501
In grad_steps = 337, loss = 0.13698001205921173
In grad_steps = 338, loss = 0.08218248188495636
In grad_steps = 339, loss = 0.4323255121707916
In grad_steps = 340, loss = 0.06982595473527908
In grad_steps = 341, loss = 0.7879247665405273
In grad_steps = 342, loss = 0.1599106341600418
In grad_steps = 343, loss = 0.3850526213645935
In grad_steps = 344, loss = 0.8911905288696289
In grad_steps = 345, loss = 0.5959877371788025
In grad_steps = 346, loss = 0.38336703181266785
In grad_steps = 347, loss = 0.499965101480484
In grad_steps = 348, loss = 0.7716114521026611
In grad_steps = 349, loss = 0.9795548319816589
In grad_steps = 350, loss = 0.24339093267917633
In grad_steps = 351, loss = 1.2462066411972046
In grad_steps = 352, loss = 0.6236686110496521
In grad_steps = 353, loss = 0.6168484687805176
In grad_steps = 354, loss = 0.6922686100006104
In grad_steps = 355, loss = 0.7305830717086792
In grad_steps = 356, loss = 0.2987178564071655
In grad_steps = 357, loss = 0.24599070847034454
In grad_steps = 358, loss = 0.9432015419006348
In grad_steps = 359, loss = 0.4953887164592743
In grad_steps = 360, loss = 0.8741930723190308
In grad_steps = 361, loss = 0.7308902740478516
In grad_steps = 362, loss = 0.6358446478843689
In grad_steps = 363, loss = 0.9213855266571045
In grad_steps = 364, loss = 0.36304140090942383
In grad_steps = 365, loss = 0.731562614440918
In grad_steps = 366, loss = 0.8156453371047974
In grad_steps = 367, loss = 0.5986998677253723
In grad_steps = 368, loss = 0.5063093304634094
In grad_steps = 369, loss = 0.4196713864803314
In grad_steps = 370, loss = 0.639509379863739
In grad_steps = 371, loss = 0.4719909131526947
In grad_steps = 372, loss = 0.41455715894699097
In grad_steps = 373, loss = 0.5702187418937683
In grad_steps = 374, loss = 0.39678460359573364
In grad_steps = 375, loss = 0.564297080039978
In grad_steps = 376, loss = 0.4780382215976715
In grad_steps = 377, loss = 0.3703911304473877
In grad_steps = 378, loss = 0.28231024742126465
In grad_steps = 379, loss = 0.20619696378707886
In grad_steps = 380, loss = 0.6094733476638794
In grad_steps = 381, loss = 0.8242350816726685
In grad_steps = 382, loss = 0.38549232482910156
In grad_steps = 383, loss = 0.2747192978858948
In grad_steps = 384, loss = 0.1163971871137619
In grad_steps = 385, loss = 0.07094945758581161
In grad_steps = 386, loss = 0.14257784187793732
In grad_steps = 387, loss = 1.5766260623931885
In grad_steps = 388, loss = 0.8236852288246155
In grad_steps = 389, loss = 0.3766380548477173
In grad_steps = 390, loss = 0.40159910917282104
In grad_steps = 391, loss = 0.34153157472610474
In grad_steps = 392, loss = 0.46189895272254944
In grad_steps = 393, loss = 0.6938030123710632
In grad_steps = 394, loss = 0.2267996370792389
In grad_steps = 395, loss = 1.2112417221069336
In grad_steps = 396, loss = 0.876807689666748
In grad_steps = 397, loss = 0.06989467889070511
In grad_steps = 398, loss = 0.8570677638053894
In grad_steps = 399, loss = 0.204338937997818
In grad_steps = 400, loss = 1.0671229362487793
In grad_steps = 401, loss = 0.2669484317302704
In grad_steps = 402, loss = 0.22981694340705872
In grad_steps = 403, loss = 0.6198105812072754
In grad_steps = 404, loss = 0.24986883997917175
In grad_steps = 405, loss = 0.3933470547199249
In grad_steps = 406, loss = 0.8105900287628174
In grad_steps = 407, loss = 0.34250932931900024
In grad_steps = 408, loss = 0.8439006805419922
In grad_steps = 409, loss = 0.531355619430542
In grad_steps = 410, loss = 0.4153575003147125
In grad_steps = 411, loss = 0.3076273202896118
In grad_steps = 412, loss = 0.7548789381980896
In grad_steps = 413, loss = 0.8863240480422974
In grad_steps = 414, loss = 1.0577898025512695
In grad_steps = 415, loss = 0.58286452293396
In grad_steps = 416, loss = 1.1647155284881592
In grad_steps = 417, loss = 0.2985990047454834
In grad_steps = 418, loss = 0.19517363607883453
In grad_steps = 419, loss = 0.24915438890457153
In grad_steps = 420, loss = 0.3103535771369934
In grad_steps = 421, loss = 0.16776928305625916
In grad_steps = 422, loss = 0.4463653564453125
In grad_steps = 423, loss = 0.7625768780708313
In grad_steps = 424, loss = 0.5765504240989685
In grad_steps = 425, loss = 0.142626091837883
In grad_steps = 426, loss = 0.12363220751285553
In grad_steps = 427, loss = 0.3421800136566162
In grad_steps = 428, loss = 0.24684105813503265
In grad_steps = 429, loss = 0.781093955039978
In grad_steps = 430, loss = 0.12237872928380966
In grad_steps = 431, loss = 1.4251272678375244
In grad_steps = 432, loss = 0.5977193117141724
In grad_steps = 433, loss = 1.365077018737793
In grad_steps = 434, loss = 0.3062240481376648
In grad_steps = 435, loss = 0.1682974100112915
In grad_steps = 436, loss = 0.20415279269218445
In grad_steps = 437, loss = 0.48210859298706055
In grad_steps = 438, loss = 0.24190223217010498
In grad_steps = 439, loss = 0.1691567301750183
In grad_steps = 440, loss = 0.14349165558815002
In grad_steps = 441, loss = 0.11911596357822418
In grad_steps = 442, loss = 0.3412761092185974
In grad_steps = 443, loss = 0.42286527156829834
In grad_steps = 444, loss = 0.8064934611320496
In grad_steps = 445, loss = 0.16131027042865753
In grad_steps = 446, loss = 0.20617161691188812
In grad_steps = 447, loss = 0.2542111277580261
In grad_steps = 448, loss = 0.26854443550109863
In grad_steps = 449, loss = 0.09610449522733688
In grad_steps = 450, loss = 0.1415635347366333
In grad_steps = 451, loss = 0.27623480558395386
In grad_steps = 452, loss = 0.23437152802944183
In grad_steps = 453, loss = 0.1936061680316925
In grad_steps = 454, loss = 0.385426789522171
In grad_steps = 455, loss = 0.1861739456653595
In grad_steps = 456, loss = 1.15324866771698
In grad_steps = 457, loss = 0.029818827286362648
In grad_steps = 458, loss = 0.2115904688835144
In grad_steps = 459, loss = 0.24696217477321625
In grad_steps = 460, loss = 0.036991994827985764
In grad_steps = 461, loss = 0.7178864479064941
In grad_steps = 462, loss = 0.24683360755443573
In grad_steps = 463, loss = 0.21127675473690033
In grad_steps = 464, loss = 0.4377915859222412
In grad_steps = 465, loss = 0.033024564385414124
In grad_steps = 466, loss = 0.15598450601100922
In grad_steps = 467, loss = 0.4668187201023102
In grad_steps = 468, loss = 0.5185694098472595
In grad_steps = 469, loss = 2.005847930908203
In grad_steps = 470, loss = 0.07932553440332413
In grad_steps = 471, loss = 1.0728139877319336
In grad_steps = 472, loss = 0.6300387978553772
In grad_steps = 473, loss = 0.8792874217033386
In grad_steps = 474, loss = 0.29855582118034363
In grad_steps = 475, loss = 0.07947304844856262
In grad_steps = 476, loss = 0.11396166682243347
In grad_steps = 477, loss = 0.7876670360565186
In grad_steps = 478, loss = 0.2245057374238968
In grad_steps = 479, loss = 0.23436538875102997
In grad_steps = 480, loss = 0.1507287174463272
In grad_steps = 481, loss = 0.20976881682872772
In grad_steps = 482, loss = 0.28131601214408875
In grad_steps = 483, loss = 0.9331995248794556
In grad_steps = 484, loss = 0.21452733874320984
In grad_steps = 485, loss = 0.513877809047699
In grad_steps = 486, loss = 0.260938435792923
In grad_steps = 487, loss = 0.4422447979450226
In grad_steps = 488, loss = 0.27871638536453247
In grad_steps = 489, loss = 0.380724161863327
In grad_steps = 490, loss = 0.19255104660987854
In grad_steps = 491, loss = 0.14631028473377228
In grad_steps = 492, loss = 0.2806694507598877
In grad_steps = 493, loss = 0.5834190845489502
In grad_steps = 494, loss = 0.3185942769050598
In grad_steps = 495, loss = 0.1398741900920868
In grad_steps = 496, loss = 0.6135217547416687
In grad_steps = 497, loss = 0.791795015335083
In grad_steps = 498, loss = 0.20345529913902283
In grad_steps = 499, loss = 0.7086150646209717
In grad_steps = 500, loss = 0.051664985716342926
In grad_steps = 501, loss = 0.042113009840250015
In grad_steps = 502, loss = 1.2607825994491577
In grad_steps = 503, loss = 0.08261854946613312
In grad_steps = 504, loss = 0.7871419191360474
In grad_steps = 505, loss = 0.04247505962848663
In grad_steps = 506, loss = 0.3306232988834381
In grad_steps = 507, loss = 0.11853551119565964
In grad_steps = 508, loss = 0.5671948790550232
In grad_steps = 509, loss = 0.7206587791442871
In grad_steps = 510, loss = 0.5777816772460938
In grad_steps = 511, loss = 0.23972740769386292
In grad_steps = 512, loss = 0.35352081060409546
In grad_steps = 513, loss = 0.16555234789848328
In grad_steps = 514, loss = 0.3766080141067505
In grad_steps = 515, loss = 0.13686922192573547
In grad_steps = 516, loss = 0.20678383111953735
In grad_steps = 517, loss = 0.42480918765068054
In grad_steps = 518, loss = 0.15809914469718933
In grad_steps = 519, loss = 0.21527066826820374
In grad_steps = 520, loss = 0.6534422039985657
In grad_steps = 521, loss = 0.47171345353126526
In grad_steps = 522, loss = 0.7163646221160889
In grad_steps = 523, loss = 0.4382029175758362
In grad_steps = 524, loss = 0.6267242431640625
In grad_steps = 525, loss = 0.5652470588684082
In grad_steps = 526, loss = 0.5768107175827026
In grad_steps = 527, loss = 0.804702639579773
In grad_steps = 528, loss = 0.5070565938949585
In grad_steps = 529, loss = 0.385492205619812
In grad_steps = 530, loss = 0.1348431408405304
In grad_steps = 531, loss = 0.14297442138195038
In grad_steps = 532, loss = 0.7728157043457031
In grad_steps = 533, loss = 0.5693252086639404
In grad_steps = 534, loss = 0.3749299645423889
In grad_steps = 535, loss = 1.4591565132141113
In grad_steps = 536, loss = 0.5256512761116028
In grad_steps = 537, loss = 0.4886130690574646
In grad_steps = 538, loss = 0.6167118549346924
In grad_steps = 539, loss = 0.5572792887687683
In grad_steps = 540, loss = 0.30830317735671997
In grad_steps = 541, loss = 0.3762315511703491
In grad_steps = 542, loss = 0.3518410921096802
In grad_steps = 543, loss = 0.3158956468105316
In grad_steps = 544, loss = 0.6686915159225464
In grad_steps = 545, loss = 0.8074015378952026
In grad_steps = 546, loss = 0.47368577122688293
In grad_steps = 547, loss = 0.6549440622329712
In grad_steps = 548, loss = 0.21332323551177979
In grad_steps = 549, loss = 0.6738417744636536
In grad_steps = 550, loss = 0.1723051518201828
In grad_steps = 551, loss = 0.22886782884597778
In grad_steps = 552, loss = 0.4689652919769287
In grad_steps = 553, loss = 0.24950753152370453
In grad_steps = 554, loss = 0.35999348759651184
In grad_steps = 555, loss = 0.17416927218437195
In grad_steps = 556, loss = 0.6024276614189148
In grad_steps = 557, loss = 0.6614199876785278
In grad_steps = 558, loss = 0.34472620487213135
In grad_steps = 559, loss = 0.37529224157333374
In grad_steps = 560, loss = 0.26788613200187683
In grad_steps = 561, loss = 0.29394301772117615
In grad_steps = 562, loss = 0.1764356940984726
In grad_steps = 563, loss = 0.6154295206069946
In grad_steps = 564, loss = 0.16865529119968414
In grad_steps = 565, loss = 0.22070348262786865
In grad_steps = 566, loss = 0.2703743577003479
In grad_steps = 567, loss = 1.2398240566253662
In grad_steps = 568, loss = 0.4095158874988556
In grad_steps = 569, loss = 0.264069139957428
In grad_steps = 570, loss = 0.5992939472198486
In grad_steps = 571, loss = 0.7128272652626038
In grad_steps = 572, loss = 0.08692692965269089
In grad_steps = 573, loss = 0.08281198889017105
In grad_steps = 574, loss = 0.2227616310119629
In grad_steps = 575, loss = 0.06671197712421417
In grad_steps = 576, loss = 0.4321313500404358
In grad_steps = 577, loss = 0.07888256013393402
In grad_steps = 578, loss = 1.7163304090499878
In grad_steps = 579, loss = 0.3506169021129608
In grad_steps = 580, loss = 0.4832751750946045
In grad_steps = 581, loss = 0.7769346237182617
In grad_steps = 582, loss = 0.7365642189979553
In grad_steps = 583, loss = 0.24546687304973602
In grad_steps = 584, loss = 0.35130807757377625
In grad_steps = 585, loss = 0.5620335936546326
In grad_steps = 586, loss = 0.2542872428894043
In grad_steps = 587, loss = 0.2783980965614319
In grad_steps = 588, loss = 0.4403528869152069
In grad_steps = 589, loss = 0.38858652114868164
In grad_steps = 590, loss = 0.1384434700012207
In grad_steps = 591, loss = 0.25789108872413635
In grad_steps = 592, loss = 0.3589377701282501
In grad_steps = 593, loss = 0.3341854214668274
In grad_steps = 594, loss = 0.2886642515659332
In grad_steps = 595, loss = 0.7113280892372131
In grad_steps = 596, loss = 0.7155466675758362
In grad_steps = 597, loss = 0.37323281168937683
In grad_steps = 598, loss = 0.3572435975074768
In grad_steps = 599, loss = 0.7424840927124023
In grad_steps = 600, loss = 1.1533311605453491
In grad_steps = 601, loss = 0.27861422300338745
In grad_steps = 602, loss = 0.6247757077217102
In grad_steps = 603, loss = 0.3051930069923401
In grad_steps = 604, loss = 0.6524470448493958
In grad_steps = 605, loss = 1.1463922262191772
In grad_steps = 606, loss = 0.2612374722957611
In grad_steps = 607, loss = 0.16118746995925903
In grad_steps = 608, loss = 0.49016353487968445
In grad_steps = 609, loss = 0.1740403026342392
In grad_steps = 610, loss = 0.20561891794204712
In grad_steps = 611, loss = 0.2633923590183258
In grad_steps = 612, loss = 0.2998356819152832
In grad_steps = 613, loss = 0.19799283146858215
In grad_steps = 614, loss = 0.17906521260738373
In grad_steps = 615, loss = 0.839199423789978
In grad_steps = 616, loss = 0.3786076009273529
In grad_steps = 617, loss = 0.21029049158096313
In grad_steps = 618, loss = 0.3687189817428589
In grad_steps = 619, loss = 0.11996239423751831
In grad_steps = 620, loss = 0.3608900308609009
In grad_steps = 621, loss = 0.3062012791633606
In grad_steps = 622, loss = 0.08344411104917526
In grad_steps = 623, loss = 0.14608225226402283
In grad_steps = 624, loss = 0.04938187822699547
In grad_steps = 625, loss = 0.15653160214424133
In grad_steps = 626, loss = 0.2650224268436432
In grad_steps = 627, loss = 0.2651044428348541
In grad_steps = 628, loss = 0.19415143132209778
In grad_steps = 629, loss = 0.6566389799118042
In grad_steps = 630, loss = 0.4698350727558136
In grad_steps = 631, loss = 0.5407067537307739
In grad_steps = 632, loss = 0.3618725836277008
In grad_steps = 633, loss = 0.4356946349143982
In grad_steps = 634, loss = 0.26507556438446045
In grad_steps = 635, loss = 1.017162561416626
In grad_steps = 636, loss = 0.19053810834884644
In grad_steps = 637, loss = 0.4921749532222748
In grad_steps = 638, loss = 0.15860047936439514
In grad_steps = 639, loss = 1.1047766208648682
In grad_steps = 640, loss = 1.3767130374908447
In grad_steps = 641, loss = 0.48604586720466614
In grad_steps = 642, loss = 0.1947895586490631
In grad_steps = 643, loss = 0.08784934133291245
In grad_steps = 644, loss = 0.6123402118682861
In grad_steps = 645, loss = 0.7279373407363892
In grad_steps = 646, loss = 0.6330610513687134
In grad_steps = 647, loss = 0.5572031140327454
In grad_steps = 648, loss = 0.15597957372665405
In grad_steps = 649, loss = 0.3478546142578125
In grad_steps = 650, loss = 0.5499154925346375
In grad_steps = 651, loss = 0.5423176884651184
In grad_steps = 652, loss = 0.32048100233078003
In grad_steps = 653, loss = 0.297268807888031
In grad_steps = 654, loss = 0.6672002077102661
In grad_steps = 655, loss = 0.5353937149047852
In grad_steps = 656, loss = 0.43188709020614624
In grad_steps = 657, loss = 0.7988020181655884
In grad_steps = 658, loss = 0.3276246190071106
In grad_steps = 659, loss = 0.4638729691505432
In grad_steps = 660, loss = 0.8303685784339905
In grad_steps = 661, loss = 0.7931798696517944
In grad_steps = 662, loss = 0.46322712302207947
In grad_steps = 663, loss = 0.228975310921669
In grad_steps = 664, loss = 0.8336744904518127
In grad_steps = 665, loss = 0.6274335980415344
In grad_steps = 666, loss = 0.19434282183647156
In grad_steps = 667, loss = 0.46008360385894775
In grad_steps = 668, loss = 0.5911859273910522
In grad_steps = 669, loss = 0.304157555103302
In grad_steps = 670, loss = 0.34453076124191284
In grad_steps = 671, loss = 0.6178158521652222
In grad_steps = 672, loss = 0.45806264877319336
In grad_steps = 673, loss = 0.8725482821464539
In grad_steps = 674, loss = 0.1855470985174179
In grad_steps = 675, loss = 1.0017684698104858
In grad_steps = 676, loss = 0.6645799875259399
In grad_steps = 677, loss = 0.23542246222496033
In grad_steps = 678, loss = 0.263202965259552
In grad_steps = 679, loss = 0.247505322098732
In grad_steps = 680, loss = 0.3512694835662842
In grad_steps = 681, loss = 0.2443593144416809
In grad_steps = 682, loss = 0.9768019914627075
In grad_steps = 683, loss = 0.19700530171394348
In grad_steps = 684, loss = 0.44119298458099365
In grad_steps = 685, loss = 0.17512819170951843
In grad_steps = 686, loss = 0.18555763363838196
In grad_steps = 687, loss = 0.3607156276702881
In grad_steps = 688, loss = 0.1579074263572693
In grad_steps = 689, loss = 0.34372442960739136
In grad_steps = 690, loss = 0.13314679265022278
In grad_steps = 691, loss = 0.7858080863952637
In grad_steps = 692, loss = 0.0676971822977066
In grad_steps = 693, loss = 0.9947087168693542
In grad_steps = 694, loss = 0.4589340090751648
In grad_steps = 695, loss = 0.12990306317806244
In grad_steps = 696, loss = 0.13357625901699066
In grad_steps = 697, loss = 0.22680886089801788
In grad_steps = 698, loss = 0.2100914567708969
In grad_steps = 699, loss = 0.22500814497470856
In grad_steps = 700, loss = 0.16381500661373138
In grad_steps = 701, loss = 0.8694743514060974
In grad_steps = 702, loss = 0.04337724298238754
In grad_steps = 703, loss = 0.10973627865314484
In grad_steps = 704, loss = 0.07027234882116318
In grad_steps = 705, loss = 1.583420753479004
In grad_steps = 706, loss = 0.16095444560050964
In grad_steps = 707, loss = 0.1528407633304596
In grad_steps = 708, loss = 0.0989440530538559
In grad_steps = 709, loss = 0.5622802972793579
In grad_steps = 710, loss = 0.13722844421863556
In grad_steps = 711, loss = 0.09855613112449646
In grad_steps = 712, loss = 0.045074671506881714
In grad_steps = 713, loss = 2.9264097213745117
In grad_steps = 714, loss = 0.07718625664710999
In grad_steps = 715, loss = 0.7601470947265625
In grad_steps = 716, loss = 0.5125264525413513
In grad_steps = 717, loss = 0.3459767699241638
In grad_steps = 718, loss = 0.07899817079305649
In grad_steps = 719, loss = 0.3998679220676422
In grad_steps = 720, loss = 2.0379931926727295
In grad_steps = 721, loss = 0.527566134929657
In grad_steps = 722, loss = 0.25905507802963257
In grad_steps = 723, loss = 0.7382298111915588
In grad_steps = 724, loss = 0.13282576203346252
In grad_steps = 725, loss = 0.230121448636055
In grad_steps = 726, loss = 0.3722628057003021
In grad_steps = 727, loss = 0.4341597557067871
In grad_steps = 728, loss = 0.2796322703361511
In grad_steps = 729, loss = 0.2284948080778122
In grad_steps = 730, loss = 0.21454870700836182
In grad_steps = 731, loss = 0.29602932929992676
In grad_steps = 732, loss = 0.21366003155708313
In grad_steps = 733, loss = 0.5368274450302124
In grad_steps = 734, loss = 0.38853758573532104
In grad_steps = 735, loss = 0.6156946420669556
In grad_steps = 736, loss = 1.631686806678772
In grad_steps = 737, loss = 0.4557032287120819
In grad_steps = 738, loss = 0.3830816447734833
In grad_steps = 739, loss = 0.17066419124603271
In grad_steps = 740, loss = 0.3074910640716553
In grad_steps = 741, loss = 0.4637151062488556
In grad_steps = 742, loss = 0.29198092222213745
In grad_steps = 743, loss = 0.29820823669433594
In grad_steps = 744, loss = 0.40545469522476196
In grad_steps = 745, loss = 0.034823473542928696
In grad_steps = 746, loss = 0.5917726755142212
In grad_steps = 747, loss = 0.23981145024299622
In grad_steps = 748, loss = 0.27405235171318054
In grad_steps = 749, loss = 0.43153199553489685
In grad_steps = 750, loss = 1.1217023134231567
In grad_steps = 751, loss = 0.4922662079334259
In grad_steps = 752, loss = 0.4786161780357361
In grad_steps = 753, loss = 0.10938028991222382
In grad_steps = 754, loss = 0.6466429829597473
In grad_steps = 755, loss = 0.11331015080213547
In grad_steps = 756, loss = 0.08104448020458221
In grad_steps = 757, loss = 0.09757988154888153
In grad_steps = 758, loss = 0.2563329339027405
In grad_steps = 759, loss = 0.6063816547393799
In grad_steps = 760, loss = 0.1336759477853775
In grad_steps = 761, loss = 0.04550055041909218
In grad_steps = 762, loss = 0.1496979296207428
In grad_steps = 763, loss = 0.411761999130249
In grad_steps = 764, loss = 0.3851047456264496
In grad_steps = 765, loss = 0.05909809097647667
In grad_steps = 766, loss = 0.27576884627342224
In grad_steps = 767, loss = 0.39278343319892883
In grad_steps = 768, loss = 0.027012621983885765
In grad_steps = 769, loss = 0.9559698104858398
In grad_steps = 770, loss = 0.71150803565979
In grad_steps = 771, loss = 0.769240140914917
In grad_steps = 772, loss = 0.5299771428108215
In grad_steps = 773, loss = 0.1410474181175232
In grad_steps = 774, loss = 0.11565417051315308
In grad_steps = 775, loss = 0.07666511088609695
In grad_steps = 776, loss = 0.07959683239459991
In grad_steps = 777, loss = 0.718237578868866
In grad_steps = 778, loss = 0.5664677023887634
In grad_steps = 779, loss = 0.6719540357589722
In grad_steps = 780, loss = 0.34922927618026733
In grad_steps = 781, loss = 0.40683478116989136
In grad_steps = 782, loss = 0.5131059885025024
In grad_steps = 783, loss = 0.37325993180274963
In grad_steps = 784, loss = 0.7915022373199463
In grad_steps = 785, loss = 0.6316685676574707
In grad_steps = 786, loss = 0.46225959062576294
In grad_steps = 787, loss = 0.30387577414512634
In grad_steps = 788, loss = 0.21842604875564575
In grad_steps = 789, loss = 0.2911086976528168
In grad_steps = 790, loss = 0.6861061453819275
In grad_steps = 791, loss = 0.36632874608039856
In grad_steps = 792, loss = 0.29896867275238037
In grad_steps = 793, loss = 0.17934057116508484
In grad_steps = 794, loss = 0.1692894697189331
In grad_steps = 795, loss = 0.2813892960548401
In grad_steps = 796, loss = 0.8320611715316772
In grad_steps = 797, loss = 0.11446397006511688
In grad_steps = 798, loss = 0.187239870429039
In grad_steps = 799, loss = 0.2753274738788605
In grad_steps = 800, loss = 0.1327625960111618
In grad_steps = 801, loss = 0.07762457430362701
In grad_steps = 802, loss = 0.06001158803701401
In grad_steps = 803, loss = 0.13451558351516724
In grad_steps = 804, loss = 0.03758109360933304
In grad_steps = 805, loss = 0.1167030856013298
In grad_steps = 806, loss = 1.1282868385314941
In grad_steps = 807, loss = 0.37324920296669006
In grad_steps = 808, loss = 0.10484318435192108
In grad_steps = 809, loss = 0.5774650573730469
In grad_steps = 810, loss = 0.1181449145078659
In grad_steps = 811, loss = 0.02299356833100319
In grad_steps = 812, loss = 0.6132210493087769
In grad_steps = 813, loss = 0.041599296033382416
In grad_steps = 814, loss = 0.08887899667024612
In grad_steps = 815, loss = 0.036080501973629
In grad_steps = 816, loss = 0.03429046645760536
In grad_steps = 817, loss = 0.5309990048408508
In grad_steps = 818, loss = 0.3381202816963196
In grad_steps = 819, loss = 0.3376826345920563
In grad_steps = 820, loss = 0.11953306198120117
In grad_steps = 821, loss = 0.574467122554779
In grad_steps = 822, loss = 0.9689279794692993
In grad_steps = 823, loss = 0.6618400812149048
In grad_steps = 824, loss = 0.16963079571723938
In grad_steps = 825, loss = 0.10678961127996445
In grad_steps = 826, loss = 0.2390478402376175
In grad_steps = 827, loss = 0.17939457297325134
In grad_steps = 828, loss = 0.6498702764511108
In grad_steps = 829, loss = 0.17456375062465668
In grad_steps = 830, loss = 1.0645407438278198
In grad_steps = 831, loss = 0.07078397274017334
In grad_steps = 832, loss = 0.1609746217727661
In grad_steps = 833, loss = 0.6425910592079163
In grad_steps = 834, loss = 0.24068130552768707
In grad_steps = 835, loss = 0.2475893497467041
In grad_steps = 836, loss = 0.74314284324646
In grad_steps = 837, loss = 0.6657351851463318
In grad_steps = 838, loss = 1.1631674766540527
In grad_steps = 839, loss = 0.2756369411945343
In grad_steps = 840, loss = 0.15738369524478912
In grad_steps = 841, loss = 0.18061035871505737
In grad_steps = 842, loss = 0.26224610209465027
In grad_steps = 843, loss = 0.7327150702476501
In grad_steps = 844, loss = 0.3290300667285919
In grad_steps = 845, loss = 0.454163134098053
In grad_steps = 846, loss = 0.5589744448661804
In grad_steps = 847, loss = 0.5091249346733093
In grad_steps = 848, loss = 0.16547951102256775
In grad_steps = 849, loss = 0.28639036417007446
In grad_steps = 850, loss = 0.16789326071739197
In grad_steps = 851, loss = 0.26067790389060974
In grad_steps = 852, loss = 0.7467334866523743
In grad_steps = 853, loss = 1.2128716707229614
In grad_steps = 854, loss = 0.24919500946998596
In grad_steps = 855, loss = 0.3513014614582062
In grad_steps = 856, loss = 0.17966105043888092
In grad_steps = 857, loss = 0.27972713112831116
In grad_steps = 858, loss = 0.29430148005485535
In grad_steps = 859, loss = 0.2190282791852951
In grad_steps = 860, loss = 0.2219129204750061
In grad_steps = 861, loss = 0.7997548580169678
In grad_steps = 862, loss = 0.06920844316482544
In grad_steps = 863, loss = 0.3475605249404907
In grad_steps = 864, loss = 0.3531128764152527
In grad_steps = 865, loss = 0.20362068712711334
In grad_steps = 866, loss = 0.5328930616378784
In grad_steps = 867, loss = 0.10312853008508682
In grad_steps = 868, loss = 0.26949548721313477
In grad_steps = 869, loss = 0.09821385145187378
In grad_steps = 870, loss = 0.43263933062553406
In grad_steps = 871, loss = 0.06089720129966736
In grad_steps = 872, loss = 0.34427610039711
In grad_steps = 873, loss = 0.2216918021440506
In grad_steps = 874, loss = 0.5440934300422668
In grad_steps = 875, loss = 0.3032037615776062
In grad_steps = 876, loss = 0.075857013463974
In grad_steps = 877, loss = 0.04869198054075241
In grad_steps = 878, loss = 0.6985833644866943
In grad_steps = 879, loss = 0.235067218542099
In grad_steps = 880, loss = 0.10006235539913177
In grad_steps = 881, loss = 0.7940719127655029
In grad_steps = 882, loss = 0.062401749193668365
In grad_steps = 883, loss = 0.7424724698066711
In grad_steps = 884, loss = 1.0542030334472656
In grad_steps = 885, loss = 0.18616168200969696
In grad_steps = 886, loss = 0.5263640284538269
In grad_steps = 887, loss = 0.5382524728775024
In grad_steps = 888, loss = 0.43400782346725464
In grad_steps = 889, loss = 0.2292705625295639
In grad_steps = 890, loss = 0.12399248778820038
In grad_steps = 891, loss = 0.6436063051223755
In grad_steps = 892, loss = 0.5198476314544678
In grad_steps = 893, loss = 0.6937774419784546
In grad_steps = 894, loss = 0.11401953548192978
In grad_steps = 895, loss = 0.08311142772436142
In grad_steps = 896, loss = 0.6865705251693726
In grad_steps = 897, loss = 0.20195746421813965
In grad_steps = 898, loss = 0.3904501795768738
In grad_steps = 899, loss = 0.47735142707824707
In grad_steps = 900, loss = 0.47160136699676514
In grad_steps = 901, loss = 0.43905431032180786
In grad_steps = 902, loss = 0.14335455000400543
In grad_steps = 903, loss = 0.6163729429244995
In grad_steps = 904, loss = 0.4952077567577362
In grad_steps = 905, loss = 0.11222304403781891
In grad_steps = 906, loss = 0.6028634309768677
In grad_steps = 907, loss = 0.2978135943412781
In grad_steps = 908, loss = 0.16746386885643005
In grad_steps = 909, loss = 0.8575131893157959
In grad_steps = 910, loss = 0.17982882261276245
In grad_steps = 911, loss = 0.42106691002845764
In grad_steps = 912, loss = 0.2774992883205414
In grad_steps = 913, loss = 0.6882529258728027
In grad_steps = 914, loss = 0.6913279294967651
In grad_steps = 915, loss = 0.7987836599349976
In grad_steps = 916, loss = 0.2385963797569275
In grad_steps = 917, loss = 0.3802356421947479
In grad_steps = 918, loss = 0.5881620049476624
In grad_steps = 919, loss = 0.1586216241121292
In grad_steps = 920, loss = 0.3561827540397644
In grad_steps = 921, loss = 0.8279200196266174
In grad_steps = 922, loss = 0.49103471636772156
In grad_steps = 923, loss = 0.2656919062137604
In grad_steps = 924, loss = 0.20708760619163513
In grad_steps = 925, loss = 0.1779923439025879
In grad_steps = 926, loss = 0.5290700793266296
In grad_steps = 927, loss = 0.08083987236022949
In grad_steps = 928, loss = 0.5714589357376099
In grad_steps = 929, loss = 0.12158989906311035
In grad_steps = 930, loss = 0.6554602980613708
In grad_steps = 931, loss = 0.14690789580345154
In grad_steps = 932, loss = 0.10259710252285004
In grad_steps = 933, loss = 0.5041933059692383
In grad_steps = 934, loss = 0.22991308569908142
In grad_steps = 935, loss = 1.1151174306869507
In grad_steps = 936, loss = 0.43921130895614624
In grad_steps = 937, loss = 0.07260654866695404
In grad_steps = 938, loss = 0.4007301926612854
In grad_steps = 939, loss = 0.2494751513004303
In grad_steps = 940, loss = 0.21698568761348724
In grad_steps = 941, loss = 0.14012694358825684
In grad_steps = 942, loss = 0.3309662640094757
In grad_steps = 943, loss = 1.4696992635726929
In grad_steps = 944, loss = 0.5191551446914673
In grad_steps = 945, loss = 0.3063441514968872
In grad_steps = 946, loss = 0.720320463180542
In grad_steps = 947, loss = 0.42044419050216675
In grad_steps = 948, loss = 0.4998975396156311
In grad_steps = 949, loss = 0.5398916006088257
In grad_steps = 950, loss = 0.5393070578575134
In grad_steps = 951, loss = 1.1268130540847778
In grad_steps = 952, loss = 0.12911561131477356
In grad_steps = 953, loss = 0.45057666301727295
In grad_steps = 954, loss = 0.27067089080810547
In grad_steps = 955, loss = 0.3535512387752533
In grad_steps = 956, loss = 0.41821515560150146
In grad_steps = 957, loss = 0.4347667992115021
In grad_steps = 958, loss = 0.35970237851142883
In grad_steps = 959, loss = 0.9844014048576355
In grad_steps = 960, loss = 0.3135080933570862
In grad_steps = 961, loss = 0.5249337553977966
In grad_steps = 962, loss = 0.17320260405540466
In grad_steps = 963, loss = 0.2746035158634186
In grad_steps = 964, loss = 0.2782130539417267
In grad_steps = 965, loss = 0.3228059709072113
In grad_steps = 966, loss = 0.3959888815879822
In grad_steps = 967, loss = 0.6628061532974243
In grad_steps = 968, loss = 0.4783588945865631
In grad_steps = 969, loss = 0.49476534128189087
In grad_steps = 970, loss = 0.1395936757326126
In grad_steps = 971, loss = 0.36872121691703796
In grad_steps = 972, loss = 0.6662250757217407
In grad_steps = 973, loss = 0.14074409008026123
In grad_steps = 974, loss = 0.2601510286331177
In grad_steps = 975, loss = 0.24269944429397583
In grad_steps = 976, loss = 0.34705641865730286
In grad_steps = 977, loss = 0.475628525018692
In grad_steps = 978, loss = 0.871967077255249
In grad_steps = 979, loss = 0.7749548554420471
In grad_steps = 980, loss = 0.7864981889724731
In grad_steps = 981, loss = 0.3809603452682495
In grad_steps = 982, loss = 0.36347782611846924
In grad_steps = 983, loss = 0.36133232712745667
In grad_steps = 984, loss = 1.1784350872039795
In grad_steps = 985, loss = 0.2874457538127899
In grad_steps = 986, loss = 0.1476409137248993
In grad_steps = 987, loss = 0.5704225301742554
In grad_steps = 988, loss = 0.4564198851585388
In grad_steps = 989, loss = 0.64190274477005
In grad_steps = 990, loss = 0.8818882703781128
In grad_steps = 991, loss = 0.7876443862915039
In grad_steps = 992, loss = 0.30123263597488403
In grad_steps = 993, loss = 0.6617166996002197
In grad_steps = 994, loss = 0.5352585911750793
In grad_steps = 995, loss = 0.1722830981016159
In grad_steps = 996, loss = 0.8288736343383789
In grad_steps = 997, loss = 0.2740105390548706
In grad_steps = 998, loss = 0.3336411118507385
In grad_steps = 999, loss = 0.1154162660241127
In grad_steps = 1000, loss = 0.10464607179164886
In grad_steps = 1001, loss = 0.7360666394233704
In grad_steps = 1002, loss = 0.20778891444206238
In grad_steps = 1003, loss = 0.14110077917575836
In grad_steps = 1004, loss = 0.3632625937461853
In grad_steps = 1005, loss = 0.8239842653274536
In grad_steps = 1006, loss = 0.1999078392982483
In grad_steps = 1007, loss = 0.24759161472320557
In grad_steps = 1008, loss = 0.691636860370636
In grad_steps = 1009, loss = 0.9865214228630066
In grad_steps = 1010, loss = 0.7532830834388733
In grad_steps = 1011, loss = 0.13216008245944977
In grad_steps = 1012, loss = 0.1504611074924469
In grad_steps = 1013, loss = 0.5028775930404663
In grad_steps = 1014, loss = 0.35687941312789917
In grad_steps = 1015, loss = 0.46250635385513306
In grad_steps = 1016, loss = 0.09497726708650589
In grad_steps = 1017, loss = 0.10231240093708038
In grad_steps = 1018, loss = 0.26009294390678406
In grad_steps = 1019, loss = 0.7426753044128418
In grad_steps = 1020, loss = 1.3108454942703247
In grad_steps = 1021, loss = 0.44603052735328674
In grad_steps = 1022, loss = 0.34396427869796753
In grad_steps = 1023, loss = 0.16298235952854156
In grad_steps = 1024, loss = 0.16984336078166962
In grad_steps = 1025, loss = 0.5469801425933838
In grad_steps = 1026, loss = 0.24413332343101501
In grad_steps = 1027, loss = 0.6599764823913574
In grad_steps = 1028, loss = 0.15430837869644165
In grad_steps = 1029, loss = 0.23285183310508728
In grad_steps = 1030, loss = 0.14790578186511993
In grad_steps = 1031, loss = 0.2915647327899933
In grad_steps = 1032, loss = 0.5273830890655518
In grad_steps = 1033, loss = 0.3746834993362427
In grad_steps = 1034, loss = 0.2568480670452118
In grad_steps = 1035, loss = 0.6077756881713867
In grad_steps = 1036, loss = 0.7643107175827026
In grad_steps = 1037, loss = 0.16404616832733154
In grad_steps = 1038, loss = 0.4412432909011841
In grad_steps = 1039, loss = 0.5233993530273438
In grad_steps = 1040, loss = 0.32054752111434937
In grad_steps = 1041, loss = 0.6796993017196655
In grad_steps = 1042, loss = 0.4943466782569885
In grad_steps = 1043, loss = 0.7899061441421509
In grad_steps = 1044, loss = 0.32316890358924866
In grad_steps = 1045, loss = 0.7088450193405151
In grad_steps = 1046, loss = 0.07398635894060135
In grad_steps = 1047, loss = 0.1335066556930542
In grad_steps = 1048, loss = 0.3688058853149414
In grad_steps = 1049, loss = 0.19393613934516907
In grad_steps = 1050, loss = 1.374680519104004
In grad_steps = 1051, loss = 0.11604239791631699
In grad_steps = 1052, loss = 0.15884333848953247
In grad_steps = 1053, loss = 0.44667792320251465
In grad_steps = 1054, loss = 0.14493539929389954
In grad_steps = 1055, loss = 0.3815672993659973
In grad_steps = 1056, loss = 0.9862087965011597
In grad_steps = 1057, loss = 0.4946339726448059
In grad_steps = 1058, loss = 0.377066433429718
In grad_steps = 1059, loss = 0.24598023295402527
In grad_steps = 1060, loss = 0.26222482323646545
In grad_steps = 1061, loss = 0.28281348943710327
In grad_steps = 1062, loss = 1.0389498472213745
In grad_steps = 1063, loss = 0.2555893063545227
In grad_steps = 1064, loss = 0.2964182198047638
In grad_steps = 1065, loss = 0.039061810821294785
In grad_steps = 1066, loss = 0.5552330613136292
In grad_steps = 1067, loss = 0.5951452851295471
In grad_steps = 1068, loss = 0.16214443743228912
In grad_steps = 1069, loss = 0.2792854309082031
In grad_steps = 1070, loss = 0.2851174473762512
In grad_steps = 1071, loss = 0.12082189321517944
In grad_steps = 1072, loss = 0.10639245808124542
In grad_steps = 1073, loss = 0.3409668207168579
In grad_steps = 1074, loss = 0.1630871742963791
In grad_steps = 1075, loss = 0.6787110567092896
In grad_steps = 1076, loss = 0.1916295439004898
In grad_steps = 1077, loss = 0.16869889199733734
In grad_steps = 1078, loss = 0.7833887934684753
In grad_steps = 1079, loss = 0.11847373843193054
In grad_steps = 1080, loss = 0.11030267179012299
In grad_steps = 1081, loss = 0.7561256289482117
In grad_steps = 1082, loss = 0.08795522898435593
In grad_steps = 1083, loss = 0.024884207174181938
In grad_steps = 1084, loss = 0.6307490468025208
In grad_steps = 1085, loss = 1.2534576654434204
In grad_steps = 1086, loss = 0.1180778294801712
In grad_steps = 1087, loss = 0.8547753095626831
In grad_steps = 1088, loss = 0.39272457361221313
In grad_steps = 1089, loss = 0.15336191654205322
In grad_steps = 1090, loss = 0.07204457372426987
In grad_steps = 1091, loss = 0.2742480933666229
In grad_steps = 1092, loss = 0.05423383414745331
In grad_steps = 1093, loss = 0.6725464463233948
In grad_steps = 1094, loss = 0.05919155478477478
In grad_steps = 1095, loss = 0.30884289741516113
In grad_steps = 1096, loss = 0.12481926381587982
In grad_steps = 1097, loss = 0.1262938678264618
In grad_steps = 1098, loss = 0.5211930274963379
In grad_steps = 1099, loss = 0.35013431310653687
In grad_steps = 1100, loss = 0.20266279578208923
In grad_steps = 1101, loss = 0.38095417618751526
In grad_steps = 1102, loss = 0.8426342606544495
In grad_steps = 1103, loss = 0.029872845858335495
In grad_steps = 1104, loss = 1.5633327960968018
In grad_steps = 1105, loss = 0.36238542199134827
In grad_steps = 1106, loss = 0.29898184537887573
In grad_steps = 1107, loss = 0.16147367656230927
In grad_steps = 1108, loss = 0.4959632456302643
In grad_steps = 1109, loss = 0.37690556049346924
In grad_steps = 1110, loss = 0.3321705460548401
In grad_steps = 1111, loss = 0.5747591853141785
In grad_steps = 1112, loss = 0.632491409778595
In grad_steps = 1113, loss = 0.32100367546081543
In grad_steps = 1114, loss = 0.317696213722229
In grad_steps = 1115, loss = 0.11384566128253937
In grad_steps = 1116, loss = 0.6546511650085449
In grad_steps = 1117, loss = 0.23535779118537903
In grad_steps = 1118, loss = 0.41094350814819336
In grad_steps = 1119, loss = 0.5569489002227783
In grad_steps = 1120, loss = 0.11682850122451782
In grad_steps = 1121, loss = 0.15228748321533203
In grad_steps = 1122, loss = 0.25615549087524414
In grad_steps = 1123, loss = 0.593948245048523
In grad_steps = 1124, loss = 0.38942858576774597
In grad_steps = 1125, loss = 0.23935627937316895
In grad_steps = 1126, loss = 0.19308693706989288
In grad_steps = 1127, loss = 0.25127145648002625
In grad_steps = 1128, loss = 0.5869037508964539
In grad_steps = 1129, loss = 0.24423344433307648
In grad_steps = 1130, loss = 0.13342319428920746
In grad_steps = 1131, loss = 0.16600066423416138
In grad_steps = 1132, loss = 0.22813305258750916
In grad_steps = 1133, loss = 0.6329565048217773
In grad_steps = 1134, loss = 0.22765180468559265
In grad_steps = 1135, loss = 0.5156693458557129
In grad_steps = 1136, loss = 0.7362122535705566
In grad_steps = 1137, loss = 0.31390783190727234
In grad_steps = 1138, loss = 0.14401622116565704
In grad_steps = 1139, loss = 0.6003060936927795
In grad_steps = 1140, loss = 0.12991684675216675
In grad_steps = 1141, loss = 1.2976272106170654
In grad_steps = 1142, loss = 0.25735554099082947
In grad_steps = 1143, loss = 0.6383991837501526
In grad_steps = 1144, loss = 0.011805775575339794
In grad_steps = 1145, loss = 0.8395530581474304
In grad_steps = 1146, loss = 1.3370753526687622
In grad_steps = 1147, loss = 0.7671559453010559
In grad_steps = 1148, loss = 0.5424877405166626
In grad_steps = 1149, loss = 0.07587523013353348
In grad_steps = 1150, loss = 0.480281800031662
In grad_steps = 1151, loss = 0.4162086546421051
In grad_steps = 1152, loss = 0.4604431986808777
In grad_steps = 1153, loss = 0.7085500955581665
In grad_steps = 1154, loss = 0.4883376955986023
In grad_steps = 1155, loss = 0.3834138512611389
In grad_steps = 1156, loss = 0.3948083519935608
In grad_steps = 1157, loss = 0.2998020648956299
In grad_steps = 1158, loss = 0.5133551955223083
In grad_steps = 1159, loss = 0.14287880063056946
In grad_steps = 1160, loss = 0.3898433744907379
In grad_steps = 1161, loss = 0.565297544002533
In grad_steps = 1162, loss = 0.42197930812835693
In grad_steps = 1163, loss = 0.4942589998245239
In grad_steps = 1164, loss = 0.22203537821769714
In grad_steps = 1165, loss = 0.2842888832092285
In grad_steps = 1166, loss = 1.1316032409667969
In grad_steps = 1167, loss = 0.5425198078155518
In grad_steps = 1168, loss = 0.44648656249046326
In grad_steps = 1169, loss = 0.06776738166809082
In grad_steps = 1170, loss = 0.2970471680164337
In grad_steps = 1171, loss = 0.46338266134262085
In grad_steps = 1172, loss = 0.46843066811561584
In grad_steps = 1173, loss = 0.2986513078212738
In grad_steps = 1174, loss = 0.3585756719112396
In grad_steps = 1175, loss = 0.604978084564209
In grad_steps = 1176, loss = 0.4129224121570587
In grad_steps = 1177, loss = 0.08647054433822632
In grad_steps = 1178, loss = 0.5798553228378296
In grad_steps = 1179, loss = 0.0659288763999939
In grad_steps = 1180, loss = 0.2885449230670929
In grad_steps = 1181, loss = 0.5715013742446899
In grad_steps = 1182, loss = 0.8997524976730347
In grad_steps = 1183, loss = 0.46386048197746277
In grad_steps = 1184, loss = 0.7946486473083496
In grad_steps = 1185, loss = 0.40819311141967773
In grad_steps = 1186, loss = 0.2066417634487152
In grad_steps = 1187, loss = 0.2410164475440979
In grad_steps = 1188, loss = 0.8870015740394592
In grad_steps = 1189, loss = 0.21292923390865326
In grad_steps = 1190, loss = 0.12023521214723587
In grad_steps = 1191, loss = 0.3135215938091278
In grad_steps = 1192, loss = 0.3791764974594116
In grad_steps = 1193, loss = 0.14876104891300201
In grad_steps = 1194, loss = 0.3597416877746582
In grad_steps = 1195, loss = 0.6297697424888611
In grad_steps = 1196, loss = 0.1331731081008911
In grad_steps = 1197, loss = 0.5883727073669434
In grad_steps = 1198, loss = 0.12536117434501648
In grad_steps = 1199, loss = 0.9134429693222046
In grad_steps = 1200, loss = 0.23924626410007477
In grad_steps = 1201, loss = 0.09042274206876755
In grad_steps = 1202, loss = 0.36892199516296387
In grad_steps = 1203, loss = 0.0709015354514122
In grad_steps = 1204, loss = 0.07781030237674713
In grad_steps = 1205, loss = 1.4710776805877686
In grad_steps = 1206, loss = 1.1606172323226929
In grad_steps = 1207, loss = 0.1349465250968933
In grad_steps = 1208, loss = 0.3739980459213257
In grad_steps = 1209, loss = 0.5313217639923096
In grad_steps = 1210, loss = 0.39294546842575073
In grad_steps = 1211, loss = 0.5603100657463074
In grad_steps = 1212, loss = 0.09747688472270966
In grad_steps = 1213, loss = 0.6777817606925964
In grad_steps = 1214, loss = 0.4340210556983948
In grad_steps = 1215, loss = 0.426712304353714
In grad_steps = 1216, loss = 0.21320196986198425
In grad_steps = 1217, loss = 0.05805984139442444
In grad_steps = 1218, loss = 0.4818030595779419
In grad_steps = 1219, loss = 0.21818020939826965
In grad_steps = 1220, loss = 0.5196505188941956
In grad_steps = 1221, loss = 0.3100358843803406
In grad_steps = 1222, loss = 0.4801231324672699
In grad_steps = 1223, loss = 0.42947056889533997
In grad_steps = 1224, loss = 1.2006359100341797
In grad_steps = 1225, loss = 0.17023931443691254
In grad_steps = 1226, loss = 0.40430188179016113
In grad_steps = 1227, loss = 0.2464333027601242
In grad_steps = 1228, loss = 0.20888198912143707
In grad_steps = 1229, loss = 0.8431320786476135
In grad_steps = 1230, loss = 0.28027844429016113
In grad_steps = 1231, loss = 0.15856881439685822
In grad_steps = 1232, loss = 0.06048736348748207
In grad_steps = 1233, loss = 0.4767008423805237
In grad_steps = 1234, loss = 0.3615129590034485
In grad_steps = 1235, loss = 0.30677860975265503
In grad_steps = 1236, loss = 0.6675523519515991
In grad_steps = 1237, loss = 0.48496437072753906
In grad_steps = 1238, loss = 0.15350863337516785
In grad_steps = 1239, loss = 0.4457581043243408
In grad_steps = 1240, loss = 0.1365799456834793
In grad_steps = 1241, loss = 0.07388287782669067
In grad_steps = 1242, loss = 0.4076232612133026
In grad_steps = 1243, loss = 0.2700088620185852
In grad_steps = 1244, loss = 0.6556126475334167
In grad_steps = 1245, loss = 0.6252264380455017
In grad_steps = 1246, loss = 0.16314716637134552
In grad_steps = 1247, loss = 0.23231935501098633
In grad_steps = 1248, loss = 0.6321144700050354
In grad_steps = 1249, loss = 0.34029701352119446
In grad_steps = 1250, loss = 0.08362419903278351
In grad_steps = 1251, loss = 0.2762790322303772
In grad_steps = 1252, loss = 0.054766133427619934
In grad_steps = 1253, loss = 0.27293264865875244
In grad_steps = 1254, loss = 0.2136346697807312
In grad_steps = 1255, loss = 0.6060388088226318
In grad_steps = 1256, loss = 0.990770697593689
In grad_steps = 1257, loss = 0.47044825553894043
In grad_steps = 1258, loss = 0.7037493586540222
In grad_steps = 1259, loss = 0.0703924298286438
In grad_steps = 1260, loss = 0.17455771565437317
In grad_steps = 1261, loss = 1.3595044612884521
In grad_steps = 1262, loss = 0.5473324656486511
In grad_steps = 1263, loss = 0.07671951502561569
In grad_steps = 1264, loss = 0.24290528893470764
In grad_steps = 1265, loss = 0.282723605632782
In grad_steps = 1266, loss = 0.7219715118408203
In grad_steps = 1267, loss = 0.7505390644073486
In grad_steps = 1268, loss = 0.6579868197441101
In grad_steps = 1269, loss = 0.09486348181962967
In grad_steps = 1270, loss = 0.15532726049423218
In grad_steps = 1271, loss = 0.5501735806465149
In grad_steps = 1272, loss = 0.5439096689224243
In grad_steps = 1273, loss = 0.2301306128501892
In grad_steps = 1274, loss = 0.3541274666786194
In grad_steps = 1275, loss = 0.2102511078119278
In grad_steps = 1276, loss = 0.41509777307510376
In grad_steps = 1277, loss = 0.17960871756076813
In grad_steps = 1278, loss = 0.2865838408470154
In grad_steps = 1279, loss = 0.18703027069568634
In grad_steps = 1280, loss = 0.18197357654571533
In grad_steps = 1281, loss = 0.4019875228404999
In grad_steps = 1282, loss = 0.6633774042129517
In grad_steps = 1283, loss = 0.5704261064529419
In grad_steps = 1284, loss = 0.19596430659294128
In grad_steps = 1285, loss = 0.3174278140068054
In grad_steps = 1286, loss = 0.20194226503372192
In grad_steps = 1287, loss = 0.11742720007896423
In grad_steps = 1288, loss = 0.41707009077072144
In grad_steps = 1289, loss = 0.12795162200927734
In grad_steps = 1290, loss = 0.047304168343544006
In grad_steps = 1291, loss = 0.291659951210022
In grad_steps = 1292, loss = 0.7418342232704163
In grad_steps = 1293, loss = 0.1414262354373932
In grad_steps = 1294, loss = 0.6276940107345581
In grad_steps = 1295, loss = 0.34754905104637146
In grad_steps = 1296, loss = 0.08743464946746826
In grad_steps = 1297, loss = 0.15333132445812225
In grad_steps = 1298, loss = 0.2252013385295868
In grad_steps = 1299, loss = 0.6296836733818054
In grad_steps = 1300, loss = 0.1486615091562271
In grad_steps = 1301, loss = 0.12239670753479004
In grad_steps = 1302, loss = 0.0386795736849308
In grad_steps = 1303, loss = 0.49654799699783325
In grad_steps = 1304, loss = 0.030598074197769165
In grad_steps = 1305, loss = 0.9423348903656006
In grad_steps = 1306, loss = 0.895003080368042
In grad_steps = 1307, loss = 0.3682849407196045
In grad_steps = 1308, loss = 1.0416722297668457
In grad_steps = 1309, loss = 0.45460617542266846
In grad_steps = 1310, loss = 0.14167755842208862
In grad_steps = 1311, loss = 0.1762830913066864
In grad_steps = 1312, loss = 0.12365490943193436
In grad_steps = 1313, loss = 0.10669208317995071
In grad_steps = 1314, loss = 0.08357933163642883
In grad_steps = 1315, loss = 0.20333002507686615
In grad_steps = 1316, loss = 1.2793967723846436
In grad_steps = 1317, loss = 0.7254623770713806
In grad_steps = 1318, loss = 0.29624462127685547
In grad_steps = 1319, loss = 0.3179667294025421
In grad_steps = 1320, loss = 0.41615599393844604
In grad_steps = 1321, loss = 0.1385970115661621
In grad_steps = 1322, loss = 0.6067900061607361
In grad_steps = 1323, loss = 0.4701594114303589
In grad_steps = 1324, loss = 0.08613014221191406
In grad_steps = 1325, loss = 0.1264665275812149
In grad_steps = 1326, loss = 0.15211421251296997
In grad_steps = 1327, loss = 0.7262024283409119
In grad_steps = 1328, loss = 0.07244755327701569
In grad_steps = 1329, loss = 0.6502339243888855
In grad_steps = 1330, loss = 0.6831055879592896
In grad_steps = 1331, loss = 0.29089516401290894
In grad_steps = 1332, loss = 0.0942305326461792
In grad_steps = 1333, loss = 0.5631139278411865
In grad_steps = 1334, loss = 0.14628483355045319
In grad_steps = 1335, loss = 0.40183770656585693
In grad_steps = 1336, loss = 0.35996055603027344
In grad_steps = 1337, loss = 0.26382678747177124
In grad_steps = 1338, loss = 0.020041130483150482
In grad_steps = 1339, loss = 0.6133780479431152
In grad_steps = 1340, loss = 0.31367093324661255
In grad_steps = 1341, loss = 0.1966494619846344
In grad_steps = 1342, loss = 0.22639265656471252
In grad_steps = 1343, loss = 0.49832683801651
In grad_steps = 1344, loss = 0.09686487913131714
In grad_steps = 1345, loss = 1.0079398155212402
In grad_steps = 1346, loss = 0.3020918369293213
In grad_steps = 1347, loss = 0.23240351676940918
In grad_steps = 1348, loss = 0.40274447202682495
In grad_steps = 1349, loss = 0.00858363974839449
In grad_steps = 1350, loss = 0.3761325478553772
In grad_steps = 1351, loss = 0.31451451778411865
In grad_steps = 1352, loss = 0.09587893635034561
In grad_steps = 1353, loss = 1.2211885452270508
In grad_steps = 1354, loss = 0.09888742864131927
In grad_steps = 1355, loss = 0.8457262516021729
In grad_steps = 1356, loss = 0.1098623126745224
In grad_steps = 1357, loss = 0.3915676474571228
In grad_steps = 1358, loss = 0.09162674844264984
In grad_steps = 1359, loss = 0.1309918761253357
In grad_steps = 1360, loss = 0.18989425897598267
In grad_steps = 1361, loss = 0.7389482855796814
In grad_steps = 1362, loss = 0.083076611161232
In grad_steps = 1363, loss = 0.1117347925901413
In grad_steps = 1364, loss = 0.47705140709877014
In grad_steps = 1365, loss = 0.4452632963657379
In grad_steps = 1366, loss = 0.38494086265563965
In grad_steps = 1367, loss = 0.04982743412256241
In grad_steps = 1368, loss = 0.6991393566131592
In grad_steps = 1369, loss = 0.24765357375144958
In grad_steps = 1370, loss = 0.09542854130268097
In grad_steps = 1371, loss = 0.2851316034793854
In grad_steps = 1372, loss = 0.6479505896568298
In grad_steps = 1373, loss = 0.0736471489071846
In grad_steps = 1374, loss = 0.9024523496627808
In grad_steps = 1375, loss = 0.03172957897186279
In grad_steps = 1376, loss = 0.5650283098220825
In grad_steps = 1377, loss = 0.06162714958190918
In grad_steps = 1378, loss = 0.5764176249504089
In grad_steps = 1379, loss = 0.19586366415023804
In grad_steps = 1380, loss = 0.281107634305954
In grad_steps = 1381, loss = 0.08334747701883316
In grad_steps = 1382, loss = 0.14569373428821564
In grad_steps = 1383, loss = 0.9641777873039246
In grad_steps = 1384, loss = 0.21315830945968628
In grad_steps = 1385, loss = 0.8253512978553772
In grad_steps = 1386, loss = 0.6465107202529907
In grad_steps = 1387, loss = 0.15897145867347717
In grad_steps = 1388, loss = 0.06382560729980469
In grad_steps = 1389, loss = 0.08381517231464386
In grad_steps = 1390, loss = 0.04836847260594368
In grad_steps = 1391, loss = 0.6739184856414795
In grad_steps = 1392, loss = 0.17346930503845215
In grad_steps = 1393, loss = 0.20465971529483795
In grad_steps = 1394, loss = 0.096872478723526
In grad_steps = 1395, loss = 0.910800039768219
In grad_steps = 1396, loss = 0.6796851754188538
In grad_steps = 1397, loss = 0.058242347091436386
In grad_steps = 1398, loss = 0.7111214399337769
In grad_steps = 1399, loss = 0.07423075288534164
In grad_steps = 1400, loss = 0.6351857781410217
In grad_steps = 1401, loss = 0.3417156934738159
In grad_steps = 1402, loss = 0.2798157334327698
In grad_steps = 1403, loss = 0.7129345536231995
In grad_steps = 1404, loss = 0.5031156539916992
In grad_steps = 1405, loss = 0.09272322803735733
In grad_steps = 1406, loss = 0.2180883288383484
In grad_steps = 1407, loss = 0.20487888157367706
In grad_steps = 1408, loss = 0.4659382104873657
In grad_steps = 1409, loss = 0.32377880811691284
In grad_steps = 1410, loss = 0.32476770877838135
In grad_steps = 1411, loss = 0.5219367146492004
In grad_steps = 1412, loss = 0.27673858404159546
In grad_steps = 1413, loss = 1.2537004947662354
In grad_steps = 1414, loss = 0.268441379070282
In grad_steps = 1415, loss = 0.17774692177772522
In grad_steps = 1416, loss = 0.2658739387989044
In grad_steps = 1417, loss = 0.260518342256546
In grad_steps = 1418, loss = 0.04722675681114197
In grad_steps = 1419, loss = 0.07802444696426392
In grad_steps = 1420, loss = 0.3460119962692261
In grad_steps = 1421, loss = 0.11426804959774017
In grad_steps = 1422, loss = 0.2040466070175171
In grad_steps = 1423, loss = 1.0714004039764404
In grad_steps = 1424, loss = 0.04921318590641022
In grad_steps = 1425, loss = 0.47492289543151855
In grad_steps = 1426, loss = 0.12393587827682495
In grad_steps = 1427, loss = 0.20515507459640503
In grad_steps = 1428, loss = 0.7874758243560791
In grad_steps = 1429, loss = 0.6045339107513428
In grad_steps = 1430, loss = 0.19073793292045593
In grad_steps = 1431, loss = 0.7833330631256104
In grad_steps = 1432, loss = 0.0413457490503788
In grad_steps = 1433, loss = 0.06351979076862335
In grad_steps = 1434, loss = 0.38245975971221924
In grad_steps = 1435, loss = 0.3171887993812561
In grad_steps = 1436, loss = 0.2479371726512909
In grad_steps = 1437, loss = 0.8337996006011963
In grad_steps = 1438, loss = 0.1545717716217041
In grad_steps = 1439, loss = 0.0429176390171051
In grad_steps = 1440, loss = 0.2879446744918823
In grad_steps = 1441, loss = 1.108504295349121
In grad_steps = 1442, loss = 0.17374420166015625
In grad_steps = 1443, loss = 1.4171432256698608
In grad_steps = 1444, loss = 0.07313629984855652
In grad_steps = 1445, loss = 0.1852734088897705
In grad_steps = 1446, loss = 0.46555930376052856
In grad_steps = 1447, loss = 0.022779816761612892
In grad_steps = 1448, loss = 0.6269268989562988
In grad_steps = 1449, loss = 0.4283663034439087
In grad_steps = 1450, loss = 0.13426652550697327
In grad_steps = 1451, loss = 0.684516191482544
In grad_steps = 1452, loss = 0.15991941094398499
In grad_steps = 1453, loss = 0.736950159072876
In grad_steps = 1454, loss = 0.24495480954647064
In grad_steps = 1455, loss = 0.7653175592422485
In grad_steps = 1456, loss = 0.4273253381252289
In grad_steps = 1457, loss = 0.3179972171783447
In grad_steps = 1458, loss = 0.21034719049930573
In grad_steps = 1459, loss = 0.18691037595272064
In grad_steps = 1460, loss = 0.11818896234035492
In grad_steps = 1461, loss = 0.5054726600646973
In grad_steps = 1462, loss = 0.16855189204216003
In grad_steps = 1463, loss = 0.7718942761421204
In grad_steps = 1464, loss = 0.33954745531082153
In grad_steps = 1465, loss = 0.4639466404914856
In grad_steps = 1466, loss = 0.42066991329193115
In grad_steps = 1467, loss = 0.3562084436416626
In grad_steps = 1468, loss = 0.0854531079530716
In grad_steps = 1469, loss = 0.3757931590080261
In grad_steps = 1470, loss = 0.18874749541282654
In grad_steps = 1471, loss = 0.10980414599180222
In grad_steps = 1472, loss = 0.5416008234024048
In grad_steps = 1473, loss = 0.13786502182483673
In grad_steps = 1474, loss = 0.9141066670417786
In grad_steps = 1475, loss = 0.42697009444236755
In grad_steps = 1476, loss = 0.5373329520225525
In grad_steps = 1477, loss = 0.19468511641025543
In grad_steps = 1478, loss = 0.5859534740447998
In grad_steps = 1479, loss = 0.29819348454475403
In grad_steps = 1480, loss = 0.13536132872104645
In grad_steps = 1481, loss = 0.1909489631652832
In grad_steps = 1482, loss = 0.1001390665769577
In grad_steps = 1483, loss = 0.5522250533103943
In grad_steps = 1484, loss = 0.32268819212913513
In grad_steps = 1485, loss = 0.29656773805618286
In grad_steps = 1486, loss = 1.2724621295928955
In grad_steps = 1487, loss = 0.14749671518802643
In grad_steps = 1488, loss = 0.8110555410385132
In grad_steps = 1489, loss = 0.44002631306648254
In grad_steps = 1490, loss = 0.6955868005752563
In grad_steps = 1491, loss = 0.6075211763381958
In grad_steps = 1492, loss = 0.09646108001470566
In grad_steps = 1493, loss = 0.5249786376953125
In grad_steps = 1494, loss = 0.3764556050300598
In grad_steps = 1495, loss = 0.13180962204933167
In grad_steps = 1496, loss = 0.10792089998722076
In grad_steps = 1497, loss = 0.23537932336330414
In grad_steps = 1498, loss = 0.3403377830982208
In grad_steps = 1499, loss = 0.8430326581001282
In grad_steps = 1500, loss = 0.25806641578674316
In grad_steps = 1501, loss = 0.29761454463005066
In grad_steps = 1502, loss = 0.5303349494934082
In grad_steps = 1503, loss = 0.399458110332489
In grad_steps = 1504, loss = 0.2595502734184265
In grad_steps = 1505, loss = 0.4932636618614197
In grad_steps = 1506, loss = 0.2854612469673157
In grad_steps = 1507, loss = 0.6677016019821167
In grad_steps = 1508, loss = 0.48420944809913635
In grad_steps = 1509, loss = 0.794452965259552
In grad_steps = 1510, loss = 0.7106004953384399
In grad_steps = 1511, loss = 0.5376882553100586
In grad_steps = 1512, loss = 0.9209257364273071
In grad_steps = 1513, loss = 0.7898290157318115
In grad_steps = 1514, loss = 0.281252384185791
In grad_steps = 1515, loss = 0.3477066159248352
In grad_steps = 1516, loss = 0.1544918417930603
In grad_steps = 1517, loss = 0.06808298826217651
In grad_steps = 1518, loss = 0.06328099966049194
In grad_steps = 1519, loss = 0.4586386978626251
In grad_steps = 1520, loss = 0.5726300477981567
In grad_steps = 1521, loss = 1.3429533243179321
In grad_steps = 1522, loss = 0.2986307740211487
In grad_steps = 1523, loss = 1.1454423666000366
In grad_steps = 1524, loss = 0.5680290460586548
In grad_steps = 1525, loss = 0.28952479362487793
In grad_steps = 1526, loss = 0.3636414408683777
In grad_steps = 1527, loss = 0.5766209959983826
In grad_steps = 1528, loss = 0.7068067789077759
In grad_steps = 1529, loss = 0.12331028282642365
In grad_steps = 1530, loss = 0.5237727165222168
In grad_steps = 1531, loss = 0.5632572174072266
In grad_steps = 1532, loss = 0.3121846318244934
In grad_steps = 1533, loss = 0.1980818212032318
In grad_steps = 1534, loss = 0.2221091389656067
In grad_steps = 1535, loss = 0.31938984990119934
In grad_steps = 1536, loss = 0.15947160124778748
In grad_steps = 1537, loss = 0.4656451642513275
In grad_steps = 1538, loss = 0.24076858162879944
In grad_steps = 1539, loss = 0.06574273854494095
In grad_steps = 1540, loss = 0.16976185142993927
In grad_steps = 1541, loss = 0.5228603482246399
In grad_steps = 1542, loss = 0.23147325217723846
In grad_steps = 1543, loss = 0.2990691363811493
In grad_steps = 1544, loss = 0.1110689714550972
In grad_steps = 1545, loss = 0.12304451316595078
In grad_steps = 1546, loss = 0.20641586184501648
In grad_steps = 1547, loss = 1.158549427986145
In grad_steps = 1548, loss = 0.05592041462659836
In grad_steps = 1549, loss = 0.019612636417150497
In grad_steps = 1550, loss = 0.20349064469337463
In grad_steps = 1551, loss = 0.5196552872657776
In grad_steps = 1552, loss = 0.12132266163825989
In grad_steps = 1553, loss = 0.042340487241744995
In grad_steps = 1554, loss = 0.4439224898815155
In grad_steps = 1555, loss = 0.08981147408485413
In grad_steps = 1556, loss = 1.1501425504684448
In grad_steps = 1557, loss = 0.20660658180713654
In grad_steps = 1558, loss = 0.6199482083320618
In grad_steps = 1559, loss = 0.6109763979911804
In grad_steps = 1560, loss = 0.1421181857585907
In grad_steps = 1561, loss = 0.3951590955257416
In grad_steps = 1562, loss = 0.37481892108917236
In grad_steps = 1563, loss = 0.1087823137640953
In grad_steps = 1564, loss = 1.4569348096847534
In grad_steps = 1565, loss = 0.5038136839866638
In grad_steps = 1566, loss = 0.1106172725558281
In grad_steps = 1567, loss = 0.4847736954689026
In grad_steps = 1568, loss = 0.36086517572402954
In grad_steps = 1569, loss = 0.22037501633167267
In grad_steps = 1570, loss = 0.5529603958129883
In grad_steps = 1571, loss = 0.8051613569259644
In grad_steps = 1572, loss = 0.23996035754680634
In grad_steps = 1573, loss = 0.23055842518806458
In grad_steps = 1574, loss = 0.7753585577011108
In grad_steps = 1575, loss = 0.6353298425674438
In grad_steps = 1576, loss = 0.4123196601867676
In grad_steps = 1577, loss = 0.6438642144203186
In grad_steps = 1578, loss = 0.09182397276163101
In grad_steps = 1579, loss = 0.4827168583869934
In grad_steps = 1580, loss = 0.153056338429451
In grad_steps = 1581, loss = 0.24350401759147644
In grad_steps = 1582, loss = 0.2376500368118286
In grad_steps = 1583, loss = 0.18916673958301544
In grad_steps = 1584, loss = 0.3136320114135742
In grad_steps = 1585, loss = 0.2576248049736023
In grad_steps = 1586, loss = 0.19787587225437164
In grad_steps = 1587, loss = 0.1798887401819229
In grad_steps = 1588, loss = 0.8701205849647522
In grad_steps = 1589, loss = 0.6732943654060364
In grad_steps = 1590, loss = 0.22090305387973785
In grad_steps = 1591, loss = 0.8096168041229248
In grad_steps = 1592, loss = 0.9474653601646423
In grad_steps = 1593, loss = 0.04281710833311081
In grad_steps = 1594, loss = 0.16977427899837494
In grad_steps = 1595, loss = 0.8615995049476624
In grad_steps = 1596, loss = 0.12032388150691986
In grad_steps = 1597, loss = 0.1747480034828186
In grad_steps = 1598, loss = 0.079067163169384
In grad_steps = 1599, loss = 0.3079046607017517
In grad_steps = 1600, loss = 0.5208497643470764
In grad_steps = 1601, loss = 0.19747596979141235
In grad_steps = 1602, loss = 0.21827301383018494
In grad_steps = 1603, loss = 0.5766989588737488
In grad_steps = 1604, loss = 0.3498973548412323
In grad_steps = 1605, loss = 0.0973135307431221
In grad_steps = 1606, loss = 0.25028297305107117
In grad_steps = 1607, loss = 0.051137395203113556
In grad_steps = 1608, loss = 0.6719636917114258
In grad_steps = 1609, loss = 0.7058930993080139
In grad_steps = 1610, loss = 0.19904306530952454
In grad_steps = 1611, loss = 0.2615623474121094
In grad_steps = 1612, loss = 0.30994653701782227
In grad_steps = 1613, loss = 0.34075284004211426
In grad_steps = 1614, loss = 0.1058574765920639
In grad_steps = 1615, loss = 0.12459992617368698
In grad_steps = 1616, loss = 0.12081380188465118
In grad_steps = 1617, loss = 0.777876079082489
In grad_steps = 1618, loss = 0.7026551961898804
In grad_steps = 1619, loss = 0.2576209604740143
In grad_steps = 1620, loss = 0.5346448421478271
In grad_steps = 1621, loss = 0.30517759919166565
In grad_steps = 1622, loss = 1.2103604078292847
In grad_steps = 1623, loss = 0.6054192781448364
In grad_steps = 1624, loss = 0.44489020109176636
In grad_steps = 1625, loss = 0.8924718499183655
In grad_steps = 1626, loss = 0.13618889451026917
In grad_steps = 1627, loss = 0.15466934442520142
In grad_steps = 1628, loss = 0.2661970853805542
In grad_steps = 1629, loss = 0.1233738362789154
In grad_steps = 1630, loss = 0.3837648928165436
In grad_steps = 1631, loss = 0.07301995158195496
In grad_steps = 1632, loss = 0.4631545841693878
In grad_steps = 1633, loss = 0.612679123878479
In grad_steps = 1634, loss = 0.24468572437763214
In grad_steps = 1635, loss = 0.2262798696756363
In grad_steps = 1636, loss = 0.3828846514225006
In grad_steps = 1637, loss = 0.48210787773132324
In grad_steps = 1638, loss = 0.21552670001983643
In grad_steps = 1639, loss = 0.18050020933151245
In grad_steps = 1640, loss = 0.28567489981651306
In grad_steps = 1641, loss = 0.3623712658882141
In grad_steps = 1642, loss = 0.056736115366220474
In grad_steps = 1643, loss = 0.6335391998291016
In grad_steps = 1644, loss = 0.24088415503501892
In grad_steps = 1645, loss = 0.2983545660972595
In grad_steps = 1646, loss = 0.27306124567985535
In grad_steps = 1647, loss = 0.7763047218322754
In grad_steps = 1648, loss = 0.05479976162314415
In grad_steps = 1649, loss = 1.283475637435913
In grad_steps = 1650, loss = 0.30647629499435425
In grad_steps = 1651, loss = 0.8902003169059753
In grad_steps = 1652, loss = 0.07966827601194382
In grad_steps = 1653, loss = 0.4268301725387573
In grad_steps = 1654, loss = 0.8252811431884766
In grad_steps = 1655, loss = 0.7688941359519958
In grad_steps = 1656, loss = 0.13580292463302612
In grad_steps = 1657, loss = 0.40206652879714966
In grad_steps = 1658, loss = 0.21508930623531342
In grad_steps = 1659, loss = 0.0829983726143837
In grad_steps = 1660, loss = 0.4282825291156769
In grad_steps = 1661, loss = 0.18938349187374115
In grad_steps = 1662, loss = 0.2655370831489563
In grad_steps = 1663, loss = 0.2256602644920349
In grad_steps = 1664, loss = 0.41967880725860596
In grad_steps = 1665, loss = 0.3790239989757538
In grad_steps = 1666, loss = 0.25366610288619995
In grad_steps = 1667, loss = 0.3068917393684387
In grad_steps = 1668, loss = 0.40142297744750977
In grad_steps = 1669, loss = 0.6303591728210449
In grad_steps = 1670, loss = 0.2175339162349701
In grad_steps = 1671, loss = 0.06494808942079544
In grad_steps = 1672, loss = 0.20277702808380127
In grad_steps = 1673, loss = 0.07670477777719498
In grad_steps = 1674, loss = 0.052726369351148605
In grad_steps = 1675, loss = 0.248604416847229
In grad_steps = 1676, loss = 0.17762796580791473
In grad_steps = 1677, loss = 0.12275303155183792
In grad_steps = 1678, loss = 0.7350631356239319
In grad_steps = 1679, loss = 1.72969388961792
In grad_steps = 1680, loss = 0.14652904868125916
In grad_steps = 1681, loss = 0.18222489953041077
In grad_steps = 1682, loss = 0.11762011051177979
In grad_steps = 1683, loss = 0.5637921094894409
In grad_steps = 1684, loss = 0.7813523411750793
In grad_steps = 1685, loss = 0.180515319108963
In grad_steps = 1686, loss = 0.07322946935892105
In grad_steps = 1687, loss = 1.0226449966430664
In grad_steps = 1688, loss = 0.2127937376499176
In grad_steps = 1689, loss = 1.3975715637207031
In grad_steps = 1690, loss = 0.15996423363685608
In grad_steps = 1691, loss = 0.44138064980506897
In grad_steps = 1692, loss = 0.24198830127716064
In grad_steps = 1693, loss = 0.2895853519439697
In grad_steps = 1694, loss = 0.08704173564910889
In grad_steps = 1695, loss = 0.5825544595718384
In grad_steps = 1696, loss = 0.0565619170665741
In grad_steps = 1697, loss = 0.5678872466087341
In grad_steps = 1698, loss = 0.5113711357116699
In grad_steps = 1699, loss = 0.21221691370010376
In grad_steps = 1700, loss = 0.43644267320632935
In grad_steps = 1701, loss = 0.7188680768013
In grad_steps = 1702, loss = 0.29924702644348145
In grad_steps = 1703, loss = 0.48737579584121704
In grad_steps = 1704, loss = 0.06511487811803818
In grad_steps = 1705, loss = 0.29872119426727295
In grad_steps = 1706, loss = 0.19954439997673035
In grad_steps = 1707, loss = 0.47978436946868896
In grad_steps = 1708, loss = 0.05203239619731903
In grad_steps = 1709, loss = 0.15972338616847992
In grad_steps = 1710, loss = 0.7767854332923889
In grad_steps = 1711, loss = 0.2532574236392975
In grad_steps = 1712, loss = 0.22556836903095245
In grad_steps = 1713, loss = 0.0600040964782238
In grad_steps = 1714, loss = 0.07878564298152924
In grad_steps = 1715, loss = 0.323458731174469
In grad_steps = 1716, loss = 0.19893907010555267
In grad_steps = 1717, loss = 0.03364153206348419
In grad_steps = 1718, loss = 0.9374041557312012
In grad_steps = 1719, loss = 0.013794156722724438
In grad_steps = 1720, loss = 0.25515586137771606
In grad_steps = 1721, loss = 0.8201637268066406
In grad_steps = 1722, loss = 0.03269939497113228
In grad_steps = 1723, loss = 0.06815560907125473
In grad_steps = 1724, loss = 0.0276534054428339
In grad_steps = 1725, loss = 0.031528521329164505
In grad_steps = 1726, loss = 0.061846762895584106
In grad_steps = 1727, loss = 0.42699530720710754
In grad_steps = 1728, loss = 0.1764403134584427
In grad_steps = 1729, loss = 0.01707274839282036
In grad_steps = 1730, loss = 0.09109627455472946
In grad_steps = 1731, loss = 0.9248770475387573
In grad_steps = 1732, loss = 0.9927488565444946
In grad_steps = 1733, loss = 0.40398529171943665
In grad_steps = 1734, loss = 0.6180101037025452
In grad_steps = 1735, loss = 0.08043495565652847
In grad_steps = 1736, loss = 0.17070487141609192
In grad_steps = 1737, loss = 0.09394951164722443
In grad_steps = 1738, loss = 0.642071008682251
In grad_steps = 1739, loss = 0.26736050844192505
In grad_steps = 1740, loss = 0.2993147373199463
In grad_steps = 1741, loss = 0.2013731747865677
In grad_steps = 1742, loss = 0.49087581038475037
In grad_steps = 1743, loss = 0.5200222134590149
In grad_steps = 1744, loss = 0.06358587741851807
In grad_steps = 1745, loss = 0.3866681754589081
In grad_steps = 1746, loss = 0.10964491963386536
In grad_steps = 1747, loss = 0.20468229055404663
In grad_steps = 1748, loss = 0.12272386252880096
In grad_steps = 1749, loss = 1.1351189613342285
In grad_steps = 1750, loss = 0.6636395454406738
In grad_steps = 1751, loss = 0.1540822684764862
In grad_steps = 1752, loss = 0.2450621873140335
In grad_steps = 1753, loss = 0.14736293256282806
In grad_steps = 1754, loss = 0.33263784646987915
In grad_steps = 1755, loss = 0.9425439238548279
In grad_steps = 1756, loss = 0.4252641797065735
In grad_steps = 1757, loss = 0.19344493746757507
In grad_steps = 1758, loss = 1.1801937818527222
In grad_steps = 1759, loss = 0.09047729521989822
In grad_steps = 1760, loss = 0.1598493605852127
In grad_steps = 1761, loss = 0.39244771003723145
In grad_steps = 1762, loss = 0.5165884494781494
In grad_steps = 1763, loss = 0.5876953601837158
In grad_steps = 1764, loss = 0.35835716128349304
In grad_steps = 1765, loss = 0.7332093715667725
In grad_steps = 1766, loss = 0.2091943919658661
In grad_steps = 1767, loss = 0.25930556654930115
In grad_steps = 1768, loss = 0.6398964524269104
In grad_steps = 1769, loss = 0.4605378210544586
In grad_steps = 1770, loss = 0.4099322557449341
In grad_steps = 1771, loss = 0.6371143460273743
In grad_steps = 1772, loss = 0.24109821021556854
In grad_steps = 1773, loss = 0.28362685441970825
In grad_steps = 1774, loss = 0.4532710611820221
In grad_steps = 1775, loss = 0.2969517111778259
In grad_steps = 1776, loss = 0.15419553220272064
In grad_steps = 1777, loss = 0.1304088830947876
In grad_steps = 1778, loss = 0.4550941288471222
In grad_steps = 1779, loss = 0.296954870223999
In grad_steps = 1780, loss = 0.21211105585098267
In grad_steps = 1781, loss = 0.14605368673801422
In grad_steps = 1782, loss = 0.27190500497817993
In grad_steps = 1783, loss = 0.29255780577659607
In grad_steps = 1784, loss = 0.42559120059013367
In grad_steps = 1785, loss = 0.07791478931903839
In grad_steps = 1786, loss = 0.052338551729917526
In grad_steps = 1787, loss = 0.036980558186769485
In grad_steps = 1788, loss = 0.12299354374408722
In grad_steps = 1789, loss = 0.1823844164609909
In grad_steps = 1790, loss = 0.17745445668697357
In grad_steps = 1791, loss = 0.9613059163093567
In grad_steps = 1792, loss = 0.7380271553993225
In grad_steps = 1793, loss = 0.7118918895721436
In grad_steps = 1794, loss = 0.6001548767089844
In grad_steps = 1795, loss = 1.7356451749801636
In grad_steps = 1796, loss = 0.14623744785785675
In grad_steps = 1797, loss = 0.1199273020029068
In grad_steps = 1798, loss = 0.14457197487354279
In grad_steps = 1799, loss = 0.2129737287759781
In grad_steps = 1800, loss = 0.011086167767643929
In grad_steps = 1801, loss = 0.6069227457046509
In grad_steps = 1802, loss = 0.03190728276968002
In grad_steps = 1803, loss = 0.7007261514663696
In grad_steps = 1804, loss = 0.04786683991551399
In grad_steps = 1805, loss = 0.35711199045181274
In grad_steps = 1806, loss = 0.1618804633617401
In grad_steps = 1807, loss = 0.15463629364967346
In grad_steps = 1808, loss = 1.318591594696045
In grad_steps = 1809, loss = 0.1599995195865631
In grad_steps = 1810, loss = 0.5735660195350647
In grad_steps = 1811, loss = 0.716974139213562
In grad_steps = 1812, loss = 0.09486769139766693
In grad_steps = 1813, loss = 0.41115614771842957
In grad_steps = 1814, loss = 0.10142321139574051
In grad_steps = 1815, loss = 1.150948405265808
In grad_steps = 1816, loss = 0.7561482191085815
In grad_steps = 1817, loss = 0.061951830983161926
In grad_steps = 1818, loss = 0.2312977910041809
In grad_steps = 1819, loss = 0.11256837844848633
In grad_steps = 1820, loss = 0.5194659233093262
In grad_steps = 1821, loss = 0.15519672632217407
In grad_steps = 1822, loss = 0.38794562220573425
In grad_steps = 1823, loss = 0.15406763553619385
In grad_steps = 1824, loss = 0.3080309331417084
In grad_steps = 1825, loss = 0.2314804643392563
In grad_steps = 1826, loss = 0.26579126715660095
In grad_steps = 1827, loss = 0.16945300996303558
In grad_steps = 1828, loss = 0.6596366167068481
In grad_steps = 1829, loss = 0.17952674627304077
In grad_steps = 1830, loss = 0.9313230514526367
In grad_steps = 1831, loss = 0.4443359971046448
In grad_steps = 1832, loss = 0.47837033867836
In grad_steps = 1833, loss = 0.10597690939903259
In grad_steps = 1834, loss = 0.0878896713256836
In grad_steps = 1835, loss = 0.05214422941207886
In grad_steps = 1836, loss = 0.08490478247404099
In grad_steps = 1837, loss = 0.874261736869812
In grad_steps = 1838, loss = 0.5268207788467407
In grad_steps = 1839, loss = 0.7114042043685913
In grad_steps = 1840, loss = 0.28041017055511475
In grad_steps = 1841, loss = 0.18858468532562256
In grad_steps = 1842, loss = 0.14690741896629333
In grad_steps = 1843, loss = 0.08020101487636566
In grad_steps = 1844, loss = 0.2724410891532898
In grad_steps = 1845, loss = 0.08245599269866943
In grad_steps = 1846, loss = 0.10075529664754868
In grad_steps = 1847, loss = 0.25417932868003845
In grad_steps = 1848, loss = 0.19546400010585785
In grad_steps = 1849, loss = 0.35099852085113525
In grad_steps = 1850, loss = 0.06943739205598831
In grad_steps = 1851, loss = 0.05996386706829071
In grad_steps = 1852, loss = 0.10106636583805084
In grad_steps = 1853, loss = 1.2475738525390625
In grad_steps = 1854, loss = 0.2006256878376007
In grad_steps = 1855, loss = 0.15221808850765228
In grad_steps = 1856, loss = 0.18996578454971313
In grad_steps = 1857, loss = 0.6069308519363403
In grad_steps = 1858, loss = 0.05126345902681351
In grad_steps = 1859, loss = 0.22312197089195251
In grad_steps = 1860, loss = 0.17796175181865692
In grad_steps = 1861, loss = 0.7435703277587891
In grad_steps = 1862, loss = 0.04899987950921059
In grad_steps = 1863, loss = 1.8154479265213013
In grad_steps = 1864, loss = 0.03794832527637482
In grad_steps = 1865, loss = 0.2987489104270935
In grad_steps = 1866, loss = 0.9852579832077026
In grad_steps = 1867, loss = 0.050624534487724304
In grad_steps = 1868, loss = 0.24668337404727936
In grad_steps = 1869, loss = 0.10720322281122208
In grad_steps = 1870, loss = 0.9161370396614075
In grad_steps = 1871, loss = 0.7822787761688232
In grad_steps = 1872, loss = 0.09302864223718643
In grad_steps = 1873, loss = 0.0735907033085823
In grad_steps = 1874, loss = 0.5275167226791382
In grad_steps = 1875, loss = 1.2077107429504395
In grad_steps = 1876, loss = 0.7204890847206116
In grad_steps = 1877, loss = 0.5223404765129089
In grad_steps = 1878, loss = 1.5488663911819458
In grad_steps = 1879, loss = 0.04289146885275841
In grad_steps = 1880, loss = 0.5307422876358032
In grad_steps = 1881, loss = 0.518425703048706
In grad_steps = 1882, loss = 0.4553714692592621
In grad_steps = 1883, loss = 0.6349068880081177
In grad_steps = 1884, loss = 0.5106589794158936
In grad_steps = 1885, loss = 0.4982118606567383
In grad_steps = 1886, loss = 0.7246958017349243
In grad_steps = 1887, loss = 0.9000183343887329
In grad_steps = 1888, loss = 0.7005693912506104
In grad_steps = 1889, loss = 0.37983787059783936
In grad_steps = 1890, loss = 0.28125861287117004
In grad_steps = 1891, loss = 0.41035544872283936
In grad_steps = 1892, loss = 0.4317016005516052
In grad_steps = 1893, loss = 0.30166125297546387
In grad_steps = 1894, loss = 0.40877124667167664
In grad_steps = 1895, loss = 0.34508877992630005
In grad_steps = 1896, loss = 0.358142226934433
In grad_steps = 1897, loss = 0.21810176968574524
In grad_steps = 1898, loss = 0.3520755171775818
In grad_steps = 1899, loss = 0.1762198805809021
In grad_steps = 1900, loss = 0.12099374830722809
In grad_steps = 1901, loss = 0.12091369181871414
In grad_steps = 1902, loss = 0.128106951713562
In grad_steps = 1903, loss = 0.5542290210723877
In grad_steps = 1904, loss = 0.5340436697006226
In grad_steps = 1905, loss = 0.6492879390716553
In grad_steps = 1906, loss = 0.021124746650457382
In grad_steps = 1907, loss = 0.089585080742836
In grad_steps = 1908, loss = 0.013110137544572353
In grad_steps = 1909, loss = 0.22783933579921722
In grad_steps = 1910, loss = 0.024028100073337555
In grad_steps = 1911, loss = 0.27808913588523865
In grad_steps = 1912, loss = 0.019802527502179146
In grad_steps = 1913, loss = 1.1651893854141235
In grad_steps = 1914, loss = 0.694071352481842
In grad_steps = 1915, loss = 0.4716382920742035
In grad_steps = 1916, loss = 0.08992310613393784
In grad_steps = 1917, loss = 0.08515239506959915
In grad_steps = 1918, loss = 0.6437264680862427
In grad_steps = 1919, loss = 0.3032885193824768
In grad_steps = 1920, loss = 0.13732631504535675
In grad_steps = 1921, loss = 0.12831953167915344
In grad_steps = 1922, loss = 0.052938856184482574
In grad_steps = 1923, loss = 0.24927297234535217
In grad_steps = 1924, loss = 1.0889835357666016
In grad_steps = 1925, loss = 0.13854044675827026
In grad_steps = 1926, loss = 0.1175975352525711
In grad_steps = 1927, loss = 0.11004658788442612
In grad_steps = 1928, loss = 0.19406773149967194
In grad_steps = 1929, loss = 0.17155811190605164
In grad_steps = 1930, loss = 0.5923073291778564
In grad_steps = 1931, loss = 0.6430288553237915
In grad_steps = 1932, loss = 0.3792426884174347
In grad_steps = 1933, loss = 0.6078153848648071
In grad_steps = 1934, loss = 0.06939265877008438
In grad_steps = 1935, loss = 0.11254112422466278
In grad_steps = 1936, loss = 0.0524163693189621
In grad_steps = 1937, loss = 0.5823241472244263
In grad_steps = 1938, loss = 0.06344672292470932
In grad_steps = 1939, loss = 0.786406934261322
In grad_steps = 1940, loss = 0.3475744128227234
In grad_steps = 1941, loss = 0.11567060649394989
In grad_steps = 1942, loss = 0.10121846199035645
In grad_steps = 1943, loss = 0.18510714173316956
In grad_steps = 1944, loss = 0.9044497609138489
In grad_steps = 1945, loss = 1.5837024450302124
In grad_steps = 1946, loss = 0.7795164585113525
In grad_steps = 1947, loss = 0.5950747728347778
In grad_steps = 1948, loss = 0.39594119787216187
In grad_steps = 1949, loss = 0.3830505311489105
In grad_steps = 1950, loss = 0.5209560394287109
In grad_steps = 1951, loss = 0.4854833781719208
In grad_steps = 1952, loss = 0.5181899666786194
In grad_steps = 1953, loss = 0.16136552393436432
In grad_steps = 1954, loss = 0.24456220865249634
In grad_steps = 1955, loss = 0.39900052547454834
In grad_steps = 1956, loss = 0.5694893002510071
In grad_steps = 1957, loss = 0.623924732208252
In grad_steps = 1958, loss = 0.11831428110599518
In grad_steps = 1959, loss = 0.2696170210838318
In grad_steps = 1960, loss = 0.21512746810913086
In grad_steps = 1961, loss = 0.4678539037704468
In grad_steps = 1962, loss = 0.47319716215133667
In grad_steps = 1963, loss = 0.5392357707023621
In grad_steps = 1964, loss = 0.22903993725776672
In grad_steps = 1965, loss = 0.33277928829193115
In grad_steps = 1966, loss = 0.21199342608451843
In grad_steps = 1967, loss = 0.1502978801727295
In grad_steps = 1968, loss = 0.29443901777267456
In grad_steps = 1969, loss = 1.0441533327102661
In grad_steps = 1970, loss = 1.0554332733154297
In grad_steps = 1971, loss = 1.006719708442688
In grad_steps = 1972, loss = 0.2980974018573761
In grad_steps = 1973, loss = 0.09143165498971939
In grad_steps = 1974, loss = 0.21194607019424438
In grad_steps = 1975, loss = 0.11568757891654968
In grad_steps = 1976, loss = 0.4750305414199829
In grad_steps = 1977, loss = 0.22600418329238892
In grad_steps = 1978, loss = 0.3109726309776306
In grad_steps = 1979, loss = 0.6492171287536621
In grad_steps = 1980, loss = 0.18411077558994293
In grad_steps = 1981, loss = 0.38535431027412415
In grad_steps = 1982, loss = 0.3063519299030304
In grad_steps = 1983, loss = 0.12270670384168625
In grad_steps = 1984, loss = 0.28039517998695374
In grad_steps = 1985, loss = 0.8193442225456238
In grad_steps = 1986, loss = 0.366349995136261
In grad_steps = 1987, loss = 0.23115642368793488
In grad_steps = 1988, loss = 0.8487554788589478
In grad_steps = 1989, loss = 0.5972604751586914
In grad_steps = 1990, loss = 0.09079496562480927
In grad_steps = 1991, loss = 0.537965714931488
In grad_steps = 1992, loss = 0.19966407120227814
In grad_steps = 1993, loss = 0.2909853458404541
In grad_steps = 1994, loss = 0.634456217288971
In grad_steps = 1995, loss = 0.24630269408226013
In grad_steps = 1996, loss = 0.28094160556793213
In grad_steps = 1997, loss = 0.27043864130973816
In grad_steps = 1998, loss = 0.10714773833751678
In grad_steps = 1999, loss = 0.1091323047876358
In grad_steps = 2000, loss = 0.1500452160835266
In grad_steps = 2001, loss = 0.9864516854286194
In grad_steps = 2002, loss = 0.15443846583366394
In grad_steps = 2003, loss = 0.1984344720840454
In grad_steps = 2004, loss = 0.11017706245183945
In grad_steps = 2005, loss = 0.11174601316452026
In grad_steps = 2006, loss = 0.17288726568222046
In grad_steps = 2007, loss = 0.12257780879735947
In grad_steps = 2008, loss = 0.15612158179283142
In grad_steps = 2009, loss = 0.04334040731191635
In grad_steps = 2010, loss = 0.21466396749019623
In grad_steps = 2011, loss = 0.19263015687465668
In grad_steps = 2012, loss = 0.6372482180595398
In grad_steps = 2013, loss = 0.4118327498435974
In grad_steps = 2014, loss = 0.046590421348810196
In grad_steps = 2015, loss = 0.13157634437084198
In grad_steps = 2016, loss = 0.040660351514816284
In grad_steps = 2017, loss = 1.071796178817749
In grad_steps = 2018, loss = 1.0014863014221191
In grad_steps = 2019, loss = 0.8156327605247498
In grad_steps = 2020, loss = 0.11716948449611664
In grad_steps = 2021, loss = 0.10346418619155884
In grad_steps = 2022, loss = 0.08047671616077423
In grad_steps = 2023, loss = 0.22755487263202667
In grad_steps = 2024, loss = 0.8828606009483337
In grad_steps = 2025, loss = 0.31395938992500305
In grad_steps = 2026, loss = 1.074615478515625
In grad_steps = 2027, loss = 0.2588083744049072
In grad_steps = 2028, loss = 0.6459540128707886
In grad_steps = 2029, loss = 0.36450567841529846
In grad_steps = 2030, loss = 0.15371417999267578
In grad_steps = 2031, loss = 0.08253011107444763
In grad_steps = 2032, loss = 0.7724840044975281
In grad_steps = 2033, loss = 0.5793426036834717
In grad_steps = 2034, loss = 0.49887603521347046
In grad_steps = 2035, loss = 0.06691659986972809
In grad_steps = 2036, loss = 0.0965271145105362
In grad_steps = 2037, loss = 0.06543836742639542
In grad_steps = 2038, loss = 0.6367148160934448
In grad_steps = 2039, loss = 0.1471145749092102
In grad_steps = 2040, loss = 0.05689936876296997
In grad_steps = 2041, loss = 0.5077872276306152
In grad_steps = 2042, loss = 0.14594118297100067
In grad_steps = 2043, loss = 0.22705329954624176
In grad_steps = 2044, loss = 0.6005470156669617
In grad_steps = 2045, loss = 0.4574822783470154
In grad_steps = 2046, loss = 0.03117946721613407
In grad_steps = 2047, loss = 0.18239586055278778
In grad_steps = 2048, loss = 0.23172932863235474
In grad_steps = 2049, loss = 0.6137208938598633
In grad_steps = 2050, loss = 0.2790650427341461
In grad_steps = 2051, loss = 0.160538911819458
In grad_steps = 2052, loss = 0.004356655292212963
Beginning epoch 2
In grad_steps = 2053, loss = 0.264093816280365
In grad_steps = 2054, loss = 0.020956456661224365
In grad_steps = 2055, loss = 0.22665514051914215
In grad_steps = 2056, loss = 0.351184219121933
In grad_steps = 2057, loss = 0.07426390051841736
In grad_steps = 2058, loss = 0.022094208747148514
In grad_steps = 2059, loss = 0.07568833976984024
In grad_steps = 2060, loss = 0.0721697136759758
In grad_steps = 2061, loss = 1.3379803895950317
In grad_steps = 2062, loss = 0.5101032257080078
In grad_steps = 2063, loss = 0.9455748200416565
In grad_steps = 2064, loss = 0.040641892701387405
In grad_steps = 2065, loss = 0.7423564195632935
In grad_steps = 2066, loss = 0.06573884189128876
In grad_steps = 2067, loss = 0.455197811126709
In grad_steps = 2068, loss = 0.5305402278900146
In grad_steps = 2069, loss = 0.2719573378562927
In grad_steps = 2070, loss = 0.45851045846939087
In grad_steps = 2071, loss = 0.2449568212032318
In grad_steps = 2072, loss = 0.3373345136642456
In grad_steps = 2073, loss = 0.10697749257087708
In grad_steps = 2074, loss = 0.7034142017364502
In grad_steps = 2075, loss = 0.8188676238059998
In grad_steps = 2076, loss = 0.1610773503780365
In grad_steps = 2077, loss = 0.05452236905694008
In grad_steps = 2078, loss = 0.35628706216812134
In grad_steps = 2079, loss = 0.3067096173763275
In grad_steps = 2080, loss = 0.9688549041748047
In grad_steps = 2081, loss = 0.15146993100643158
In grad_steps = 2082, loss = 0.1504208743572235
In grad_steps = 2083, loss = 0.5626934766769409
In grad_steps = 2084, loss = 0.39811673760414124
In grad_steps = 2085, loss = 0.33203423023223877
In grad_steps = 2086, loss = 0.5460390448570251
In grad_steps = 2087, loss = 0.16648028790950775
In grad_steps = 2088, loss = 0.3024970293045044
In grad_steps = 2089, loss = 0.4114626348018646
In grad_steps = 2090, loss = 0.2090103030204773
In grad_steps = 2091, loss = 0.23798808455467224
In grad_steps = 2092, loss = 0.3372683823108673
In grad_steps = 2093, loss = 0.5300441980361938
In grad_steps = 2094, loss = 0.0982603132724762
In grad_steps = 2095, loss = 0.21977809071540833
In grad_steps = 2096, loss = 0.17730534076690674
In grad_steps = 2097, loss = 0.26690173149108887
In grad_steps = 2098, loss = 0.2133719027042389
In grad_steps = 2099, loss = 0.7383028268814087
In grad_steps = 2100, loss = 0.6113336086273193
In grad_steps = 2101, loss = 0.31859302520751953
In grad_steps = 2102, loss = 0.11891400814056396
In grad_steps = 2103, loss = 0.22686710953712463
In grad_steps = 2104, loss = 0.04901280999183655
In grad_steps = 2105, loss = 0.11207006871700287
In grad_steps = 2106, loss = 0.013126586563885212
In grad_steps = 2107, loss = 0.060491010546684265
In grad_steps = 2108, loss = 0.06998321413993835
In grad_steps = 2109, loss = 0.5347763299942017
In grad_steps = 2110, loss = 0.4816958010196686
In grad_steps = 2111, loss = 0.17957787215709686
In grad_steps = 2112, loss = 0.7623516917228699
In grad_steps = 2113, loss = 1.6857882738113403
In grad_steps = 2114, loss = 0.09462828189134598
In grad_steps = 2115, loss = 0.18369030952453613
In grad_steps = 2116, loss = 0.2851145267486572
In grad_steps = 2117, loss = 0.10077496618032455
In grad_steps = 2118, loss = 0.2644508481025696
In grad_steps = 2119, loss = 0.41046735644340515
In grad_steps = 2120, loss = 0.04479840397834778
In grad_steps = 2121, loss = 0.2607545256614685
In grad_steps = 2122, loss = 0.1753920614719391
In grad_steps = 2123, loss = 0.12981683015823364
In grad_steps = 2124, loss = 0.08311489969491959
In grad_steps = 2125, loss = 0.29560258984565735
In grad_steps = 2126, loss = 0.18431693315505981
In grad_steps = 2127, loss = 0.03653460741043091
In grad_steps = 2128, loss = 0.3376578688621521
In grad_steps = 2129, loss = 0.07501741498708725
In grad_steps = 2130, loss = 0.2729153633117676
In grad_steps = 2131, loss = 0.361518532037735
In grad_steps = 2132, loss = 0.516755998134613
In grad_steps = 2133, loss = 0.43761715292930603
In grad_steps = 2134, loss = 0.3646118938922882
In grad_steps = 2135, loss = 0.12164594233036041
In grad_steps = 2136, loss = 0.5743240118026733
In grad_steps = 2137, loss = 0.058716416358947754
In grad_steps = 2138, loss = 0.6860576868057251
In grad_steps = 2139, loss = 0.17756131291389465
In grad_steps = 2140, loss = 0.15361544489860535
In grad_steps = 2141, loss = 1.012884497642517
In grad_steps = 2142, loss = 0.10796728730201721
In grad_steps = 2143, loss = 0.8711748123168945
In grad_steps = 2144, loss = 0.3544822335243225
In grad_steps = 2145, loss = 0.4278606176376343
In grad_steps = 2146, loss = 0.3971847891807556
In grad_steps = 2147, loss = 0.40065962076187134
In grad_steps = 2148, loss = 0.03238163888454437
In grad_steps = 2149, loss = 0.26262879371643066
In grad_steps = 2150, loss = 0.5245919227600098
In grad_steps = 2151, loss = 0.6251645088195801
In grad_steps = 2152, loss = 0.22645822167396545
In grad_steps = 2153, loss = 0.5825315117835999
In grad_steps = 2154, loss = 0.10696165263652802
In grad_steps = 2155, loss = 0.7013673186302185
In grad_steps = 2156, loss = 0.10363410413265228
In grad_steps = 2157, loss = 0.37785592675209045
In grad_steps = 2158, loss = 0.25324174761772156
In grad_steps = 2159, loss = 0.3031637668609619
In grad_steps = 2160, loss = 0.20451515913009644
In grad_steps = 2161, loss = 0.4964424967765808
In grad_steps = 2162, loss = 0.07580641657114029
In grad_steps = 2163, loss = 0.15421335399150848
In grad_steps = 2164, loss = 0.03244777396321297
In grad_steps = 2165, loss = 0.05553499981760979
In grad_steps = 2166, loss = 0.3082426190376282
In grad_steps = 2167, loss = 0.08780743181705475
In grad_steps = 2168, loss = 0.6546746492385864
In grad_steps = 2169, loss = 0.2980188727378845
In grad_steps = 2170, loss = 0.13033059239387512
In grad_steps = 2171, loss = 0.14927953481674194
In grad_steps = 2172, loss = 0.13323703408241272
In grad_steps = 2173, loss = 0.37846049666404724
In grad_steps = 2174, loss = 0.14006124436855316
In grad_steps = 2175, loss = 0.042497407644987106
In grad_steps = 2176, loss = 0.27825260162353516
In grad_steps = 2177, loss = 0.4004848599433899
In grad_steps = 2178, loss = 0.40243440866470337
In grad_steps = 2179, loss = 0.04257246106863022
In grad_steps = 2180, loss = 0.7611382603645325
In grad_steps = 2181, loss = 0.3240843117237091
In grad_steps = 2182, loss = 0.3011506199836731
In grad_steps = 2183, loss = 0.1075356975197792
In grad_steps = 2184, loss = 0.18826457858085632
In grad_steps = 2185, loss = 0.2679254412651062
In grad_steps = 2186, loss = 0.6393109560012817
In grad_steps = 2187, loss = 0.05015013366937637
In grad_steps = 2188, loss = 0.2523554861545563
In grad_steps = 2189, loss = 0.014386615715920925
In grad_steps = 2190, loss = 0.038275983184576035
In grad_steps = 2191, loss = 1.3401870727539062
In grad_steps = 2192, loss = 0.07724630832672119
In grad_steps = 2193, loss = 1.2166675329208374
In grad_steps = 2194, loss = 0.01807287707924843
In grad_steps = 2195, loss = 0.10661283135414124
In grad_steps = 2196, loss = 0.02601570077240467
In grad_steps = 2197, loss = 0.7464261054992676
In grad_steps = 2198, loss = 0.06670836359262466
In grad_steps = 2199, loss = 0.34045690298080444
In grad_steps = 2200, loss = 0.18276607990264893
In grad_steps = 2201, loss = 0.08859987556934357
In grad_steps = 2202, loss = 0.1427881419658661
In grad_steps = 2203, loss = 0.5123633146286011
In grad_steps = 2204, loss = 0.06186598166823387
In grad_steps = 2205, loss = 0.7833932042121887
In grad_steps = 2206, loss = 0.286247193813324
In grad_steps = 2207, loss = 0.3257294297218323
In grad_steps = 2208, loss = 0.05908846855163574
In grad_steps = 2209, loss = 0.07948025315999985
In grad_steps = 2210, loss = 0.047499943524599075
In grad_steps = 2211, loss = 0.03273536637425423
In grad_steps = 2212, loss = 0.09308210760354996
In grad_steps = 2213, loss = 0.06368117779493332
In grad_steps = 2214, loss = 0.34410154819488525
In grad_steps = 2215, loss = 0.1421801745891571
In grad_steps = 2216, loss = 0.1762171983718872
In grad_steps = 2217, loss = 0.0962214469909668
In grad_steps = 2218, loss = 0.024152792990207672
In grad_steps = 2219, loss = 0.01691831275820732
In grad_steps = 2220, loss = 0.04127734154462814
In grad_steps = 2221, loss = 0.6668784618377686
In grad_steps = 2222, loss = 0.07961824536323547
In grad_steps = 2223, loss = 0.03421453386545181
In grad_steps = 2224, loss = 0.1631564348936081
In grad_steps = 2225, loss = 0.4060591459274292
In grad_steps = 2226, loss = 0.8239556550979614
In grad_steps = 2227, loss = 0.04448014125227928
In grad_steps = 2228, loss = 0.8584674000740051
In grad_steps = 2229, loss = 0.011405697092413902
In grad_steps = 2230, loss = 0.011847589164972305
In grad_steps = 2231, loss = 0.6085179448127747
In grad_steps = 2232, loss = 0.5182124972343445
In grad_steps = 2233, loss = 0.05857115983963013
In grad_steps = 2234, loss = 0.04045148938894272
In grad_steps = 2235, loss = 0.1516159623861313
In grad_steps = 2236, loss = 0.8939365148544312
In grad_steps = 2237, loss = 0.0928674191236496
In grad_steps = 2238, loss = 0.4305867850780487
In grad_steps = 2239, loss = 0.046388737857341766
In grad_steps = 2240, loss = 0.3829704523086548
In grad_steps = 2241, loss = 0.21426744759082794
In grad_steps = 2242, loss = 0.20089447498321533
In grad_steps = 2243, loss = 0.09780262410640717
In grad_steps = 2244, loss = 0.2132529318332672
In grad_steps = 2245, loss = 0.16176925599575043
In grad_steps = 2246, loss = 0.3652951121330261
In grad_steps = 2247, loss = 0.18254639208316803
In grad_steps = 2248, loss = 0.17670290172100067
In grad_steps = 2249, loss = 0.10182276368141174
In grad_steps = 2250, loss = 0.5368608236312866
In grad_steps = 2251, loss = 0.21917857229709625
In grad_steps = 2252, loss = 1.0083284378051758
In grad_steps = 2253, loss = 0.029562663286924362
In grad_steps = 2254, loss = 0.4700818657875061
In grad_steps = 2255, loss = 0.04224056005477905
In grad_steps = 2256, loss = 0.42691749334335327
In grad_steps = 2257, loss = 0.19507506489753723
In grad_steps = 2258, loss = 0.16860033571720123
In grad_steps = 2259, loss = 0.08428122848272324
In grad_steps = 2260, loss = 0.3715360164642334
In grad_steps = 2261, loss = 0.18907254934310913
In grad_steps = 2262, loss = 0.057105652987957
In grad_steps = 2263, loss = 0.3674861192703247
In grad_steps = 2264, loss = 0.44301432371139526
In grad_steps = 2265, loss = 0.5517972111701965
In grad_steps = 2266, loss = 0.09487345069646835
In grad_steps = 2267, loss = 0.06742554903030396
In grad_steps = 2268, loss = 0.08984722197055817
In grad_steps = 2269, loss = 1.0291646718978882
In grad_steps = 2270, loss = 0.8076868653297424
In grad_steps = 2271, loss = 0.6738008856773376
In grad_steps = 2272, loss = 0.4061330556869507
In grad_steps = 2273, loss = 0.310086727142334
In grad_steps = 2274, loss = 0.5809251070022583
In grad_steps = 2275, loss = 0.319545179605484
In grad_steps = 2276, loss = 0.2802988588809967
In grad_steps = 2277, loss = 0.15215685963630676
In grad_steps = 2278, loss = 0.3254293203353882
In grad_steps = 2279, loss = 0.21950949728488922
In grad_steps = 2280, loss = 0.19684968888759613
In grad_steps = 2281, loss = 0.18725405633449554
In grad_steps = 2282, loss = 0.24713528156280518
In grad_steps = 2283, loss = 0.1495755910873413
In grad_steps = 2284, loss = 0.6878652572631836
In grad_steps = 2285, loss = 1.054889440536499
In grad_steps = 2286, loss = 0.061693623661994934
In grad_steps = 2287, loss = 0.9582592248916626
In grad_steps = 2288, loss = 0.3612514138221741
In grad_steps = 2289, loss = 0.49274498224258423
In grad_steps = 2290, loss = 0.1482979953289032
In grad_steps = 2291, loss = 0.10969330370426178
In grad_steps = 2292, loss = 0.19485466182231903
In grad_steps = 2293, loss = 0.08640936017036438
In grad_steps = 2294, loss = 0.3815368115901947
In grad_steps = 2295, loss = 0.1538505256175995
In grad_steps = 2296, loss = 0.29653748869895935
In grad_steps = 2297, loss = 0.25618767738342285
In grad_steps = 2298, loss = 0.1781744509935379
In grad_steps = 2299, loss = 0.11717594414949417
In grad_steps = 2300, loss = 0.17715699970722198
In grad_steps = 2301, loss = 1.168128252029419
In grad_steps = 2302, loss = 0.14832703769207
In grad_steps = 2303, loss = 0.2221457064151764
In grad_steps = 2304, loss = 0.19669240713119507
In grad_steps = 2305, loss = 0.11634553968906403
In grad_steps = 2306, loss = 0.17355918884277344
In grad_steps = 2307, loss = 0.06034214049577713
In grad_steps = 2308, loss = 0.6527106761932373
In grad_steps = 2309, loss = 1.0828999280929565
In grad_steps = 2310, loss = 0.19115516543388367
In grad_steps = 2311, loss = 0.5403519868850708
In grad_steps = 2312, loss = 0.15920615196228027
In grad_steps = 2313, loss = 0.7763840556144714
In grad_steps = 2314, loss = 0.8744744062423706
In grad_steps = 2315, loss = 0.8471651077270508
In grad_steps = 2316, loss = 0.2850506901741028
In grad_steps = 2317, loss = 0.35844850540161133
In grad_steps = 2318, loss = 0.31539613008499146
In grad_steps = 2319, loss = 0.29561638832092285
In grad_steps = 2320, loss = 0.9295055270195007
In grad_steps = 2321, loss = 0.21233147382736206
In grad_steps = 2322, loss = 0.26058849692344666
In grad_steps = 2323, loss = 0.2625034749507904
In grad_steps = 2324, loss = 0.8118965029716492
In grad_steps = 2325, loss = 0.2861686944961548
In grad_steps = 2326, loss = 0.11520114541053772
In grad_steps = 2327, loss = 0.39070284366607666
In grad_steps = 2328, loss = 0.5858362913131714
In grad_steps = 2329, loss = 0.26000091433525085
In grad_steps = 2330, loss = 0.1754242181777954
In grad_steps = 2331, loss = 0.3581624925136566
In grad_steps = 2332, loss = 0.1809879094362259
In grad_steps = 2333, loss = 0.6565704345703125
In grad_steps = 2334, loss = 0.1238035261631012
In grad_steps = 2335, loss = 0.6144646406173706
In grad_steps = 2336, loss = 0.589322566986084
In grad_steps = 2337, loss = 0.22296208143234253
In grad_steps = 2338, loss = 0.18234390020370483
In grad_steps = 2339, loss = 0.10261036455631256
In grad_steps = 2340, loss = 0.44896721839904785
In grad_steps = 2341, loss = 0.47146618366241455
In grad_steps = 2342, loss = 0.373187780380249
In grad_steps = 2343, loss = 0.31030112504959106
In grad_steps = 2344, loss = 0.44223207235336304
In grad_steps = 2345, loss = 0.31406712532043457
In grad_steps = 2346, loss = 0.20430181920528412
In grad_steps = 2347, loss = 0.19147726893424988
In grad_steps = 2348, loss = 0.2672134041786194
In grad_steps = 2349, loss = 0.5416831970214844
In grad_steps = 2350, loss = 0.2342909276485443
In grad_steps = 2351, loss = 0.04762538522481918
In grad_steps = 2352, loss = 0.08835572749376297
In grad_steps = 2353, loss = 0.8537511229515076
In grad_steps = 2354, loss = 0.4819357395172119
In grad_steps = 2355, loss = 0.24258071184158325
In grad_steps = 2356, loss = 0.06667504459619522
In grad_steps = 2357, loss = 0.06372451782226562
In grad_steps = 2358, loss = 0.03562357276678085
In grad_steps = 2359, loss = 0.18579334020614624
In grad_steps = 2360, loss = 1.052046775817871
In grad_steps = 2361, loss = 1.0088469982147217
In grad_steps = 2362, loss = 0.06620080769062042
In grad_steps = 2363, loss = 0.6693196296691895
In grad_steps = 2364, loss = 0.2662696838378906
In grad_steps = 2365, loss = 0.0501873679459095
In grad_steps = 2366, loss = 0.08640018105506897
In grad_steps = 2367, loss = 0.11337213963270187
In grad_steps = 2368, loss = 0.021175190806388855
In grad_steps = 2369, loss = 1.1607391834259033
In grad_steps = 2370, loss = 0.5821632742881775
In grad_steps = 2371, loss = 0.20039331912994385
In grad_steps = 2372, loss = 0.04424351081252098
In grad_steps = 2373, loss = 0.3276810348033905
In grad_steps = 2374, loss = 0.21828392148017883
In grad_steps = 2375, loss = 0.6980313658714294
In grad_steps = 2376, loss = 0.22445690631866455
In grad_steps = 2377, loss = 0.07495022565126419
In grad_steps = 2378, loss = 0.38422176241874695
In grad_steps = 2379, loss = 0.2779765725135803
In grad_steps = 2380, loss = 0.25040459632873535
In grad_steps = 2381, loss = 0.027057070285081863
In grad_steps = 2382, loss = 0.7214686274528503
In grad_steps = 2383, loss = 0.23971198499202728
In grad_steps = 2384, loss = 0.05277836322784424
In grad_steps = 2385, loss = 0.042879942804574966
In grad_steps = 2386, loss = 0.2408105731010437
In grad_steps = 2387, loss = 0.29502153396606445
In grad_steps = 2388, loss = 0.07175250351428986
In grad_steps = 2389, loss = 0.18234622478485107
In grad_steps = 2390, loss = 0.04844668135046959
In grad_steps = 2391, loss = 0.06161661446094513
In grad_steps = 2392, loss = 0.29806044697761536
In grad_steps = 2393, loss = 0.029902592301368713
In grad_steps = 2394, loss = 0.8077344298362732
In grad_steps = 2395, loss = 0.07946701347827911
In grad_steps = 2396, loss = 0.6795586347579956
In grad_steps = 2397, loss = 0.1385406255722046
In grad_steps = 2398, loss = 0.8353015184402466
In grad_steps = 2399, loss = 0.06657205522060394
In grad_steps = 2400, loss = 0.29318898916244507
In grad_steps = 2401, loss = 0.25194138288497925
In grad_steps = 2402, loss = 0.10497907549142838
In grad_steps = 2403, loss = 0.06737218052148819
In grad_steps = 2404, loss = 0.10057536512613297
In grad_steps = 2405, loss = 0.3473432958126068
In grad_steps = 2406, loss = 0.8862910270690918
In grad_steps = 2407, loss = 0.08336931467056274
In grad_steps = 2408, loss = 0.44088757038116455
In grad_steps = 2409, loss = 0.5102399587631226
In grad_steps = 2410, loss = 0.5892818570137024
In grad_steps = 2411, loss = 0.21887457370758057
In grad_steps = 2412, loss = 0.7919438481330872
In grad_steps = 2413, loss = 0.42463067173957825
In grad_steps = 2414, loss = 0.03683852404356003
In grad_steps = 2415, loss = 0.4552505910396576
In grad_steps = 2416, loss = 0.7631818652153015
In grad_steps = 2417, loss = 0.07721611112356186
In grad_steps = 2418, loss = 0.8692377805709839
In grad_steps = 2419, loss = 0.3832671344280243
In grad_steps = 2420, loss = 0.27202171087265015
In grad_steps = 2421, loss = 0.4129650294780731
In grad_steps = 2422, loss = 0.43831324577331543
In grad_steps = 2423, loss = 0.7390522360801697
In grad_steps = 2424, loss = 0.08639419078826904
In grad_steps = 2425, loss = 0.16776302456855774
In grad_steps = 2426, loss = 0.5302006006240845
In grad_steps = 2427, loss = 0.16702857613563538
In grad_steps = 2428, loss = 0.36301273107528687
In grad_steps = 2429, loss = 0.37386220693588257
In grad_steps = 2430, loss = 0.25805673003196716
In grad_steps = 2431, loss = 0.2161821573972702
In grad_steps = 2432, loss = 0.2610042691230774
In grad_steps = 2433, loss = 0.4763132631778717
In grad_steps = 2434, loss = 0.40664321184158325
In grad_steps = 2435, loss = 0.17636798322200775
In grad_steps = 2436, loss = 0.06846699118614197
In grad_steps = 2437, loss = 0.07281097769737244
In grad_steps = 2438, loss = 0.06234338879585266
In grad_steps = 2439, loss = 0.044510211795568466
In grad_steps = 2440, loss = 0.8269597291946411
In grad_steps = 2441, loss = 0.1785757690668106
In grad_steps = 2442, loss = 0.26256540417671204
In grad_steps = 2443, loss = 0.218990758061409
In grad_steps = 2444, loss = 0.7346171140670776
In grad_steps = 2445, loss = 0.09241558611392975
In grad_steps = 2446, loss = 0.05702830106019974
In grad_steps = 2447, loss = 0.10141626745462418
In grad_steps = 2448, loss = 1.3783349990844727
In grad_steps = 2449, loss = 0.7107074856758118
In grad_steps = 2450, loss = 0.005172436591237783
In grad_steps = 2451, loss = 1.227648377418518
In grad_steps = 2452, loss = 0.1132735088467598
In grad_steps = 2453, loss = 0.9008070826530457
In grad_steps = 2454, loss = 0.07911983877420425
In grad_steps = 2455, loss = 0.21749305725097656
In grad_steps = 2456, loss = 0.08698251098394394
In grad_steps = 2457, loss = 0.061813876032829285
In grad_steps = 2458, loss = 0.08710455894470215
In grad_steps = 2459, loss = 0.32227060198783875
In grad_steps = 2460, loss = 0.041197650134563446
In grad_steps = 2461, loss = 0.07601986080408096
In grad_steps = 2462, loss = 0.5123640298843384
In grad_steps = 2463, loss = 0.21912528574466705
In grad_steps = 2464, loss = 0.14274761080741882
In grad_steps = 2465, loss = 0.2065843641757965
In grad_steps = 2466, loss = 0.5222516059875488
In grad_steps = 2467, loss = 0.49665385484695435
In grad_steps = 2468, loss = 0.6855111122131348
In grad_steps = 2469, loss = 1.217687726020813
In grad_steps = 2470, loss = 0.23214499652385712
In grad_steps = 2471, loss = 0.11511807888746262
In grad_steps = 2472, loss = 0.4146924614906311
In grad_steps = 2473, loss = 0.554709792137146
In grad_steps = 2474, loss = 0.09032019972801208
In grad_steps = 2475, loss = 0.18974828720092773
In grad_steps = 2476, loss = 0.2176792323589325
In grad_steps = 2477, loss = 0.08308200538158417
In grad_steps = 2478, loss = 0.06179676949977875
In grad_steps = 2479, loss = 0.18506307899951935
In grad_steps = 2480, loss = 0.18811771273612976
In grad_steps = 2481, loss = 0.13776767253875732
In grad_steps = 2482, loss = 0.6654824614524841
In grad_steps = 2483, loss = 0.1164625734090805
In grad_steps = 2484, loss = 0.2407560646533966
In grad_steps = 2485, loss = 0.38378530740737915
In grad_steps = 2486, loss = 0.7647547721862793
In grad_steps = 2487, loss = 0.11693404614925385
In grad_steps = 2488, loss = 0.11746004968881607
In grad_steps = 2489, loss = 0.04514627903699875
In grad_steps = 2490, loss = 0.09441787004470825
In grad_steps = 2491, loss = 0.049372971057891846
In grad_steps = 2492, loss = 0.052765894681215286
In grad_steps = 2493, loss = 0.05941692739725113
In grad_steps = 2494, loss = 0.019203446805477142
In grad_steps = 2495, loss = 0.1852601170539856
In grad_steps = 2496, loss = 0.08554165810346603
In grad_steps = 2497, loss = 0.5271520018577576
In grad_steps = 2498, loss = 0.05939123407006264
In grad_steps = 2499, loss = 0.17094025015830994
In grad_steps = 2500, loss = 0.06998376548290253
In grad_steps = 2501, loss = 0.030175210908055305
In grad_steps = 2502, loss = 0.04191956669092178
In grad_steps = 2503, loss = 0.015759611502289772
In grad_steps = 2504, loss = 0.03971030190587044
In grad_steps = 2505, loss = 0.08619466423988342
In grad_steps = 2506, loss = 0.006831051781773567
In grad_steps = 2507, loss = 0.1119239553809166
In grad_steps = 2508, loss = 0.19344516098499298
In grad_steps = 2509, loss = 0.7302837371826172
In grad_steps = 2510, loss = 0.005897447466850281
In grad_steps = 2511, loss = 0.05714399367570877
In grad_steps = 2512, loss = 0.004594285972416401
In grad_steps = 2513, loss = 0.006974364630877972
In grad_steps = 2514, loss = 0.19699181616306305
In grad_steps = 2515, loss = 0.17031289637088776
In grad_steps = 2516, loss = 0.07715259492397308
In grad_steps = 2517, loss = 0.1244824081659317
In grad_steps = 2518, loss = 0.004656289704144001
In grad_steps = 2519, loss = 0.6864981651306152
In grad_steps = 2520, loss = 0.5108246207237244
In grad_steps = 2521, loss = 0.08607824146747589
In grad_steps = 2522, loss = 1.577083706855774
In grad_steps = 2523, loss = 0.030746661126613617
In grad_steps = 2524, loss = 0.5910027623176575
In grad_steps = 2525, loss = 1.114725112915039
In grad_steps = 2526, loss = 0.5460835695266724
In grad_steps = 2527, loss = 1.1644929647445679
In grad_steps = 2528, loss = 0.07461155951023102
In grad_steps = 2529, loss = 0.23657837510108948
In grad_steps = 2530, loss = 0.3620462715625763
In grad_steps = 2531, loss = 0.19064483046531677
In grad_steps = 2532, loss = 0.13956430554389954
In grad_steps = 2533, loss = 0.06607905775308609
In grad_steps = 2534, loss = 0.0917847752571106
In grad_steps = 2535, loss = 0.047545045614242554
In grad_steps = 2536, loss = 0.5517992377281189
In grad_steps = 2537, loss = 0.047012537717819214
In grad_steps = 2538, loss = 0.15521827340126038
In grad_steps = 2539, loss = 0.19688643515110016
In grad_steps = 2540, loss = 0.7026938199996948
In grad_steps = 2541, loss = 0.16122812032699585
In grad_steps = 2542, loss = 0.055674999952316284
In grad_steps = 2543, loss = 0.14345189929008484
In grad_steps = 2544, loss = 0.07890330255031586
In grad_steps = 2545, loss = 0.03916577249765396
In grad_steps = 2546, loss = 0.10416419059038162
In grad_steps = 2547, loss = 0.08469858020544052
In grad_steps = 2548, loss = 0.074336938560009
In grad_steps = 2549, loss = 0.11114999651908875
In grad_steps = 2550, loss = 2.0644140243530273
In grad_steps = 2551, loss = 0.3549012839794159
In grad_steps = 2552, loss = 0.046371638774871826
In grad_steps = 2553, loss = 0.02456073649227619
In grad_steps = 2554, loss = 0.01773988828063011
In grad_steps = 2555, loss = 1.1406006813049316
In grad_steps = 2556, loss = 0.04578114300966263
In grad_steps = 2557, loss = 1.3855719566345215
In grad_steps = 2558, loss = 0.03993938863277435
In grad_steps = 2559, loss = 0.47881707549095154
In grad_steps = 2560, loss = 0.29605498909950256
In grad_steps = 2561, loss = 0.2502742111682892
In grad_steps = 2562, loss = 0.9832820296287537
In grad_steps = 2563, loss = 0.15666821599006653
In grad_steps = 2564, loss = 0.4138895571231842
In grad_steps = 2565, loss = 0.33377206325531006
In grad_steps = 2566, loss = 0.1403922587633133
In grad_steps = 2567, loss = 0.22846215963363647
In grad_steps = 2568, loss = 0.16732513904571533
In grad_steps = 2569, loss = 0.12877970933914185
In grad_steps = 2570, loss = 0.49982723593711853
In grad_steps = 2571, loss = 0.2989145517349243
In grad_steps = 2572, loss = 0.13338489830493927
In grad_steps = 2573, loss = 0.35943472385406494
In grad_steps = 2574, loss = 0.19572101533412933
In grad_steps = 2575, loss = 0.47597894072532654
In grad_steps = 2576, loss = 0.4904009699821472
In grad_steps = 2577, loss = 0.15501508116722107
In grad_steps = 2578, loss = 0.35248488187789917
In grad_steps = 2579, loss = 0.36614733934402466
In grad_steps = 2580, loss = 0.34756726026535034
In grad_steps = 2581, loss = 0.6950239539146423
In grad_steps = 2582, loss = 0.04547770321369171
In grad_steps = 2583, loss = 0.031208744272589684
In grad_steps = 2584, loss = 0.02213900163769722
In grad_steps = 2585, loss = 1.5340466499328613
In grad_steps = 2586, loss = 0.846489667892456
In grad_steps = 2587, loss = 0.04032079502940178
In grad_steps = 2588, loss = 1.5946083068847656
In grad_steps = 2589, loss = 0.36615049839019775
In grad_steps = 2590, loss = 0.45217493176460266
In grad_steps = 2591, loss = 0.7197134494781494
In grad_steps = 2592, loss = 0.14624883234500885
In grad_steps = 2593, loss = 0.13415241241455078
In grad_steps = 2594, loss = 0.4781673550605774
In grad_steps = 2595, loss = 0.20016337931156158
In grad_steps = 2596, loss = 0.1493743509054184
In grad_steps = 2597, loss = 0.5994994640350342
In grad_steps = 2598, loss = 0.3766646981239319
In grad_steps = 2599, loss = 0.39251500368118286
In grad_steps = 2600, loss = 0.2803557515144348
In grad_steps = 2601, loss = 0.18394355475902557
In grad_steps = 2602, loss = 0.5049185156822205
In grad_steps = 2603, loss = 0.24063575267791748
In grad_steps = 2604, loss = 0.31154537200927734
In grad_steps = 2605, loss = 0.3183676600456238
In grad_steps = 2606, loss = 0.290029913187027
In grad_steps = 2607, loss = 0.11325594037771225
In grad_steps = 2608, loss = 0.0961277037858963
In grad_steps = 2609, loss = 0.752831220626831
In grad_steps = 2610, loss = 0.46763181686401367
In grad_steps = 2611, loss = 0.04194815456867218
In grad_steps = 2612, loss = 0.1767028272151947
In grad_steps = 2613, loss = 0.18157123029232025
In grad_steps = 2614, loss = 0.06270071119070053
In grad_steps = 2615, loss = 0.10361205786466599
In grad_steps = 2616, loss = 0.1031452864408493
In grad_steps = 2617, loss = 0.32574284076690674
In grad_steps = 2618, loss = 0.5975133180618286
In grad_steps = 2619, loss = 0.41610366106033325
In grad_steps = 2620, loss = 1.1052203178405762
In grad_steps = 2621, loss = 0.17094182968139648
In grad_steps = 2622, loss = 0.20375065505504608
In grad_steps = 2623, loss = 0.6091201901435852
In grad_steps = 2624, loss = 0.4877302646636963
In grad_steps = 2625, loss = 0.07267294824123383
In grad_steps = 2626, loss = 0.05106627941131592
In grad_steps = 2627, loss = 0.36586064100265503
In grad_steps = 2628, loss = 0.08312001824378967
In grad_steps = 2629, loss = 0.8782491087913513
In grad_steps = 2630, loss = 0.507945716381073
In grad_steps = 2631, loss = 0.9794144630432129
In grad_steps = 2632, loss = 0.07903334498405457
In grad_steps = 2633, loss = 0.16048941016197205
In grad_steps = 2634, loss = 0.1019069030880928
In grad_steps = 2635, loss = 0.6846922636032104
In grad_steps = 2636, loss = 0.2234436571598053
In grad_steps = 2637, loss = 0.231672465801239
In grad_steps = 2638, loss = 0.4607425928115845
In grad_steps = 2639, loss = 0.17717283964157104
In grad_steps = 2640, loss = 0.19557738304138184
In grad_steps = 2641, loss = 0.17387615144252777
In grad_steps = 2642, loss = 0.24054700136184692
In grad_steps = 2643, loss = 0.2774147391319275
In grad_steps = 2644, loss = 0.3460507094860077
In grad_steps = 2645, loss = 0.1782240867614746
In grad_steps = 2646, loss = 0.3594837784767151
In grad_steps = 2647, loss = 0.3585272431373596
In grad_steps = 2648, loss = 0.6276758313179016
In grad_steps = 2649, loss = 0.1363115757703781
In grad_steps = 2650, loss = 0.5656204223632812
In grad_steps = 2651, loss = 0.13314372301101685
In grad_steps = 2652, loss = 0.7464396953582764
In grad_steps = 2653, loss = 0.45205962657928467
In grad_steps = 2654, loss = 0.10309574007987976
In grad_steps = 2655, loss = 0.5098193883895874
In grad_steps = 2656, loss = 0.041334524750709534
In grad_steps = 2657, loss = 0.37551742792129517
In grad_steps = 2658, loss = 0.9997806549072266
In grad_steps = 2659, loss = 0.24010147154331207
In grad_steps = 2660, loss = 0.4206930994987488
In grad_steps = 2661, loss = 0.5262476205825806
In grad_steps = 2662, loss = 0.06114627420902252
In grad_steps = 2663, loss = 0.20647285878658295
In grad_steps = 2664, loss = 0.2409292608499527
In grad_steps = 2665, loss = 0.3882427513599396
In grad_steps = 2666, loss = 0.16706493496894836
In grad_steps = 2667, loss = 0.08701126277446747
In grad_steps = 2668, loss = 0.17882411181926727
In grad_steps = 2669, loss = 0.10943648219108582
In grad_steps = 2670, loss = 0.11769038438796997
In grad_steps = 2671, loss = 0.1970791220664978
In grad_steps = 2672, loss = 0.09134452790021896
In grad_steps = 2673, loss = 0.0605374276638031
In grad_steps = 2674, loss = 0.04990151897072792
In grad_steps = 2675, loss = 0.018109461292624474
In grad_steps = 2676, loss = 0.049455322325229645
In grad_steps = 2677, loss = 0.0474030002951622
In grad_steps = 2678, loss = 0.01553196832537651
In grad_steps = 2679, loss = 0.07600153237581253
In grad_steps = 2680, loss = 0.14354445040225983
In grad_steps = 2681, loss = 0.13375866413116455
In grad_steps = 2682, loss = 1.0037697553634644
In grad_steps = 2683, loss = 0.011748240329325199
In grad_steps = 2684, loss = 0.19138886034488678
In grad_steps = 2685, loss = 0.4251074492931366
In grad_steps = 2686, loss = 0.40059882402420044
In grad_steps = 2687, loss = 0.04952641576528549
In grad_steps = 2688, loss = 0.5040943622589111
In grad_steps = 2689, loss = 0.06623151153326035
In grad_steps = 2690, loss = 0.0490100122988224
In grad_steps = 2691, loss = 0.0354011133313179
In grad_steps = 2692, loss = 0.052470751106739044
In grad_steps = 2693, loss = 1.381886601448059
In grad_steps = 2694, loss = 0.8750124573707581
In grad_steps = 2695, loss = 0.3477121889591217
In grad_steps = 2696, loss = 0.027176033705472946
In grad_steps = 2697, loss = 0.721642792224884
In grad_steps = 2698, loss = 0.04231869801878929
In grad_steps = 2699, loss = 0.02731659635901451
In grad_steps = 2700, loss = 0.6067814230918884
In grad_steps = 2701, loss = 0.14127175509929657
In grad_steps = 2702, loss = 0.1339477300643921
In grad_steps = 2703, loss = 0.4863418638706207
In grad_steps = 2704, loss = 0.0888819620013237
In grad_steps = 2705, loss = 0.21991531550884247
In grad_steps = 2706, loss = 0.1268344521522522
In grad_steps = 2707, loss = 0.5394725799560547
In grad_steps = 2708, loss = 0.22286182641983032
In grad_steps = 2709, loss = 0.5781991481781006
In grad_steps = 2710, loss = 0.27518728375434875
In grad_steps = 2711, loss = 0.07105966657400131
In grad_steps = 2712, loss = 0.08388683944940567
In grad_steps = 2713, loss = 0.4069627523422241
In grad_steps = 2714, loss = 0.72079998254776
In grad_steps = 2715, loss = 0.837262749671936
In grad_steps = 2716, loss = 0.13842865824699402
In grad_steps = 2717, loss = 0.7317379117012024
In grad_steps = 2718, loss = 0.47374677658081055
In grad_steps = 2719, loss = 0.16876332461833954
In grad_steps = 2720, loss = 0.5387147665023804
In grad_steps = 2721, loss = 0.4683408737182617
In grad_steps = 2722, loss = 0.2226957082748413
In grad_steps = 2723, loss = 0.2807314991950989
In grad_steps = 2724, loss = 0.5942465662956238
In grad_steps = 2725, loss = 0.07340317964553833
In grad_steps = 2726, loss = 0.8946760892868042
In grad_steps = 2727, loss = 0.029877815395593643
In grad_steps = 2728, loss = 0.5563606023788452
In grad_steps = 2729, loss = 0.38758814334869385
In grad_steps = 2730, loss = 0.08850876986980438
In grad_steps = 2731, loss = 0.08625663816928864
In grad_steps = 2732, loss = 0.10061535239219666
In grad_steps = 2733, loss = 0.25282689929008484
In grad_steps = 2734, loss = 0.0682419016957283
In grad_steps = 2735, loss = 0.8104218244552612
In grad_steps = 2736, loss = 0.07298490405082703
In grad_steps = 2737, loss = 0.2207179069519043
In grad_steps = 2738, loss = 0.16230279207229614
In grad_steps = 2739, loss = 0.08246690034866333
In grad_steps = 2740, loss = 0.08078894019126892
In grad_steps = 2741, loss = 0.1543046534061432
In grad_steps = 2742, loss = 0.20251905918121338
In grad_steps = 2743, loss = 0.08581574261188507
In grad_steps = 2744, loss = 0.13582727313041687
In grad_steps = 2745, loss = 0.016885118559002876
In grad_steps = 2746, loss = 1.0010440349578857
In grad_steps = 2747, loss = 0.3225747346878052
In grad_steps = 2748, loss = 0.0507306270301342
In grad_steps = 2749, loss = 0.028328897431492805
In grad_steps = 2750, loss = 0.08682197332382202
In grad_steps = 2751, loss = 0.06261605769395828
In grad_steps = 2752, loss = 0.04333457723259926
In grad_steps = 2753, loss = 0.23282840847969055
In grad_steps = 2754, loss = 0.23442208766937256
In grad_steps = 2755, loss = 0.015999672934412956
In grad_steps = 2756, loss = 0.02873021923005581
In grad_steps = 2757, loss = 0.10023427754640579
In grad_steps = 2758, loss = 1.3081592321395874
In grad_steps = 2759, loss = 0.2666587233543396
In grad_steps = 2760, loss = 0.021256059408187866
In grad_steps = 2761, loss = 0.07024064660072327
In grad_steps = 2762, loss = 0.22020550072193146
In grad_steps = 2763, loss = 0.04824388772249222
In grad_steps = 2764, loss = 0.01748526468873024
In grad_steps = 2765, loss = 0.03624162822961807
In grad_steps = 2766, loss = 1.6898249387741089
In grad_steps = 2767, loss = 0.40598297119140625
In grad_steps = 2768, loss = 1.0762332677841187
In grad_steps = 2769, loss = 0.240232452750206
In grad_steps = 2770, loss = 0.08874986320734024
In grad_steps = 2771, loss = 0.037167057394981384
In grad_steps = 2772, loss = 0.3041314482688904
In grad_steps = 2773, loss = 1.5780441761016846
In grad_steps = 2774, loss = 0.6561678647994995
In grad_steps = 2775, loss = 0.105999656021595
In grad_steps = 2776, loss = 0.16672852635383606
In grad_steps = 2777, loss = 0.03747060149908066
In grad_steps = 2778, loss = 0.14244432747364044
In grad_steps = 2779, loss = 0.16372916102409363
In grad_steps = 2780, loss = 0.21754325926303864
In grad_steps = 2781, loss = 0.13904613256454468
In grad_steps = 2782, loss = 0.11075487732887268
In grad_steps = 2783, loss = 0.05758778378367424
In grad_steps = 2784, loss = 0.1062624603509903
In grad_steps = 2785, loss = 0.10344758629798889
In grad_steps = 2786, loss = 0.17155787348747253
In grad_steps = 2787, loss = 0.17486493289470673
In grad_steps = 2788, loss = 0.23874372243881226
In grad_steps = 2789, loss = 1.0738006830215454
In grad_steps = 2790, loss = 0.4188300371170044
In grad_steps = 2791, loss = 0.10932137072086334
In grad_steps = 2792, loss = 0.1017652079463005
In grad_steps = 2793, loss = 0.42630311846733093
In grad_steps = 2794, loss = 0.2050362378358841
In grad_steps = 2795, loss = 0.16174525022506714
In grad_steps = 2796, loss = 0.2378988265991211
In grad_steps = 2797, loss = 0.14351530373096466
In grad_steps = 2798, loss = 0.01211063377559185
In grad_steps = 2799, loss = 0.08151857554912567
In grad_steps = 2800, loss = 0.05413631722331047
In grad_steps = 2801, loss = 0.02113932929933071
In grad_steps = 2802, loss = 0.08827875554561615
In grad_steps = 2803, loss = 0.9315740466117859
In grad_steps = 2804, loss = 0.31125375628471375
In grad_steps = 2805, loss = 0.40549346804618835
In grad_steps = 2806, loss = 0.020181555300951004
In grad_steps = 2807, loss = 0.4398314654827118
In grad_steps = 2808, loss = 0.020904317498207092
In grad_steps = 2809, loss = 0.03180035203695297
In grad_steps = 2810, loss = 0.033544979989528656
In grad_steps = 2811, loss = 0.029713887721300125
In grad_steps = 2812, loss = 0.09670624881982803
In grad_steps = 2813, loss = 0.01987905241549015
In grad_steps = 2814, loss = 0.2527826130390167
In grad_steps = 2815, loss = 0.02194966748356819
In grad_steps = 2816, loss = 0.23894740641117096
In grad_steps = 2817, loss = 0.027863722294569016
In grad_steps = 2818, loss = 0.22592924535274506
In grad_steps = 2819, loss = 0.22548969089984894
In grad_steps = 2820, loss = 0.014460385777056217
In grad_steps = 2821, loss = 0.06158478185534477
In grad_steps = 2822, loss = 0.2043323665857315
In grad_steps = 2823, loss = 0.021800216287374496
In grad_steps = 2824, loss = 0.5248956680297852
In grad_steps = 2825, loss = 0.3586467504501343
In grad_steps = 2826, loss = 0.0613267682492733
In grad_steps = 2827, loss = 0.012781507335603237
In grad_steps = 2828, loss = 0.02707498148083687
In grad_steps = 2829, loss = 0.01640694960951805
In grad_steps = 2830, loss = 0.33314067125320435
In grad_steps = 2831, loss = 0.43044641613960266
In grad_steps = 2832, loss = 0.3100978136062622
In grad_steps = 2833, loss = 0.06282835453748703
In grad_steps = 2834, loss = 1.3189109563827515
In grad_steps = 2835, loss = 0.797122061252594
In grad_steps = 2836, loss = 0.3493627905845642
In grad_steps = 2837, loss = 0.42132946848869324
In grad_steps = 2838, loss = 0.7474363446235657
In grad_steps = 2839, loss = 0.3324945867061615
In grad_steps = 2840, loss = 0.07776139676570892
In grad_steps = 2841, loss = 0.0860983282327652
In grad_steps = 2842, loss = 0.326089084148407
In grad_steps = 2843, loss = 0.2782631516456604
In grad_steps = 2844, loss = 0.1102527603507042
In grad_steps = 2845, loss = 0.12136968970298767
In grad_steps = 2846, loss = 0.14957590401172638
In grad_steps = 2847, loss = 0.15426701307296753
In grad_steps = 2848, loss = 0.05589555576443672
In grad_steps = 2849, loss = 0.3920899033546448
In grad_steps = 2850, loss = 0.08292260020971298
In grad_steps = 2851, loss = 0.08682262152433395
In grad_steps = 2852, loss = 0.1355649083852768
In grad_steps = 2853, loss = 0.24989312887191772
In grad_steps = 2854, loss = 0.03195440024137497
In grad_steps = 2855, loss = 0.016704512760043144
In grad_steps = 2856, loss = 0.06213972717523575
In grad_steps = 2857, loss = 0.013534042052924633
In grad_steps = 2858, loss = 0.22501569986343384
In grad_steps = 2859, loss = 1.1236107349395752
In grad_steps = 2860, loss = 0.023770175874233246
In grad_steps = 2861, loss = 0.06616467982530594
In grad_steps = 2862, loss = 0.18428577482700348
In grad_steps = 2863, loss = 0.04246155545115471
In grad_steps = 2864, loss = 0.008609367534518242
In grad_steps = 2865, loss = 0.10863158851861954
In grad_steps = 2866, loss = 0.03553437814116478
In grad_steps = 2867, loss = 0.28887397050857544
In grad_steps = 2868, loss = 0.01490189228206873
In grad_steps = 2869, loss = 0.01444060169160366
In grad_steps = 2870, loss = 0.12380050122737885
In grad_steps = 2871, loss = 0.038295675069093704
In grad_steps = 2872, loss = 0.7681693434715271
In grad_steps = 2873, loss = 0.01528744213283062
In grad_steps = 2874, loss = 0.9623188972473145
In grad_steps = 2875, loss = 0.0448414646089077
In grad_steps = 2876, loss = 0.03380356729030609
In grad_steps = 2877, loss = 0.02479429543018341
In grad_steps = 2878, loss = 0.0535358302295208
In grad_steps = 2879, loss = 0.042591020464897156
In grad_steps = 2880, loss = 0.04033806547522545
In grad_steps = 2881, loss = 0.6566646099090576
In grad_steps = 2882, loss = 0.1571495682001114
In grad_steps = 2883, loss = 1.0814976692199707
In grad_steps = 2884, loss = 0.024262769147753716
In grad_steps = 2885, loss = 0.08855543285608292
In grad_steps = 2886, loss = 0.5051043629646301
In grad_steps = 2887, loss = 0.17079031467437744
In grad_steps = 2888, loss = 0.2566696107387543
In grad_steps = 2889, loss = 0.3960360884666443
In grad_steps = 2890, loss = 0.2813969850540161
In grad_steps = 2891, loss = 0.839357852935791
In grad_steps = 2892, loss = 0.11735861748456955
In grad_steps = 2893, loss = 0.06056976318359375
In grad_steps = 2894, loss = 0.10891149193048477
In grad_steps = 2895, loss = 0.05471516400575638
In grad_steps = 2896, loss = 0.787750780582428
In grad_steps = 2897, loss = 0.4493755102157593
In grad_steps = 2898, loss = 0.09169299155473709
In grad_steps = 2899, loss = 0.6916750073432922
In grad_steps = 2900, loss = 0.9814857244491577
In grad_steps = 2901, loss = 0.05041980370879173
In grad_steps = 2902, loss = 0.14990007877349854
In grad_steps = 2903, loss = 0.11665091663599014
In grad_steps = 2904, loss = 0.04186621308326721
In grad_steps = 2905, loss = 0.2096923440694809
In grad_steps = 2906, loss = 0.4568617343902588
In grad_steps = 2907, loss = 0.07786567509174347
In grad_steps = 2908, loss = 0.18590103089809418
In grad_steps = 2909, loss = 0.07503563910722733
In grad_steps = 2910, loss = 0.31463751196861267
In grad_steps = 2911, loss = 0.1247069388628006
In grad_steps = 2912, loss = 0.051587432622909546
In grad_steps = 2913, loss = 0.05733789503574371
In grad_steps = 2914, loss = 0.7429006099700928
In grad_steps = 2915, loss = 0.06024257093667984
In grad_steps = 2916, loss = 0.0802890807390213
In grad_steps = 2917, loss = 0.1335601508617401
In grad_steps = 2918, loss = 0.01294011902064085
In grad_steps = 2919, loss = 0.08985904604196548
In grad_steps = 2920, loss = 0.03183799237012863
In grad_steps = 2921, loss = 0.1691933423280716
In grad_steps = 2922, loss = 0.05392074212431908
In grad_steps = 2923, loss = 0.6121239066123962
In grad_steps = 2924, loss = 0.022484857589006424
In grad_steps = 2925, loss = 0.327726811170578
In grad_steps = 2926, loss = 0.10471150279045105
In grad_steps = 2927, loss = 0.22723807394504547
In grad_steps = 2928, loss = 0.05439544469118118
In grad_steps = 2929, loss = 0.0974588692188263
In grad_steps = 2930, loss = 0.02017815038561821
In grad_steps = 2931, loss = 1.5520379543304443
In grad_steps = 2932, loss = 2.1912262439727783
In grad_steps = 2933, loss = 0.02368534542620182
In grad_steps = 2934, loss = 0.6805561184883118
In grad_steps = 2935, loss = 0.054079074412584305
In grad_steps = 2936, loss = 0.6708033084869385
In grad_steps = 2937, loss = 0.1784791350364685
In grad_steps = 2938, loss = 0.31634676456451416
In grad_steps = 2939, loss = 0.20344305038452148
In grad_steps = 2940, loss = 0.8452058434486389
In grad_steps = 2941, loss = 0.08512738347053528
In grad_steps = 2942, loss = 0.07562081515789032
In grad_steps = 2943, loss = 0.12156225740909576
In grad_steps = 2944, loss = 0.7382282614707947
In grad_steps = 2945, loss = 0.5115055441856384
In grad_steps = 2946, loss = 0.33169084787368774
In grad_steps = 2947, loss = 0.07325758039951324
In grad_steps = 2948, loss = 0.14854760468006134
In grad_steps = 2949, loss = 0.3644045293331146
In grad_steps = 2950, loss = 0.24783647060394287
In grad_steps = 2951, loss = 0.29744604229927063
In grad_steps = 2952, loss = 0.5138056874275208
In grad_steps = 2953, loss = 0.8421046137809753
In grad_steps = 2954, loss = 0.28602075576782227
In grad_steps = 2955, loss = 0.48396623134613037
In grad_steps = 2956, loss = 1.0301668643951416
In grad_steps = 2957, loss = 0.5405880212783813
In grad_steps = 2958, loss = 0.09329389035701752
In grad_steps = 2959, loss = 0.5365087985992432
In grad_steps = 2960, loss = 0.1193084567785263
In grad_steps = 2961, loss = 0.062470678240060806
In grad_steps = 2962, loss = 0.4197007417678833
In grad_steps = 2963, loss = 0.1673646718263626
In grad_steps = 2964, loss = 0.5763890147209167
In grad_steps = 2965, loss = 0.050118397921323776
In grad_steps = 2966, loss = 0.18068765103816986
In grad_steps = 2967, loss = 0.25971829891204834
In grad_steps = 2968, loss = 0.5719888806343079
In grad_steps = 2969, loss = 0.07395970821380615
In grad_steps = 2970, loss = 0.10323881357908249
In grad_steps = 2971, loss = 0.11070112138986588
In grad_steps = 2972, loss = 0.060367338359355927
In grad_steps = 2973, loss = 0.06858903169631958
In grad_steps = 2974, loss = 0.9614460468292236
In grad_steps = 2975, loss = 0.3694365918636322
In grad_steps = 2976, loss = 0.11719011515378952
In grad_steps = 2977, loss = 0.060441091656684875
In grad_steps = 2978, loss = 0.06568551063537598
In grad_steps = 2979, loss = 0.5181777477264404
In grad_steps = 2980, loss = 0.07045052945613861
In grad_steps = 2981, loss = 0.23399406671524048
In grad_steps = 2982, loss = 0.020795311778783798
In grad_steps = 2983, loss = 0.2981238067150116
In grad_steps = 2984, loss = 0.047240372747182846
In grad_steps = 2985, loss = 0.03694808483123779
In grad_steps = 2986, loss = 0.30922001600265503
In grad_steps = 2987, loss = 0.05963502451777458
In grad_steps = 2988, loss = 0.3763572573661804
In grad_steps = 2989, loss = 0.11656337231397629
In grad_steps = 2990, loss = 0.03293517604470253
In grad_steps = 2991, loss = 0.022731341421604156
In grad_steps = 2992, loss = 0.6234196424484253
In grad_steps = 2993, loss = 0.06028218939900398
In grad_steps = 2994, loss = 0.22589774429798126
In grad_steps = 2995, loss = 0.012703693471848965
In grad_steps = 2996, loss = 1.45444655418396
In grad_steps = 2997, loss = 0.09523844718933105
In grad_steps = 2998, loss = 0.07795187085866928
In grad_steps = 2999, loss = 0.47920912504196167
In grad_steps = 3000, loss = 0.1360267996788025
In grad_steps = 3001, loss = 0.05131843686103821
In grad_steps = 3002, loss = 0.9968691468238831
In grad_steps = 3003, loss = 0.29806584119796753
In grad_steps = 3004, loss = 1.1679999828338623
In grad_steps = 3005, loss = 0.01549894455820322
In grad_steps = 3006, loss = 0.158852219581604
In grad_steps = 3007, loss = 0.2107831984758377
In grad_steps = 3008, loss = 0.3302627205848694
In grad_steps = 3009, loss = 0.15517377853393555
In grad_steps = 3010, loss = 0.4198712408542633
In grad_steps = 3011, loss = 0.5716361999511719
In grad_steps = 3012, loss = 1.056016445159912
In grad_steps = 3013, loss = 0.1047254428267479
In grad_steps = 3014, loss = 0.4961026608943939
In grad_steps = 3015, loss = 0.20229732990264893
In grad_steps = 3016, loss = 0.19700145721435547
In grad_steps = 3017, loss = 0.23008355498313904
In grad_steps = 3018, loss = 0.45295602083206177
In grad_steps = 3019, loss = 0.3308004140853882
In grad_steps = 3020, loss = 0.4734964072704315
In grad_steps = 3021, loss = 0.46974480152130127
In grad_steps = 3022, loss = 0.3950490355491638
In grad_steps = 3023, loss = 0.08614424616098404
In grad_steps = 3024, loss = 0.27439042925834656
In grad_steps = 3025, loss = 0.5997856855392456
In grad_steps = 3026, loss = 0.2946275770664215
In grad_steps = 3027, loss = 0.21623146533966064
In grad_steps = 3028, loss = 0.11624370515346527
In grad_steps = 3029, loss = 0.18070311844348907
In grad_steps = 3030, loss = 0.178791344165802
In grad_steps = 3031, loss = 0.6527010798454285
In grad_steps = 3032, loss = 0.7968524694442749
In grad_steps = 3033, loss = 0.807425320148468
In grad_steps = 3034, loss = 0.8420557379722595
In grad_steps = 3035, loss = 0.8223485350608826
In grad_steps = 3036, loss = 0.4871792793273926
In grad_steps = 3037, loss = 0.30353155732154846
In grad_steps = 3038, loss = 0.40868571400642395
In grad_steps = 3039, loss = 0.0802229642868042
In grad_steps = 3040, loss = 0.4762578010559082
In grad_steps = 3041, loss = 0.2547452747821808
In grad_steps = 3042, loss = 0.4301139712333679
In grad_steps = 3043, loss = 0.8713623285293579
In grad_steps = 3044, loss = 0.7952960133552551
In grad_steps = 3045, loss = 0.2706216871738434
In grad_steps = 3046, loss = 0.416079580783844
In grad_steps = 3047, loss = 0.4605659246444702
In grad_steps = 3048, loss = 0.1436270922422409
In grad_steps = 3049, loss = 0.46543172001838684
In grad_steps = 3050, loss = 0.14131765067577362
In grad_steps = 3051, loss = 0.4072387218475342
In grad_steps = 3052, loss = 0.024063661694526672
In grad_steps = 3053, loss = 0.10665974766016006
In grad_steps = 3054, loss = 0.55596923828125
In grad_steps = 3055, loss = 0.10438017547130585
In grad_steps = 3056, loss = 0.1790844351053238
In grad_steps = 3057, loss = 0.2401517629623413
In grad_steps = 3058, loss = 0.30172234773635864
In grad_steps = 3059, loss = 0.1779506802558899
In grad_steps = 3060, loss = 0.0309402197599411
In grad_steps = 3061, loss = 0.08826018124818802
In grad_steps = 3062, loss = 0.9944388270378113
In grad_steps = 3063, loss = 0.024374045431613922
In grad_steps = 3064, loss = 0.048597514629364014
In grad_steps = 3065, loss = 0.016325265169143677
In grad_steps = 3066, loss = 0.08131080865859985
In grad_steps = 3067, loss = 0.1769353449344635
In grad_steps = 3068, loss = 0.031199611723423004
In grad_steps = 3069, loss = 0.0016261691926047206
In grad_steps = 3070, loss = 0.007725982461124659
In grad_steps = 3071, loss = 0.09586908668279648
In grad_steps = 3072, loss = 1.176619052886963
In grad_steps = 3073, loss = 1.776678204536438
In grad_steps = 3074, loss = 0.10773647576570511
In grad_steps = 3075, loss = 0.08299800008535385
In grad_steps = 3076, loss = 0.028266198933124542
In grad_steps = 3077, loss = 0.15003478527069092
In grad_steps = 3078, loss = 1.2320001125335693
In grad_steps = 3079, loss = 0.09906680881977081
In grad_steps = 3080, loss = 0.44523268938064575
In grad_steps = 3081, loss = 0.15375536680221558
In grad_steps = 3082, loss = 0.09243346005678177
In grad_steps = 3083, loss = 0.3549349308013916
In grad_steps = 3084, loss = 0.3832900822162628
In grad_steps = 3085, loss = 0.6018079519271851
In grad_steps = 3086, loss = 0.09285002946853638
In grad_steps = 3087, loss = 0.11562079936265945
In grad_steps = 3088, loss = 0.5225818753242493
In grad_steps = 3089, loss = 0.6011749505996704
In grad_steps = 3090, loss = 0.18637309968471527
In grad_steps = 3091, loss = 0.3652462661266327
In grad_steps = 3092, loss = 0.22826912999153137
In grad_steps = 3093, loss = 0.2115856409072876
In grad_steps = 3094, loss = 0.2471083551645279
In grad_steps = 3095, loss = 0.21641051769256592
In grad_steps = 3096, loss = 0.6069768667221069
In grad_steps = 3097, loss = 0.21914765238761902
In grad_steps = 3098, loss = 0.5111949443817139
In grad_steps = 3099, loss = 0.03479352220892906
In grad_steps = 3100, loss = 0.04058658331632614
In grad_steps = 3101, loss = 0.06983701139688492
In grad_steps = 3102, loss = 0.16186968982219696
In grad_steps = 3103, loss = 0.8546584844589233
In grad_steps = 3104, loss = 0.04206673428416252
In grad_steps = 3105, loss = 0.205475315451622
In grad_steps = 3106, loss = 0.43447554111480713
In grad_steps = 3107, loss = 0.19930236041545868
In grad_steps = 3108, loss = 0.07280510663986206
In grad_steps = 3109, loss = 0.5843942761421204
In grad_steps = 3110, loss = 0.37797194719314575
In grad_steps = 3111, loss = 0.7566596865653992
In grad_steps = 3112, loss = 0.09729748964309692
In grad_steps = 3113, loss = 0.9278192520141602
In grad_steps = 3114, loss = 0.22735655307769775
In grad_steps = 3115, loss = 0.5198681950569153
In grad_steps = 3116, loss = 0.11093736439943314
In grad_steps = 3117, loss = 0.06178070232272148
In grad_steps = 3118, loss = 0.02850724756717682
In grad_steps = 3119, loss = 0.5261648893356323
In grad_steps = 3120, loss = 0.6702245473861694
In grad_steps = 3121, loss = 0.1204618290066719
In grad_steps = 3122, loss = 0.09357740730047226
In grad_steps = 3123, loss = 0.2749921381473541
In grad_steps = 3124, loss = 0.13664792478084564
In grad_steps = 3125, loss = 0.10319729149341583
In grad_steps = 3126, loss = 0.23935244977474213
In grad_steps = 3127, loss = 0.17222140729427338
In grad_steps = 3128, loss = 0.22240345180034637
In grad_steps = 3129, loss = 0.09059654176235199
In grad_steps = 3130, loss = 0.17353393137454987
In grad_steps = 3131, loss = 0.6016579270362854
In grad_steps = 3132, loss = 0.10792069882154465
In grad_steps = 3133, loss = 0.38272491097450256
In grad_steps = 3134, loss = 0.5063889622688293
In grad_steps = 3135, loss = 0.09822184592485428
In grad_steps = 3136, loss = 0.04051872342824936
In grad_steps = 3137, loss = 0.3960084915161133
In grad_steps = 3138, loss = 0.436224102973938
In grad_steps = 3139, loss = 0.18173839151859283
In grad_steps = 3140, loss = 0.42778676748275757
In grad_steps = 3141, loss = 0.3941737711429596
In grad_steps = 3142, loss = 0.12135238945484161
In grad_steps = 3143, loss = 0.12129524350166321
In grad_steps = 3144, loss = 0.24285843968391418
In grad_steps = 3145, loss = 0.07911577075719833
In grad_steps = 3146, loss = 0.14648747444152832
In grad_steps = 3147, loss = 0.030106278136372566
In grad_steps = 3148, loss = 0.12875300645828247
In grad_steps = 3149, loss = 0.013447670266032219
In grad_steps = 3150, loss = 0.01480149757117033
In grad_steps = 3151, loss = 0.2763517498970032
In grad_steps = 3152, loss = 0.12435118108987808
In grad_steps = 3153, loss = 0.22955098748207092
In grad_steps = 3154, loss = 0.2909949719905853
In grad_steps = 3155, loss = 0.7759986519813538
In grad_steps = 3156, loss = 0.006964887026697397
In grad_steps = 3157, loss = 0.6016583442687988
In grad_steps = 3158, loss = 0.13073042035102844
In grad_steps = 3159, loss = 0.05519428849220276
In grad_steps = 3160, loss = 0.03412371128797531
In grad_steps = 3161, loss = 0.4489818513393402
In grad_steps = 3162, loss = 0.03650110960006714
In grad_steps = 3163, loss = 0.617006778717041
In grad_steps = 3164, loss = 0.2678341865539551
In grad_steps = 3165, loss = 0.30250534415245056
In grad_steps = 3166, loss = 0.0763065293431282
In grad_steps = 3167, loss = 0.11312493681907654
In grad_steps = 3168, loss = 0.0763944461941719
In grad_steps = 3169, loss = 0.4194255471229553
In grad_steps = 3170, loss = 0.01716928742825985
In grad_steps = 3171, loss = 0.3360190689563751
In grad_steps = 3172, loss = 0.8066086173057556
In grad_steps = 3173, loss = 0.009311794303357601
In grad_steps = 3174, loss = 0.06658559292554855
In grad_steps = 3175, loss = 0.0701604038476944
In grad_steps = 3176, loss = 0.6306124925613403
In grad_steps = 3177, loss = 0.03159263730049133
In grad_steps = 3178, loss = 0.07362677156925201
In grad_steps = 3179, loss = 0.12391065806150436
In grad_steps = 3180, loss = 0.16727645695209503
In grad_steps = 3181, loss = 0.34919676184654236
In grad_steps = 3182, loss = 0.09045092016458511
In grad_steps = 3183, loss = 0.021429279819130898
In grad_steps = 3184, loss = 0.0346447229385376
In grad_steps = 3185, loss = 0.2145639955997467
In grad_steps = 3186, loss = 0.48827046155929565
In grad_steps = 3187, loss = 0.14093971252441406
In grad_steps = 3188, loss = 0.07365863025188446
In grad_steps = 3189, loss = 0.7286957502365112
In grad_steps = 3190, loss = 0.004661065526306629
In grad_steps = 3191, loss = 0.06310457736253738
In grad_steps = 3192, loss = 0.15633732080459595
In grad_steps = 3193, loss = 0.061328403651714325
In grad_steps = 3194, loss = 1.2129147052764893
In grad_steps = 3195, loss = 0.08728192001581192
In grad_steps = 3196, loss = 0.23051804304122925
In grad_steps = 3197, loss = 0.013316559605300426
In grad_steps = 3198, loss = 0.1873057633638382
In grad_steps = 3199, loss = 1.5171600580215454
In grad_steps = 3200, loss = 0.07890092581510544
In grad_steps = 3201, loss = 0.18325912952423096
In grad_steps = 3202, loss = 0.0774979293346405
In grad_steps = 3203, loss = 0.3312686085700989
In grad_steps = 3204, loss = 0.16434673964977264
In grad_steps = 3205, loss = 0.24922135472297668
In grad_steps = 3206, loss = 0.9400089979171753
In grad_steps = 3207, loss = 0.14251770079135895
In grad_steps = 3208, loss = 0.22724226117134094
In grad_steps = 3209, loss = 0.23128752410411835
In grad_steps = 3210, loss = 0.1433180868625641
In grad_steps = 3211, loss = 0.09348352998495102
In grad_steps = 3212, loss = 0.12093666195869446
In grad_steps = 3213, loss = 0.11896952241659164
In grad_steps = 3214, loss = 0.47918859124183655
In grad_steps = 3215, loss = 0.17285069823265076
In grad_steps = 3216, loss = 0.27082937955856323
In grad_steps = 3217, loss = 0.09731324017047882
In grad_steps = 3218, loss = 0.4087909162044525
In grad_steps = 3219, loss = 1.7338483333587646
In grad_steps = 3220, loss = 0.714532732963562
In grad_steps = 3221, loss = 0.6277428865432739
In grad_steps = 3222, loss = 0.01685168407857418
In grad_steps = 3223, loss = 0.07188189774751663
In grad_steps = 3224, loss = 0.1049855574965477
In grad_steps = 3225, loss = 0.4282812774181366
In grad_steps = 3226, loss = 0.29593929648399353
In grad_steps = 3227, loss = 0.11126116663217545
In grad_steps = 3228, loss = 0.402340292930603
In grad_steps = 3229, loss = 0.12418203055858612
In grad_steps = 3230, loss = 0.06599869579076767
In grad_steps = 3231, loss = 0.38918638229370117
In grad_steps = 3232, loss = 0.09954475611448288
In grad_steps = 3233, loss = 0.08379524946212769
In grad_steps = 3234, loss = 0.2563720941543579
In grad_steps = 3235, loss = 1.4279462099075317
In grad_steps = 3236, loss = 0.8170550465583801
In grad_steps = 3237, loss = 0.9079147577285767
In grad_steps = 3238, loss = 0.19724224507808685
In grad_steps = 3239, loss = 0.12208246439695358
In grad_steps = 3240, loss = 0.16524285078048706
In grad_steps = 3241, loss = 0.3411714434623718
In grad_steps = 3242, loss = 0.10356228798627853
In grad_steps = 3243, loss = 0.10085132718086243
In grad_steps = 3244, loss = 0.07139631360769272
In grad_steps = 3245, loss = 0.10889612138271332
In grad_steps = 3246, loss = 0.12609846889972687
In grad_steps = 3247, loss = 0.22726593911647797
In grad_steps = 3248, loss = 0.5249391794204712
In grad_steps = 3249, loss = 0.09461232274770737
In grad_steps = 3250, loss = 0.7527492642402649
In grad_steps = 3251, loss = 0.18210333585739136
In grad_steps = 3252, loss = 0.4761306047439575
In grad_steps = 3253, loss = 0.0660373643040657
In grad_steps = 3254, loss = 0.07999320328235626
In grad_steps = 3255, loss = 0.3574419915676117
In grad_steps = 3256, loss = 0.025888293981552124
In grad_steps = 3257, loss = 0.06361617147922516
In grad_steps = 3258, loss = 1.15767240524292
In grad_steps = 3259, loss = 0.9386324882507324
In grad_steps = 3260, loss = 0.0699864849448204
In grad_steps = 3261, loss = 0.14885270595550537
In grad_steps = 3262, loss = 0.28474971652030945
In grad_steps = 3263, loss = 0.2893243730068207
In grad_steps = 3264, loss = 0.5505688786506653
In grad_steps = 3265, loss = 0.02533731237053871
In grad_steps = 3266, loss = 0.7844833135604858
In grad_steps = 3267, loss = 0.2744983434677124
In grad_steps = 3268, loss = 0.15496356785297394
In grad_steps = 3269, loss = 0.4449693560600281
In grad_steps = 3270, loss = 0.028432102873921394
In grad_steps = 3271, loss = 0.06602682173252106
In grad_steps = 3272, loss = 0.09073849022388458
In grad_steps = 3273, loss = 0.2087632715702057
In grad_steps = 3274, loss = 0.3583221435546875
In grad_steps = 3275, loss = 0.1491067111492157
In grad_steps = 3276, loss = 0.15565815567970276
In grad_steps = 3277, loss = 1.3916431665420532
In grad_steps = 3278, loss = 0.030700869858264923
In grad_steps = 3279, loss = 0.08835132420063019
In grad_steps = 3280, loss = 0.06075174733996391
In grad_steps = 3281, loss = 0.4812321364879608
In grad_steps = 3282, loss = 0.20328529179096222
In grad_steps = 3283, loss = 0.12437039613723755
In grad_steps = 3284, loss = 0.052307214587926865
In grad_steps = 3285, loss = 0.013319100253283978
In grad_steps = 3286, loss = 0.3829793632030487
In grad_steps = 3287, loss = 0.10719495266675949
In grad_steps = 3288, loss = 0.11052833497524261
In grad_steps = 3289, loss = 0.22140580415725708
In grad_steps = 3290, loss = 0.47340694069862366
In grad_steps = 3291, loss = 0.05017857998609543
In grad_steps = 3292, loss = 0.31584203243255615
In grad_steps = 3293, loss = 0.02072967402637005
In grad_steps = 3294, loss = 0.0468241386115551
In grad_steps = 3295, loss = 0.009706653654575348
In grad_steps = 3296, loss = 0.10405068099498749
In grad_steps = 3297, loss = 0.5250089764595032
In grad_steps = 3298, loss = 0.7757994532585144
In grad_steps = 3299, loss = 0.059098243713378906
In grad_steps = 3300, loss = 0.15135273337364197
In grad_steps = 3301, loss = 0.6728168725967407
In grad_steps = 3302, loss = 0.2852475643157959
In grad_steps = 3303, loss = 0.0671389251947403
In grad_steps = 3304, loss = 0.2512901723384857
In grad_steps = 3305, loss = 0.504899263381958
In grad_steps = 3306, loss = 0.10863178223371506
In grad_steps = 3307, loss = 0.04580969735980034
In grad_steps = 3308, loss = 0.351905882358551
In grad_steps = 3309, loss = 0.2006601244211197
In grad_steps = 3310, loss = 0.11126402765512466
In grad_steps = 3311, loss = 0.5264464020729065
In grad_steps = 3312, loss = 0.03773901239037514
In grad_steps = 3313, loss = 0.16246254742145538
In grad_steps = 3314, loss = 1.1393715143203735
In grad_steps = 3315, loss = 0.9219534993171692
In grad_steps = 3316, loss = 0.2311910092830658
In grad_steps = 3317, loss = 0.02535407990217209
In grad_steps = 3318, loss = 0.13658829033374786
In grad_steps = 3319, loss = 0.6018108129501343
In grad_steps = 3320, loss = 0.5243182182312012
In grad_steps = 3321, loss = 0.5741478204727173
In grad_steps = 3322, loss = 0.0604926161468029
In grad_steps = 3323, loss = 0.06536378711462021
In grad_steps = 3324, loss = 0.25292539596557617
In grad_steps = 3325, loss = 0.39028459787368774
In grad_steps = 3326, loss = 0.0840861052274704
In grad_steps = 3327, loss = 0.18061195313930511
In grad_steps = 3328, loss = 0.1882646381855011
In grad_steps = 3329, loss = 0.41900405287742615
In grad_steps = 3330, loss = 0.1933993250131607
In grad_steps = 3331, loss = 0.16171479225158691
In grad_steps = 3332, loss = 0.1783943623304367
In grad_steps = 3333, loss = 0.08751675486564636
In grad_steps = 3334, loss = 0.3406123220920563
In grad_steps = 3335, loss = 0.1474805325269699
In grad_steps = 3336, loss = 0.6825322508811951
In grad_steps = 3337, loss = 0.44085144996643066
In grad_steps = 3338, loss = 0.17063747346401215
In grad_steps = 3339, loss = 0.08660057932138443
In grad_steps = 3340, loss = 0.04264840483665466
In grad_steps = 3341, loss = 0.20580926537513733
In grad_steps = 3342, loss = 0.0625830739736557
In grad_steps = 3343, loss = 0.02805701270699501
In grad_steps = 3344, loss = 0.04754764586687088
In grad_steps = 3345, loss = 0.30687928199768066
In grad_steps = 3346, loss = 0.23489606380462646
In grad_steps = 3347, loss = 0.1819210648536682
In grad_steps = 3348, loss = 0.11654448509216309
In grad_steps = 3349, loss = 0.13175417482852936
In grad_steps = 3350, loss = 0.03621983528137207
In grad_steps = 3351, loss = 0.07131513208150864
In grad_steps = 3352, loss = 0.5065315961837769
In grad_steps = 3353, loss = 0.04689439758658409
In grad_steps = 3354, loss = 0.023855075240135193
In grad_steps = 3355, loss = 0.021729296073317528
In grad_steps = 3356, loss = 0.034190550446510315
In grad_steps = 3357, loss = 0.20265249907970428
In grad_steps = 3358, loss = 0.2005195915699005
In grad_steps = 3359, loss = 0.6979730129241943
In grad_steps = 3360, loss = 0.01627494767308235
In grad_steps = 3361, loss = 0.9100845456123352
In grad_steps = 3362, loss = 0.05891236662864685
In grad_steps = 3363, loss = 0.05834499001502991
In grad_steps = 3364, loss = 0.048115041106939316
In grad_steps = 3365, loss = 0.03663976490497589
In grad_steps = 3366, loss = 0.02240273728966713
In grad_steps = 3367, loss = 0.025738555938005447
In grad_steps = 3368, loss = 0.08569798618555069
In grad_steps = 3369, loss = 1.3568572998046875
In grad_steps = 3370, loss = 0.7204965353012085
In grad_steps = 3371, loss = 0.07763662934303284
In grad_steps = 3372, loss = 0.04955509305000305
In grad_steps = 3373, loss = 0.2174779623746872
In grad_steps = 3374, loss = 0.017882809042930603
In grad_steps = 3375, loss = 0.7781704664230347
In grad_steps = 3376, loss = 0.0603337362408638
In grad_steps = 3377, loss = 0.049589987844228745
In grad_steps = 3378, loss = 0.07450539618730545
In grad_steps = 3379, loss = 0.05869383364915848
In grad_steps = 3380, loss = 0.9059734344482422
In grad_steps = 3381, loss = 0.026374494656920433
In grad_steps = 3382, loss = 0.7497222423553467
In grad_steps = 3383, loss = 0.7492271065711975
In grad_steps = 3384, loss = 0.1366802453994751
In grad_steps = 3385, loss = 0.04227142035961151
In grad_steps = 3386, loss = 0.6311548948287964
In grad_steps = 3387, loss = 0.08044327795505524
In grad_steps = 3388, loss = 0.0629328265786171
In grad_steps = 3389, loss = 0.37325501441955566
In grad_steps = 3390, loss = 0.07507047057151794
In grad_steps = 3391, loss = 0.10355666279792786
In grad_steps = 3392, loss = 0.17574039101600647
In grad_steps = 3393, loss = 0.5309205055236816
In grad_steps = 3394, loss = 0.12674568593502045
In grad_steps = 3395, loss = 0.20970575511455536
In grad_steps = 3396, loss = 0.39960789680480957
In grad_steps = 3397, loss = 0.08755060285329819
In grad_steps = 3398, loss = 0.8984652161598206
In grad_steps = 3399, loss = 0.07524167001247406
In grad_steps = 3400, loss = 0.10422451794147491
In grad_steps = 3401, loss = 0.31496933102607727
In grad_steps = 3402, loss = 0.005883358884602785
In grad_steps = 3403, loss = 0.3328884243965149
In grad_steps = 3404, loss = 0.2728191614151001
In grad_steps = 3405, loss = 0.2572565972805023
In grad_steps = 3406, loss = 0.1979963481426239
In grad_steps = 3407, loss = 0.07994285225868225
In grad_steps = 3408, loss = 0.3561348617076874
In grad_steps = 3409, loss = 0.16932573914527893
In grad_steps = 3410, loss = 0.13869760930538177
In grad_steps = 3411, loss = 0.07079239934682846
In grad_steps = 3412, loss = 0.25472861528396606
In grad_steps = 3413, loss = 0.03615374118089676
In grad_steps = 3414, loss = 0.7866154313087463
In grad_steps = 3415, loss = 0.07513006776571274
In grad_steps = 3416, loss = 0.2817891538143158
In grad_steps = 3417, loss = 0.19554662704467773
In grad_steps = 3418, loss = 0.07226012647151947
In grad_steps = 3419, loss = 0.04301963746547699
In grad_steps = 3420, loss = 0.022532515227794647
In grad_steps = 3421, loss = 0.6752029657363892
In grad_steps = 3422, loss = 0.5701968669891357
In grad_steps = 3423, loss = 0.1616678386926651
In grad_steps = 3424, loss = 0.5598005652427673
In grad_steps = 3425, loss = 0.7561140656471252
In grad_steps = 3426, loss = 0.040596045553684235
In grad_steps = 3427, loss = 0.9569705128669739
In grad_steps = 3428, loss = 0.019511451944708824
In grad_steps = 3429, loss = 0.24068787693977356
In grad_steps = 3430, loss = 0.061928171664476395
In grad_steps = 3431, loss = 0.6016643047332764
In grad_steps = 3432, loss = 0.20094159245491028
In grad_steps = 3433, loss = 0.1569136381149292
In grad_steps = 3434, loss = 0.09615002572536469
In grad_steps = 3435, loss = 0.17130354046821594
In grad_steps = 3436, loss = 0.25821202993392944
In grad_steps = 3437, loss = 0.0794469565153122
In grad_steps = 3438, loss = 0.21820230782032013
In grad_steps = 3439, loss = 0.21578243374824524
In grad_steps = 3440, loss = 0.2602233290672302
In grad_steps = 3441, loss = 0.04190075024962425
In grad_steps = 3442, loss = 0.07571856677532196
In grad_steps = 3443, loss = 0.004886786453425884
In grad_steps = 3444, loss = 0.7067774534225464
In grad_steps = 3445, loss = 0.13563969731330872
In grad_steps = 3446, loss = 0.09291855990886688
In grad_steps = 3447, loss = 0.06465845555067062
In grad_steps = 3448, loss = 0.08992413431406021
In grad_steps = 3449, loss = 0.47895169258117676
In grad_steps = 3450, loss = 0.009158316999673843
In grad_steps = 3451, loss = 0.1213541030883789
In grad_steps = 3452, loss = 0.027242261916399002
In grad_steps = 3453, loss = 0.8123968839645386
In grad_steps = 3454, loss = 0.035490866750478745
In grad_steps = 3455, loss = 0.09196871519088745
In grad_steps = 3456, loss = 0.6489956378936768
In grad_steps = 3457, loss = 0.11481614410877228
In grad_steps = 3458, loss = 0.011775288730859756
In grad_steps = 3459, loss = 0.039692021906375885
In grad_steps = 3460, loss = 0.10936573147773743
In grad_steps = 3461, loss = 0.6970292329788208
In grad_steps = 3462, loss = 0.07272227853536606
In grad_steps = 3463, loss = 0.14466193318367004
In grad_steps = 3464, loss = 0.21411378681659698
In grad_steps = 3465, loss = 0.0726446807384491
In grad_steps = 3466, loss = 1.395511507987976
In grad_steps = 3467, loss = 0.10238677263259888
In grad_steps = 3468, loss = 0.08773157745599747
In grad_steps = 3469, loss = 0.1529415249824524
In grad_steps = 3470, loss = 0.48980575799942017
In grad_steps = 3471, loss = 0.036836862564086914
In grad_steps = 3472, loss = 0.05885305255651474
In grad_steps = 3473, loss = 0.09439504146575928
In grad_steps = 3474, loss = 0.21981818974018097
In grad_steps = 3475, loss = 0.04973629489541054
In grad_steps = 3476, loss = 0.25181901454925537
In grad_steps = 3477, loss = 0.026670880615711212
In grad_steps = 3478, loss = 0.920356273651123
In grad_steps = 3479, loss = 0.042940281331539154
In grad_steps = 3480, loss = 0.07046768814325333
In grad_steps = 3481, loss = 0.3741695284843445
In grad_steps = 3482, loss = 0.03593836724758148
In grad_steps = 3483, loss = 0.04288632795214653
In grad_steps = 3484, loss = 0.9581080675125122
In grad_steps = 3485, loss = 0.020259540528059006
In grad_steps = 3486, loss = 0.024997416883707047
In grad_steps = 3487, loss = 0.6063488721847534
In grad_steps = 3488, loss = 0.182373508810997
In grad_steps = 3489, loss = 0.03766998276114464
In grad_steps = 3490, loss = 1.012449026107788
In grad_steps = 3491, loss = 0.11127561330795288
In grad_steps = 3492, loss = 0.027868548408150673
In grad_steps = 3493, loss = 0.16750669479370117
In grad_steps = 3494, loss = 1.18757164478302
In grad_steps = 3495, loss = 0.07183930277824402
In grad_steps = 3496, loss = 1.155712366104126
In grad_steps = 3497, loss = 0.0628976970911026
In grad_steps = 3498, loss = 0.37066972255706787
In grad_steps = 3499, loss = 0.14883258938789368
In grad_steps = 3500, loss = 0.018056759610772133
In grad_steps = 3501, loss = 0.7239896059036255
In grad_steps = 3502, loss = 0.033611029386520386
In grad_steps = 3503, loss = 0.037976816296577454
In grad_steps = 3504, loss = 0.21288929879665375
In grad_steps = 3505, loss = 0.24645519256591797
In grad_steps = 3506, loss = 0.7783845067024231
In grad_steps = 3507, loss = 0.06778557598590851
In grad_steps = 3508, loss = 0.3775537312030792
In grad_steps = 3509, loss = 0.24863526225090027
In grad_steps = 3510, loss = 0.7632223963737488
In grad_steps = 3511, loss = 0.2524995505809784
In grad_steps = 3512, loss = 0.05553717911243439
In grad_steps = 3513, loss = 0.08862985670566559
In grad_steps = 3514, loss = 0.41412773728370667
In grad_steps = 3515, loss = 0.12295956164598465
In grad_steps = 3516, loss = 0.9150384664535522
In grad_steps = 3517, loss = 0.18581931293010712
In grad_steps = 3518, loss = 0.3778702914714813
In grad_steps = 3519, loss = 0.14207445085048676
In grad_steps = 3520, loss = 0.5510130524635315
In grad_steps = 3521, loss = 0.06466139853000641
In grad_steps = 3522, loss = 0.1104012057185173
In grad_steps = 3523, loss = 0.07299541682004929
In grad_steps = 3524, loss = 0.11442022025585175
In grad_steps = 3525, loss = 0.3442586064338684
In grad_steps = 3526, loss = 0.12026000022888184
In grad_steps = 3527, loss = 1.1367179155349731
In grad_steps = 3528, loss = 0.14456717669963837
In grad_steps = 3529, loss = 0.11722530424594879
In grad_steps = 3530, loss = 0.0948794037103653
In grad_steps = 3531, loss = 0.12290235608816147
In grad_steps = 3532, loss = 0.23557786643505096
In grad_steps = 3533, loss = 0.0945342481136322
In grad_steps = 3534, loss = 0.05078926682472229
In grad_steps = 3535, loss = 0.05901133269071579
In grad_steps = 3536, loss = 0.1930437982082367
In grad_steps = 3537, loss = 0.13057659566402435
In grad_steps = 3538, loss = 0.6293433904647827
In grad_steps = 3539, loss = 0.7267856597900391
In grad_steps = 3540, loss = 0.051210395991802216
In grad_steps = 3541, loss = 0.4204963445663452
In grad_steps = 3542, loss = 0.476373553276062
In grad_steps = 3543, loss = 0.8816802501678467
In grad_steps = 3544, loss = 0.6114263534545898
In grad_steps = 3545, loss = 0.040856726467609406
In grad_steps = 3546, loss = 0.07208976149559021
In grad_steps = 3547, loss = 0.1989537477493286
In grad_steps = 3548, loss = 0.08990295231342316
In grad_steps = 3549, loss = 0.042927876114845276
In grad_steps = 3550, loss = 0.04069323092699051
In grad_steps = 3551, loss = 0.8771737813949585
In grad_steps = 3552, loss = 0.6485169529914856
In grad_steps = 3553, loss = 0.1377006471157074
In grad_steps = 3554, loss = 0.5610912442207336
In grad_steps = 3555, loss = 0.42125001549720764
In grad_steps = 3556, loss = 0.38976067304611206
In grad_steps = 3557, loss = 0.32886573672294617
In grad_steps = 3558, loss = 0.5195440649986267
In grad_steps = 3559, loss = 0.18708345293998718
In grad_steps = 3560, loss = 0.42099496722221375
In grad_steps = 3561, loss = 0.1763722151517868
In grad_steps = 3562, loss = 0.9921830892562866
In grad_steps = 3563, loss = 0.7501970529556274
In grad_steps = 3564, loss = 0.22790729999542236
In grad_steps = 3565, loss = 1.656269907951355
In grad_steps = 3566, loss = 0.2550809681415558
In grad_steps = 3567, loss = 0.22801287472248077
In grad_steps = 3568, loss = 0.17487287521362305
In grad_steps = 3569, loss = 0.20288459956645966
In grad_steps = 3570, loss = 0.1843133121728897
In grad_steps = 3571, loss = 0.09738405048847198
In grad_steps = 3572, loss = 0.19992375373840332
In grad_steps = 3573, loss = 0.13315242528915405
In grad_steps = 3574, loss = 0.9541724920272827
In grad_steps = 3575, loss = 0.041197970509529114
In grad_steps = 3576, loss = 0.5420952439308167
In grad_steps = 3577, loss = 0.16370625793933868
In grad_steps = 3578, loss = 0.49992385506629944
In grad_steps = 3579, loss = 0.28475111722946167
In grad_steps = 3580, loss = 0.2652719020843506
In grad_steps = 3581, loss = 0.8188683986663818
In grad_steps = 3582, loss = 0.059214673936367035
In grad_steps = 3583, loss = 0.6229759454727173
In grad_steps = 3584, loss = 0.656910240650177
In grad_steps = 3585, loss = 0.07575064897537231
In grad_steps = 3586, loss = 0.08715039491653442
In grad_steps = 3587, loss = 0.07109656929969788
In grad_steps = 3588, loss = 0.2722622752189636
In grad_steps = 3589, loss = 0.09890513867139816
In grad_steps = 3590, loss = 0.32602906227111816
In grad_steps = 3591, loss = 0.1234164834022522
In grad_steps = 3592, loss = 0.032563649117946625
In grad_steps = 3593, loss = 0.07632343471050262
In grad_steps = 3594, loss = 0.33979105949401855
In grad_steps = 3595, loss = 0.2592456638813019
In grad_steps = 3596, loss = 0.21225157380104065
In grad_steps = 3597, loss = 0.08961720019578934
In grad_steps = 3598, loss = 0.035290494561195374
In grad_steps = 3599, loss = 0.09717053174972534
In grad_steps = 3600, loss = 0.6059708595275879
In grad_steps = 3601, loss = 0.05561848357319832
In grad_steps = 3602, loss = 0.05353190004825592
In grad_steps = 3603, loss = 0.05656501650810242
In grad_steps = 3604, loss = 0.24866463243961334
In grad_steps = 3605, loss = 0.12546956539154053
In grad_steps = 3606, loss = 0.05930231139063835
In grad_steps = 3607, loss = 0.44246718287467957
In grad_steps = 3608, loss = 0.1263851821422577
In grad_steps = 3609, loss = 0.1265774816274643
In grad_steps = 3610, loss = 0.13167673349380493
In grad_steps = 3611, loss = 0.45443665981292725
In grad_steps = 3612, loss = 0.3399111032485962
In grad_steps = 3613, loss = 0.023042507469654083
In grad_steps = 3614, loss = 0.14127738773822784
In grad_steps = 3615, loss = 0.017020918428897858
In grad_steps = 3616, loss = 0.16072773933410645
In grad_steps = 3617, loss = 1.2046105861663818
In grad_steps = 3618, loss = 0.9330143928527832
In grad_steps = 3619, loss = 0.007329144515097141
In grad_steps = 3620, loss = 0.16956545412540436
In grad_steps = 3621, loss = 0.16475863754749298
In grad_steps = 3622, loss = 0.00813747476786375
In grad_steps = 3623, loss = 0.4035854935646057
In grad_steps = 3624, loss = 1.3814337253570557
In grad_steps = 3625, loss = 0.07225409895181656
In grad_steps = 3626, loss = 0.1264871507883072
In grad_steps = 3627, loss = 0.7721533179283142
In grad_steps = 3628, loss = 0.5601184964179993
In grad_steps = 3629, loss = 0.2435014843940735
In grad_steps = 3630, loss = 0.45532870292663574
In grad_steps = 3631, loss = 0.019553670659661293
In grad_steps = 3632, loss = 0.8018022179603577
In grad_steps = 3633, loss = 0.0507611520588398
In grad_steps = 3634, loss = 0.19395588338375092
In grad_steps = 3635, loss = 0.08134365826845169
In grad_steps = 3636, loss = 0.1442616581916809
In grad_steps = 3637, loss = 0.0906890407204628
In grad_steps = 3638, loss = 0.2732529640197754
In grad_steps = 3639, loss = 0.10247595608234406
In grad_steps = 3640, loss = 0.17590132355690002
In grad_steps = 3641, loss = 0.8628800511360168
In grad_steps = 3642, loss = 0.3617345094680786
In grad_steps = 3643, loss = 0.06477876007556915
In grad_steps = 3644, loss = 0.5130726099014282
In grad_steps = 3645, loss = 0.06680487841367722
In grad_steps = 3646, loss = 0.043314337730407715
In grad_steps = 3647, loss = 0.1278030127286911
In grad_steps = 3648, loss = 0.5201412439346313
In grad_steps = 3649, loss = 0.08263503015041351
In grad_steps = 3650, loss = 0.10248792916536331
In grad_steps = 3651, loss = 0.3029763698577881
In grad_steps = 3652, loss = 0.7070527076721191
In grad_steps = 3653, loss = 0.4644891023635864
In grad_steps = 3654, loss = 0.11934292316436768
In grad_steps = 3655, loss = 0.1372322142124176
In grad_steps = 3656, loss = 0.763600766658783
In grad_steps = 3657, loss = 0.09244987368583679
In grad_steps = 3658, loss = 0.09737207740545273
In grad_steps = 3659, loss = 0.5350359082221985
In grad_steps = 3660, loss = 0.06739027053117752
In grad_steps = 3661, loss = 0.4192908704280853
In grad_steps = 3662, loss = 0.26111266016960144
In grad_steps = 3663, loss = 0.5538284182548523
In grad_steps = 3664, loss = 0.3861287534236908
In grad_steps = 3665, loss = 0.10428442060947418
In grad_steps = 3666, loss = 0.30532190203666687
In grad_steps = 3667, loss = 0.13715261220932007
In grad_steps = 3668, loss = 0.08125779032707214
In grad_steps = 3669, loss = 0.06730878353118896
In grad_steps = 3670, loss = 0.16237641870975494
In grad_steps = 3671, loss = 0.4075019359588623
In grad_steps = 3672, loss = 0.03500615432858467
In grad_steps = 3673, loss = 0.3654710054397583
In grad_steps = 3674, loss = 0.13685059547424316
In grad_steps = 3675, loss = 0.8402445316314697
In grad_steps = 3676, loss = 0.33200016617774963
In grad_steps = 3677, loss = 0.031122051179409027
In grad_steps = 3678, loss = 0.3696160614490509
In grad_steps = 3679, loss = 0.03995516523718834
In grad_steps = 3680, loss = 0.03720472753047943
In grad_steps = 3681, loss = 0.09875200688838959
In grad_steps = 3682, loss = 0.03400679677724838
In grad_steps = 3683, loss = 0.28200894594192505
In grad_steps = 3684, loss = 0.05160408467054367
In grad_steps = 3685, loss = 0.41664525866508484
In grad_steps = 3686, loss = 0.3699306845664978
In grad_steps = 3687, loss = 0.07059379667043686
In grad_steps = 3688, loss = 0.05552724003791809
In grad_steps = 3689, loss = 0.09997764974832535
In grad_steps = 3690, loss = 0.22993525862693787
In grad_steps = 3691, loss = 0.012320597656071186
In grad_steps = 3692, loss = 0.22261106967926025
In grad_steps = 3693, loss = 0.019520675763487816
In grad_steps = 3694, loss = 0.015313254669308662
In grad_steps = 3695, loss = 0.009604593738913536
In grad_steps = 3696, loss = 0.8053358197212219
In grad_steps = 3697, loss = 0.06393774598836899
In grad_steps = 3698, loss = 0.3586329519748688
In grad_steps = 3699, loss = 0.09011167287826538
In grad_steps = 3700, loss = 0.28727471828460693
In grad_steps = 3701, loss = 0.01828281208872795
In grad_steps = 3702, loss = 1.281764030456543
In grad_steps = 3703, loss = 0.11654658615589142
In grad_steps = 3704, loss = 0.8319056034088135
In grad_steps = 3705, loss = 0.06678485125303268
In grad_steps = 3706, loss = 0.317234069108963
In grad_steps = 3707, loss = 0.40187332034111023
In grad_steps = 3708, loss = 0.7120084166526794
In grad_steps = 3709, loss = 0.07223580032587051
In grad_steps = 3710, loss = 0.5494457483291626
In grad_steps = 3711, loss = 0.15558040142059326
In grad_steps = 3712, loss = 0.0435551181435585
In grad_steps = 3713, loss = 0.4640718698501587
In grad_steps = 3714, loss = 0.4826905131340027
In grad_steps = 3715, loss = 0.21055924892425537
In grad_steps = 3716, loss = 0.5065581798553467
In grad_steps = 3717, loss = 0.06777434796094894
In grad_steps = 3718, loss = 0.20777446031570435
In grad_steps = 3719, loss = 0.25407832860946655
In grad_steps = 3720, loss = 0.06078214943408966
In grad_steps = 3721, loss = 0.17115715146064758
In grad_steps = 3722, loss = 0.4079601466655731
In grad_steps = 3723, loss = 0.062098197638988495
In grad_steps = 3724, loss = 0.014184216037392616
In grad_steps = 3725, loss = 0.0880466103553772
In grad_steps = 3726, loss = 0.040428951382637024
In grad_steps = 3727, loss = 0.04188692569732666
In grad_steps = 3728, loss = 0.10371805727481842
In grad_steps = 3729, loss = 0.06060432642698288
In grad_steps = 3730, loss = 0.07731952518224716
In grad_steps = 3731, loss = 0.17081883549690247
In grad_steps = 3732, loss = 1.406063199043274
In grad_steps = 3733, loss = 0.18873198330402374
In grad_steps = 3734, loss = 0.057769306004047394
In grad_steps = 3735, loss = 0.1390731781721115
In grad_steps = 3736, loss = 0.08737324178218842
In grad_steps = 3737, loss = 0.9517384767532349
In grad_steps = 3738, loss = 0.041459523141384125
In grad_steps = 3739, loss = 0.02790578082203865
In grad_steps = 3740, loss = 0.14186488091945648
In grad_steps = 3741, loss = 0.06881901621818542
In grad_steps = 3742, loss = 0.1107964813709259
In grad_steps = 3743, loss = 0.04575726389884949
In grad_steps = 3744, loss = 0.25405630469322205
In grad_steps = 3745, loss = 0.062012121081352234
In grad_steps = 3746, loss = 0.5122062563896179
In grad_steps = 3747, loss = 0.08113276958465576
In grad_steps = 3748, loss = 0.007429067976772785
In grad_steps = 3749, loss = 0.04044236242771149
In grad_steps = 3750, loss = 0.05018091946840286
In grad_steps = 3751, loss = 0.4343452751636505
In grad_steps = 3752, loss = 0.11937990039587021
In grad_steps = 3753, loss = 0.04242919385433197
In grad_steps = 3754, loss = 1.024796962738037
In grad_steps = 3755, loss = 0.04602360352873802
In grad_steps = 3756, loss = 0.29758769273757935
In grad_steps = 3757, loss = 0.009267954155802727
In grad_steps = 3758, loss = 0.044032901525497437
In grad_steps = 3759, loss = 0.4827055037021637
In grad_steps = 3760, loss = 0.5631169080734253
In grad_steps = 3761, loss = 0.012028925120830536
In grad_steps = 3762, loss = 0.0675714910030365
In grad_steps = 3763, loss = 1.1832473278045654
In grad_steps = 3764, loss = 0.1390591561794281
In grad_steps = 3765, loss = 0.051755163818597794
In grad_steps = 3766, loss = 0.40068167448043823
In grad_steps = 3767, loss = 0.028512075543403625
In grad_steps = 3768, loss = 0.08711717277765274
In grad_steps = 3769, loss = 0.05047858506441116
In grad_steps = 3770, loss = 0.01952175423502922
In grad_steps = 3771, loss = 1.1872981786727905
In grad_steps = 3772, loss = 0.009976493194699287
In grad_steps = 3773, loss = 0.04798265919089317
In grad_steps = 3774, loss = 0.1033087819814682
In grad_steps = 3775, loss = 0.02944127470254898
In grad_steps = 3776, loss = 0.23592734336853027
In grad_steps = 3777, loss = 0.02729577012360096
In grad_steps = 3778, loss = 0.04014071449637413
In grad_steps = 3779, loss = 0.16724200546741486
In grad_steps = 3780, loss = 0.05117648467421532
In grad_steps = 3781, loss = 0.05832308530807495
In grad_steps = 3782, loss = 0.011312538757920265
In grad_steps = 3783, loss = 0.07172796130180359
In grad_steps = 3784, loss = 1.1084489822387695
In grad_steps = 3785, loss = 0.396947979927063
In grad_steps = 3786, loss = 0.16930052638053894
In grad_steps = 3787, loss = 0.6066179871559143
In grad_steps = 3788, loss = 0.06767197698354721
In grad_steps = 3789, loss = 0.36526069045066833
In grad_steps = 3790, loss = 0.11465077102184296
In grad_steps = 3791, loss = 0.4976853132247925
In grad_steps = 3792, loss = 0.15894731879234314
In grad_steps = 3793, loss = 0.28591665625572205
In grad_steps = 3794, loss = 0.057586174458265305
In grad_steps = 3795, loss = 0.09647202491760254
In grad_steps = 3796, loss = 0.20835161209106445
In grad_steps = 3797, loss = 0.1093081533908844
In grad_steps = 3798, loss = 0.4271745979785919
In grad_steps = 3799, loss = 0.03787260130047798
In grad_steps = 3800, loss = 0.06498238444328308
In grad_steps = 3801, loss = 0.04374818131327629
In grad_steps = 3802, loss = 1.6013333797454834
In grad_steps = 3803, loss = 0.44994890689849854
In grad_steps = 3804, loss = 0.04093678295612335
In grad_steps = 3805, loss = 0.05657526105642319
In grad_steps = 3806, loss = 0.06117401272058487
In grad_steps = 3807, loss = 0.17331333458423615
In grad_steps = 3808, loss = 0.7532077431678772
In grad_steps = 3809, loss = 0.09443306177854538
In grad_steps = 3810, loss = 0.10757366567850113
In grad_steps = 3811, loss = 0.9077441096305847
In grad_steps = 3812, loss = 0.21186481416225433
In grad_steps = 3813, loss = 0.06693312525749207
In grad_steps = 3814, loss = 0.10051581263542175
In grad_steps = 3815, loss = 0.10862889140844345
In grad_steps = 3816, loss = 0.764676570892334
In grad_steps = 3817, loss = 0.10879618674516678
In grad_steps = 3818, loss = 0.6067805290222168
In grad_steps = 3819, loss = 0.11620292067527771
In grad_steps = 3820, loss = 0.14966373145580292
In grad_steps = 3821, loss = 0.35631364583969116
In grad_steps = 3822, loss = 0.2850734293460846
In grad_steps = 3823, loss = 0.45051857829093933
In grad_steps = 3824, loss = 0.41904810070991516
In grad_steps = 3825, loss = 0.037453606724739075
In grad_steps = 3826, loss = 0.09385216236114502
In grad_steps = 3827, loss = 0.10263700038194656
In grad_steps = 3828, loss = 0.22284424304962158
In grad_steps = 3829, loss = 0.028768418356776237
In grad_steps = 3830, loss = 0.07841411232948303
In grad_steps = 3831, loss = 0.1558985710144043
In grad_steps = 3832, loss = 0.05057786777615547
In grad_steps = 3833, loss = 0.06656911969184875
In grad_steps = 3834, loss = 0.02924131229519844
In grad_steps = 3835, loss = 0.02653837949037552
In grad_steps = 3836, loss = 0.12489543855190277
In grad_steps = 3837, loss = 0.5616752505302429
In grad_steps = 3838, loss = 0.04149935394525528
In grad_steps = 3839, loss = 0.025361116975545883
In grad_steps = 3840, loss = 0.03248251602053642
In grad_steps = 3841, loss = 0.05989091843366623
In grad_steps = 3842, loss = 0.058395590633153915
In grad_steps = 3843, loss = 0.10523541271686554
In grad_steps = 3844, loss = 0.41204434633255005
In grad_steps = 3845, loss = 0.05459005385637283
In grad_steps = 3846, loss = 0.23202183842658997
In grad_steps = 3847, loss = 0.013877819292247295
In grad_steps = 3848, loss = 1.228644847869873
In grad_steps = 3849, loss = 0.017455890774726868
In grad_steps = 3850, loss = 0.4967901110649109
In grad_steps = 3851, loss = 0.08778387308120728
In grad_steps = 3852, loss = 0.02872401848435402
In grad_steps = 3853, loss = 0.015594935975968838
In grad_steps = 3854, loss = 0.4322053790092468
In grad_steps = 3855, loss = 0.012028430588543415
In grad_steps = 3856, loss = 0.9868781566619873
In grad_steps = 3857, loss = 0.43435245752334595
In grad_steps = 3858, loss = 0.48683086037635803
In grad_steps = 3859, loss = 0.1283072531223297
In grad_steps = 3860, loss = 0.25759389996528625
In grad_steps = 3861, loss = 1.2609553337097168
In grad_steps = 3862, loss = 0.4897841811180115
In grad_steps = 3863, loss = 0.38914698362350464
In grad_steps = 3864, loss = 0.6996937990188599
In grad_steps = 3865, loss = 0.08359883725643158
In grad_steps = 3866, loss = 0.8333176374435425
In grad_steps = 3867, loss = 0.12391970306634903
In grad_steps = 3868, loss = 0.8534573316574097
In grad_steps = 3869, loss = 0.24196857213974
In grad_steps = 3870, loss = 0.08005978167057037
In grad_steps = 3871, loss = 0.25410011410713196
In grad_steps = 3872, loss = 0.2716938853263855
In grad_steps = 3873, loss = 0.39648473262786865
In grad_steps = 3874, loss = 0.1383497714996338
In grad_steps = 3875, loss = 0.36991193890571594
In grad_steps = 3876, loss = 0.19252343475818634
In grad_steps = 3877, loss = 0.21777844429016113
In grad_steps = 3878, loss = 0.3216811418533325
In grad_steps = 3879, loss = 0.255096435546875
In grad_steps = 3880, loss = 0.11194579303264618
In grad_steps = 3881, loss = 0.4536852240562439
In grad_steps = 3882, loss = 0.17966459691524506
In grad_steps = 3883, loss = 0.6398576498031616
In grad_steps = 3884, loss = 0.06800577044487
In grad_steps = 3885, loss = 0.2527283728122711
In grad_steps = 3886, loss = 0.051011957228183746
In grad_steps = 3887, loss = 0.047643475234508514
In grad_steps = 3888, loss = 0.018055683001875877
In grad_steps = 3889, loss = 0.0749453529715538
In grad_steps = 3890, loss = 0.46410977840423584
In grad_steps = 3891, loss = 0.23834338784217834
In grad_steps = 3892, loss = 0.06604327261447906
In grad_steps = 3893, loss = 0.12270120531320572
In grad_steps = 3894, loss = 0.027880875393748283
In grad_steps = 3895, loss = 0.0741744190454483
In grad_steps = 3896, loss = 0.010379412211477757
In grad_steps = 3897, loss = 0.08832065761089325
In grad_steps = 3898, loss = 0.018853014335036278
In grad_steps = 3899, loss = 0.5147277116775513
In grad_steps = 3900, loss = 0.33765166997909546
In grad_steps = 3901, loss = 0.01448888424783945
In grad_steps = 3902, loss = 1.1984245777130127
In grad_steps = 3903, loss = 0.020289186388254166
In grad_steps = 3904, loss = 0.009712442755699158
In grad_steps = 3905, loss = 0.009907744824886322
In grad_steps = 3906, loss = 2.0046801567077637
In grad_steps = 3907, loss = 0.34152957797050476
In grad_steps = 3908, loss = 0.23380523920059204
In grad_steps = 3909, loss = 0.06865585595369339
In grad_steps = 3910, loss = 0.5492342710494995
In grad_steps = 3911, loss = 0.060576993972063065
In grad_steps = 3912, loss = 0.28391149640083313
In grad_steps = 3913, loss = 0.17070578038692474
In grad_steps = 3914, loss = 0.12044788151979446
In grad_steps = 3915, loss = 0.03279993683099747
In grad_steps = 3916, loss = 0.6407251358032227
In grad_steps = 3917, loss = 0.054139479994773865
In grad_steps = 3918, loss = 0.32793429493904114
In grad_steps = 3919, loss = 1.090728521347046
In grad_steps = 3920, loss = 0.019496871158480644
In grad_steps = 3921, loss = 0.0502827987074852
In grad_steps = 3922, loss = 0.2915419638156891
In grad_steps = 3923, loss = 1.2094762325286865
In grad_steps = 3924, loss = 0.11835938692092896
In grad_steps = 3925, loss = 0.10297510027885437
In grad_steps = 3926, loss = 0.22561194002628326
In grad_steps = 3927, loss = 0.2587696313858032
In grad_steps = 3928, loss = 1.069578766822815
In grad_steps = 3929, loss = 0.7205549478530884
In grad_steps = 3930, loss = 0.12882882356643677
In grad_steps = 3931, loss = 0.8349176645278931
In grad_steps = 3932, loss = 0.02417944371700287
In grad_steps = 3933, loss = 0.5892185568809509
In grad_steps = 3934, loss = 0.3927130699157715
In grad_steps = 3935, loss = 0.2917981445789337
In grad_steps = 3936, loss = 0.31155553460121155
In grad_steps = 3937, loss = 0.6354103088378906
In grad_steps = 3938, loss = 0.24678832292556763
In grad_steps = 3939, loss = 0.45821282267570496
In grad_steps = 3940, loss = 0.5484611988067627
In grad_steps = 3941, loss = 0.5373737812042236
In grad_steps = 3942, loss = 0.22963309288024902
In grad_steps = 3943, loss = 0.2613331377506256
In grad_steps = 3944, loss = 0.25937125086784363
In grad_steps = 3945, loss = 0.29541265964508057
In grad_steps = 3946, loss = 0.2162628471851349
In grad_steps = 3947, loss = 0.1783706545829773
In grad_steps = 3948, loss = 0.0842367485165596
In grad_steps = 3949, loss = 0.17719997465610504
In grad_steps = 3950, loss = 0.15552185475826263
In grad_steps = 3951, loss = 0.10646810382604599
In grad_steps = 3952, loss = 0.07894810289144516
In grad_steps = 3953, loss = 0.07932214438915253
In grad_steps = 3954, loss = 0.09721965342760086
In grad_steps = 3955, loss = 0.05249364674091339
In grad_steps = 3956, loss = 0.5005086660385132
In grad_steps = 3957, loss = 0.36620983481407166
In grad_steps = 3958, loss = 0.11039531975984573
In grad_steps = 3959, loss = 0.01041797362267971
In grad_steps = 3960, loss = 0.1322854906320572
In grad_steps = 3961, loss = 0.012570478022098541
In grad_steps = 3962, loss = 0.09709856659173965
In grad_steps = 3963, loss = 0.026501979678869247
In grad_steps = 3964, loss = 0.025267869234085083
In grad_steps = 3965, loss = 0.009272323921322823
In grad_steps = 3966, loss = 1.031837821006775
In grad_steps = 3967, loss = 0.628150463104248
In grad_steps = 3968, loss = 0.4310967028141022
In grad_steps = 3969, loss = 0.04322017729282379
In grad_steps = 3970, loss = 0.013280419632792473
In grad_steps = 3971, loss = 0.32758256793022156
In grad_steps = 3972, loss = 0.0574096255004406
In grad_steps = 3973, loss = 0.04408741742372513
In grad_steps = 3974, loss = 0.05229124054312706
In grad_steps = 3975, loss = 0.0396440215408802
In grad_steps = 3976, loss = 0.048208944499492645
In grad_steps = 3977, loss = 0.5283462405204773
In grad_steps = 3978, loss = 0.11164438724517822
In grad_steps = 3979, loss = 0.15648303925991058
In grad_steps = 3980, loss = 0.11437717080116272
In grad_steps = 3981, loss = 0.06195941939949989
In grad_steps = 3982, loss = 0.03406303748488426
In grad_steps = 3983, loss = 0.469512403011322
In grad_steps = 3984, loss = 0.6617108583450317
In grad_steps = 3985, loss = 0.028555726632475853
In grad_steps = 3986, loss = 0.15083032846450806
In grad_steps = 3987, loss = 0.0786522701382637
In grad_steps = 3988, loss = 0.014442572370171547
In grad_steps = 3989, loss = 0.26718467473983765
In grad_steps = 3990, loss = 0.10058453679084778
In grad_steps = 3991, loss = 0.06815962493419647
In grad_steps = 3992, loss = 0.479412317276001
In grad_steps = 3993, loss = 0.3796344995498657
In grad_steps = 3994, loss = 0.019127968698740005
In grad_steps = 3995, loss = 0.021488459780812263
In grad_steps = 3996, loss = 0.0824122503399849
In grad_steps = 3997, loss = 0.5457550287246704
In grad_steps = 3998, loss = 0.6711240410804749
In grad_steps = 3999, loss = 0.5378261804580688
In grad_steps = 4000, loss = 0.3631990849971771
In grad_steps = 4001, loss = 0.14119252562522888
In grad_steps = 4002, loss = 0.3190023601055145
In grad_steps = 4003, loss = 0.12801118195056915
In grad_steps = 4004, loss = 0.15642452239990234
In grad_steps = 4005, loss = 0.6496873497962952
In grad_steps = 4006, loss = 0.170060932636261
In grad_steps = 4007, loss = 0.39224332571029663
In grad_steps = 4008, loss = 0.10434085130691528
In grad_steps = 4009, loss = 0.28923824429512024
In grad_steps = 4010, loss = 0.08288135379552841
In grad_steps = 4011, loss = 0.09787464141845703
In grad_steps = 4012, loss = 0.16935667395591736
In grad_steps = 4013, loss = 0.011107546277344227
In grad_steps = 4014, loss = 0.029335277155041695
In grad_steps = 4015, loss = 0.23220887780189514
In grad_steps = 4016, loss = 0.0651649609208107
In grad_steps = 4017, loss = 0.184192955493927
In grad_steps = 4018, loss = 0.09343820065259933
In grad_steps = 4019, loss = 0.0653594583272934
In grad_steps = 4020, loss = 0.010084432549774647
In grad_steps = 4021, loss = 0.39610522985458374
In grad_steps = 4022, loss = 0.6227555871009827
In grad_steps = 4023, loss = 0.04498458281159401
In grad_steps = 4024, loss = 0.9044649004936218
In grad_steps = 4025, loss = 0.011951757594943047
In grad_steps = 4026, loss = 0.016063271090388298
In grad_steps = 4027, loss = 0.0325251966714859
In grad_steps = 4028, loss = 0.5327515006065369
In grad_steps = 4029, loss = 0.3023412227630615
In grad_steps = 4030, loss = 0.05192705988883972
In grad_steps = 4031, loss = 0.12325359135866165
In grad_steps = 4032, loss = 0.4584755599498749
In grad_steps = 4033, loss = 0.21801075339317322
In grad_steps = 4034, loss = 0.05629032850265503
In grad_steps = 4035, loss = 0.6147841811180115
In grad_steps = 4036, loss = 0.108651302754879
In grad_steps = 4037, loss = 0.19195161759853363
In grad_steps = 4038, loss = 0.6085004210472107
In grad_steps = 4039, loss = 0.10804562270641327
In grad_steps = 4040, loss = 0.0389053039252758
In grad_steps = 4041, loss = 2.651439905166626
In grad_steps = 4042, loss = 0.6919914484024048
In grad_steps = 4043, loss = 0.00995247345417738
In grad_steps = 4044, loss = 0.940634548664093
In grad_steps = 4045, loss = 0.13007843494415283
In grad_steps = 4046, loss = 0.2612696588039398
In grad_steps = 4047, loss = 0.602865993976593
In grad_steps = 4048, loss = 0.09960160404443741
In grad_steps = 4049, loss = 0.28663355112075806
In grad_steps = 4050, loss = 0.20166949927806854
In grad_steps = 4051, loss = 0.10300691425800323
In grad_steps = 4052, loss = 0.17884932458400726
In grad_steps = 4053, loss = 0.08681818097829819
In grad_steps = 4054, loss = 0.30110791325569153
In grad_steps = 4055, loss = 0.22293731570243835
In grad_steps = 4056, loss = 0.0709153339266777
In grad_steps = 4057, loss = 0.402248740196228
In grad_steps = 4058, loss = 0.11357422173023224
In grad_steps = 4059, loss = 0.22852538526058197
In grad_steps = 4060, loss = 0.037132080644369125
In grad_steps = 4061, loss = 0.1078726202249527
In grad_steps = 4062, loss = 0.07168640196323395
In grad_steps = 4063, loss = 0.2326028048992157
In grad_steps = 4064, loss = 0.11323590576648712
In grad_steps = 4065, loss = 0.18419606983661652
In grad_steps = 4066, loss = 0.3740193247795105
In grad_steps = 4067, loss = 0.02939838543534279
In grad_steps = 4068, loss = 0.21470779180526733
In grad_steps = 4069, loss = 0.1288631558418274
In grad_steps = 4070, loss = 0.29809507727622986
In grad_steps = 4071, loss = 0.24081215262413025
In grad_steps = 4072, loss = 0.09303846955299377
In grad_steps = 4073, loss = 0.014845729805529118
In grad_steps = 4074, loss = 0.005116094835102558
In grad_steps = 4075, loss = 0.2840343415737152
In grad_steps = 4076, loss = 0.00692439079284668
In grad_steps = 4077, loss = 0.15464191138744354
In grad_steps = 4078, loss = 0.27877604961395264
In grad_steps = 4079, loss = 0.3543010354042053
In grad_steps = 4080, loss = 0.8087810277938843
In grad_steps = 4081, loss = 2.714857816696167
In grad_steps = 4082, loss = 0.02359139733016491
In grad_steps = 4083, loss = 0.011015607044100761
In grad_steps = 4084, loss = 0.04360781982541084
In grad_steps = 4085, loss = 1.1665804386138916
In grad_steps = 4086, loss = 0.6146126389503479
In grad_steps = 4087, loss = 0.66718590259552
In grad_steps = 4088, loss = 0.015635348856449127
In grad_steps = 4089, loss = 0.052994050085544586
In grad_steps = 4090, loss = 0.06149652227759361
In grad_steps = 4091, loss = 0.6433008909225464
In grad_steps = 4092, loss = 0.10495282709598541
In grad_steps = 4093, loss = 0.04793288931250572
In grad_steps = 4094, loss = 0.3756556510925293
In grad_steps = 4095, loss = 0.13922809064388275
In grad_steps = 4096, loss = 0.24295397102832794
In grad_steps = 4097, loss = 0.31781110167503357
In grad_steps = 4098, loss = 0.15022920072078705
In grad_steps = 4099, loss = 0.05753854662179947
In grad_steps = 4100, loss = 0.1765187680721283
In grad_steps = 4101, loss = 0.12898680567741394
In grad_steps = 4102, loss = 0.4251473546028137
In grad_steps = 4103, loss = 0.07790015637874603
In grad_steps = 4104, loss = 0.17749229073524475
In grad_steps = 4105, loss = 0.004142869729548693
Elapsed time: 2340.0469930171967 seconds for ensemble 3 with 2 epochs
LoRA instance 3 evaluation complete. Data saved to /pscratch/sd/t/tianle/lucid/other_source/SURP_2024/results/tmp_lora_ensemble/Llama3-8B-set-5/test_data_instance_3_seed_30280.npz.
lora instance i = 3 Successfully finished.
Training lora instance 4
DEBUG: Total parameters: 8037076992
DEBUG: Trainable parameters: 6815744
Beginning epoch 1
In grad_steps = 0, loss = 0.7379498481750488
In grad_steps = 1, loss = 0.6383206844329834
In grad_steps = 2, loss = 1.6093676090240479
In grad_steps = 3, loss = 0.7997822165489197
In grad_steps = 4, loss = 0.6222766041755676
In grad_steps = 5, loss = 0.8302825689315796
In grad_steps = 6, loss = 0.5251904726028442
In grad_steps = 7, loss = 0.5675976276397705
In grad_steps = 8, loss = 1.1825662851333618
In grad_steps = 9, loss = 0.9708204865455627
In grad_steps = 10, loss = 0.757611870765686
In grad_steps = 11, loss = 0.7113206386566162
In grad_steps = 12, loss = 0.7485948801040649
In grad_steps = 13, loss = 0.5883256793022156
In grad_steps = 14, loss = 0.9431278705596924
In grad_steps = 15, loss = 0.5370775461196899
In grad_steps = 16, loss = 0.7165396213531494
In grad_steps = 17, loss = 0.6954242587089539
In grad_steps = 18, loss = 0.7821848392486572
In grad_steps = 19, loss = 0.7426869869232178
In grad_steps = 20, loss = 0.6725357174873352
In grad_steps = 21, loss = 0.6499752402305603
In grad_steps = 22, loss = 0.6794649362564087
In grad_steps = 23, loss = 0.7912227511405945
In grad_steps = 24, loss = 0.6340291500091553
In grad_steps = 25, loss = 0.7522122859954834
In grad_steps = 26, loss = 0.7288929224014282
In grad_steps = 27, loss = 0.7216823101043701
In grad_steps = 28, loss = 0.5721908807754517
In grad_steps = 29, loss = 0.6903616786003113
In grad_steps = 30, loss = 0.8108426332473755
In grad_steps = 31, loss = 0.6221823692321777
In grad_steps = 32, loss = 0.6492095589637756
In grad_steps = 33, loss = 0.6731716990470886
In grad_steps = 34, loss = 0.6267898082733154
In grad_steps = 35, loss = 0.688860297203064
In grad_steps = 36, loss = 0.6571845412254333
In grad_steps = 37, loss = 0.6888575553894043
In grad_steps = 38, loss = 0.783689558506012
In grad_steps = 39, loss = 0.6915628910064697
In grad_steps = 40, loss = 0.7440242767333984
In grad_steps = 41, loss = 0.6796414256095886
In grad_steps = 42, loss = 0.6261720061302185
In grad_steps = 43, loss = 0.6700721979141235
In grad_steps = 44, loss = 0.5837749242782593
In grad_steps = 45, loss = 0.730903148651123
In grad_steps = 46, loss = 0.7189751267433167
In grad_steps = 47, loss = 0.6698384881019592
In grad_steps = 48, loss = 0.5100131630897522
In grad_steps = 49, loss = 0.6640627980232239
In grad_steps = 50, loss = 0.7223255634307861
In grad_steps = 51, loss = 0.6882797479629517
In grad_steps = 52, loss = 0.5574905276298523
In grad_steps = 53, loss = 0.741359293460846
In grad_steps = 54, loss = 0.6389086246490479
In grad_steps = 55, loss = 0.6149314045906067
In grad_steps = 56, loss = 0.6887350082397461
In grad_steps = 57, loss = 0.7087734341621399
In grad_steps = 58, loss = 0.688770592212677
In grad_steps = 59, loss = 0.7075973749160767
In grad_steps = 60, loss = 0.7396727800369263
In grad_steps = 61, loss = 0.6140816807746887
In grad_steps = 62, loss = 0.6297163963317871
In grad_steps = 63, loss = 0.6013821363449097
In grad_steps = 64, loss = 0.49583080410957336
In grad_steps = 65, loss = 0.5523207187652588
In grad_steps = 66, loss = 0.5770391225814819
In grad_steps = 67, loss = 0.4173353314399719
In grad_steps = 68, loss = 0.5386799573898315
In grad_steps = 69, loss = 0.506294846534729
In grad_steps = 70, loss = 0.9161332845687866
In grad_steps = 71, loss = 0.33553507924079895
In grad_steps = 72, loss = 1.0111535787582397
In grad_steps = 73, loss = 0.8315244913101196
In grad_steps = 74, loss = 0.7066366672515869
In grad_steps = 75, loss = 0.6879815459251404
In grad_steps = 76, loss = 0.4146198332309723
In grad_steps = 77, loss = 0.41857701539993286
In grad_steps = 78, loss = 0.6562114357948303
In grad_steps = 79, loss = 0.6108056902885437
In grad_steps = 80, loss = 0.6474550366401672
In grad_steps = 81, loss = 0.24956165254116058
In grad_steps = 82, loss = 0.9909662008285522
In grad_steps = 83, loss = 0.7858834862709045
In grad_steps = 84, loss = 0.7506827116012573
In grad_steps = 85, loss = 0.777131199836731
In grad_steps = 86, loss = 0.8077558279037476
In grad_steps = 87, loss = 0.6142220497131348
In grad_steps = 88, loss = 0.5046796798706055
In grad_steps = 89, loss = 0.5899657607078552
In grad_steps = 90, loss = 0.6523486375808716
In grad_steps = 91, loss = 0.6854797601699829
In grad_steps = 92, loss = 0.6052237749099731
In grad_steps = 93, loss = 0.802159309387207
In grad_steps = 94, loss = 0.6687465906143188
In grad_steps = 95, loss = 0.3481289744377136
In grad_steps = 96, loss = 0.6881169676780701
In grad_steps = 97, loss = 0.7449168562889099
In grad_steps = 98, loss = 0.7648712396621704
In grad_steps = 99, loss = 0.5319569110870361
In grad_steps = 100, loss = 0.8387628197669983
In grad_steps = 101, loss = 0.9658778309822083
In grad_steps = 102, loss = 0.7857139706611633
In grad_steps = 103, loss = 0.4294509291648865
In grad_steps = 104, loss = 0.5499723553657532
In grad_steps = 105, loss = 0.5965350866317749
In grad_steps = 106, loss = 0.5358449816703796
In grad_steps = 107, loss = 0.5936455726623535
In grad_steps = 108, loss = 0.488957941532135
In grad_steps = 109, loss = 0.5387647151947021
In grad_steps = 110, loss = 0.9963995218276978
In grad_steps = 111, loss = 0.3845822811126709
In grad_steps = 112, loss = 0.2785024642944336
In grad_steps = 113, loss = 0.5764709115028381
In grad_steps = 114, loss = 0.2514535188674927
In grad_steps = 115, loss = 0.9502161741256714
In grad_steps = 116, loss = 0.3003993630409241
In grad_steps = 117, loss = 0.48709291219711304
In grad_steps = 118, loss = 0.5117897987365723
In grad_steps = 119, loss = 0.7734342217445374
In grad_steps = 120, loss = 0.3440769910812378
In grad_steps = 121, loss = 0.4031440317630768
In grad_steps = 122, loss = 0.3388403356075287
In grad_steps = 123, loss = 0.7975367307662964
In grad_steps = 124, loss = 0.6590412259101868
In grad_steps = 125, loss = 0.7833038568496704
In grad_steps = 126, loss = 0.12620380520820618
In grad_steps = 127, loss = 0.6342453956604004
In grad_steps = 128, loss = 0.5977820158004761
In grad_steps = 129, loss = 0.5658822059631348
In grad_steps = 130, loss = 0.9219483733177185
In grad_steps = 131, loss = 0.6516973972320557
In grad_steps = 132, loss = 0.6885550022125244
In grad_steps = 133, loss = 0.5105434656143188
In grad_steps = 134, loss = 0.7106099128723145
In grad_steps = 135, loss = 0.6057571172714233
In grad_steps = 136, loss = 0.5522005558013916
In grad_steps = 137, loss = 0.3144954442977905
In grad_steps = 138, loss = 0.535793125629425
In grad_steps = 139, loss = 0.3376392722129822
In grad_steps = 140, loss = 0.775966465473175
In grad_steps = 141, loss = 0.4413478970527649
In grad_steps = 142, loss = 0.46405029296875
In grad_steps = 143, loss = 0.2230812907218933
In grad_steps = 144, loss = 0.6786543130874634
In grad_steps = 145, loss = 0.35717684030532837
In grad_steps = 146, loss = 0.6341227293014526
In grad_steps = 147, loss = 0.26366904377937317
In grad_steps = 148, loss = 0.24988499283790588
In grad_steps = 149, loss = 0.3784645199775696
In grad_steps = 150, loss = 0.923373818397522
In grad_steps = 151, loss = 0.5285572409629822
In grad_steps = 152, loss = 1.412142038345337
In grad_steps = 153, loss = 1.1009474992752075
In grad_steps = 154, loss = 0.7530760765075684
In grad_steps = 155, loss = 0.21793051064014435
In grad_steps = 156, loss = 0.5044222474098206
In grad_steps = 157, loss = 0.17145967483520508
In grad_steps = 158, loss = 0.2549176812171936
In grad_steps = 159, loss = 0.5730050802230835
In grad_steps = 160, loss = 0.24809136986732483
In grad_steps = 161, loss = 0.7685856819152832
In grad_steps = 162, loss = 0.5091847777366638
In grad_steps = 163, loss = 0.35061419010162354
In grad_steps = 164, loss = 0.7277162075042725
In grad_steps = 165, loss = 0.272621750831604
In grad_steps = 166, loss = 0.17280426621437073
In grad_steps = 167, loss = 0.653435468673706
In grad_steps = 168, loss = 0.6434634327888489
In grad_steps = 169, loss = 0.13799281418323517
In grad_steps = 170, loss = 0.2203601896762848
In grad_steps = 171, loss = 0.4100269675254822
In grad_steps = 172, loss = 0.3297306001186371
In grad_steps = 173, loss = 0.138839453458786
In grad_steps = 174, loss = 0.14035136997699738
In grad_steps = 175, loss = 0.6558322310447693
In grad_steps = 176, loss = 0.054184526205062866
In grad_steps = 177, loss = 0.15550851821899414
In grad_steps = 178, loss = 0.5207278728485107
In grad_steps = 179, loss = 1.429857611656189
In grad_steps = 180, loss = 0.19457750022411346
In grad_steps = 181, loss = 0.5565696358680725
In grad_steps = 182, loss = 0.7987380623817444
In grad_steps = 183, loss = 0.3356626033782959
In grad_steps = 184, loss = 0.5817267894744873
In grad_steps = 185, loss = 0.5867885947227478
In grad_steps = 186, loss = 0.3872448205947876
In grad_steps = 187, loss = 0.4949989318847656
In grad_steps = 188, loss = 0.5906487107276917
In grad_steps = 189, loss = 0.31133580207824707
In grad_steps = 190, loss = 0.18995697796344757
In grad_steps = 191, loss = 0.32354971766471863
In grad_steps = 192, loss = 0.5792217254638672
In grad_steps = 193, loss = 0.523314356803894
In grad_steps = 194, loss = 0.8842242956161499
In grad_steps = 195, loss = 0.7293245792388916
In grad_steps = 196, loss = 0.6259068846702576
In grad_steps = 197, loss = 0.6203106045722961
In grad_steps = 198, loss = 0.5270825028419495
In grad_steps = 199, loss = 1.0196335315704346
In grad_steps = 200, loss = 0.30284833908081055
In grad_steps = 201, loss = 0.8133442401885986
In grad_steps = 202, loss = 0.6058865189552307
In grad_steps = 203, loss = 0.3752164840698242
In grad_steps = 204, loss = 0.560804545879364
In grad_steps = 205, loss = 0.569358229637146
In grad_steps = 206, loss = 0.29349616169929504
In grad_steps = 207, loss = 0.4651871919631958
In grad_steps = 208, loss = 0.2954179346561432
In grad_steps = 209, loss = 0.1890895664691925
In grad_steps = 210, loss = 0.24061062932014465
In grad_steps = 211, loss = 0.9829603433609009
In grad_steps = 212, loss = 0.4192698895931244
In grad_steps = 213, loss = 0.3620073199272156
In grad_steps = 214, loss = 0.13690640032291412
In grad_steps = 215, loss = 0.22299395501613617
In grad_steps = 216, loss = 0.5297722816467285
In grad_steps = 217, loss = 0.4113417863845825
In grad_steps = 218, loss = 1.8743016719818115
In grad_steps = 219, loss = 0.49835342168807983
In grad_steps = 220, loss = 0.364807665348053
In grad_steps = 221, loss = 0.9999001026153564
In grad_steps = 222, loss = 0.5558044910430908
In grad_steps = 223, loss = 0.5680719614028931
In grad_steps = 224, loss = 0.20346979796886444
In grad_steps = 225, loss = 0.09263899177312851
In grad_steps = 226, loss = 0.7207027673721313
In grad_steps = 227, loss = 0.48720240592956543
In grad_steps = 228, loss = 0.4226057529449463
In grad_steps = 229, loss = 0.31807106733322144
In grad_steps = 230, loss = 0.32858654856681824
In grad_steps = 231, loss = 0.6877279281616211
In grad_steps = 232, loss = 0.9702196717262268
In grad_steps = 233, loss = 0.29363757371902466
In grad_steps = 234, loss = 0.6377291679382324
In grad_steps = 235, loss = 0.6246846318244934
In grad_steps = 236, loss = 0.6208442449569702
In grad_steps = 237, loss = 0.20523591339588165
In grad_steps = 238, loss = 0.12472398579120636
In grad_steps = 239, loss = 0.2787400186061859
In grad_steps = 240, loss = 0.16276293992996216
In grad_steps = 241, loss = 0.5468943119049072
In grad_steps = 242, loss = 0.31126582622528076
In grad_steps = 243, loss = 0.3423634171485901
In grad_steps = 244, loss = 1.2104690074920654
In grad_steps = 245, loss = 0.21679317951202393
In grad_steps = 246, loss = 0.14685434103012085
In grad_steps = 247, loss = 0.35958200693130493
In grad_steps = 248, loss = 1.3680758476257324
In grad_steps = 249, loss = 0.14354166388511658
In grad_steps = 250, loss = 0.6145512461662292
In grad_steps = 251, loss = 0.957656979560852
In grad_steps = 252, loss = 0.17205505073070526
In grad_steps = 253, loss = 0.6513398289680481
In grad_steps = 254, loss = 0.5754161477088928
In grad_steps = 255, loss = 0.5440655946731567
In grad_steps = 256, loss = 0.8945324420928955
In grad_steps = 257, loss = 0.6887677311897278
In grad_steps = 258, loss = 0.8399231433868408
In grad_steps = 259, loss = 0.6196532845497131
In grad_steps = 260, loss = 0.660961389541626
In grad_steps = 261, loss = 0.9804143905639648
In grad_steps = 262, loss = 0.5114935636520386
In grad_steps = 263, loss = 0.6054404973983765
In grad_steps = 264, loss = 0.4834989309310913
In grad_steps = 265, loss = 0.6403466463088989
In grad_steps = 266, loss = 0.9356878995895386
In grad_steps = 267, loss = 0.8864724636077881
In grad_steps = 268, loss = 0.5384848713874817
In grad_steps = 269, loss = 0.2997758388519287
In grad_steps = 270, loss = 0.46834486722946167
In grad_steps = 271, loss = 0.6585426330566406
In grad_steps = 272, loss = 0.4513007402420044
In grad_steps = 273, loss = 0.516232430934906
In grad_steps = 274, loss = 0.39943239092826843
In grad_steps = 275, loss = 0.5454486608505249
In grad_steps = 276, loss = 0.5344645977020264
In grad_steps = 277, loss = 0.3663594722747803
In grad_steps = 278, loss = 0.45349764823913574
In grad_steps = 279, loss = 0.6805165410041809
In grad_steps = 280, loss = 0.6354531645774841
In grad_steps = 281, loss = 0.44473108649253845
In grad_steps = 282, loss = 0.5688031911849976
In grad_steps = 283, loss = 0.6277582049369812
In grad_steps = 284, loss = 0.5938965082168579
In grad_steps = 285, loss = 0.3070968985557556
In grad_steps = 286, loss = 0.16876345872879028
In grad_steps = 287, loss = 1.093991756439209
In grad_steps = 288, loss = 0.9950756430625916
In grad_steps = 289, loss = 0.42694738507270813
In grad_steps = 290, loss = 0.3262401521205902
In grad_steps = 291, loss = 1.066681981086731
In grad_steps = 292, loss = 0.5714914798736572
In grad_steps = 293, loss = 0.7174961566925049
In grad_steps = 294, loss = 0.4212302267551422
In grad_steps = 295, loss = 0.7763362526893616
In grad_steps = 296, loss = 0.44283801317214966
In grad_steps = 297, loss = 0.3541974723339081
In grad_steps = 298, loss = 0.38057971000671387
In grad_steps = 299, loss = 0.53009033203125
In grad_steps = 300, loss = 0.6165053844451904
In grad_steps = 301, loss = 0.5928021669387817
In grad_steps = 302, loss = 0.3608554005622864
In grad_steps = 303, loss = 0.4917984902858734
In grad_steps = 304, loss = 0.16304567456245422
In grad_steps = 305, loss = 0.2781384587287903
In grad_steps = 306, loss = 0.64130699634552
In grad_steps = 307, loss = 0.3736923038959503
In grad_steps = 308, loss = 0.6672007441520691
In grad_steps = 309, loss = 0.5685408711433411
In grad_steps = 310, loss = 0.19721455872058868
In grad_steps = 311, loss = 1.4387977123260498
In grad_steps = 312, loss = 0.2039298117160797
In grad_steps = 313, loss = 0.22694607079029083
In grad_steps = 314, loss = 0.5306426286697388
In grad_steps = 315, loss = 0.12783539295196533
In grad_steps = 316, loss = 1.1824963092803955
In grad_steps = 317, loss = 0.5152643918991089
In grad_steps = 318, loss = 0.34748443961143494
In grad_steps = 319, loss = 0.1496993750333786
In grad_steps = 320, loss = 0.6050705909729004
In grad_steps = 321, loss = 0.20843547582626343
In grad_steps = 322, loss = 0.9126589298248291
In grad_steps = 323, loss = 0.3595404624938965
In grad_steps = 324, loss = 0.43553340435028076
In grad_steps = 325, loss = 0.23965562880039215
In grad_steps = 326, loss = 0.43611204624176025
In grad_steps = 327, loss = 0.28328776359558105
In grad_steps = 328, loss = 0.23616334795951843
In grad_steps = 329, loss = 0.8261827230453491
In grad_steps = 330, loss = 0.2166009396314621
In grad_steps = 331, loss = 0.09642718732357025
In grad_steps = 332, loss = 0.11260469257831573
In grad_steps = 333, loss = 0.16298611462116241
In grad_steps = 334, loss = 0.4428550601005554
In grad_steps = 335, loss = 0.3782764971256256
In grad_steps = 336, loss = 0.6591947674751282
In grad_steps = 337, loss = 0.13206882774829865
In grad_steps = 338, loss = 0.08269059658050537
In grad_steps = 339, loss = 0.6636350154876709
In grad_steps = 340, loss = 0.07647214829921722
In grad_steps = 341, loss = 0.7632958292961121
In grad_steps = 342, loss = 0.1449715942144394
In grad_steps = 343, loss = 0.29721763730049133
In grad_steps = 344, loss = 1.0235600471496582
In grad_steps = 345, loss = 0.6631180047988892
In grad_steps = 346, loss = 0.4351833760738373
In grad_steps = 347, loss = 0.5611684918403625
In grad_steps = 348, loss = 0.7097538709640503
In grad_steps = 349, loss = 1.0125666856765747
In grad_steps = 350, loss = 0.14702445268630981
In grad_steps = 351, loss = 1.2695846557617188
In grad_steps = 352, loss = 0.6962161660194397
In grad_steps = 353, loss = 0.5531153678894043
In grad_steps = 354, loss = 0.7518260478973389
In grad_steps = 355, loss = 0.7578933835029602
In grad_steps = 356, loss = 0.3694266080856323
In grad_steps = 357, loss = 0.319039523601532
In grad_steps = 358, loss = 0.8814160823822021
In grad_steps = 359, loss = 0.5190985202789307
In grad_steps = 360, loss = 0.8454625606536865
In grad_steps = 361, loss = 0.745462954044342
In grad_steps = 362, loss = 0.6311584711074829
In grad_steps = 363, loss = 0.9284182786941528
In grad_steps = 364, loss = 0.370431125164032
In grad_steps = 365, loss = 0.7190194129943848
In grad_steps = 366, loss = 0.7628873586654663
In grad_steps = 367, loss = 0.5380698442459106
In grad_steps = 368, loss = 0.4709298610687256
In grad_steps = 369, loss = 0.3982016146183014
In grad_steps = 370, loss = 0.532311737537384
In grad_steps = 371, loss = 0.5265196561813354
In grad_steps = 372, loss = 0.4542273283004761
In grad_steps = 373, loss = 0.5532094836235046
In grad_steps = 374, loss = 0.3924775719642639
In grad_steps = 375, loss = 0.48068735003471375
In grad_steps = 376, loss = 0.42633694410324097
In grad_steps = 377, loss = 0.3633297383785248
In grad_steps = 378, loss = 0.3098137080669403
In grad_steps = 379, loss = 0.2513639032840729
In grad_steps = 380, loss = 0.5664164423942566
In grad_steps = 381, loss = 0.7908994555473328
In grad_steps = 382, loss = 0.3640039265155792
In grad_steps = 383, loss = 0.3114825189113617
In grad_steps = 384, loss = 0.09295035898685455
In grad_steps = 385, loss = 0.046548034995794296
In grad_steps = 386, loss = 0.11958558857440948
In grad_steps = 387, loss = 1.838122010231018
In grad_steps = 388, loss = 0.9594537019729614
In grad_steps = 389, loss = 0.3600116968154907
In grad_steps = 390, loss = 0.3650367259979248
In grad_steps = 391, loss = 0.3234850764274597
In grad_steps = 392, loss = 0.5591868758201599
In grad_steps = 393, loss = 0.5760725736618042
In grad_steps = 394, loss = 0.34982162714004517
In grad_steps = 395, loss = 1.0946922302246094
In grad_steps = 396, loss = 0.712820291519165
In grad_steps = 397, loss = 0.12678509950637817
In grad_steps = 398, loss = 0.8427720069885254
In grad_steps = 399, loss = 0.25899869203567505
In grad_steps = 400, loss = 0.9714148044586182
In grad_steps = 401, loss = 0.3091161847114563
In grad_steps = 402, loss = 0.25492608547210693
In grad_steps = 403, loss = 0.6810200214385986
In grad_steps = 404, loss = 0.3093768358230591
In grad_steps = 405, loss = 0.4321834444999695
In grad_steps = 406, loss = 0.7994452714920044
In grad_steps = 407, loss = 0.3457259237766266
In grad_steps = 408, loss = 0.7362478971481323
In grad_steps = 409, loss = 0.49693483114242554
In grad_steps = 410, loss = 0.350699782371521
In grad_steps = 411, loss = 0.2835436165332794
In grad_steps = 412, loss = 0.7110834717750549
In grad_steps = 413, loss = 0.8771107196807861
In grad_steps = 414, loss = 1.149914026260376
In grad_steps = 415, loss = 0.5273500084877014
In grad_steps = 416, loss = 1.2418692111968994
In grad_steps = 417, loss = 0.2705942690372467
In grad_steps = 418, loss = 0.18819987773895264
In grad_steps = 419, loss = 0.27629828453063965
In grad_steps = 420, loss = 0.3579517602920532
In grad_steps = 421, loss = 0.14141049981117249
In grad_steps = 422, loss = 0.4869571924209595
In grad_steps = 423, loss = 0.8817926049232483
In grad_steps = 424, loss = 0.8034073710441589
In grad_steps = 425, loss = 0.13700145483016968
In grad_steps = 426, loss = 0.12486593425273895
In grad_steps = 427, loss = 0.3594673275947571
In grad_steps = 428, loss = 0.26560306549072266
In grad_steps = 429, loss = 0.8072828054428101
In grad_steps = 430, loss = 0.18994013965129852
In grad_steps = 431, loss = 1.2085983753204346
In grad_steps = 432, loss = 0.6137210130691528
In grad_steps = 433, loss = 0.9461120367050171
In grad_steps = 434, loss = 0.30864840745925903
In grad_steps = 435, loss = 0.24644935131072998
In grad_steps = 436, loss = 0.23017975687980652
In grad_steps = 437, loss = 0.3597014248371124
In grad_steps = 438, loss = 0.25158461928367615
In grad_steps = 439, loss = 0.1853506863117218
In grad_steps = 440, loss = 0.10982535034418106
In grad_steps = 441, loss = 0.07710357010364532
In grad_steps = 442, loss = 0.3096817433834076
In grad_steps = 443, loss = 0.36895498633384705
In grad_steps = 444, loss = 0.9252652525901794
In grad_steps = 445, loss = 0.13510851562023163
In grad_steps = 446, loss = 0.18497301638126373
In grad_steps = 447, loss = 0.1538405418395996
In grad_steps = 448, loss = 0.21225325763225555
In grad_steps = 449, loss = 0.07078693807125092
In grad_steps = 450, loss = 0.10626229643821716
In grad_steps = 451, loss = 0.2557797431945801
In grad_steps = 452, loss = 0.1204315721988678
In grad_steps = 453, loss = 0.1078515574336052
In grad_steps = 454, loss = 0.37709423899650574
In grad_steps = 455, loss = 0.4415234625339508
In grad_steps = 456, loss = 1.2041735649108887
In grad_steps = 457, loss = 0.01556959468871355
In grad_steps = 458, loss = 0.16651524603366852
In grad_steps = 459, loss = 0.021626412868499756
In grad_steps = 460, loss = 0.03893743455410004
In grad_steps = 461, loss = 0.8178068399429321
In grad_steps = 462, loss = 0.23264747858047485
In grad_steps = 463, loss = 0.16660429537296295
In grad_steps = 464, loss = 0.36114612221717834
In grad_steps = 465, loss = 0.02418307401239872
In grad_steps = 466, loss = 0.2760821580886841
In grad_steps = 467, loss = 0.42724692821502686
In grad_steps = 468, loss = 0.37584632635116577
In grad_steps = 469, loss = 2.2432994842529297
In grad_steps = 470, loss = 0.137423574924469
In grad_steps = 471, loss = 0.834601879119873
In grad_steps = 472, loss = 0.46634921431541443
In grad_steps = 473, loss = 0.8075864911079407
In grad_steps = 474, loss = 0.3794841170310974
In grad_steps = 475, loss = 0.09045103192329407
In grad_steps = 476, loss = 0.12575571238994598
In grad_steps = 477, loss = 0.7638739347457886
In grad_steps = 478, loss = 0.19191378355026245
In grad_steps = 479, loss = 0.22849811613559723
In grad_steps = 480, loss = 0.17792730033397675
In grad_steps = 481, loss = 0.18743176758289337
In grad_steps = 482, loss = 0.32853183150291443
In grad_steps = 483, loss = 0.907447099685669
In grad_steps = 484, loss = 0.22055396437644958
In grad_steps = 485, loss = 0.6317043304443359
In grad_steps = 486, loss = 0.20002856850624084
In grad_steps = 487, loss = 0.5024393796920776
In grad_steps = 488, loss = 0.17228220403194427
In grad_steps = 489, loss = 0.44334176182746887
In grad_steps = 490, loss = 0.242661714553833
In grad_steps = 491, loss = 0.17227289080619812
In grad_steps = 492, loss = 0.29167282581329346
In grad_steps = 493, loss = 0.5659891366958618
In grad_steps = 494, loss = 0.2812422513961792
In grad_steps = 495, loss = 0.2714221179485321
In grad_steps = 496, loss = 0.3992637097835541
In grad_steps = 497, loss = 0.7002660632133484
In grad_steps = 498, loss = 0.44141045212745667
In grad_steps = 499, loss = 0.49396300315856934
In grad_steps = 500, loss = 0.07072154432535172
In grad_steps = 501, loss = 0.062375400215387344
In grad_steps = 502, loss = 1.0470346212387085
In grad_steps = 503, loss = 0.09660251438617706
In grad_steps = 504, loss = 0.8042017817497253
In grad_steps = 505, loss = 0.051507774740457535
In grad_steps = 506, loss = 0.35283857583999634
In grad_steps = 507, loss = 0.10695330053567886
In grad_steps = 508, loss = 0.40103772282600403
In grad_steps = 509, loss = 0.8780363202095032
In grad_steps = 510, loss = 0.5880323052406311
In grad_steps = 511, loss = 0.21817481517791748
In grad_steps = 512, loss = 0.35765668749809265
In grad_steps = 513, loss = 0.13629764318466187
In grad_steps = 514, loss = 0.3761412799358368
In grad_steps = 515, loss = 0.08751098811626434
In grad_steps = 516, loss = 0.24931654334068298
In grad_steps = 517, loss = 0.37932848930358887
In grad_steps = 518, loss = 0.15423744916915894
In grad_steps = 519, loss = 0.16506367921829224
In grad_steps = 520, loss = 0.6736966967582703
In grad_steps = 521, loss = 0.33785155415534973
In grad_steps = 522, loss = 0.46491414308547974
In grad_steps = 523, loss = 0.7347651720046997
In grad_steps = 524, loss = 0.6264598369598389
In grad_steps = 525, loss = 0.40727272629737854
In grad_steps = 526, loss = 0.7709489464759827
In grad_steps = 527, loss = 1.0845415592193604
In grad_steps = 528, loss = 0.646987795829773
In grad_steps = 529, loss = 0.3723933696746826
In grad_steps = 530, loss = 0.10351672023534775
In grad_steps = 531, loss = 0.11731621623039246
In grad_steps = 532, loss = 0.7027127742767334
In grad_steps = 533, loss = 0.6393320560455322
In grad_steps = 534, loss = 0.3665629029273987
In grad_steps = 535, loss = 1.2379720211029053
In grad_steps = 536, loss = 0.5364013910293579
In grad_steps = 537, loss = 0.512759804725647
In grad_steps = 538, loss = 0.6853575706481934
In grad_steps = 539, loss = 0.5778179168701172
In grad_steps = 540, loss = 0.30141696333885193
In grad_steps = 541, loss = 0.4251064658164978
In grad_steps = 542, loss = 0.4063257575035095
In grad_steps = 543, loss = 0.45290428400039673
In grad_steps = 544, loss = 0.6495987176895142
In grad_steps = 545, loss = 0.6758014559745789
In grad_steps = 546, loss = 0.4512925148010254
In grad_steps = 547, loss = 0.5330181121826172
In grad_steps = 548, loss = 0.2683963477611542
In grad_steps = 549, loss = 0.6698827743530273
In grad_steps = 550, loss = 0.3029714524745941
In grad_steps = 551, loss = 0.2905222177505493
In grad_steps = 552, loss = 0.5276970267295837
In grad_steps = 553, loss = 0.33139583468437195
In grad_steps = 554, loss = 0.3369614779949188
In grad_steps = 555, loss = 0.1908809393644333
In grad_steps = 556, loss = 0.5002015829086304
In grad_steps = 557, loss = 0.6417578458786011
In grad_steps = 558, loss = 0.21969999372959137
In grad_steps = 559, loss = 0.41779959201812744
In grad_steps = 560, loss = 0.3445591926574707
In grad_steps = 561, loss = 0.2217779904603958
In grad_steps = 562, loss = 0.18265871703624725
In grad_steps = 563, loss = 0.49811744689941406
In grad_steps = 564, loss = 0.26101410388946533
In grad_steps = 565, loss = 0.2134740650653839
In grad_steps = 566, loss = 0.28587254881858826
In grad_steps = 567, loss = 1.3972091674804688
In grad_steps = 568, loss = 0.40938395261764526
In grad_steps = 569, loss = 0.2852950990200043
In grad_steps = 570, loss = 0.5388686656951904
In grad_steps = 571, loss = 0.7410092949867249
In grad_steps = 572, loss = 0.08121299743652344
In grad_steps = 573, loss = 0.08532901853322983
In grad_steps = 574, loss = 0.23170343041419983
In grad_steps = 575, loss = 0.0526217520236969
In grad_steps = 576, loss = 0.4193669855594635
In grad_steps = 577, loss = 0.06883608549833298
In grad_steps = 578, loss = 1.8250999450683594
In grad_steps = 579, loss = 0.21262191236019135
In grad_steps = 580, loss = 0.4294482469558716
In grad_steps = 581, loss = 0.8412801623344421
In grad_steps = 582, loss = 0.7110118865966797
In grad_steps = 583, loss = 0.27029019594192505
In grad_steps = 584, loss = 0.36949601769447327
In grad_steps = 585, loss = 0.5658887028694153
In grad_steps = 586, loss = 0.27148985862731934
In grad_steps = 587, loss = 0.20304256677627563
In grad_steps = 588, loss = 0.3695501685142517
In grad_steps = 589, loss = 0.31606751680374146
In grad_steps = 590, loss = 0.1579226553440094
In grad_steps = 591, loss = 0.2717248499393463
In grad_steps = 592, loss = 0.38531917333602905
In grad_steps = 593, loss = 0.19163638353347778
In grad_steps = 594, loss = 0.38180339336395264
In grad_steps = 595, loss = 0.7484779953956604
In grad_steps = 596, loss = 0.6549643874168396
In grad_steps = 597, loss = 0.4319055676460266
In grad_steps = 598, loss = 0.28091925382614136
In grad_steps = 599, loss = 0.8804923892021179
In grad_steps = 600, loss = 0.8487530946731567
In grad_steps = 601, loss = 0.2998267710208893
In grad_steps = 602, loss = 0.6201000213623047
In grad_steps = 603, loss = 0.35770824551582336
In grad_steps = 604, loss = 0.6631225943565369
In grad_steps = 605, loss = 1.2945160865783691
In grad_steps = 606, loss = 0.19250163435935974
In grad_steps = 607, loss = 0.18536829948425293
In grad_steps = 608, loss = 0.4360617399215698
In grad_steps = 609, loss = 0.17982326447963715
In grad_steps = 610, loss = 0.16463464498519897
In grad_steps = 611, loss = 0.2567916512489319
In grad_steps = 612, loss = 0.33885887265205383
In grad_steps = 613, loss = 0.19502486288547516
In grad_steps = 614, loss = 0.18502378463745117
In grad_steps = 615, loss = 0.8289470672607422
In grad_steps = 616, loss = 0.3284772038459778
In grad_steps = 617, loss = 0.21656067669391632
In grad_steps = 618, loss = 0.3327547609806061
In grad_steps = 619, loss = 0.12491242587566376
In grad_steps = 620, loss = 0.3223487436771393
In grad_steps = 621, loss = 0.20231805741786957
In grad_steps = 622, loss = 0.07666002959012985
In grad_steps = 623, loss = 0.15274271368980408
In grad_steps = 624, loss = 0.03811823949217796
In grad_steps = 625, loss = 0.1594206541776657
In grad_steps = 626, loss = 0.129071444272995
In grad_steps = 627, loss = 0.3233095407485962
In grad_steps = 628, loss = 0.2751636803150177
In grad_steps = 629, loss = 0.7240421772003174
In grad_steps = 630, loss = 0.2736821174621582
In grad_steps = 631, loss = 0.36218270659446716
In grad_steps = 632, loss = 0.5252925753593445
In grad_steps = 633, loss = 0.494020015001297
In grad_steps = 634, loss = 0.2644992172718048
In grad_steps = 635, loss = 1.0205509662628174
In grad_steps = 636, loss = 0.3178926110267639
In grad_steps = 637, loss = 0.23161259293556213
In grad_steps = 638, loss = 0.14365464448928833
In grad_steps = 639, loss = 1.0918138027191162
In grad_steps = 640, loss = 1.4960246086120605
In grad_steps = 641, loss = 0.6693742275238037
In grad_steps = 642, loss = 0.1021166518330574
In grad_steps = 643, loss = 0.06469413638114929
In grad_steps = 644, loss = 0.5483121275901794
In grad_steps = 645, loss = 0.6843464970588684
In grad_steps = 646, loss = 0.6740517020225525
In grad_steps = 647, loss = 0.5673519372940063
In grad_steps = 648, loss = 0.16954505443572998
In grad_steps = 649, loss = 0.3578617572784424
In grad_steps = 650, loss = 0.5397260189056396
In grad_steps = 651, loss = 0.5735830068588257
In grad_steps = 652, loss = 0.36192208528518677
In grad_steps = 653, loss = 0.35717278718948364
In grad_steps = 654, loss = 0.6510539054870605
In grad_steps = 655, loss = 0.46308401226997375
In grad_steps = 656, loss = 0.46485283970832825
In grad_steps = 657, loss = 0.7442299127578735
In grad_steps = 658, loss = 0.24980580806732178
In grad_steps = 659, loss = 0.5624415874481201
In grad_steps = 660, loss = 0.8443458080291748
In grad_steps = 661, loss = 0.875819206237793
In grad_steps = 662, loss = 0.49340030550956726
In grad_steps = 663, loss = 0.2208196222782135
In grad_steps = 664, loss = 0.9130147695541382
In grad_steps = 665, loss = 0.6221532225608826
In grad_steps = 666, loss = 0.1990727335214615
In grad_steps = 667, loss = 0.4899422526359558
In grad_steps = 668, loss = 0.603056013584137
In grad_steps = 669, loss = 0.3176136016845703
In grad_steps = 670, loss = 0.40135544538497925
In grad_steps = 671, loss = 0.6025837063789368
In grad_steps = 672, loss = 0.5121447443962097
In grad_steps = 673, loss = 0.8622342944145203
In grad_steps = 674, loss = 0.22786355018615723
In grad_steps = 675, loss = 1.0383765697479248
In grad_steps = 676, loss = 0.6359711289405823
In grad_steps = 677, loss = 0.2594018578529358
In grad_steps = 678, loss = 0.23434968292713165
In grad_steps = 679, loss = 0.27021849155426025
In grad_steps = 680, loss = 0.34176018834114075
In grad_steps = 681, loss = 0.25229787826538086
In grad_steps = 682, loss = 0.9153333902359009
In grad_steps = 683, loss = 0.2035183310508728
In grad_steps = 684, loss = 0.43108612298965454
In grad_steps = 685, loss = 0.2179233729839325
In grad_steps = 686, loss = 0.20306220650672913
In grad_steps = 687, loss = 0.38367265462875366
In grad_steps = 688, loss = 0.2267020046710968
In grad_steps = 689, loss = 0.4446232318878174
In grad_steps = 690, loss = 0.17097637057304382
In grad_steps = 691, loss = 0.7745665311813354
In grad_steps = 692, loss = 0.07245121896266937
In grad_steps = 693, loss = 1.032157063484192
In grad_steps = 694, loss = 0.4194115400314331
In grad_steps = 695, loss = 0.11297851800918579
In grad_steps = 696, loss = 0.15042085945606232
In grad_steps = 697, loss = 0.2573884427547455
In grad_steps = 698, loss = 0.09597501158714294
In grad_steps = 699, loss = 0.24062040448188782
In grad_steps = 700, loss = 0.17174509167671204
In grad_steps = 701, loss = 0.798928439617157
In grad_steps = 702, loss = 0.04367726296186447
In grad_steps = 703, loss = 0.1014927327632904
In grad_steps = 704, loss = 0.15386290848255157
In grad_steps = 705, loss = 1.3814164400100708
In grad_steps = 706, loss = 0.19905662536621094
In grad_steps = 707, loss = 0.12165071815252304
In grad_steps = 708, loss = 0.10575929284095764
In grad_steps = 709, loss = 0.6746008396148682
In grad_steps = 710, loss = 0.09247565269470215
In grad_steps = 711, loss = 0.17961136996746063
In grad_steps = 712, loss = 0.0967140793800354
In grad_steps = 713, loss = 2.7880687713623047
In grad_steps = 714, loss = 0.059551794081926346
In grad_steps = 715, loss = 0.7634832262992859
In grad_steps = 716, loss = 0.23577666282653809
In grad_steps = 717, loss = 0.24177244305610657
In grad_steps = 718, loss = 0.06136781722307205
In grad_steps = 719, loss = 0.37779852747917175
In grad_steps = 720, loss = 2.190962314605713
In grad_steps = 721, loss = 0.5812045931816101
In grad_steps = 722, loss = 0.2216489017009735
In grad_steps = 723, loss = 0.616829514503479
In grad_steps = 724, loss = 0.11809759587049484
In grad_steps = 725, loss = 0.2103380262851715
In grad_steps = 726, loss = 0.32825613021850586
In grad_steps = 727, loss = 0.3934467136859894
In grad_steps = 728, loss = 0.3418716490268707
In grad_steps = 729, loss = 0.26405763626098633
In grad_steps = 730, loss = 0.18940141797065735
In grad_steps = 731, loss = 0.24557262659072876
In grad_steps = 732, loss = 0.22337743639945984
In grad_steps = 733, loss = 0.5179826617240906
In grad_steps = 734, loss = 0.3382609486579895
In grad_steps = 735, loss = 0.5350995659828186
In grad_steps = 736, loss = 1.6163458824157715
In grad_steps = 737, loss = 0.5032097101211548
In grad_steps = 738, loss = 0.4588109254837036
In grad_steps = 739, loss = 0.2060532420873642
In grad_steps = 740, loss = 0.3969537019729614
In grad_steps = 741, loss = 0.2970719039440155
In grad_steps = 742, loss = 0.41075965762138367
In grad_steps = 743, loss = 0.3293013274669647
In grad_steps = 744, loss = 0.28470298647880554
In grad_steps = 745, loss = 0.04018267244100571
In grad_steps = 746, loss = 0.4783439040184021
In grad_steps = 747, loss = 0.18242663145065308
In grad_steps = 748, loss = 0.2711985111236572
In grad_steps = 749, loss = 0.2957897484302521
In grad_steps = 750, loss = 1.1393778324127197
In grad_steps = 751, loss = 0.6687140464782715
In grad_steps = 752, loss = 0.3859475255012512
In grad_steps = 753, loss = 0.10032004863023758
In grad_steps = 754, loss = 0.717261016368866
In grad_steps = 755, loss = 0.10251384973526001
In grad_steps = 756, loss = 0.07524493336677551
In grad_steps = 757, loss = 0.08098844438791275
In grad_steps = 758, loss = 0.23569950461387634
In grad_steps = 759, loss = 0.6958354115486145
In grad_steps = 760, loss = 0.13518598675727844
In grad_steps = 761, loss = 0.03867363557219505
In grad_steps = 762, loss = 0.31984248757362366
In grad_steps = 763, loss = 0.4754014313220978
In grad_steps = 764, loss = 0.33602428436279297
In grad_steps = 765, loss = 0.0950000062584877
In grad_steps = 766, loss = 0.19880519807338715
In grad_steps = 767, loss = 0.3518446683883667
In grad_steps = 768, loss = 0.05540734902024269
In grad_steps = 769, loss = 0.8352429866790771
In grad_steps = 770, loss = 0.6013062000274658
In grad_steps = 771, loss = 0.711001992225647
In grad_steps = 772, loss = 0.6100044846534729
In grad_steps = 773, loss = 0.20799043774604797
In grad_steps = 774, loss = 0.14129279553890228
In grad_steps = 775, loss = 0.06639906764030457
In grad_steps = 776, loss = 0.0743098109960556
In grad_steps = 777, loss = 0.6256058216094971
In grad_steps = 778, loss = 0.5125443339347839
In grad_steps = 779, loss = 0.6923633813858032
In grad_steps = 780, loss = 0.15094980597496033
In grad_steps = 781, loss = 0.41164398193359375
In grad_steps = 782, loss = 0.42133569717407227
In grad_steps = 783, loss = 0.6314035654067993
In grad_steps = 784, loss = 0.9336685538291931
In grad_steps = 785, loss = 0.6701363325119019
In grad_steps = 786, loss = 0.37828588485717773
In grad_steps = 787, loss = 0.40370410680770874
In grad_steps = 788, loss = 0.22919650375843048
In grad_steps = 789, loss = 0.3655349314212799
In grad_steps = 790, loss = 0.7530194520950317
In grad_steps = 791, loss = 0.2215314507484436
In grad_steps = 792, loss = 0.28957220911979675
In grad_steps = 793, loss = 0.22211652994155884
In grad_steps = 794, loss = 0.21920886635780334
In grad_steps = 795, loss = 0.25469303131103516
In grad_steps = 796, loss = 0.7738931179046631
In grad_steps = 797, loss = 0.15005117654800415
In grad_steps = 798, loss = 0.12024831771850586
In grad_steps = 799, loss = 0.22455519437789917
In grad_steps = 800, loss = 0.1716609001159668
In grad_steps = 801, loss = 0.12284807860851288
In grad_steps = 802, loss = 0.08172183483839035
In grad_steps = 803, loss = 0.1134280115365982
In grad_steps = 804, loss = 0.025113705545663834
In grad_steps = 805, loss = 0.10313688218593597
In grad_steps = 806, loss = 1.242598056793213
In grad_steps = 807, loss = 0.32193195819854736
In grad_steps = 808, loss = 0.23278002440929413
In grad_steps = 809, loss = 0.9332507252693176
In grad_steps = 810, loss = 0.10050938278436661
In grad_steps = 811, loss = 0.023562027141451836
In grad_steps = 812, loss = 0.7475539445877075
In grad_steps = 813, loss = 0.061597809195518494
In grad_steps = 814, loss = 0.07337865978479385
In grad_steps = 815, loss = 0.03824032470583916
In grad_steps = 816, loss = 0.04765366390347481
In grad_steps = 817, loss = 1.2038381099700928
In grad_steps = 818, loss = 1.3382000923156738
In grad_steps = 819, loss = 0.895675003528595
In grad_steps = 820, loss = 0.10535240173339844
In grad_steps = 821, loss = 0.34203922748565674
In grad_steps = 822, loss = 0.7906372547149658
In grad_steps = 823, loss = 0.6919438242912292
In grad_steps = 824, loss = 0.2577350437641144
In grad_steps = 825, loss = 0.17693085968494415
In grad_steps = 826, loss = 0.2327277958393097
In grad_steps = 827, loss = 0.4830619692802429
In grad_steps = 828, loss = 0.3614775538444519
In grad_steps = 829, loss = 0.32451534271240234
In grad_steps = 830, loss = 0.4362841844558716
In grad_steps = 831, loss = 0.12936754524707794
In grad_steps = 832, loss = 0.2437390387058258
In grad_steps = 833, loss = 0.6402188539505005
In grad_steps = 834, loss = 0.3466050922870636
In grad_steps = 835, loss = 0.2464682161808014
In grad_steps = 836, loss = 0.5675610303878784
In grad_steps = 837, loss = 0.5538588762283325
In grad_steps = 838, loss = 0.9935281276702881
In grad_steps = 839, loss = 0.17348676919937134
In grad_steps = 840, loss = 0.17808032035827637
In grad_steps = 841, loss = 0.07844845950603485
In grad_steps = 842, loss = 0.429798424243927
In grad_steps = 843, loss = 0.933185338973999
In grad_steps = 844, loss = 0.5799683332443237
In grad_steps = 845, loss = 0.5770926475524902
In grad_steps = 846, loss = 0.6328728795051575
In grad_steps = 847, loss = 0.3086335361003876
In grad_steps = 848, loss = 0.1405944973230362
In grad_steps = 849, loss = 0.23214814066886902
In grad_steps = 850, loss = 0.3081324100494385
In grad_steps = 851, loss = 0.23749719560146332
In grad_steps = 852, loss = 0.5011278390884399
In grad_steps = 853, loss = 0.9003117680549622
In grad_steps = 854, loss = 0.18023961782455444
In grad_steps = 855, loss = 0.36358118057250977
In grad_steps = 856, loss = 0.146744966506958
In grad_steps = 857, loss = 0.2865552306175232
In grad_steps = 858, loss = 0.36348509788513184
In grad_steps = 859, loss = 0.26723653078079224
In grad_steps = 860, loss = 0.28180554509162903
In grad_steps = 861, loss = 0.8739171624183655
In grad_steps = 862, loss = 0.06815127283334732
In grad_steps = 863, loss = 0.2826308012008667
In grad_steps = 864, loss = 0.3776969909667969
In grad_steps = 865, loss = 0.16224434971809387
In grad_steps = 866, loss = 0.49776825308799744
In grad_steps = 867, loss = 0.09091601520776749
In grad_steps = 868, loss = 0.1548551321029663
In grad_steps = 869, loss = 0.15919716656208038
In grad_steps = 870, loss = 0.6336869597434998
In grad_steps = 871, loss = 0.04480281099677086
In grad_steps = 872, loss = 0.34793782234191895
In grad_steps = 873, loss = 0.17964057624340057
In grad_steps = 874, loss = 0.5571320652961731
In grad_steps = 875, loss = 0.18917952477931976
In grad_steps = 876, loss = 0.07595495879650116
In grad_steps = 877, loss = 0.05504561588168144
In grad_steps = 878, loss = 0.870405375957489
In grad_steps = 879, loss = 0.878411591053009
In grad_steps = 880, loss = 0.1789192408323288
In grad_steps = 881, loss = 0.8400975465774536
In grad_steps = 882, loss = 0.10560809075832367
In grad_steps = 883, loss = 0.7086539268493652
In grad_steps = 884, loss = 0.6737867593765259
In grad_steps = 885, loss = 0.2508847117424011
In grad_steps = 886, loss = 0.38964638113975525
In grad_steps = 887, loss = 0.5403887033462524
In grad_steps = 888, loss = 0.32403764128685
In grad_steps = 889, loss = 0.19249063730239868
In grad_steps = 890, loss = 0.1248025894165039
In grad_steps = 891, loss = 0.7408656477928162
In grad_steps = 892, loss = 0.5373062491416931
In grad_steps = 893, loss = 0.6223682165145874
In grad_steps = 894, loss = 0.10871405899524689
In grad_steps = 895, loss = 0.20112429559230804
In grad_steps = 896, loss = 0.4451240301132202
In grad_steps = 897, loss = 0.19651466608047485
In grad_steps = 898, loss = 0.9827210307121277
In grad_steps = 899, loss = 0.477378785610199
In grad_steps = 900, loss = 0.7958081364631653
In grad_steps = 901, loss = 0.44496631622314453
In grad_steps = 902, loss = 0.21171315014362335
In grad_steps = 903, loss = 0.676828920841217
In grad_steps = 904, loss = 0.4580431282520294
In grad_steps = 905, loss = 0.18840450048446655
In grad_steps = 906, loss = 0.6235275268554688
In grad_steps = 907, loss = 0.33690276741981506
In grad_steps = 908, loss = 0.17383363842964172
In grad_steps = 909, loss = 0.806121826171875
In grad_steps = 910, loss = 0.23554188013076782
In grad_steps = 911, loss = 0.44980907440185547
In grad_steps = 912, loss = 0.3505917191505432
In grad_steps = 913, loss = 0.43099918961524963
In grad_steps = 914, loss = 0.5492741465568542
In grad_steps = 915, loss = 0.7692931294441223
In grad_steps = 916, loss = 0.2453659325838089
In grad_steps = 917, loss = 0.543850839138031
In grad_steps = 918, loss = 0.42995399236679077
In grad_steps = 919, loss = 0.1870286911725998
In grad_steps = 920, loss = 0.29318463802337646
In grad_steps = 921, loss = 0.8977285623550415
In grad_steps = 922, loss = 0.5592204928398132
In grad_steps = 923, loss = 0.17679262161254883
In grad_steps = 924, loss = 0.18079406023025513
In grad_steps = 925, loss = 0.20971672236919403
In grad_steps = 926, loss = 0.4485231041908264
In grad_steps = 927, loss = 0.11042051762342453
In grad_steps = 928, loss = 0.691565215587616
In grad_steps = 929, loss = 0.15891845524311066
In grad_steps = 930, loss = 0.7572240829467773
In grad_steps = 931, loss = 0.11793160438537598
In grad_steps = 932, loss = 0.07616132497787476
In grad_steps = 933, loss = 0.526999831199646
In grad_steps = 934, loss = 0.13807299733161926
In grad_steps = 935, loss = 1.0861530303955078
In grad_steps = 936, loss = 0.429787278175354
In grad_steps = 937, loss = 0.0916793942451477
In grad_steps = 938, loss = 0.39822882413864136
In grad_steps = 939, loss = 0.26187172532081604
In grad_steps = 940, loss = 0.13357563316822052
In grad_steps = 941, loss = 0.17813366651535034
In grad_steps = 942, loss = 0.25465288758277893
In grad_steps = 943, loss = 1.2647614479064941
In grad_steps = 944, loss = 0.5924360752105713
In grad_steps = 945, loss = 0.12497670948505402
In grad_steps = 946, loss = 0.8195784091949463
In grad_steps = 947, loss = 0.2980203628540039
In grad_steps = 948, loss = 0.3608633577823639
In grad_steps = 949, loss = 0.42745697498321533
In grad_steps = 950, loss = 0.8012447953224182
In grad_steps = 951, loss = 1.1470677852630615
In grad_steps = 952, loss = 0.1194484606385231
In grad_steps = 953, loss = 0.2741152048110962
In grad_steps = 954, loss = 0.13863788545131683
In grad_steps = 955, loss = 0.3980739414691925
In grad_steps = 956, loss = 0.3900989592075348
In grad_steps = 957, loss = 0.466562956571579
In grad_steps = 958, loss = 0.4865191876888275
In grad_steps = 959, loss = 1.3498954772949219
In grad_steps = 960, loss = 0.17940478026866913
In grad_steps = 961, loss = 0.5140772461891174
In grad_steps = 962, loss = 0.16808943450450897
In grad_steps = 963, loss = 0.22667451202869415
In grad_steps = 964, loss = 0.31425195932388306
In grad_steps = 965, loss = 0.4052964448928833
In grad_steps = 966, loss = 0.4006125330924988
In grad_steps = 967, loss = 0.7519258856773376
In grad_steps = 968, loss = 0.5980592966079712
In grad_steps = 969, loss = 0.6672401428222656
In grad_steps = 970, loss = 0.12439782172441483
In grad_steps = 971, loss = 0.4203851819038391
In grad_steps = 972, loss = 0.40451350808143616
In grad_steps = 973, loss = 0.16958096623420715
In grad_steps = 974, loss = 0.3905909061431885
In grad_steps = 975, loss = 0.17814166843891144
In grad_steps = 976, loss = 0.3583093583583832
In grad_steps = 977, loss = 0.3823406994342804
In grad_steps = 978, loss = 0.8564842343330383
In grad_steps = 979, loss = 0.7521229386329651
In grad_steps = 980, loss = 0.9221512675285339
In grad_steps = 981, loss = 0.44190841913223267
In grad_steps = 982, loss = 0.6278693079948425
In grad_steps = 983, loss = 0.47521325945854187
In grad_steps = 984, loss = 0.6283780932426453
In grad_steps = 985, loss = 0.4655187129974365
In grad_steps = 986, loss = 0.25472092628479004
In grad_steps = 987, loss = 0.8358123302459717
In grad_steps = 988, loss = 0.5886174440383911
In grad_steps = 989, loss = 0.5019151568412781
In grad_steps = 990, loss = 0.6555685997009277
In grad_steps = 991, loss = 0.6755484342575073
In grad_steps = 992, loss = 0.3540494441986084
In grad_steps = 993, loss = 0.6566393375396729
In grad_steps = 994, loss = 0.4844907522201538
In grad_steps = 995, loss = 0.21973511576652527
In grad_steps = 996, loss = 0.6896679997444153
In grad_steps = 997, loss = 0.16230884194374084
In grad_steps = 998, loss = 0.36455070972442627
In grad_steps = 999, loss = 0.0752309188246727
In grad_steps = 1000, loss = 0.09853073954582214
In grad_steps = 1001, loss = 0.5662099123001099
In grad_steps = 1002, loss = 0.22967615723609924
In grad_steps = 1003, loss = 0.1747702807188034
In grad_steps = 1004, loss = 0.3405389189720154
In grad_steps = 1005, loss = 0.7230998277664185
In grad_steps = 1006, loss = 0.21095702052116394
In grad_steps = 1007, loss = 0.42697980999946594
In grad_steps = 1008, loss = 0.36513230204582214
In grad_steps = 1009, loss = 1.0282008647918701
In grad_steps = 1010, loss = 0.8167914152145386
In grad_steps = 1011, loss = 0.10832639038562775
In grad_steps = 1012, loss = 0.15367938578128815
In grad_steps = 1013, loss = 0.5815101861953735
In grad_steps = 1014, loss = 0.1822054088115692
In grad_steps = 1015, loss = 0.4472276568412781
In grad_steps = 1016, loss = 0.06564100831747055
In grad_steps = 1017, loss = 0.11051222681999207
In grad_steps = 1018, loss = 0.24571499228477478
In grad_steps = 1019, loss = 0.6888992786407471
In grad_steps = 1020, loss = 1.670396089553833
In grad_steps = 1021, loss = 0.560259997844696
In grad_steps = 1022, loss = 0.37711799144744873
In grad_steps = 1023, loss = 0.12649418413639069
In grad_steps = 1024, loss = 0.19519522786140442
In grad_steps = 1025, loss = 0.5654861330986023
In grad_steps = 1026, loss = 0.2794741988182068
In grad_steps = 1027, loss = 0.8765104413032532
In grad_steps = 1028, loss = 0.11922456324100494
In grad_steps = 1029, loss = 0.13979217410087585
In grad_steps = 1030, loss = 0.272795706987381
In grad_steps = 1031, loss = 0.4771951138973236
In grad_steps = 1032, loss = 0.593430757522583
In grad_steps = 1033, loss = 0.4803660809993744
In grad_steps = 1034, loss = 0.19343742728233337
In grad_steps = 1035, loss = 0.6277021765708923
In grad_steps = 1036, loss = 0.6530638337135315
In grad_steps = 1037, loss = 0.22786766290664673
In grad_steps = 1038, loss = 0.46615633368492126
In grad_steps = 1039, loss = 0.7173733115196228
In grad_steps = 1040, loss = 0.30158767104148865
In grad_steps = 1041, loss = 0.5098637938499451
In grad_steps = 1042, loss = 0.56575608253479
In grad_steps = 1043, loss = 0.7366114854812622
In grad_steps = 1044, loss = 0.3957764804363251
In grad_steps = 1045, loss = 0.4426061511039734
In grad_steps = 1046, loss = 0.09014566242694855
In grad_steps = 1047, loss = 0.13923686742782593
In grad_steps = 1048, loss = 0.3945816159248352
In grad_steps = 1049, loss = 0.2757617235183716
In grad_steps = 1050, loss = 1.2416679859161377
In grad_steps = 1051, loss = 0.09774042665958405
In grad_steps = 1052, loss = 0.17941202223300934
In grad_steps = 1053, loss = 0.6125544309616089
In grad_steps = 1054, loss = 0.1143956407904625
In grad_steps = 1055, loss = 0.3802657723426819
In grad_steps = 1056, loss = 0.9925143718719482
In grad_steps = 1057, loss = 0.4601683020591736
In grad_steps = 1058, loss = 0.2609376013278961
In grad_steps = 1059, loss = 0.1488024890422821
In grad_steps = 1060, loss = 0.22883166372776031
In grad_steps = 1061, loss = 0.2995627820491791
In grad_steps = 1062, loss = 1.090536117553711
In grad_steps = 1063, loss = 0.20449599623680115
In grad_steps = 1064, loss = 0.32225701212882996
In grad_steps = 1065, loss = 0.04161414876580238
In grad_steps = 1066, loss = 0.6861547231674194
In grad_steps = 1067, loss = 0.5893121361732483
In grad_steps = 1068, loss = 0.15271013975143433
In grad_steps = 1069, loss = 0.27432215213775635
In grad_steps = 1070, loss = 0.21401368081569672
In grad_steps = 1071, loss = 0.09427747130393982
In grad_steps = 1072, loss = 0.09476877748966217
In grad_steps = 1073, loss = 0.37011492252349854
In grad_steps = 1074, loss = 0.2350720763206482
In grad_steps = 1075, loss = 0.45063772797584534
In grad_steps = 1076, loss = 0.22539353370666504
In grad_steps = 1077, loss = 0.18906071782112122
In grad_steps = 1078, loss = 0.6462417244911194
In grad_steps = 1079, loss = 0.1261628419160843
In grad_steps = 1080, loss = 0.07248826324939728
In grad_steps = 1081, loss = 0.8375838994979858
In grad_steps = 1082, loss = 0.10708653926849365
In grad_steps = 1083, loss = 0.0686308741569519
In grad_steps = 1084, loss = 0.47972893714904785
In grad_steps = 1085, loss = 1.305149793624878
In grad_steps = 1086, loss = 0.10202377289533615
In grad_steps = 1087, loss = 0.8203545808792114
In grad_steps = 1088, loss = 0.3819723427295685
In grad_steps = 1089, loss = 0.1253616362810135
In grad_steps = 1090, loss = 0.06338199973106384
In grad_steps = 1091, loss = 0.5235538482666016
In grad_steps = 1092, loss = 0.04801924154162407
In grad_steps = 1093, loss = 0.7605800032615662
In grad_steps = 1094, loss = 0.05085944011807442
In grad_steps = 1095, loss = 0.396087110042572
In grad_steps = 1096, loss = 0.14140355587005615
In grad_steps = 1097, loss = 0.1192510575056076
In grad_steps = 1098, loss = 0.33862730860710144
In grad_steps = 1099, loss = 0.5769548416137695
In grad_steps = 1100, loss = 0.19273561239242554
In grad_steps = 1101, loss = 0.5529907941818237
In grad_steps = 1102, loss = 0.8122654557228088
In grad_steps = 1103, loss = 0.03323591127991676
In grad_steps = 1104, loss = 1.3652629852294922
In grad_steps = 1105, loss = 0.3504493832588196
In grad_steps = 1106, loss = 0.3965110778808594
In grad_steps = 1107, loss = 0.15146486461162567
In grad_steps = 1108, loss = 0.5527466535568237
In grad_steps = 1109, loss = 0.33549728989601135
In grad_steps = 1110, loss = 0.3705615699291229
In grad_steps = 1111, loss = 0.3937256634235382
In grad_steps = 1112, loss = 0.5766907930374146
In grad_steps = 1113, loss = 0.3575919270515442
In grad_steps = 1114, loss = 0.2758723795413971
In grad_steps = 1115, loss = 0.11903662979602814
In grad_steps = 1116, loss = 0.6316373348236084
In grad_steps = 1117, loss = 0.3489617705345154
In grad_steps = 1118, loss = 0.4413324296474457
In grad_steps = 1119, loss = 0.5540207028388977
In grad_steps = 1120, loss = 0.0928945243358612
In grad_steps = 1121, loss = 0.18015016615390778
In grad_steps = 1122, loss = 0.18807347118854523
In grad_steps = 1123, loss = 0.424473375082016
In grad_steps = 1124, loss = 0.423602432012558
In grad_steps = 1125, loss = 0.20128518342971802
In grad_steps = 1126, loss = 0.17311173677444458
In grad_steps = 1127, loss = 0.22225068509578705
In grad_steps = 1128, loss = 0.7783006429672241
In grad_steps = 1129, loss = 0.2065524011850357
In grad_steps = 1130, loss = 0.1430778205394745
In grad_steps = 1131, loss = 0.1675199568271637
In grad_steps = 1132, loss = 0.25660276412963867
In grad_steps = 1133, loss = 0.7100375294685364
In grad_steps = 1134, loss = 0.17243248224258423
In grad_steps = 1135, loss = 0.25935426354408264
In grad_steps = 1136, loss = 0.7139905095100403
In grad_steps = 1137, loss = 0.1640724539756775
In grad_steps = 1138, loss = 0.18239207565784454
In grad_steps = 1139, loss = 0.7749136686325073
In grad_steps = 1140, loss = 0.10904599726200104
In grad_steps = 1141, loss = 1.1575576066970825
In grad_steps = 1142, loss = 0.24491657316684723
In grad_steps = 1143, loss = 0.6617932915687561
In grad_steps = 1144, loss = 0.01695880852639675
In grad_steps = 1145, loss = 0.38961878418922424
In grad_steps = 1146, loss = 1.5495853424072266
In grad_steps = 1147, loss = 0.5673027038574219
In grad_steps = 1148, loss = 0.289869099855423
In grad_steps = 1149, loss = 0.07607363164424896
In grad_steps = 1150, loss = 0.4938608705997467
In grad_steps = 1151, loss = 0.4023323059082031
In grad_steps = 1152, loss = 0.49691516160964966
In grad_steps = 1153, loss = 0.7560113668441772
In grad_steps = 1154, loss = 0.3525320589542389
In grad_steps = 1155, loss = 0.3606001138687134
In grad_steps = 1156, loss = 0.32146334648132324
In grad_steps = 1157, loss = 0.2747715711593628
In grad_steps = 1158, loss = 0.43157416582107544
In grad_steps = 1159, loss = 0.12151309847831726
In grad_steps = 1160, loss = 0.4018232822418213
In grad_steps = 1161, loss = 0.546022891998291
In grad_steps = 1162, loss = 0.26482415199279785
In grad_steps = 1163, loss = 0.43010202050209045
In grad_steps = 1164, loss = 0.16482210159301758
In grad_steps = 1165, loss = 0.31072747707366943
In grad_steps = 1166, loss = 1.4731194972991943
In grad_steps = 1167, loss = 0.9091680645942688
In grad_steps = 1168, loss = 0.4557024836540222
In grad_steps = 1169, loss = 0.0769767016172409
In grad_steps = 1170, loss = 0.2526889443397522
In grad_steps = 1171, loss = 0.41821885108947754
In grad_steps = 1172, loss = 0.34072908759117126
In grad_steps = 1173, loss = 0.37582579255104065
In grad_steps = 1174, loss = 0.2694627046585083
In grad_steps = 1175, loss = 0.633266031742096
In grad_steps = 1176, loss = 0.51142817735672
In grad_steps = 1177, loss = 0.11284378916025162
In grad_steps = 1178, loss = 0.8268611431121826
In grad_steps = 1179, loss = 0.092691570520401
In grad_steps = 1180, loss = 0.3123495578765869
In grad_steps = 1181, loss = 0.44519633054733276
In grad_steps = 1182, loss = 1.0391241312026978
In grad_steps = 1183, loss = 0.5134510397911072
In grad_steps = 1184, loss = 0.8174499273300171
In grad_steps = 1185, loss = 0.40632808208465576
In grad_steps = 1186, loss = 0.2668282389640808
In grad_steps = 1187, loss = 0.26296406984329224
In grad_steps = 1188, loss = 0.6949930787086487
In grad_steps = 1189, loss = 0.20486661791801453
In grad_steps = 1190, loss = 0.1779295951128006
In grad_steps = 1191, loss = 0.1831786334514618
In grad_steps = 1192, loss = 0.30098360776901245
In grad_steps = 1193, loss = 0.20338940620422363
In grad_steps = 1194, loss = 0.2576095461845398
In grad_steps = 1195, loss = 0.5024922490119934
In grad_steps = 1196, loss = 0.1335756480693817
In grad_steps = 1197, loss = 0.690800666809082
In grad_steps = 1198, loss = 0.11565134674310684
In grad_steps = 1199, loss = 0.9632435441017151
In grad_steps = 1200, loss = 0.1815333515405655
In grad_steps = 1201, loss = 0.10223443806171417
In grad_steps = 1202, loss = 0.3102758228778839
In grad_steps = 1203, loss = 0.07534971833229065
In grad_steps = 1204, loss = 0.10517057776451111
In grad_steps = 1205, loss = 1.1639829874038696
In grad_steps = 1206, loss = 1.2671213150024414
In grad_steps = 1207, loss = 0.10058648139238358
In grad_steps = 1208, loss = 0.40461188554763794
In grad_steps = 1209, loss = 0.6056555509567261
In grad_steps = 1210, loss = 0.6096236109733582
In grad_steps = 1211, loss = 0.506493866443634
In grad_steps = 1212, loss = 0.11733365803956985
In grad_steps = 1213, loss = 0.7111203074455261
In grad_steps = 1214, loss = 0.36083725094795227
In grad_steps = 1215, loss = 0.44292595982551575
In grad_steps = 1216, loss = 0.1816920042037964
In grad_steps = 1217, loss = 0.07285243272781372
In grad_steps = 1218, loss = 0.5518946051597595
In grad_steps = 1219, loss = 0.27814608812332153
In grad_steps = 1220, loss = 0.49132663011550903
In grad_steps = 1221, loss = 0.5132874250411987
In grad_steps = 1222, loss = 0.5032471418380737
In grad_steps = 1223, loss = 0.49246686697006226
In grad_steps = 1224, loss = 1.0845527648925781
In grad_steps = 1225, loss = 0.21399684250354767
In grad_steps = 1226, loss = 0.3853103816509247
In grad_steps = 1227, loss = 0.30605992674827576
In grad_steps = 1228, loss = 0.21403327584266663
In grad_steps = 1229, loss = 1.2008970975875854
In grad_steps = 1230, loss = 0.3194299340248108
In grad_steps = 1231, loss = 0.1697879433631897
In grad_steps = 1232, loss = 0.07276859134435654
In grad_steps = 1233, loss = 0.40336793661117554
In grad_steps = 1234, loss = 0.3710501790046692
In grad_steps = 1235, loss = 0.3225724399089813
In grad_steps = 1236, loss = 0.8464186191558838
In grad_steps = 1237, loss = 0.5717877149581909
In grad_steps = 1238, loss = 0.1900317370891571
In grad_steps = 1239, loss = 0.38265156745910645
In grad_steps = 1240, loss = 0.14165987074375153
In grad_steps = 1241, loss = 0.10991962999105453
In grad_steps = 1242, loss = 0.3556801676750183
In grad_steps = 1243, loss = 0.2238871455192566
In grad_steps = 1244, loss = 0.6604911684989929
In grad_steps = 1245, loss = 0.5390982031822205
In grad_steps = 1246, loss = 0.17715048789978027
In grad_steps = 1247, loss = 0.19371500611305237
In grad_steps = 1248, loss = 0.6485844254493713
In grad_steps = 1249, loss = 0.3454055190086365
In grad_steps = 1250, loss = 0.09930238127708435
In grad_steps = 1251, loss = 0.27631875872612
In grad_steps = 1252, loss = 0.06343735009431839
In grad_steps = 1253, loss = 0.11375844478607178
In grad_steps = 1254, loss = 0.18849743902683258
In grad_steps = 1255, loss = 1.1930216550827026
In grad_steps = 1256, loss = 0.9809771180152893
In grad_steps = 1257, loss = 0.6697863936424255
In grad_steps = 1258, loss = 0.6627398133277893
In grad_steps = 1259, loss = 0.07034087181091309
In grad_steps = 1260, loss = 0.3635260760784149
In grad_steps = 1261, loss = 1.1086848974227905
In grad_steps = 1262, loss = 0.5137584209442139
In grad_steps = 1263, loss = 0.05778764188289642
In grad_steps = 1264, loss = 0.22899416089057922
In grad_steps = 1265, loss = 0.310194730758667
In grad_steps = 1266, loss = 0.7632039189338684
In grad_steps = 1267, loss = 0.74294513463974
In grad_steps = 1268, loss = 0.707470178604126
In grad_steps = 1269, loss = 0.10377511382102966
In grad_steps = 1270, loss = 0.1272183507680893
In grad_steps = 1271, loss = 0.6562246084213257
In grad_steps = 1272, loss = 0.6089156270027161
In grad_steps = 1273, loss = 0.2594888210296631
In grad_steps = 1274, loss = 0.3404388725757599
In grad_steps = 1275, loss = 0.20598676800727844
In grad_steps = 1276, loss = 0.37577003240585327
In grad_steps = 1277, loss = 0.19114813208580017
In grad_steps = 1278, loss = 0.23817026615142822
In grad_steps = 1279, loss = 0.20157523453235626
In grad_steps = 1280, loss = 0.1629178673028946
In grad_steps = 1281, loss = 0.43642812967300415
In grad_steps = 1282, loss = 0.6715594530105591
In grad_steps = 1283, loss = 0.4829524755477905
In grad_steps = 1284, loss = 0.15802480280399323
In grad_steps = 1285, loss = 0.25193890929222107
In grad_steps = 1286, loss = 0.1789971888065338
In grad_steps = 1287, loss = 0.10906178504228592
In grad_steps = 1288, loss = 0.23820967972278595
In grad_steps = 1289, loss = 0.1499967873096466
In grad_steps = 1290, loss = 0.040840033441782
In grad_steps = 1291, loss = 0.16986224055290222
In grad_steps = 1292, loss = 1.0876960754394531
In grad_steps = 1293, loss = 0.10618278384208679
In grad_steps = 1294, loss = 0.6648812294006348
In grad_steps = 1295, loss = 0.25627678632736206
In grad_steps = 1296, loss = 0.10352181643247604
In grad_steps = 1297, loss = 0.0759480893611908
In grad_steps = 1298, loss = 0.198725625872612
In grad_steps = 1299, loss = 0.291562020778656
In grad_steps = 1300, loss = 0.281581848859787
In grad_steps = 1301, loss = 0.1910495162010193
In grad_steps = 1302, loss = 0.025062192231416702
In grad_steps = 1303, loss = 0.5981234312057495
In grad_steps = 1304, loss = 0.034377291798591614
In grad_steps = 1305, loss = 0.6666730046272278
In grad_steps = 1306, loss = 0.8329582810401917
In grad_steps = 1307, loss = 0.20506830513477325
In grad_steps = 1308, loss = 1.234053373336792
In grad_steps = 1309, loss = 0.4092216491699219
In grad_steps = 1310, loss = 0.28757378458976746
In grad_steps = 1311, loss = 0.20761731266975403
In grad_steps = 1312, loss = 0.15125888586044312
In grad_steps = 1313, loss = 0.1382458209991455
In grad_steps = 1314, loss = 0.10051971673965454
In grad_steps = 1315, loss = 0.16843196749687195
In grad_steps = 1316, loss = 1.1171398162841797
In grad_steps = 1317, loss = 0.9858095645904541
In grad_steps = 1318, loss = 0.4402550160884857
In grad_steps = 1319, loss = 0.2547852694988251
In grad_steps = 1320, loss = 0.3982601463794708
In grad_steps = 1321, loss = 0.16004085540771484
In grad_steps = 1322, loss = 0.5487990975379944
In grad_steps = 1323, loss = 0.49251633882522583
In grad_steps = 1324, loss = 0.20348815619945526
In grad_steps = 1325, loss = 0.11748451739549637
In grad_steps = 1326, loss = 0.1359080970287323
In grad_steps = 1327, loss = 0.8101131319999695
In grad_steps = 1328, loss = 0.08173436671495438
In grad_steps = 1329, loss = 0.7148141264915466
In grad_steps = 1330, loss = 0.6858497858047485
In grad_steps = 1331, loss = 0.4484882950782776
In grad_steps = 1332, loss = 0.10637751966714859
In grad_steps = 1333, loss = 0.5920095443725586
In grad_steps = 1334, loss = 0.16872724890708923
In grad_steps = 1335, loss = 0.5259473919868469
In grad_steps = 1336, loss = 0.3407452404499054
In grad_steps = 1337, loss = 0.2097933292388916
In grad_steps = 1338, loss = 0.07883560657501221
In grad_steps = 1339, loss = 0.5552043914794922
In grad_steps = 1340, loss = 0.4652411639690399
In grad_steps = 1341, loss = 0.2182040512561798
In grad_steps = 1342, loss = 0.3430555462837219
In grad_steps = 1343, loss = 0.4662529528141022
In grad_steps = 1344, loss = 0.15356750786304474
In grad_steps = 1345, loss = 0.8553764820098877
In grad_steps = 1346, loss = 0.2790924310684204
In grad_steps = 1347, loss = 0.40670597553253174
In grad_steps = 1348, loss = 0.42820852994918823
In grad_steps = 1349, loss = 0.009329135529696941
In grad_steps = 1350, loss = 0.575242817401886
In grad_steps = 1351, loss = 0.5743157863616943
In grad_steps = 1352, loss = 0.16067998111248016
In grad_steps = 1353, loss = 0.8559854626655579
In grad_steps = 1354, loss = 0.16004298627376556
In grad_steps = 1355, loss = 0.7463434934616089
In grad_steps = 1356, loss = 0.1211567372083664
In grad_steps = 1357, loss = 0.4805402457714081
In grad_steps = 1358, loss = 0.3753888010978699
In grad_steps = 1359, loss = 0.16620497405529022
In grad_steps = 1360, loss = 0.17081205546855927
In grad_steps = 1361, loss = 0.7082772254943848
In grad_steps = 1362, loss = 0.06538406014442444
In grad_steps = 1363, loss = 0.19540923833847046
In grad_steps = 1364, loss = 0.37366607785224915
In grad_steps = 1365, loss = 0.33796417713165283
In grad_steps = 1366, loss = 0.37262487411499023
In grad_steps = 1367, loss = 0.04778271168470383
In grad_steps = 1368, loss = 0.7664507031440735
In grad_steps = 1369, loss = 0.1365259736776352
In grad_steps = 1370, loss = 0.09095929563045502
In grad_steps = 1371, loss = 0.2299075424671173
In grad_steps = 1372, loss = 0.5905691981315613
In grad_steps = 1373, loss = 0.061418481171131134
In grad_steps = 1374, loss = 0.9957081079483032
In grad_steps = 1375, loss = 0.03334540128707886
In grad_steps = 1376, loss = 0.516261875629425
In grad_steps = 1377, loss = 0.06181953847408295
In grad_steps = 1378, loss = 0.5773008465766907
In grad_steps = 1379, loss = 0.1902063488960266
In grad_steps = 1380, loss = 0.2987937927246094
In grad_steps = 1381, loss = 0.0721282809972763
In grad_steps = 1382, loss = 0.17621174454689026
In grad_steps = 1383, loss = 0.7670273780822754
In grad_steps = 1384, loss = 0.139840230345726
In grad_steps = 1385, loss = 1.4166691303253174
In grad_steps = 1386, loss = 0.5286471843719482
In grad_steps = 1387, loss = 0.23699957132339478
In grad_steps = 1388, loss = 0.07586579024791718
In grad_steps = 1389, loss = 0.08673637360334396
In grad_steps = 1390, loss = 0.05240057408809662
In grad_steps = 1391, loss = 0.6098421812057495
In grad_steps = 1392, loss = 0.2136136293411255
In grad_steps = 1393, loss = 0.14012795686721802
In grad_steps = 1394, loss = 0.1315682977437973
In grad_steps = 1395, loss = 0.7086228728294373
In grad_steps = 1396, loss = 0.659203827381134
In grad_steps = 1397, loss = 0.06005850434303284
In grad_steps = 1398, loss = 0.4956846535205841
In grad_steps = 1399, loss = 0.08030014485120773
In grad_steps = 1400, loss = 0.6234706044197083
In grad_steps = 1401, loss = 0.31625011563301086
In grad_steps = 1402, loss = 0.17177149653434753
In grad_steps = 1403, loss = 0.667008101940155
In grad_steps = 1404, loss = 0.5109904408454895
In grad_steps = 1405, loss = 0.10451433807611465
In grad_steps = 1406, loss = 0.48347151279449463
In grad_steps = 1407, loss = 0.2604498565196991
In grad_steps = 1408, loss = 0.40213626623153687
In grad_steps = 1409, loss = 0.2358255684375763
In grad_steps = 1410, loss = 0.3179187774658203
In grad_steps = 1411, loss = 0.5294842720031738
In grad_steps = 1412, loss = 0.1931053102016449
In grad_steps = 1413, loss = 1.087096095085144
In grad_steps = 1414, loss = 0.6386864185333252
In grad_steps = 1415, loss = 0.1434372067451477
In grad_steps = 1416, loss = 0.20680712163448334
In grad_steps = 1417, loss = 0.15200507640838623
In grad_steps = 1418, loss = 0.06426265835762024
In grad_steps = 1419, loss = 0.07761788368225098
In grad_steps = 1420, loss = 0.30547329783439636
In grad_steps = 1421, loss = 0.10633742064237595
In grad_steps = 1422, loss = 0.38241884112358093
In grad_steps = 1423, loss = 1.0808255672454834
In grad_steps = 1424, loss = 0.11248559504747391
In grad_steps = 1425, loss = 0.5797916054725647
In grad_steps = 1426, loss = 0.08837898075580597
In grad_steps = 1427, loss = 0.24806652963161469
In grad_steps = 1428, loss = 0.5241872668266296
In grad_steps = 1429, loss = 0.2748590111732483
In grad_steps = 1430, loss = 0.13025109469890594
In grad_steps = 1431, loss = 0.7731924057006836
In grad_steps = 1432, loss = 0.050884876400232315
In grad_steps = 1433, loss = 0.04585222154855728
In grad_steps = 1434, loss = 0.3512985408306122
In grad_steps = 1435, loss = 0.2739216685295105
In grad_steps = 1436, loss = 0.2566370368003845
In grad_steps = 1437, loss = 0.8093928098678589
In grad_steps = 1438, loss = 0.24795246124267578
In grad_steps = 1439, loss = 0.04584338515996933
In grad_steps = 1440, loss = 0.4963096082210541
In grad_steps = 1441, loss = 0.9773468971252441
In grad_steps = 1442, loss = 0.1790463775396347
In grad_steps = 1443, loss = 1.2821885347366333
In grad_steps = 1444, loss = 0.056664951145648956
In grad_steps = 1445, loss = 0.17242354154586792
In grad_steps = 1446, loss = 0.19609971344470978
In grad_steps = 1447, loss = 0.02451237104833126
In grad_steps = 1448, loss = 0.5030977129936218
In grad_steps = 1449, loss = 0.33871665596961975
In grad_steps = 1450, loss = 0.21923218667507172
In grad_steps = 1451, loss = 0.7415469288825989
In grad_steps = 1452, loss = 0.14363494515419006
In grad_steps = 1453, loss = 0.7916613817214966
In grad_steps = 1454, loss = 0.1850321888923645
In grad_steps = 1455, loss = 0.6620968580245972
In grad_steps = 1456, loss = 0.38579055666923523
In grad_steps = 1457, loss = 0.4647684693336487
In grad_steps = 1458, loss = 0.26036781072616577
In grad_steps = 1459, loss = 0.22386829555034637
In grad_steps = 1460, loss = 0.0803900957107544
In grad_steps = 1461, loss = 0.42803725600242615
In grad_steps = 1462, loss = 0.13242101669311523
In grad_steps = 1463, loss = 0.902095377445221
In grad_steps = 1464, loss = 0.5667121410369873
In grad_steps = 1465, loss = 0.3737591505050659
In grad_steps = 1466, loss = 0.12054748088121414
In grad_steps = 1467, loss = 0.5577433705329895
In grad_steps = 1468, loss = 0.10710623860359192
In grad_steps = 1469, loss = 0.39415013790130615
In grad_steps = 1470, loss = 0.37171727418899536
In grad_steps = 1471, loss = 0.10184115171432495
In grad_steps = 1472, loss = 0.5136220455169678
In grad_steps = 1473, loss = 0.12542004883289337
In grad_steps = 1474, loss = 0.8291689157485962
In grad_steps = 1475, loss = 0.5001784563064575
In grad_steps = 1476, loss = 0.3985537588596344
In grad_steps = 1477, loss = 0.21112141013145447
In grad_steps = 1478, loss = 0.5056641101837158
In grad_steps = 1479, loss = 0.2589815855026245
In grad_steps = 1480, loss = 0.19976258277893066
In grad_steps = 1481, loss = 0.1735255867242813
In grad_steps = 1482, loss = 0.10859379172325134
In grad_steps = 1483, loss = 0.5648307204246521
In grad_steps = 1484, loss = 0.3071550130844116
In grad_steps = 1485, loss = 0.32941046357154846
In grad_steps = 1486, loss = 1.15311598777771
In grad_steps = 1487, loss = 0.16654004156589508
In grad_steps = 1488, loss = 0.6959591507911682
In grad_steps = 1489, loss = 0.4411918520927429
In grad_steps = 1490, loss = 0.7548487186431885
In grad_steps = 1491, loss = 0.4216526448726654
In grad_steps = 1492, loss = 0.07692599296569824
In grad_steps = 1493, loss = 0.5061777830123901
In grad_steps = 1494, loss = 0.5899040699005127
In grad_steps = 1495, loss = 0.17421922087669373
In grad_steps = 1496, loss = 0.0814497098326683
In grad_steps = 1497, loss = 0.32735174894332886
In grad_steps = 1498, loss = 0.39002442359924316
In grad_steps = 1499, loss = 0.7246301174163818
In grad_steps = 1500, loss = 0.19367250800132751
In grad_steps = 1501, loss = 0.27263668179512024
In grad_steps = 1502, loss = 0.6305333971977234
In grad_steps = 1503, loss = 0.42798325419425964
In grad_steps = 1504, loss = 0.33570319414138794
In grad_steps = 1505, loss = 0.43220043182373047
In grad_steps = 1506, loss = 0.2732800841331482
In grad_steps = 1507, loss = 0.8112419843673706
In grad_steps = 1508, loss = 0.3731732666492462
In grad_steps = 1509, loss = 0.8111644983291626
In grad_steps = 1510, loss = 0.9218395352363586
In grad_steps = 1511, loss = 0.7837613821029663
In grad_steps = 1512, loss = 0.9526352286338806
In grad_steps = 1513, loss = 0.6642893552780151
In grad_steps = 1514, loss = 0.3173242509365082
In grad_steps = 1515, loss = 0.4368196129798889
In grad_steps = 1516, loss = 0.1472068727016449
In grad_steps = 1517, loss = 0.08363865315914154
In grad_steps = 1518, loss = 0.07514198869466782
In grad_steps = 1519, loss = 0.45953214168548584
In grad_steps = 1520, loss = 0.5342611074447632
In grad_steps = 1521, loss = 1.2196927070617676
In grad_steps = 1522, loss = 0.20234891772270203
In grad_steps = 1523, loss = 1.0762094259262085
In grad_steps = 1524, loss = 0.6545636653900146
In grad_steps = 1525, loss = 0.34489771723747253
In grad_steps = 1526, loss = 0.4208865165710449
In grad_steps = 1527, loss = 0.5878332853317261
In grad_steps = 1528, loss = 0.5967657566070557
In grad_steps = 1529, loss = 0.10724066942930222
In grad_steps = 1530, loss = 0.5945525169372559
In grad_steps = 1531, loss = 0.6541580557823181
In grad_steps = 1532, loss = 0.545062243938446
In grad_steps = 1533, loss = 0.3335978388786316
In grad_steps = 1534, loss = 0.3271991014480591
In grad_steps = 1535, loss = 0.3026842474937439
In grad_steps = 1536, loss = 0.1517954021692276
In grad_steps = 1537, loss = 0.42747625708580017
In grad_steps = 1538, loss = 0.17472650110721588
In grad_steps = 1539, loss = 0.11319898068904877
In grad_steps = 1540, loss = 0.2034316211938858
In grad_steps = 1541, loss = 0.43603694438934326
In grad_steps = 1542, loss = 0.21715039014816284
In grad_steps = 1543, loss = 0.1577140986919403
In grad_steps = 1544, loss = 0.09863130748271942
In grad_steps = 1545, loss = 0.21732991933822632
In grad_steps = 1546, loss = 0.1147812008857727
In grad_steps = 1547, loss = 1.3573362827301025
In grad_steps = 1548, loss = 0.03315991163253784
In grad_steps = 1549, loss = 0.017103096470236778
In grad_steps = 1550, loss = 0.19320718944072723
In grad_steps = 1551, loss = 0.6630679368972778
In grad_steps = 1552, loss = 0.11719046533107758
In grad_steps = 1553, loss = 0.05205301195383072
In grad_steps = 1554, loss = 0.2918747663497925
In grad_steps = 1555, loss = 0.19735704362392426
In grad_steps = 1556, loss = 1.1701494455337524
In grad_steps = 1557, loss = 0.8738260865211487
In grad_steps = 1558, loss = 0.5365254282951355
In grad_steps = 1559, loss = 0.5864668488502502
In grad_steps = 1560, loss = 0.21396587789058685
In grad_steps = 1561, loss = 0.3171539008617401
In grad_steps = 1562, loss = 0.3865402936935425
In grad_steps = 1563, loss = 0.08456233888864517
In grad_steps = 1564, loss = 1.1789933443069458
In grad_steps = 1565, loss = 0.5733485221862793
In grad_steps = 1566, loss = 0.13491934537887573
In grad_steps = 1567, loss = 0.4876652657985687
In grad_steps = 1568, loss = 0.4349247217178345
In grad_steps = 1569, loss = 0.1622844934463501
In grad_steps = 1570, loss = 0.4422197937965393
In grad_steps = 1571, loss = 0.8242199420928955
In grad_steps = 1572, loss = 0.24957343935966492
In grad_steps = 1573, loss = 0.28238287568092346
In grad_steps = 1574, loss = 0.6877315044403076
In grad_steps = 1575, loss = 0.6732712984085083
In grad_steps = 1576, loss = 0.5335427522659302
In grad_steps = 1577, loss = 0.5646159648895264
In grad_steps = 1578, loss = 0.09058979153633118
In grad_steps = 1579, loss = 0.5875862836837769
In grad_steps = 1580, loss = 0.14220285415649414
In grad_steps = 1581, loss = 0.3699267506599426
In grad_steps = 1582, loss = 0.21077731251716614
In grad_steps = 1583, loss = 0.174115389585495
In grad_steps = 1584, loss = 0.23767900466918945
In grad_steps = 1585, loss = 0.24208930134773254
In grad_steps = 1586, loss = 0.1967460662126541
In grad_steps = 1587, loss = 0.28573930263519287
In grad_steps = 1588, loss = 1.0557827949523926
In grad_steps = 1589, loss = 0.6776865720748901
In grad_steps = 1590, loss = 0.324831485748291
In grad_steps = 1591, loss = 0.8790560364723206
In grad_steps = 1592, loss = 0.47488945722579956
In grad_steps = 1593, loss = 0.049955107271671295
In grad_steps = 1594, loss = 0.15533773601055145
In grad_steps = 1595, loss = 0.8796173930168152
In grad_steps = 1596, loss = 0.1411479264497757
In grad_steps = 1597, loss = 0.3094530701637268
In grad_steps = 1598, loss = 0.11844885349273682
In grad_steps = 1599, loss = 0.20140914618968964
In grad_steps = 1600, loss = 0.46305403113365173
In grad_steps = 1601, loss = 0.17526689171791077
In grad_steps = 1602, loss = 0.1838265359401703
In grad_steps = 1603, loss = 0.5415892601013184
In grad_steps = 1604, loss = 0.2505907416343689
In grad_steps = 1605, loss = 0.09158272296190262
In grad_steps = 1606, loss = 0.3354770541191101
In grad_steps = 1607, loss = 0.03442356735467911
In grad_steps = 1608, loss = 0.6617950797080994
In grad_steps = 1609, loss = 0.6333038210868835
In grad_steps = 1610, loss = 0.32888421416282654
In grad_steps = 1611, loss = 0.2030085176229477
In grad_steps = 1612, loss = 0.5020715594291687
In grad_steps = 1613, loss = 0.3401554524898529
In grad_steps = 1614, loss = 0.052925966680049896
In grad_steps = 1615, loss = 0.17193959653377533
In grad_steps = 1616, loss = 0.07744362950325012
In grad_steps = 1617, loss = 0.8486234545707703
In grad_steps = 1618, loss = 0.7072259187698364
In grad_steps = 1619, loss = 0.6031236052513123
In grad_steps = 1620, loss = 0.44896265864372253
In grad_steps = 1621, loss = 0.38899853825569153
In grad_steps = 1622, loss = 1.1078357696533203
In grad_steps = 1623, loss = 0.6091058254241943
In grad_steps = 1624, loss = 0.42753833532333374
In grad_steps = 1625, loss = 0.8736076354980469
In grad_steps = 1626, loss = 0.23111507296562195
In grad_steps = 1627, loss = 0.23103728890419006
In grad_steps = 1628, loss = 0.35082346200942993
In grad_steps = 1629, loss = 0.17872710525989532
In grad_steps = 1630, loss = 0.4512593150138855
In grad_steps = 1631, loss = 0.16191059350967407
In grad_steps = 1632, loss = 0.35788244009017944
In grad_steps = 1633, loss = 0.627782940864563
In grad_steps = 1634, loss = 0.3025699257850647
In grad_steps = 1635, loss = 0.22159022092819214
In grad_steps = 1636, loss = 0.42427584528923035
In grad_steps = 1637, loss = 0.39499858021736145
In grad_steps = 1638, loss = 0.17007416486740112
In grad_steps = 1639, loss = 0.24282130599021912
In grad_steps = 1640, loss = 0.28429725766181946
In grad_steps = 1641, loss = 0.23620662093162537
In grad_steps = 1642, loss = 0.07345455884933472
In grad_steps = 1643, loss = 0.964063286781311
In grad_steps = 1644, loss = 0.2488241344690323
In grad_steps = 1645, loss = 0.2477111667394638
In grad_steps = 1646, loss = 0.2956312894821167
In grad_steps = 1647, loss = 0.6471012234687805
In grad_steps = 1648, loss = 0.07737311720848083
In grad_steps = 1649, loss = 1.561416506767273
In grad_steps = 1650, loss = 0.24406510591506958
In grad_steps = 1651, loss = 0.9644749164581299
In grad_steps = 1652, loss = 0.06756620109081268
In grad_steps = 1653, loss = 0.343818724155426
In grad_steps = 1654, loss = 0.5563763380050659
In grad_steps = 1655, loss = 0.6997110843658447
In grad_steps = 1656, loss = 0.1897161602973938
In grad_steps = 1657, loss = 0.45333507657051086
In grad_steps = 1658, loss = 0.16832903027534485
In grad_steps = 1659, loss = 0.1258210688829422
In grad_steps = 1660, loss = 0.6448264122009277
In grad_steps = 1661, loss = 0.26720720529556274
In grad_steps = 1662, loss = 0.2764577865600586
In grad_steps = 1663, loss = 0.2640800476074219
In grad_steps = 1664, loss = 0.2583203911781311
In grad_steps = 1665, loss = 0.21015749871730804
In grad_steps = 1666, loss = 0.23651230335235596
In grad_steps = 1667, loss = 0.31011760234832764
In grad_steps = 1668, loss = 0.33955079317092896
In grad_steps = 1669, loss = 0.4862419366836548
In grad_steps = 1670, loss = 0.18274903297424316
In grad_steps = 1671, loss = 0.058010999113321304
In grad_steps = 1672, loss = 0.14387162029743195
In grad_steps = 1673, loss = 0.0779133290052414
In grad_steps = 1674, loss = 0.05668813735246658
In grad_steps = 1675, loss = 0.1460656076669693
In grad_steps = 1676, loss = 0.1093423143029213
In grad_steps = 1677, loss = 0.05813729017972946
In grad_steps = 1678, loss = 0.8205547332763672
In grad_steps = 1679, loss = 1.5685174465179443
In grad_steps = 1680, loss = 0.17393408715724945
In grad_steps = 1681, loss = 0.08589273691177368
In grad_steps = 1682, loss = 0.4771755635738373
In grad_steps = 1683, loss = 0.20324914157390594
In grad_steps = 1684, loss = 0.7975687384605408
In grad_steps = 1685, loss = 0.1275518238544464
In grad_steps = 1686, loss = 0.10892750322818756
In grad_steps = 1687, loss = 0.5638024806976318
In grad_steps = 1688, loss = 0.16655589640140533
In grad_steps = 1689, loss = 1.3628195524215698
In grad_steps = 1690, loss = 0.20554985105991364
In grad_steps = 1691, loss = 0.3269612789154053
In grad_steps = 1692, loss = 0.20027834177017212
In grad_steps = 1693, loss = 0.25748559832572937
In grad_steps = 1694, loss = 0.04342188686132431
In grad_steps = 1695, loss = 0.752668559551239
In grad_steps = 1696, loss = 0.0549895316362381
In grad_steps = 1697, loss = 0.6554398536682129
In grad_steps = 1698, loss = 0.635392427444458
In grad_steps = 1699, loss = 0.2291559875011444
In grad_steps = 1700, loss = 0.47192293405532837
In grad_steps = 1701, loss = 0.6783149838447571
In grad_steps = 1702, loss = 0.5045919418334961
In grad_steps = 1703, loss = 0.26001572608947754
In grad_steps = 1704, loss = 0.06333266198635101
In grad_steps = 1705, loss = 0.5035321712493896
In grad_steps = 1706, loss = 0.11558829247951508
In grad_steps = 1707, loss = 0.4843844771385193
In grad_steps = 1708, loss = 0.06010975316166878
In grad_steps = 1709, loss = 0.15422746539115906
In grad_steps = 1710, loss = 0.9395783543586731
In grad_steps = 1711, loss = 0.2133123278617859
In grad_steps = 1712, loss = 0.26843127608299255
In grad_steps = 1713, loss = 0.08806973695755005
In grad_steps = 1714, loss = 0.18621788918972015
In grad_steps = 1715, loss = 0.3688586950302124
In grad_steps = 1716, loss = 0.2670591175556183
In grad_steps = 1717, loss = 0.03503137081861496
In grad_steps = 1718, loss = 0.970475435256958
In grad_steps = 1719, loss = 0.04943665862083435
In grad_steps = 1720, loss = 0.46267884969711304
In grad_steps = 1721, loss = 0.6276373267173767
In grad_steps = 1722, loss = 0.02914978563785553
In grad_steps = 1723, loss = 0.10280995815992355
In grad_steps = 1724, loss = 0.043256521224975586
In grad_steps = 1725, loss = 0.04251492768526077
In grad_steps = 1726, loss = 0.04116075485944748
In grad_steps = 1727, loss = 0.36267781257629395
In grad_steps = 1728, loss = 0.32810238003730774
In grad_steps = 1729, loss = 0.02354976162314415
In grad_steps = 1730, loss = 0.0918695479631424
In grad_steps = 1731, loss = 0.8892695307731628
In grad_steps = 1732, loss = 1.3973731994628906
In grad_steps = 1733, loss = 0.24146661162376404
In grad_steps = 1734, loss = 0.5509033799171448
In grad_steps = 1735, loss = 0.12679076194763184
In grad_steps = 1736, loss = 0.18066070973873138
In grad_steps = 1737, loss = 0.09170468151569366
In grad_steps = 1738, loss = 0.7695112824440002
In grad_steps = 1739, loss = 0.5372445583343506
In grad_steps = 1740, loss = 0.3513173758983612
In grad_steps = 1741, loss = 0.21106210350990295
In grad_steps = 1742, loss = 0.3823651671409607
In grad_steps = 1743, loss = 0.5528684854507446
In grad_steps = 1744, loss = 0.24978359043598175
In grad_steps = 1745, loss = 0.4309648871421814
In grad_steps = 1746, loss = 0.23644012212753296
In grad_steps = 1747, loss = 0.19561408460140228
In grad_steps = 1748, loss = 0.23292052745819092
In grad_steps = 1749, loss = 1.1016747951507568
In grad_steps = 1750, loss = 0.40169039368629456
In grad_steps = 1751, loss = 0.22867420315742493
In grad_steps = 1752, loss = 0.3193427622318268
In grad_steps = 1753, loss = 0.14637549221515656
In grad_steps = 1754, loss = 0.3408837616443634
In grad_steps = 1755, loss = 0.7650938034057617
In grad_steps = 1756, loss = 0.2947705388069153
In grad_steps = 1757, loss = 0.19861821830272675
In grad_steps = 1758, loss = 1.0696542263031006
In grad_steps = 1759, loss = 0.0745781660079956
In grad_steps = 1760, loss = 0.1846214234828949
In grad_steps = 1761, loss = 0.27761247754096985
In grad_steps = 1762, loss = 0.6162179708480835
In grad_steps = 1763, loss = 0.17237359285354614
In grad_steps = 1764, loss = 0.501592755317688
In grad_steps = 1765, loss = 0.6688098907470703
In grad_steps = 1766, loss = 0.24166475236415863
In grad_steps = 1767, loss = 0.3020298480987549
In grad_steps = 1768, loss = 0.7616826891899109
In grad_steps = 1769, loss = 0.4444398581981659
In grad_steps = 1770, loss = 0.3356059491634369
In grad_steps = 1771, loss = 0.5655169486999512
In grad_steps = 1772, loss = 0.24789376556873322
In grad_steps = 1773, loss = 0.14698757231235504
In grad_steps = 1774, loss = 0.3026254177093506
In grad_steps = 1775, loss = 0.3371833860874176
In grad_steps = 1776, loss = 0.27087169885635376
In grad_steps = 1777, loss = 0.30684590339660645
In grad_steps = 1778, loss = 0.5363293290138245
In grad_steps = 1779, loss = 0.31162595748901367
In grad_steps = 1780, loss = 0.29620617628097534
In grad_steps = 1781, loss = 0.07286223769187927
In grad_steps = 1782, loss = 0.13354812562465668
In grad_steps = 1783, loss = 0.19902077317237854
In grad_steps = 1784, loss = 0.5012156367301941
In grad_steps = 1785, loss = 0.18163561820983887
In grad_steps = 1786, loss = 0.029680872336030006
In grad_steps = 1787, loss = 0.0472722165286541
In grad_steps = 1788, loss = 0.21212467551231384
In grad_steps = 1789, loss = 0.21963924169540405
In grad_steps = 1790, loss = 0.17994920909404755
In grad_steps = 1791, loss = 0.34388381242752075
In grad_steps = 1792, loss = 0.3860253393650055
In grad_steps = 1793, loss = 0.6900517344474792
In grad_steps = 1794, loss = 0.6405404210090637
In grad_steps = 1795, loss = 1.5410100221633911
In grad_steps = 1796, loss = 0.08275134861469269
In grad_steps = 1797, loss = 0.578973114490509
In grad_steps = 1798, loss = 0.1524866372346878
In grad_steps = 1799, loss = 0.20276322960853577
In grad_steps = 1800, loss = 0.012605641037225723
In grad_steps = 1801, loss = 0.4132140576839447
In grad_steps = 1802, loss = 0.022039974108338356
In grad_steps = 1803, loss = 0.8939498662948608
In grad_steps = 1804, loss = 0.21242967247962952
In grad_steps = 1805, loss = 0.202676460146904
In grad_steps = 1806, loss = 0.3009355664253235
In grad_steps = 1807, loss = 0.3080942928791046
In grad_steps = 1808, loss = 1.220684289932251
In grad_steps = 1809, loss = 0.19761452078819275
In grad_steps = 1810, loss = 0.3227044939994812
In grad_steps = 1811, loss = 0.8815021514892578
In grad_steps = 1812, loss = 0.09084229171276093
In grad_steps = 1813, loss = 0.47674041986465454
In grad_steps = 1814, loss = 0.2005568891763687
In grad_steps = 1815, loss = 0.9552111029624939
In grad_steps = 1816, loss = 0.4318189024925232
In grad_steps = 1817, loss = 0.06725313514471054
In grad_steps = 1818, loss = 0.20276600122451782
In grad_steps = 1819, loss = 0.17715539038181305
In grad_steps = 1820, loss = 0.5333507061004639
In grad_steps = 1821, loss = 0.09935416281223297
In grad_steps = 1822, loss = 0.4737413227558136
In grad_steps = 1823, loss = 0.21295735239982605
In grad_steps = 1824, loss = 0.25185662508010864
In grad_steps = 1825, loss = 0.27729547023773193
In grad_steps = 1826, loss = 0.39011549949645996
In grad_steps = 1827, loss = 0.11346733570098877
In grad_steps = 1828, loss = 0.7820294499397278
In grad_steps = 1829, loss = 0.1915128529071808
In grad_steps = 1830, loss = 0.868341326713562
In grad_steps = 1831, loss = 0.12066283077001572
In grad_steps = 1832, loss = 0.4028910994529724
In grad_steps = 1833, loss = 0.07809498906135559
In grad_steps = 1834, loss = 0.1201830804347992
In grad_steps = 1835, loss = 0.034241728484630585
In grad_steps = 1836, loss = 0.08551931381225586
In grad_steps = 1837, loss = 0.8356757164001465
In grad_steps = 1838, loss = 0.5330839157104492
In grad_steps = 1839, loss = 0.5038211345672607
In grad_steps = 1840, loss = 0.23563623428344727
In grad_steps = 1841, loss = 0.16847126185894012
In grad_steps = 1842, loss = 0.14231988787651062
In grad_steps = 1843, loss = 0.05441151559352875
In grad_steps = 1844, loss = 0.21286766231060028
In grad_steps = 1845, loss = 0.060339219868183136
In grad_steps = 1846, loss = 0.07120180130004883
In grad_steps = 1847, loss = 0.3119334876537323
In grad_steps = 1848, loss = 0.11813676357269287
In grad_steps = 1849, loss = 0.6367348432540894
In grad_steps = 1850, loss = 0.08275648206472397
In grad_steps = 1851, loss = 0.04897209629416466
In grad_steps = 1852, loss = 0.07551825046539307
In grad_steps = 1853, loss = 1.5692676305770874
In grad_steps = 1854, loss = 0.29002541303634644
In grad_steps = 1855, loss = 0.14579002559185028
In grad_steps = 1856, loss = 0.242069274187088
In grad_steps = 1857, loss = 0.44682905077934265
In grad_steps = 1858, loss = 0.04869817942380905
In grad_steps = 1859, loss = 0.1361425518989563
In grad_steps = 1860, loss = 0.25924038887023926
In grad_steps = 1861, loss = 0.6616989970207214
In grad_steps = 1862, loss = 0.15799883008003235
In grad_steps = 1863, loss = 1.7294704914093018
In grad_steps = 1864, loss = 0.06617268174886703
In grad_steps = 1865, loss = 0.24733972549438477
In grad_steps = 1866, loss = 1.0169897079467773
In grad_steps = 1867, loss = 0.043363507837057114
In grad_steps = 1868, loss = 0.1833726018667221
In grad_steps = 1869, loss = 0.2211228460073471
In grad_steps = 1870, loss = 0.9440926909446716
In grad_steps = 1871, loss = 0.6893036365509033
In grad_steps = 1872, loss = 0.08543848991394043
In grad_steps = 1873, loss = 0.13988974690437317
In grad_steps = 1874, loss = 0.4595004916191101
In grad_steps = 1875, loss = 1.1466243267059326
In grad_steps = 1876, loss = 0.527874767780304
In grad_steps = 1877, loss = 0.5247827172279358
In grad_steps = 1878, loss = 1.7574121952056885
In grad_steps = 1879, loss = 0.04539760202169418
In grad_steps = 1880, loss = 0.5949351787567139
In grad_steps = 1881, loss = 0.5725472569465637
In grad_steps = 1882, loss = 0.4258047342300415
In grad_steps = 1883, loss = 0.4501490890979767
In grad_steps = 1884, loss = 0.37540239095687866
In grad_steps = 1885, loss = 0.41623741388320923
In grad_steps = 1886, loss = 0.6405066251754761
In grad_steps = 1887, loss = 0.7882586121559143
In grad_steps = 1888, loss = 0.6280040740966797
In grad_steps = 1889, loss = 0.3477666974067688
In grad_steps = 1890, loss = 0.267880916595459
In grad_steps = 1891, loss = 0.4390147626399994
In grad_steps = 1892, loss = 0.3394553065299988
In grad_steps = 1893, loss = 0.26586949825286865
In grad_steps = 1894, loss = 0.2974228262901306
In grad_steps = 1895, loss = 0.2680188715457916
In grad_steps = 1896, loss = 0.24018114805221558
In grad_steps = 1897, loss = 0.12245228886604309
In grad_steps = 1898, loss = 0.2581251263618469
In grad_steps = 1899, loss = 0.16086989641189575
In grad_steps = 1900, loss = 0.07601916790008545
In grad_steps = 1901, loss = 0.13549403846263885
In grad_steps = 1902, loss = 0.07413054257631302
In grad_steps = 1903, loss = 0.2335931807756424
In grad_steps = 1904, loss = 0.5950672030448914
In grad_steps = 1905, loss = 0.7395395040512085
In grad_steps = 1906, loss = 0.013289665803313255
In grad_steps = 1907, loss = 0.149309441447258
In grad_steps = 1908, loss = 0.008688108064234257
In grad_steps = 1909, loss = 0.33577463030815125
In grad_steps = 1910, loss = 0.026547590270638466
In grad_steps = 1911, loss = 0.24457642436027527
In grad_steps = 1912, loss = 0.024530021473765373
In grad_steps = 1913, loss = 1.1087701320648193
In grad_steps = 1914, loss = 0.9795681238174438
In grad_steps = 1915, loss = 0.4053114056587219
In grad_steps = 1916, loss = 0.124998077750206
In grad_steps = 1917, loss = 0.04738185554742813
In grad_steps = 1918, loss = 0.5157417058944702
In grad_steps = 1919, loss = 0.19798952341079712
In grad_steps = 1920, loss = 0.17367210984230042
In grad_steps = 1921, loss = 0.11811943352222443
In grad_steps = 1922, loss = 0.012778197415173054
In grad_steps = 1923, loss = 0.15487360954284668
In grad_steps = 1924, loss = 1.1661370992660522
In grad_steps = 1925, loss = 0.17494724690914154
In grad_steps = 1926, loss = 0.0795806348323822
In grad_steps = 1927, loss = 0.10285669565200806
In grad_steps = 1928, loss = 0.11760829389095306
In grad_steps = 1929, loss = 0.07339703291654587
In grad_steps = 1930, loss = 0.9161978960037231
In grad_steps = 1931, loss = 0.7457007765769958
In grad_steps = 1932, loss = 0.29125845432281494
In grad_steps = 1933, loss = 0.5314038991928101
In grad_steps = 1934, loss = 0.05906583368778229
In grad_steps = 1935, loss = 0.25665929913520813
In grad_steps = 1936, loss = 0.044993408024311066
In grad_steps = 1937, loss = 0.13617907464504242
In grad_steps = 1938, loss = 0.05667353421449661
In grad_steps = 1939, loss = 0.9869194030761719
In grad_steps = 1940, loss = 0.41718411445617676
In grad_steps = 1941, loss = 0.2173648476600647
In grad_steps = 1942, loss = 0.14129802584648132
In grad_steps = 1943, loss = 0.17807331681251526
In grad_steps = 1944, loss = 0.9957838654518127
In grad_steps = 1945, loss = 1.4728387594223022
In grad_steps = 1946, loss = 0.6862726211547852
In grad_steps = 1947, loss = 0.8069072961807251
In grad_steps = 1948, loss = 0.7182362079620361
In grad_steps = 1949, loss = 0.2969039976596832
In grad_steps = 1950, loss = 0.5715188980102539
In grad_steps = 1951, loss = 0.5542001724243164
In grad_steps = 1952, loss = 0.5752346515655518
In grad_steps = 1953, loss = 0.1797960102558136
In grad_steps = 1954, loss = 0.3625872731208801
In grad_steps = 1955, loss = 0.4556582272052765
In grad_steps = 1956, loss = 0.520583987236023
In grad_steps = 1957, loss = 0.6242324113845825
In grad_steps = 1958, loss = 0.1961992084980011
In grad_steps = 1959, loss = 0.2597109079360962
In grad_steps = 1960, loss = 0.2657912075519562
In grad_steps = 1961, loss = 0.3379926085472107
In grad_steps = 1962, loss = 0.5402225852012634
In grad_steps = 1963, loss = 0.45507344603538513
In grad_steps = 1964, loss = 0.16190482676029205
In grad_steps = 1965, loss = 0.4392348527908325
In grad_steps = 1966, loss = 0.32322975993156433
In grad_steps = 1967, loss = 0.26520681381225586
In grad_steps = 1968, loss = 0.4572726786136627
In grad_steps = 1969, loss = 1.000179409980774
In grad_steps = 1970, loss = 0.5162898898124695
In grad_steps = 1971, loss = 1.0121753215789795
In grad_steps = 1972, loss = 0.25347036123275757
In grad_steps = 1973, loss = 0.09712672978639603
In grad_steps = 1974, loss = 0.26333820819854736
In grad_steps = 1975, loss = 0.0672941729426384
In grad_steps = 1976, loss = 0.3972690999507904
In grad_steps = 1977, loss = 0.2024168074131012
In grad_steps = 1978, loss = 0.23714974522590637
In grad_steps = 1979, loss = 0.6387292146682739
In grad_steps = 1980, loss = 0.1775454431772232
In grad_steps = 1981, loss = 0.3389495611190796
In grad_steps = 1982, loss = 0.2667984664440155
In grad_steps = 1983, loss = 0.08636507391929626
In grad_steps = 1984, loss = 0.23383724689483643
In grad_steps = 1985, loss = 0.7573286890983582
In grad_steps = 1986, loss = 0.37336793541908264
In grad_steps = 1987, loss = 0.2988966703414917
In grad_steps = 1988, loss = 1.0212435722351074
In grad_steps = 1989, loss = 0.6792240142822266
In grad_steps = 1990, loss = 0.07243534922599792
In grad_steps = 1991, loss = 0.5391588807106018
In grad_steps = 1992, loss = 0.3111248016357422
In grad_steps = 1993, loss = 0.30056899785995483
In grad_steps = 1994, loss = 0.6902377009391785
In grad_steps = 1995, loss = 0.24495048820972443
In grad_steps = 1996, loss = 0.3867175281047821
In grad_steps = 1997, loss = 0.26136189699172974
In grad_steps = 1998, loss = 0.10875241458415985
In grad_steps = 1999, loss = 0.16228626668453217
In grad_steps = 2000, loss = 0.1469482183456421
In grad_steps = 2001, loss = 0.9276267290115356
In grad_steps = 2002, loss = 0.2522146701812744
In grad_steps = 2003, loss = 0.2065107524394989
In grad_steps = 2004, loss = 0.180707186460495
In grad_steps = 2005, loss = 0.18629004061222076
In grad_steps = 2006, loss = 0.19783318042755127
In grad_steps = 2007, loss = 0.14806556701660156
In grad_steps = 2008, loss = 0.20738035440444946
In grad_steps = 2009, loss = 0.055847156792879105
In grad_steps = 2010, loss = 0.15199579298496246
In grad_steps = 2011, loss = 0.2741696834564209
In grad_steps = 2012, loss = 0.6789121031761169
In grad_steps = 2013, loss = 0.4656142592430115
In grad_steps = 2014, loss = 0.0916721373796463
In grad_steps = 2015, loss = 0.21911326050758362
In grad_steps = 2016, loss = 0.0922439694404602
In grad_steps = 2017, loss = 0.49481865763664246
In grad_steps = 2018, loss = 0.5375367999076843
In grad_steps = 2019, loss = 0.38155022263526917
In grad_steps = 2020, loss = 0.16876931488513947
In grad_steps = 2021, loss = 0.08075231313705444
In grad_steps = 2022, loss = 0.03365315496921539
In grad_steps = 2023, loss = 0.1758207529783249
In grad_steps = 2024, loss = 0.49947798252105713
In grad_steps = 2025, loss = 0.30588802695274353
In grad_steps = 2026, loss = 0.9316177368164062
In grad_steps = 2027, loss = 0.3520523011684418
In grad_steps = 2028, loss = 0.6699368953704834
In grad_steps = 2029, loss = 0.05321953818202019
In grad_steps = 2030, loss = 0.07954701781272888
In grad_steps = 2031, loss = 0.017091138288378716
In grad_steps = 2032, loss = 1.023394227027893
In grad_steps = 2033, loss = 0.6703259348869324
In grad_steps = 2034, loss = 0.5227351784706116
In grad_steps = 2035, loss = 0.02635962888598442
In grad_steps = 2036, loss = 0.05378623306751251
In grad_steps = 2037, loss = 0.0894622728228569
In grad_steps = 2038, loss = 0.6847712397575378
In grad_steps = 2039, loss = 0.09894204139709473
In grad_steps = 2040, loss = 0.05273205786943436
In grad_steps = 2041, loss = 0.5192676186561584
In grad_steps = 2042, loss = 0.12078168988227844
In grad_steps = 2043, loss = 0.25861549377441406
In grad_steps = 2044, loss = 0.6631894707679749
In grad_steps = 2045, loss = 0.6075100302696228
In grad_steps = 2046, loss = 0.04983147233724594
In grad_steps = 2047, loss = 0.21598252654075623
In grad_steps = 2048, loss = 0.36192312836647034
In grad_steps = 2049, loss = 0.517963171005249
In grad_steps = 2050, loss = 0.3372366726398468
In grad_steps = 2051, loss = 0.2106020152568817
In grad_steps = 2052, loss = 0.010789391584694386
Beginning epoch 2
In grad_steps = 2053, loss = 0.34031468629837036
In grad_steps = 2054, loss = 0.05125977843999863
In grad_steps = 2055, loss = 0.21407265961170197
In grad_steps = 2056, loss = 0.2820545732975006
In grad_steps = 2057, loss = 0.15936966240406036
In grad_steps = 2058, loss = 0.04938800632953644
In grad_steps = 2059, loss = 0.11080265045166016
In grad_steps = 2060, loss = 0.11348943412303925
In grad_steps = 2061, loss = 0.7642159461975098
In grad_steps = 2062, loss = 0.6589934229850769
In grad_steps = 2063, loss = 0.9412368535995483
In grad_steps = 2064, loss = 0.056299448013305664
In grad_steps = 2065, loss = 0.29334160685539246
In grad_steps = 2066, loss = 0.02888796478509903
In grad_steps = 2067, loss = 0.3537023961544037
In grad_steps = 2068, loss = 0.5608794093132019
In grad_steps = 2069, loss = 0.12200406938791275
In grad_steps = 2070, loss = 0.4480997920036316
In grad_steps = 2071, loss = 0.3227134346961975
In grad_steps = 2072, loss = 0.5990114808082581
In grad_steps = 2073, loss = 0.03159851208329201
In grad_steps = 2074, loss = 0.49295032024383545
In grad_steps = 2075, loss = 1.1037731170654297
In grad_steps = 2076, loss = 0.17433202266693115
In grad_steps = 2077, loss = 0.04017578437924385
In grad_steps = 2078, loss = 0.3673999309539795
In grad_steps = 2079, loss = 0.31762048602104187
In grad_steps = 2080, loss = 1.0098211765289307
In grad_steps = 2081, loss = 0.12651315331459045
In grad_steps = 2082, loss = 0.11669126152992249
In grad_steps = 2083, loss = 0.5903082489967346
In grad_steps = 2084, loss = 0.40328407287597656
In grad_steps = 2085, loss = 0.188482403755188
In grad_steps = 2086, loss = 0.6403146386146545
In grad_steps = 2087, loss = 0.15119987726211548
In grad_steps = 2088, loss = 0.41109946370124817
In grad_steps = 2089, loss = 0.3030451536178589
In grad_steps = 2090, loss = 0.2229594886302948
In grad_steps = 2091, loss = 0.35646817088127136
In grad_steps = 2092, loss = 0.28348106145858765
In grad_steps = 2093, loss = 0.47884896397590637
In grad_steps = 2094, loss = 0.07321910560131073
In grad_steps = 2095, loss = 0.1977584809064865
In grad_steps = 2096, loss = 0.19192743301391602
In grad_steps = 2097, loss = 0.27815958857536316
In grad_steps = 2098, loss = 0.117631696164608
In grad_steps = 2099, loss = 1.2614562511444092
In grad_steps = 2100, loss = 0.5105658173561096
In grad_steps = 2101, loss = 0.11992640048265457
In grad_steps = 2102, loss = 0.07145664095878601
In grad_steps = 2103, loss = 0.4345812201499939
In grad_steps = 2104, loss = 0.04610564559698105
In grad_steps = 2105, loss = 0.07448890060186386
In grad_steps = 2106, loss = 0.024303240701556206
In grad_steps = 2107, loss = 0.046312641352415085
In grad_steps = 2108, loss = 0.09120333939790726
In grad_steps = 2109, loss = 0.5578405857086182
In grad_steps = 2110, loss = 0.4215315878391266
In grad_steps = 2111, loss = 0.08196572959423065
In grad_steps = 2112, loss = 0.8412119150161743
In grad_steps = 2113, loss = 1.657541036605835
In grad_steps = 2114, loss = 0.09478521347045898
In grad_steps = 2115, loss = 0.44291985034942627
In grad_steps = 2116, loss = 0.24321182072162628
In grad_steps = 2117, loss = 0.12822872400283813
In grad_steps = 2118, loss = 0.1659637987613678
In grad_steps = 2119, loss = 0.4603445827960968
In grad_steps = 2120, loss = 0.06136322021484375
In grad_steps = 2121, loss = 0.08296652883291245
In grad_steps = 2122, loss = 0.12296457588672638
In grad_steps = 2123, loss = 0.9434782862663269
In grad_steps = 2124, loss = 0.08166304230690002
In grad_steps = 2125, loss = 0.4283040165901184
In grad_steps = 2126, loss = 0.3696117103099823
In grad_steps = 2127, loss = 0.04547795653343201
In grad_steps = 2128, loss = 0.6840837597846985
In grad_steps = 2129, loss = 0.17253056168556213
In grad_steps = 2130, loss = 0.18115226924419403
In grad_steps = 2131, loss = 0.5713416337966919
In grad_steps = 2132, loss = 0.5142048597335815
In grad_steps = 2133, loss = 0.6505939364433289
In grad_steps = 2134, loss = 0.27596572041511536
In grad_steps = 2135, loss = 0.16039270162582397
In grad_steps = 2136, loss = 0.39673709869384766
In grad_steps = 2137, loss = 0.1837758868932724
In grad_steps = 2138, loss = 0.34500136971473694
In grad_steps = 2139, loss = 0.16135229170322418
In grad_steps = 2140, loss = 0.15794052183628082
In grad_steps = 2141, loss = 0.9694395065307617
In grad_steps = 2142, loss = 0.18605992197990417
In grad_steps = 2143, loss = 0.6172960996627808
In grad_steps = 2144, loss = 0.7632379531860352
In grad_steps = 2145, loss = 0.19161532819271088
In grad_steps = 2146, loss = 0.19179987907409668
In grad_steps = 2147, loss = 0.16357460618019104
In grad_steps = 2148, loss = 0.07178021222352982
In grad_steps = 2149, loss = 0.19695615768432617
In grad_steps = 2150, loss = 0.17565292119979858
In grad_steps = 2151, loss = 0.29696598649024963
In grad_steps = 2152, loss = 0.1846882700920105
In grad_steps = 2153, loss = 0.8953484892845154
In grad_steps = 2154, loss = 0.07829046994447708
In grad_steps = 2155, loss = 0.488741934299469
In grad_steps = 2156, loss = 0.3070085346698761
In grad_steps = 2157, loss = 0.5049664378166199
In grad_steps = 2158, loss = 0.13359662890434265
In grad_steps = 2159, loss = 0.11676927655935287
In grad_steps = 2160, loss = 0.09639348089694977
In grad_steps = 2161, loss = 0.40136584639549255
In grad_steps = 2162, loss = 0.05537157505750656
In grad_steps = 2163, loss = 0.2901032269001007
In grad_steps = 2164, loss = 0.026147790253162384
In grad_steps = 2165, loss = 0.06365042924880981
In grad_steps = 2166, loss = 0.2086365669965744
In grad_steps = 2167, loss = 0.05476653575897217
In grad_steps = 2168, loss = 0.8144469261169434
In grad_steps = 2169, loss = 0.08504442125558853
In grad_steps = 2170, loss = 0.12042085081338882
In grad_steps = 2171, loss = 0.15389148890972137
In grad_steps = 2172, loss = 0.18255507946014404
In grad_steps = 2173, loss = 0.24362125992774963
In grad_steps = 2174, loss = 0.16153396666049957
In grad_steps = 2175, loss = 0.10185964405536652
In grad_steps = 2176, loss = 0.21124720573425293
In grad_steps = 2177, loss = 0.1452871859073639
In grad_steps = 2178, loss = 0.6356414556503296
In grad_steps = 2179, loss = 0.02054937183856964
In grad_steps = 2180, loss = 0.6582549810409546
In grad_steps = 2181, loss = 0.21332493424415588
In grad_steps = 2182, loss = 0.06470045447349548
In grad_steps = 2183, loss = 0.11884723603725433
In grad_steps = 2184, loss = 0.28578799962997437
In grad_steps = 2185, loss = 0.6082044243812561
In grad_steps = 2186, loss = 0.3764929473400116
In grad_steps = 2187, loss = 0.06970490515232086
In grad_steps = 2188, loss = 0.20548854768276215
In grad_steps = 2189, loss = 0.07857409119606018
In grad_steps = 2190, loss = 0.025818584486842155
In grad_steps = 2191, loss = 1.0688377618789673
In grad_steps = 2192, loss = 0.11355561017990112
In grad_steps = 2193, loss = 1.1769288778305054
In grad_steps = 2194, loss = 0.037573330104351044
In grad_steps = 2195, loss = 0.10431060194969177
In grad_steps = 2196, loss = 0.03295831009745598
In grad_steps = 2197, loss = 1.0156381130218506
In grad_steps = 2198, loss = 0.09569720923900604
In grad_steps = 2199, loss = 0.2494795173406601
In grad_steps = 2200, loss = 0.172830268740654
In grad_steps = 2201, loss = 0.08260256052017212
In grad_steps = 2202, loss = 0.11850260198116302
In grad_steps = 2203, loss = 0.5990654826164246
In grad_steps = 2204, loss = 0.12095103412866592
In grad_steps = 2205, loss = 1.2744230031967163
In grad_steps = 2206, loss = 0.2071313112974167
In grad_steps = 2207, loss = 0.4170784056186676
In grad_steps = 2208, loss = 0.09534414112567902
In grad_steps = 2209, loss = 0.13044030964374542
In grad_steps = 2210, loss = 0.05811574310064316
In grad_steps = 2211, loss = 0.20561747252941132
In grad_steps = 2212, loss = 0.23608939349651337
In grad_steps = 2213, loss = 0.07388205081224442
In grad_steps = 2214, loss = 0.5135154724121094
In grad_steps = 2215, loss = 0.34180521965026855
In grad_steps = 2216, loss = 0.1599404662847519
In grad_steps = 2217, loss = 0.2575925886631012
In grad_steps = 2218, loss = 0.0494801327586174
In grad_steps = 2219, loss = 0.043483756482601166
In grad_steps = 2220, loss = 0.12060162425041199
In grad_steps = 2221, loss = 0.5284560918807983
In grad_steps = 2222, loss = 0.11904211342334747
In grad_steps = 2223, loss = 0.24524685740470886
In grad_steps = 2224, loss = 0.3534775972366333
In grad_steps = 2225, loss = 0.13775916397571564
In grad_steps = 2226, loss = 0.44632697105407715
In grad_steps = 2227, loss = 0.02611522004008293
In grad_steps = 2228, loss = 0.8563746809959412
In grad_steps = 2229, loss = 0.014981484040617943
In grad_steps = 2230, loss = 0.06931260228157043
In grad_steps = 2231, loss = 0.3641486167907715
In grad_steps = 2232, loss = 0.551138699054718
In grad_steps = 2233, loss = 0.09572833776473999
In grad_steps = 2234, loss = 0.07539596408605576
In grad_steps = 2235, loss = 0.09286767989397049
In grad_steps = 2236, loss = 0.9883339405059814
In grad_steps = 2237, loss = 0.05819471925497055
In grad_steps = 2238, loss = 0.37836357951164246
In grad_steps = 2239, loss = 0.040348999202251434
In grad_steps = 2240, loss = 0.411750465631485
In grad_steps = 2241, loss = 0.10323614627122879
In grad_steps = 2242, loss = 0.40023335814476013
In grad_steps = 2243, loss = 0.06910134851932526
In grad_steps = 2244, loss = 0.2418481856584549
In grad_steps = 2245, loss = 0.3062421977519989
In grad_steps = 2246, loss = 0.5529007315635681
In grad_steps = 2247, loss = 0.11271050572395325
In grad_steps = 2248, loss = 0.08650943636894226
In grad_steps = 2249, loss = 0.0752362310886383
In grad_steps = 2250, loss = 0.3948037326335907
In grad_steps = 2251, loss = 0.17012320458889008
In grad_steps = 2252, loss = 0.8192877769470215
In grad_steps = 2253, loss = 0.03232594579458237
In grad_steps = 2254, loss = 0.35382717847824097
In grad_steps = 2255, loss = 0.03072388842701912
In grad_steps = 2256, loss = 0.2662082314491272
In grad_steps = 2257, loss = 0.27138063311576843
In grad_steps = 2258, loss = 0.18388307094573975
In grad_steps = 2259, loss = 0.06273166090250015
In grad_steps = 2260, loss = 0.29027998447418213
In grad_steps = 2261, loss = 0.11737922579050064
In grad_steps = 2262, loss = 0.11559359729290009
In grad_steps = 2263, loss = 0.3429874777793884
In grad_steps = 2264, loss = 0.4377880394458771
In grad_steps = 2265, loss = 0.5325061082839966
In grad_steps = 2266, loss = 0.2584434151649475
In grad_steps = 2267, loss = 0.056195978075265884
In grad_steps = 2268, loss = 0.1539240926504135
In grad_steps = 2269, loss = 0.9589964747428894
In grad_steps = 2270, loss = 0.7462747097015381
In grad_steps = 2271, loss = 0.6028620600700378
In grad_steps = 2272, loss = 0.31182312965393066
In grad_steps = 2273, loss = 0.19547870755195618
In grad_steps = 2274, loss = 0.882591187953949
In grad_steps = 2275, loss = 0.19894887506961823
In grad_steps = 2276, loss = 0.24234697222709656
In grad_steps = 2277, loss = 0.15118277072906494
In grad_steps = 2278, loss = 0.2752520740032196
In grad_steps = 2279, loss = 0.34426769614219666
In grad_steps = 2280, loss = 0.11863675713539124
In grad_steps = 2281, loss = 0.26221662759780884
In grad_steps = 2282, loss = 0.3049381971359253
In grad_steps = 2283, loss = 0.16031989455223083
In grad_steps = 2284, loss = 0.8053993582725525
In grad_steps = 2285, loss = 1.02487051486969
In grad_steps = 2286, loss = 0.06756636500358582
In grad_steps = 2287, loss = 1.0816280841827393
In grad_steps = 2288, loss = 0.4006066620349884
In grad_steps = 2289, loss = 0.507123589515686
In grad_steps = 2290, loss = 0.12554366886615753
In grad_steps = 2291, loss = 0.0982549786567688
In grad_steps = 2292, loss = 0.21959979832172394
In grad_steps = 2293, loss = 0.08754082024097443
In grad_steps = 2294, loss = 0.28602874279022217
In grad_steps = 2295, loss = 0.2704881727695465
In grad_steps = 2296, loss = 0.23777157068252563
In grad_steps = 2297, loss = 0.2222839593887329
In grad_steps = 2298, loss = 0.17505288124084473
In grad_steps = 2299, loss = 0.09111438691616058
In grad_steps = 2300, loss = 0.18773725628852844
In grad_steps = 2301, loss = 0.8440122604370117
In grad_steps = 2302, loss = 0.12351062893867493
In grad_steps = 2303, loss = 0.36629578471183777
In grad_steps = 2304, loss = 0.19859588146209717
In grad_steps = 2305, loss = 0.07113895565271378
In grad_steps = 2306, loss = 0.06856942921876907
In grad_steps = 2307, loss = 0.05555425584316254
In grad_steps = 2308, loss = 0.5841265320777893
In grad_steps = 2309, loss = 0.7979516983032227
In grad_steps = 2310, loss = 0.24015969038009644
In grad_steps = 2311, loss = 0.8268445730209351
In grad_steps = 2312, loss = 0.12099728733301163
In grad_steps = 2313, loss = 0.675786018371582
In grad_steps = 2314, loss = 1.1152851581573486
In grad_steps = 2315, loss = 0.9397271871566772
In grad_steps = 2316, loss = 0.37934237718582153
In grad_steps = 2317, loss = 0.30363142490386963
In grad_steps = 2318, loss = 0.21091514825820923
In grad_steps = 2319, loss = 0.23438632488250732
In grad_steps = 2320, loss = 0.9746972322463989
In grad_steps = 2321, loss = 0.29168203473091125
In grad_steps = 2322, loss = 0.2610473930835724
In grad_steps = 2323, loss = 0.1884014755487442
In grad_steps = 2324, loss = 0.610925555229187
In grad_steps = 2325, loss = 0.36141300201416016
In grad_steps = 2326, loss = 0.13541315495967865
In grad_steps = 2327, loss = 0.37624165415763855
In grad_steps = 2328, loss = 0.4812973141670227
In grad_steps = 2329, loss = 0.2098609358072281
In grad_steps = 2330, loss = 0.1704883575439453
In grad_steps = 2331, loss = 0.384589284658432
In grad_steps = 2332, loss = 0.1035294383764267
In grad_steps = 2333, loss = 0.46880674362182617
In grad_steps = 2334, loss = 0.07844612747430801
In grad_steps = 2335, loss = 0.5615642070770264
In grad_steps = 2336, loss = 0.5950808525085449
In grad_steps = 2337, loss = 0.18965590000152588
In grad_steps = 2338, loss = 0.10350791364908218
In grad_steps = 2339, loss = 0.10935946553945541
In grad_steps = 2340, loss = 0.5719553828239441
In grad_steps = 2341, loss = 0.37441927194595337
In grad_steps = 2342, loss = 0.34529879689216614
In grad_steps = 2343, loss = 0.20007358491420746
In grad_steps = 2344, loss = 0.5229660868644714
In grad_steps = 2345, loss = 0.3627321124076843
In grad_steps = 2346, loss = 0.17432066798210144
In grad_steps = 2347, loss = 0.20540137588977814
In grad_steps = 2348, loss = 0.4913303554058075
In grad_steps = 2349, loss = 0.6471734046936035
In grad_steps = 2350, loss = 0.48629361391067505
In grad_steps = 2351, loss = 0.03399554640054703
In grad_steps = 2352, loss = 0.07994114607572556
In grad_steps = 2353, loss = 0.8952512145042419
In grad_steps = 2354, loss = 0.23961833119392395
In grad_steps = 2355, loss = 0.26590725779533386
In grad_steps = 2356, loss = 0.05814814567565918
In grad_steps = 2357, loss = 0.033528875559568405
In grad_steps = 2358, loss = 0.03833184763789177
In grad_steps = 2359, loss = 0.26996058225631714
In grad_steps = 2360, loss = 1.049090027809143
In grad_steps = 2361, loss = 1.2203145027160645
In grad_steps = 2362, loss = 0.21314556896686554
In grad_steps = 2363, loss = 0.4825788736343384
In grad_steps = 2364, loss = 0.31002309918403625
In grad_steps = 2365, loss = 0.06138412281870842
In grad_steps = 2366, loss = 0.10321967303752899
In grad_steps = 2367, loss = 0.11404219269752502
In grad_steps = 2368, loss = 0.05971938744187355
In grad_steps = 2369, loss = 1.2188754081726074
In grad_steps = 2370, loss = 0.345266193151474
In grad_steps = 2371, loss = 0.1453753113746643
In grad_steps = 2372, loss = 0.0465242974460125
In grad_steps = 2373, loss = 0.2566014230251312
In grad_steps = 2374, loss = 0.16654449701309204
In grad_steps = 2375, loss = 0.5778256058692932
In grad_steps = 2376, loss = 0.1395220160484314
In grad_steps = 2377, loss = 0.05217130482196808
In grad_steps = 2378, loss = 0.42730963230133057
In grad_steps = 2379, loss = 0.6172342300415039
In grad_steps = 2380, loss = 0.14926990866661072
In grad_steps = 2381, loss = 0.01996776834130287
In grad_steps = 2382, loss = 0.5622292160987854
In grad_steps = 2383, loss = 0.19847173988819122
In grad_steps = 2384, loss = 0.03414912149310112
In grad_steps = 2385, loss = 0.029004834592342377
In grad_steps = 2386, loss = 0.2119893729686737
In grad_steps = 2387, loss = 0.36930835247039795
In grad_steps = 2388, loss = 0.06152338907122612
In grad_steps = 2389, loss = 0.5436844825744629
In grad_steps = 2390, loss = 0.035329416394233704
In grad_steps = 2391, loss = 0.04347017779946327
In grad_steps = 2392, loss = 0.48813268542289734
In grad_steps = 2393, loss = 0.036478664726018906
In grad_steps = 2394, loss = 0.7486976385116577
In grad_steps = 2395, loss = 0.05168383568525314
In grad_steps = 2396, loss = 0.864219605922699
In grad_steps = 2397, loss = 0.1168561652302742
In grad_steps = 2398, loss = 0.6365441083908081
In grad_steps = 2399, loss = 0.07425972074270248
In grad_steps = 2400, loss = 0.5573775768280029
In grad_steps = 2401, loss = 0.2832348644733429
In grad_steps = 2402, loss = 0.1703321635723114
In grad_steps = 2403, loss = 0.05973206087946892
In grad_steps = 2404, loss = 0.10726291686296463
In grad_steps = 2405, loss = 0.19843174517154694
In grad_steps = 2406, loss = 0.7904545664787292
In grad_steps = 2407, loss = 0.11120231449604034
In grad_steps = 2408, loss = 0.2630445063114166
In grad_steps = 2409, loss = 0.6932995915412903
In grad_steps = 2410, loss = 0.30303487181663513
In grad_steps = 2411, loss = 0.26483893394470215
In grad_steps = 2412, loss = 0.758734405040741
In grad_steps = 2413, loss = 0.2974224388599396
In grad_steps = 2414, loss = 0.23694002628326416
In grad_steps = 2415, loss = 0.29275304079055786
In grad_steps = 2416, loss = 0.8240673542022705
In grad_steps = 2417, loss = 0.12044501304626465
In grad_steps = 2418, loss = 0.745526909828186
In grad_steps = 2419, loss = 0.30142515897750854
In grad_steps = 2420, loss = 0.3411831259727478
In grad_steps = 2421, loss = 0.47069528698921204
In grad_steps = 2422, loss = 0.3201372027397156
In grad_steps = 2423, loss = 0.7274852991104126
In grad_steps = 2424, loss = 0.08207222074270248
In grad_steps = 2425, loss = 0.27696025371551514
In grad_steps = 2426, loss = 0.5279679894447327
In grad_steps = 2427, loss = 0.13451701402664185
In grad_steps = 2428, loss = 0.2904891073703766
In grad_steps = 2429, loss = 0.26424670219421387
In grad_steps = 2430, loss = 0.18746688961982727
In grad_steps = 2431, loss = 0.21910250186920166
In grad_steps = 2432, loss = 0.14882498979568481
In grad_steps = 2433, loss = 0.32707488536834717
In grad_steps = 2434, loss = 0.3827051818370819
In grad_steps = 2435, loss = 0.08590255677700043
In grad_steps = 2436, loss = 0.04906652495265007
In grad_steps = 2437, loss = 0.04829289764165878
In grad_steps = 2438, loss = 0.05238188058137894
In grad_steps = 2439, loss = 0.03624272346496582
In grad_steps = 2440, loss = 0.7688708901405334
In grad_steps = 2441, loss = 0.3710216283798218
In grad_steps = 2442, loss = 0.7256737947463989
In grad_steps = 2443, loss = 0.05130697041749954
In grad_steps = 2444, loss = 0.6822608113288879
In grad_steps = 2445, loss = 0.21211639046669006
In grad_steps = 2446, loss = 0.06333862990140915
In grad_steps = 2447, loss = 0.1462232768535614
In grad_steps = 2448, loss = 1.2619304656982422
In grad_steps = 2449, loss = 0.724602222442627
In grad_steps = 2450, loss = 0.006598262581974268
In grad_steps = 2451, loss = 1.2446227073669434
In grad_steps = 2452, loss = 0.08903288841247559
In grad_steps = 2453, loss = 0.9339065551757812
In grad_steps = 2454, loss = 0.09423767030239105
In grad_steps = 2455, loss = 0.11216062307357788
In grad_steps = 2456, loss = 0.11927486211061478
In grad_steps = 2457, loss = 0.08028048276901245
In grad_steps = 2458, loss = 0.11612983047962189
In grad_steps = 2459, loss = 0.42317646741867065
In grad_steps = 2460, loss = 0.03546527028083801
In grad_steps = 2461, loss = 0.0667642131447792
In grad_steps = 2462, loss = 0.20807859301567078
In grad_steps = 2463, loss = 0.21997499465942383
In grad_steps = 2464, loss = 0.27077364921569824
In grad_steps = 2465, loss = 0.1298244297504425
In grad_steps = 2466, loss = 0.2985229194164276
In grad_steps = 2467, loss = 0.39749541878700256
In grad_steps = 2468, loss = 0.5162197947502136
In grad_steps = 2469, loss = 1.2357864379882812
In grad_steps = 2470, loss = 0.10991104692220688
In grad_steps = 2471, loss = 0.2404801845550537
In grad_steps = 2472, loss = 0.2389821857213974
In grad_steps = 2473, loss = 0.5994396805763245
In grad_steps = 2474, loss = 0.11895756423473358
In grad_steps = 2475, loss = 0.21119797229766846
In grad_steps = 2476, loss = 0.20774370431900024
In grad_steps = 2477, loss = 0.06164325773715973
In grad_steps = 2478, loss = 0.04183238744735718
In grad_steps = 2479, loss = 0.13897523283958435
In grad_steps = 2480, loss = 0.10838580131530762
In grad_steps = 2481, loss = 0.40751227736473083
In grad_steps = 2482, loss = 0.8415660858154297
In grad_steps = 2483, loss = 0.07433810830116272
In grad_steps = 2484, loss = 0.5504605770111084
In grad_steps = 2485, loss = 0.22794994711875916
In grad_steps = 2486, loss = 0.907342791557312
In grad_steps = 2487, loss = 0.04807427525520325
In grad_steps = 2488, loss = 0.0908484011888504
In grad_steps = 2489, loss = 0.07665011286735535
In grad_steps = 2490, loss = 0.18689942359924316
In grad_steps = 2491, loss = 0.048627860844135284
In grad_steps = 2492, loss = 0.23902654647827148
In grad_steps = 2493, loss = 0.0786745548248291
In grad_steps = 2494, loss = 0.030757587403059006
In grad_steps = 2495, loss = 0.27155017852783203
In grad_steps = 2496, loss = 0.07090774178504944
In grad_steps = 2497, loss = 0.4808785617351532
In grad_steps = 2498, loss = 0.08604919165372849
In grad_steps = 2499, loss = 0.11602281033992767
In grad_steps = 2500, loss = 0.2066323459148407
In grad_steps = 2501, loss = 0.0639776960015297
In grad_steps = 2502, loss = 0.05992666631937027
In grad_steps = 2503, loss = 0.021291200071573257
In grad_steps = 2504, loss = 0.05003484711050987
In grad_steps = 2505, loss = 0.17547038197517395
In grad_steps = 2506, loss = 0.01696675829589367
In grad_steps = 2507, loss = 0.07586783170700073
In grad_steps = 2508, loss = 0.08198574185371399
In grad_steps = 2509, loss = 0.8042991161346436
In grad_steps = 2510, loss = 0.008104109205305576
In grad_steps = 2511, loss = 0.04881829023361206
In grad_steps = 2512, loss = 0.008816307410597801
In grad_steps = 2513, loss = 0.008293211460113525
In grad_steps = 2514, loss = 0.31146982312202454
In grad_steps = 2515, loss = 0.3533831834793091
In grad_steps = 2516, loss = 0.05077306926250458
In grad_steps = 2517, loss = 0.11526563763618469
In grad_steps = 2518, loss = 0.007968587800860405
In grad_steps = 2519, loss = 0.08202658593654633
In grad_steps = 2520, loss = 0.4802294671535492
In grad_steps = 2521, loss = 0.10990363359451294
In grad_steps = 2522, loss = 1.0811045169830322
In grad_steps = 2523, loss = 0.01138238050043583
In grad_steps = 2524, loss = 0.2703268826007843
In grad_steps = 2525, loss = 0.46499213576316833
In grad_steps = 2526, loss = 0.9334758520126343
In grad_steps = 2527, loss = 1.1942839622497559
In grad_steps = 2528, loss = 0.02975521609187126
In grad_steps = 2529, loss = 0.06412849575281143
In grad_steps = 2530, loss = 0.38708123564720154
In grad_steps = 2531, loss = 0.19943572580814362
In grad_steps = 2532, loss = 0.08961375802755356
In grad_steps = 2533, loss = 0.04045453667640686
In grad_steps = 2534, loss = 0.044946279376745224
In grad_steps = 2535, loss = 0.04791232571005821
In grad_steps = 2536, loss = 0.4466017186641693
In grad_steps = 2537, loss = 0.03296725079417229
In grad_steps = 2538, loss = 0.1433590054512024
In grad_steps = 2539, loss = 0.12898686528205872
In grad_steps = 2540, loss = 0.6016929149627686
In grad_steps = 2541, loss = 0.1315595507621765
In grad_steps = 2542, loss = 0.06555642932653427
In grad_steps = 2543, loss = 0.1699860841035843
In grad_steps = 2544, loss = 0.2660510241985321
In grad_steps = 2545, loss = 0.11027251929044724
In grad_steps = 2546, loss = 0.14082540571689606
In grad_steps = 2547, loss = 0.11813883483409882
In grad_steps = 2548, loss = 0.07160578668117523
In grad_steps = 2549, loss = 0.6317465901374817
In grad_steps = 2550, loss = 1.2048263549804688
In grad_steps = 2551, loss = 0.16169677674770355
In grad_steps = 2552, loss = 0.06476012617349625
In grad_steps = 2553, loss = 0.03437879681587219
In grad_steps = 2554, loss = 0.027123458683490753
In grad_steps = 2555, loss = 1.2273907661437988
In grad_steps = 2556, loss = 0.05847426503896713
In grad_steps = 2557, loss = 0.5572142601013184
In grad_steps = 2558, loss = 0.021719785407185555
In grad_steps = 2559, loss = 0.2889868915081024
In grad_steps = 2560, loss = 0.14089393615722656
In grad_steps = 2561, loss = 0.2613241374492645
In grad_steps = 2562, loss = 0.45237475633621216
In grad_steps = 2563, loss = 0.12952770292758942
In grad_steps = 2564, loss = 0.10977881401777267
In grad_steps = 2565, loss = 0.21789351105690002
In grad_steps = 2566, loss = 0.07337823510169983
In grad_steps = 2567, loss = 0.08616798371076584
In grad_steps = 2568, loss = 0.050364404916763306
In grad_steps = 2569, loss = 0.04287925735116005
In grad_steps = 2570, loss = 0.15825198590755463
In grad_steps = 2571, loss = 0.05803072080016136
In grad_steps = 2572, loss = 0.047927454113960266
In grad_steps = 2573, loss = 0.5270128846168518
In grad_steps = 2574, loss = 0.44159767031669617
In grad_steps = 2575, loss = 0.5193198323249817
In grad_steps = 2576, loss = 0.5885658264160156
In grad_steps = 2577, loss = 0.1116940826177597
In grad_steps = 2578, loss = 0.24354912340641022
In grad_steps = 2579, loss = 0.28965747356414795
In grad_steps = 2580, loss = 0.5894291400909424
In grad_steps = 2581, loss = 0.6067594289779663
In grad_steps = 2582, loss = 0.05147932469844818
In grad_steps = 2583, loss = 0.08009158074855804
In grad_steps = 2584, loss = 0.03989081084728241
In grad_steps = 2585, loss = 1.0332605838775635
In grad_steps = 2586, loss = 0.913338303565979
In grad_steps = 2587, loss = 0.07492633163928986
In grad_steps = 2588, loss = 1.6888558864593506
In grad_steps = 2589, loss = 0.4226539134979248
In grad_steps = 2590, loss = 0.623205840587616
In grad_steps = 2591, loss = 0.6514427661895752
In grad_steps = 2592, loss = 0.3018009662628174
In grad_steps = 2593, loss = 0.2267276644706726
In grad_steps = 2594, loss = 0.3218287229537964
In grad_steps = 2595, loss = 0.22200649976730347
In grad_steps = 2596, loss = 0.2037791907787323
In grad_steps = 2597, loss = 0.3872115910053253
In grad_steps = 2598, loss = 0.28708937764167786
In grad_steps = 2599, loss = 0.4441130459308624
In grad_steps = 2600, loss = 0.26871854066848755
In grad_steps = 2601, loss = 0.290584921836853
In grad_steps = 2602, loss = 0.5881682634353638
In grad_steps = 2603, loss = 0.21011552214622498
In grad_steps = 2604, loss = 0.2631077468395233
In grad_steps = 2605, loss = 0.30888819694519043
In grad_steps = 2606, loss = 0.33711108565330505
In grad_steps = 2607, loss = 0.08928287774324417
In grad_steps = 2608, loss = 0.11011821031570435
In grad_steps = 2609, loss = 0.2460022270679474
In grad_steps = 2610, loss = 0.45121240615844727
In grad_steps = 2611, loss = 0.029759526252746582
In grad_steps = 2612, loss = 0.31947678327560425
In grad_steps = 2613, loss = 0.1546025425195694
In grad_steps = 2614, loss = 0.10143186151981354
In grad_steps = 2615, loss = 0.11004963517189026
In grad_steps = 2616, loss = 0.03603029251098633
In grad_steps = 2617, loss = 0.16229325532913208
In grad_steps = 2618, loss = 0.160142183303833
In grad_steps = 2619, loss = 0.5186715126037598
In grad_steps = 2620, loss = 1.1684001684188843
In grad_steps = 2621, loss = 0.09668849408626556
In grad_steps = 2622, loss = 0.20402032136917114
In grad_steps = 2623, loss = 0.1567016839981079
In grad_steps = 2624, loss = 0.6833085417747498
In grad_steps = 2625, loss = 0.03371618315577507
In grad_steps = 2626, loss = 0.017346810549497604
In grad_steps = 2627, loss = 0.6838511228561401
In grad_steps = 2628, loss = 0.04916295036673546
In grad_steps = 2629, loss = 0.8850674629211426
In grad_steps = 2630, loss = 0.5879694819450378
In grad_steps = 2631, loss = 0.7970765233039856
In grad_steps = 2632, loss = 0.08121384680271149
In grad_steps = 2633, loss = 0.29603418707847595
In grad_steps = 2634, loss = 0.1298871785402298
In grad_steps = 2635, loss = 0.5963224768638611
In grad_steps = 2636, loss = 0.2050032913684845
In grad_steps = 2637, loss = 0.1440991759300232
In grad_steps = 2638, loss = 0.41880181431770325
In grad_steps = 2639, loss = 0.21231462061405182
In grad_steps = 2640, loss = 0.2644079327583313
In grad_steps = 2641, loss = 0.17107054591178894
In grad_steps = 2642, loss = 0.15210546553134918
In grad_steps = 2643, loss = 0.46122297644615173
In grad_steps = 2644, loss = 0.2606937289237976
In grad_steps = 2645, loss = 0.2159268856048584
In grad_steps = 2646, loss = 0.3765122592449188
In grad_steps = 2647, loss = 0.35327520966529846
In grad_steps = 2648, loss = 0.3738163411617279
In grad_steps = 2649, loss = 0.391558974981308
In grad_steps = 2650, loss = 0.35207074880599976
In grad_steps = 2651, loss = 0.1397639960050583
In grad_steps = 2652, loss = 0.7816066741943359
In grad_steps = 2653, loss = 0.7621283531188965
In grad_steps = 2654, loss = 0.058835580945014954
In grad_steps = 2655, loss = 0.2659236788749695
In grad_steps = 2656, loss = 0.032027844339609146
In grad_steps = 2657, loss = 0.5585408806800842
In grad_steps = 2658, loss = 1.3234723806381226
In grad_steps = 2659, loss = 0.228798970580101
In grad_steps = 2660, loss = 0.3427436053752899
In grad_steps = 2661, loss = 0.5826305747032166
In grad_steps = 2662, loss = 0.04274154081940651
In grad_steps = 2663, loss = 0.21250994503498077
In grad_steps = 2664, loss = 0.16483014822006226
In grad_steps = 2665, loss = 0.2887139916419983
In grad_steps = 2666, loss = 0.08251604437828064
In grad_steps = 2667, loss = 0.12331578880548477
In grad_steps = 2668, loss = 0.38830628991127014
In grad_steps = 2669, loss = 0.13417771458625793
In grad_steps = 2670, loss = 0.15293477475643158
In grad_steps = 2671, loss = 0.25891542434692383
In grad_steps = 2672, loss = 0.10043733566999435
In grad_steps = 2673, loss = 0.04971950501203537
In grad_steps = 2674, loss = 0.04388701915740967
In grad_steps = 2675, loss = 0.03349428251385689
In grad_steps = 2676, loss = 0.039515044540166855
In grad_steps = 2677, loss = 0.015223458409309387
In grad_steps = 2678, loss = 0.11321797966957092
In grad_steps = 2679, loss = 0.20977632701396942
In grad_steps = 2680, loss = 0.032277848571538925
In grad_steps = 2681, loss = 0.03827710449695587
In grad_steps = 2682, loss = 0.7702904939651489
In grad_steps = 2683, loss = 0.00923706404864788
In grad_steps = 2684, loss = 0.08070623129606247
In grad_steps = 2685, loss = 0.355766236782074
In grad_steps = 2686, loss = 0.11627745628356934
In grad_steps = 2687, loss = 0.0712413340806961
In grad_steps = 2688, loss = 0.25736796855926514
In grad_steps = 2689, loss = 0.04403858259320259
In grad_steps = 2690, loss = 0.24611525237560272
In grad_steps = 2691, loss = 0.05255908891558647
In grad_steps = 2692, loss = 0.026648398488759995
In grad_steps = 2693, loss = 1.4644074440002441
In grad_steps = 2694, loss = 0.44114071130752563
In grad_steps = 2695, loss = 0.055740613490343094
In grad_steps = 2696, loss = 0.012894357554614544
In grad_steps = 2697, loss = 0.2165915071964264
In grad_steps = 2698, loss = 0.11182389408349991
In grad_steps = 2699, loss = 0.01749778538942337
In grad_steps = 2700, loss = 1.2200785875320435
In grad_steps = 2701, loss = 0.1400778591632843
In grad_steps = 2702, loss = 0.14198382198810577
In grad_steps = 2703, loss = 0.7222235798835754
In grad_steps = 2704, loss = 0.050397418439388275
In grad_steps = 2705, loss = 0.1789507120847702
In grad_steps = 2706, loss = 0.18260058760643005
In grad_steps = 2707, loss = 1.0347459316253662
In grad_steps = 2708, loss = 0.637173056602478
In grad_steps = 2709, loss = 0.6295176148414612
In grad_steps = 2710, loss = 0.4146376848220825
In grad_steps = 2711, loss = 0.12540647387504578
In grad_steps = 2712, loss = 0.1740470975637436
In grad_steps = 2713, loss = 0.2753753364086151
In grad_steps = 2714, loss = 0.4915866255760193
In grad_steps = 2715, loss = 0.5238228440284729
In grad_steps = 2716, loss = 0.07599322497844696
In grad_steps = 2717, loss = 0.6949397325515747
In grad_steps = 2718, loss = 0.385443776845932
In grad_steps = 2719, loss = 0.15143296122550964
In grad_steps = 2720, loss = 0.3042319118976593
In grad_steps = 2721, loss = 0.5403017401695251
In grad_steps = 2722, loss = 0.21195560693740845
In grad_steps = 2723, loss = 0.22258992493152618
In grad_steps = 2724, loss = 0.43195033073425293
In grad_steps = 2725, loss = 0.1458643525838852
In grad_steps = 2726, loss = 0.927687406539917
In grad_steps = 2727, loss = 0.024400971829891205
In grad_steps = 2728, loss = 0.5240919589996338
In grad_steps = 2729, loss = 0.4892219305038452
In grad_steps = 2730, loss = 0.0742524117231369
In grad_steps = 2731, loss = 0.05925693362951279
In grad_steps = 2732, loss = 0.09510942548513412
In grad_steps = 2733, loss = 0.40685224533081055
In grad_steps = 2734, loss = 0.06037820130586624
In grad_steps = 2735, loss = 1.253942847251892
In grad_steps = 2736, loss = 0.04802107810974121
In grad_steps = 2737, loss = 0.25891759991645813
In grad_steps = 2738, loss = 0.28845706582069397
In grad_steps = 2739, loss = 0.20181310176849365
In grad_steps = 2740, loss = 0.0731617659330368
In grad_steps = 2741, loss = 0.05449547991156578
In grad_steps = 2742, loss = 0.17525607347488403
In grad_steps = 2743, loss = 0.10968760401010513
In grad_steps = 2744, loss = 0.14332938194274902
In grad_steps = 2745, loss = 0.026805993169546127
In grad_steps = 2746, loss = 1.16298246383667
In grad_steps = 2747, loss = 0.23226022720336914
In grad_steps = 2748, loss = 0.04682973027229309
In grad_steps = 2749, loss = 0.04431769251823425
In grad_steps = 2750, loss = 0.1544453501701355
In grad_steps = 2751, loss = 0.12829671800136566
In grad_steps = 2752, loss = 0.07457708567380905
In grad_steps = 2753, loss = 0.4486013352870941
In grad_steps = 2754, loss = 0.19465869665145874
In grad_steps = 2755, loss = 0.028928987681865692
In grad_steps = 2756, loss = 0.06626445055007935
In grad_steps = 2757, loss = 0.025822214782238007
In grad_steps = 2758, loss = 1.4088160991668701
In grad_steps = 2759, loss = 0.23082241415977478
In grad_steps = 2760, loss = 0.0374322310090065
In grad_steps = 2761, loss = 0.08840519189834595
In grad_steps = 2762, loss = 0.19180428981781006
In grad_steps = 2763, loss = 0.17218360304832458
In grad_steps = 2764, loss = 0.01779577136039734
In grad_steps = 2765, loss = 0.09820305556058884
In grad_steps = 2766, loss = 1.0373014211654663
In grad_steps = 2767, loss = 0.04148624837398529
In grad_steps = 2768, loss = 0.8130220174789429
In grad_steps = 2769, loss = 0.09634934365749359
In grad_steps = 2770, loss = 0.05067719519138336
In grad_steps = 2771, loss = 0.0802803784608841
In grad_steps = 2772, loss = 0.40518319606781006
In grad_steps = 2773, loss = 2.3332180976867676
In grad_steps = 2774, loss = 0.6269079446792603
In grad_steps = 2775, loss = 0.054752156138420105
In grad_steps = 2776, loss = 0.15953470766544342
In grad_steps = 2777, loss = 0.04044002667069435
In grad_steps = 2778, loss = 0.10534197837114334
In grad_steps = 2779, loss = 0.11710797250270844
In grad_steps = 2780, loss = 0.09437757730484009
In grad_steps = 2781, loss = 0.1896292269229889
In grad_steps = 2782, loss = 0.15090614557266235
In grad_steps = 2783, loss = 0.0718718022108078
In grad_steps = 2784, loss = 0.08330883085727692
In grad_steps = 2785, loss = 0.12021763622760773
In grad_steps = 2786, loss = 0.2758040428161621
In grad_steps = 2787, loss = 0.17595034837722778
In grad_steps = 2788, loss = 0.5898821949958801
In grad_steps = 2789, loss = 1.1055259704589844
In grad_steps = 2790, loss = 0.3222123980522156
In grad_steps = 2791, loss = 0.3326263427734375
In grad_steps = 2792, loss = 0.06805731356143951
In grad_steps = 2793, loss = 0.38638490438461304
In grad_steps = 2794, loss = 0.10502804070711136
In grad_steps = 2795, loss = 0.26990002393722534
In grad_steps = 2796, loss = 0.1868496835231781
In grad_steps = 2797, loss = 0.14193208515644073
In grad_steps = 2798, loss = 0.022388437762856483
In grad_steps = 2799, loss = 0.22943799197673798
In grad_steps = 2800, loss = 0.1406305432319641
In grad_steps = 2801, loss = 0.038170021027326584
In grad_steps = 2802, loss = 0.2941771149635315
In grad_steps = 2803, loss = 1.0447856187820435
In grad_steps = 2804, loss = 0.16108299791812897
In grad_steps = 2805, loss = 0.6126707196235657
In grad_steps = 2806, loss = 0.047686949372291565
In grad_steps = 2807, loss = 0.8998805284500122
In grad_steps = 2808, loss = 0.04961322993040085
In grad_steps = 2809, loss = 0.03504033386707306
In grad_steps = 2810, loss = 0.08940558135509491
In grad_steps = 2811, loss = 0.06953494250774384
In grad_steps = 2812, loss = 0.08420126140117645
In grad_steps = 2813, loss = 0.033886365592479706
In grad_steps = 2814, loss = 0.0266699381172657
In grad_steps = 2815, loss = 0.10017995536327362
In grad_steps = 2816, loss = 0.1650353968143463
In grad_steps = 2817, loss = 0.026864387094974518
In grad_steps = 2818, loss = 0.13055920600891113
In grad_steps = 2819, loss = 0.27716201543807983
In grad_steps = 2820, loss = 0.026887401938438416
In grad_steps = 2821, loss = 0.27517449855804443
In grad_steps = 2822, loss = 0.1419355273246765
In grad_steps = 2823, loss = 0.018580611795186996
In grad_steps = 2824, loss = 0.6253595948219299
In grad_steps = 2825, loss = 0.4303121566772461
In grad_steps = 2826, loss = 0.024385010823607445
In grad_steps = 2827, loss = 0.27328944206237793
In grad_steps = 2828, loss = 0.02273900806903839
In grad_steps = 2829, loss = 0.013178584165871143
In grad_steps = 2830, loss = 0.29824337363243103
In grad_steps = 2831, loss = 0.5202149152755737
In grad_steps = 2832, loss = 0.22538383305072784
In grad_steps = 2833, loss = 0.05886180326342583
In grad_steps = 2834, loss = 0.9978926777839661
In grad_steps = 2835, loss = 0.4415394067764282
In grad_steps = 2836, loss = 0.1897643506526947
In grad_steps = 2837, loss = 0.27663561701774597
In grad_steps = 2838, loss = 0.857732355594635
In grad_steps = 2839, loss = 0.5546557307243347
In grad_steps = 2840, loss = 0.09888830780982971
In grad_steps = 2841, loss = 0.04775790497660637
In grad_steps = 2842, loss = 0.22691145539283752
In grad_steps = 2843, loss = 0.35395580530166626
In grad_steps = 2844, loss = 0.060109253972768784
In grad_steps = 2845, loss = 0.14586204290390015
In grad_steps = 2846, loss = 0.10370543599128723
In grad_steps = 2847, loss = 0.13196519017219543
In grad_steps = 2848, loss = 0.09533016383647919
In grad_steps = 2849, loss = 0.41054466366767883
In grad_steps = 2850, loss = 0.04395304247736931
In grad_steps = 2851, loss = 0.07668298482894897
In grad_steps = 2852, loss = 0.06587603688240051
In grad_steps = 2853, loss = 0.05192102491855621
In grad_steps = 2854, loss = 0.022929951548576355
In grad_steps = 2855, loss = 0.018760181963443756
In grad_steps = 2856, loss = 0.07386286556720734
In grad_steps = 2857, loss = 0.023704782128334045
In grad_steps = 2858, loss = 0.1223456859588623
In grad_steps = 2859, loss = 0.8639433979988098
In grad_steps = 2860, loss = 0.03975638374686241
In grad_steps = 2861, loss = 0.04549176245927811
In grad_steps = 2862, loss = 0.2527555823326111
In grad_steps = 2863, loss = 0.021321695297956467
In grad_steps = 2864, loss = 0.010856302455067635
In grad_steps = 2865, loss = 0.05862110108137131
In grad_steps = 2866, loss = 0.029141752049326897
In grad_steps = 2867, loss = 0.3260290026664734
In grad_steps = 2868, loss = 0.028163809329271317
In grad_steps = 2869, loss = 0.013966877944767475
In grad_steps = 2870, loss = 0.06914006918668747
In grad_steps = 2871, loss = 0.05953942984342575
In grad_steps = 2872, loss = 0.7273172736167908
In grad_steps = 2873, loss = 0.011127586476504803
In grad_steps = 2874, loss = 0.8688750267028809
In grad_steps = 2875, loss = 0.0781438797712326
In grad_steps = 2876, loss = 0.0994616150856018
In grad_steps = 2877, loss = 0.012205736711621284
In grad_steps = 2878, loss = 0.1616668701171875
In grad_steps = 2879, loss = 0.3470348119735718
In grad_steps = 2880, loss = 0.08018867671489716
In grad_steps = 2881, loss = 0.4681991636753082
In grad_steps = 2882, loss = 0.1480783373117447
In grad_steps = 2883, loss = 0.5321748852729797
In grad_steps = 2884, loss = 0.015400661155581474
In grad_steps = 2885, loss = 0.30463045835494995
In grad_steps = 2886, loss = 0.6472890377044678
In grad_steps = 2887, loss = 0.2682713270187378
In grad_steps = 2888, loss = 0.709136962890625
In grad_steps = 2889, loss = 0.6401752829551697
In grad_steps = 2890, loss = 0.5545440912246704
In grad_steps = 2891, loss = 0.682986855506897
In grad_steps = 2892, loss = 0.37849605083465576
In grad_steps = 2893, loss = 0.07566552609205246
In grad_steps = 2894, loss = 0.15737932920455933
In grad_steps = 2895, loss = 0.17446330189704895
In grad_steps = 2896, loss = 0.663261890411377
In grad_steps = 2897, loss = 0.14125192165374756
In grad_steps = 2898, loss = 0.24334843456745148
In grad_steps = 2899, loss = 0.3803509473800659
In grad_steps = 2900, loss = 0.5164303183555603
In grad_steps = 2901, loss = 0.14699441194534302
In grad_steps = 2902, loss = 0.1887248307466507
In grad_steps = 2903, loss = 0.11088195443153381
In grad_steps = 2904, loss = 0.17146196961402893
In grad_steps = 2905, loss = 0.5659252405166626
In grad_steps = 2906, loss = 0.8856522440910339
In grad_steps = 2907, loss = 0.17117753624916077
In grad_steps = 2908, loss = 0.455980509519577
In grad_steps = 2909, loss = 0.24581456184387207
In grad_steps = 2910, loss = 0.2540202736854553
In grad_steps = 2911, loss = 0.08451668173074722
In grad_steps = 2912, loss = 0.2005191594362259
In grad_steps = 2913, loss = 0.15584807097911835
In grad_steps = 2914, loss = 0.5681158304214478
In grad_steps = 2915, loss = 0.06538185477256775
In grad_steps = 2916, loss = 0.22204242646694183
In grad_steps = 2917, loss = 0.13750189542770386
In grad_steps = 2918, loss = 0.07741347700357437
In grad_steps = 2919, loss = 0.14156830310821533
In grad_steps = 2920, loss = 0.06637626141309738
In grad_steps = 2921, loss = 0.2751193344593048
In grad_steps = 2922, loss = 0.0168182123452425
In grad_steps = 2923, loss = 0.9015744924545288
In grad_steps = 2924, loss = 0.009643752127885818
In grad_steps = 2925, loss = 0.3640053868293762
In grad_steps = 2926, loss = 0.17069752514362335
In grad_steps = 2927, loss = 0.27493488788604736
In grad_steps = 2928, loss = 0.028528500348329544
In grad_steps = 2929, loss = 0.015634343028068542
In grad_steps = 2930, loss = 0.008888582699000835
In grad_steps = 2931, loss = 0.6376425623893738
In grad_steps = 2932, loss = 0.14133627712726593
In grad_steps = 2933, loss = 0.012445017695426941
In grad_steps = 2934, loss = 0.21702831983566284
In grad_steps = 2935, loss = 0.0453646145761013
In grad_steps = 2936, loss = 0.09140293300151825
In grad_steps = 2937, loss = 0.06712352484464645
In grad_steps = 2938, loss = 0.06095373257994652
In grad_steps = 2939, loss = 0.21240785717964172
In grad_steps = 2940, loss = 1.3043183088302612
In grad_steps = 2941, loss = 0.02710115723311901
In grad_steps = 2942, loss = 0.03081803023815155
In grad_steps = 2943, loss = 0.19564679265022278
In grad_steps = 2944, loss = 0.5159827470779419
In grad_steps = 2945, loss = 0.5468727946281433
In grad_steps = 2946, loss = 0.6663705706596375
In grad_steps = 2947, loss = 0.05500701069831848
In grad_steps = 2948, loss = 0.039108846336603165
In grad_steps = 2949, loss = 0.27170276641845703
In grad_steps = 2950, loss = 0.06096291542053223
In grad_steps = 2951, loss = 0.5392799973487854
In grad_steps = 2952, loss = 0.31413009762763977
In grad_steps = 2953, loss = 0.27372586727142334
In grad_steps = 2954, loss = 0.34605419635772705
In grad_steps = 2955, loss = 0.1534605473279953
In grad_steps = 2956, loss = 0.8922147750854492
In grad_steps = 2957, loss = 0.34837836027145386
In grad_steps = 2958, loss = 0.04010055959224701
In grad_steps = 2959, loss = 0.6732535362243652
In grad_steps = 2960, loss = 0.17052391171455383
In grad_steps = 2961, loss = 0.028727732598781586
In grad_steps = 2962, loss = 0.37102869153022766
In grad_steps = 2963, loss = 0.11740881949663162
In grad_steps = 2964, loss = 0.6695488691329956
In grad_steps = 2965, loss = 0.09198000282049179
In grad_steps = 2966, loss = 0.2866186797618866
In grad_steps = 2967, loss = 0.2810151278972626
In grad_steps = 2968, loss = 0.9027199745178223
In grad_steps = 2969, loss = 0.07361847907304764
In grad_steps = 2970, loss = 0.29574453830718994
In grad_steps = 2971, loss = 0.057052019983530045
In grad_steps = 2972, loss = 0.1436721235513687
In grad_steps = 2973, loss = 0.17462344467639923
In grad_steps = 2974, loss = 0.8352029919624329
In grad_steps = 2975, loss = 0.5382828116416931
In grad_steps = 2976, loss = 0.158632293343544
In grad_steps = 2977, loss = 0.11356300115585327
In grad_steps = 2978, loss = 0.09619717299938202
In grad_steps = 2979, loss = 0.28151872754096985
In grad_steps = 2980, loss = 0.025106284767389297
In grad_steps = 2981, loss = 0.35643815994262695
In grad_steps = 2982, loss = 0.04776667430996895
In grad_steps = 2983, loss = 0.3598875403404236
In grad_steps = 2984, loss = 0.07806915044784546
In grad_steps = 2985, loss = 0.02936876006424427
In grad_steps = 2986, loss = 0.34111201763153076
In grad_steps = 2987, loss = 0.06595027446746826
In grad_steps = 2988, loss = 0.9978024959564209
In grad_steps = 2989, loss = 0.27801379561424255
In grad_steps = 2990, loss = 0.07411326467990875
In grad_steps = 2991, loss = 0.07192331552505493
In grad_steps = 2992, loss = 0.16513526439666748
In grad_steps = 2993, loss = 0.06632962822914124
In grad_steps = 2994, loss = 0.06688852608203888
In grad_steps = 2995, loss = 0.04673505201935768
In grad_steps = 2996, loss = 1.0429136753082275
In grad_steps = 2997, loss = 0.2705298960208893
In grad_steps = 2998, loss = 0.3493143916130066
In grad_steps = 2999, loss = 0.4187246561050415
In grad_steps = 3000, loss = 0.26434773206710815
In grad_steps = 3001, loss = 0.1587834507226944
In grad_steps = 3002, loss = 0.23929308354854584
In grad_steps = 3003, loss = 0.22717070579528809
In grad_steps = 3004, loss = 0.871939480304718
In grad_steps = 3005, loss = 0.006563818547874689
In grad_steps = 3006, loss = 0.2045002281665802
In grad_steps = 3007, loss = 0.11175815761089325
In grad_steps = 3008, loss = 0.17892134189605713
In grad_steps = 3009, loss = 0.028307795524597168
In grad_steps = 3010, loss = 0.16382257640361786
In grad_steps = 3011, loss = 1.2903600931167603
In grad_steps = 3012, loss = 0.9839166402816772
In grad_steps = 3013, loss = 0.026484476402401924
In grad_steps = 3014, loss = 0.20855006575584412
In grad_steps = 3015, loss = 0.12381692230701447
In grad_steps = 3016, loss = 0.24443857371807098
In grad_steps = 3017, loss = 0.23599494993686676
In grad_steps = 3018, loss = 0.13992831110954285
In grad_steps = 3019, loss = 0.13867171108722687
In grad_steps = 3020, loss = 0.26204174757003784
In grad_steps = 3021, loss = 0.21347485482692719
In grad_steps = 3022, loss = 0.32755595445632935
In grad_steps = 3023, loss = 0.09128554910421371
In grad_steps = 3024, loss = 0.047377027571201324
In grad_steps = 3025, loss = 1.0028080940246582
In grad_steps = 3026, loss = 0.2746119797229767
In grad_steps = 3027, loss = 0.05522174388170242
In grad_steps = 3028, loss = 0.13322871923446655
In grad_steps = 3029, loss = 0.13207027316093445
In grad_steps = 3030, loss = 0.08176462352275848
In grad_steps = 3031, loss = 1.0269861221313477
In grad_steps = 3032, loss = 1.1995891332626343
In grad_steps = 3033, loss = 1.2021373510360718
In grad_steps = 3034, loss = 0.8991626501083374
In grad_steps = 3035, loss = 1.1546136140823364
In grad_steps = 3036, loss = 0.29821258783340454
In grad_steps = 3037, loss = 0.4124198853969574
In grad_steps = 3038, loss = 0.5107383131980896
In grad_steps = 3039, loss = 0.07170175760984421
In grad_steps = 3040, loss = 0.8614134788513184
In grad_steps = 3041, loss = 0.3239373564720154
In grad_steps = 3042, loss = 0.44820934534072876
In grad_steps = 3043, loss = 0.7005951404571533
In grad_steps = 3044, loss = 1.081535816192627
In grad_steps = 3045, loss = 0.17477534711360931
In grad_steps = 3046, loss = 0.37339434027671814
In grad_steps = 3047, loss = 0.40164774656295776
In grad_steps = 3048, loss = 0.1635320782661438
In grad_steps = 3049, loss = 0.5411684513092041
In grad_steps = 3050, loss = 0.12064032256603241
In grad_steps = 3051, loss = 0.5230348110198975
In grad_steps = 3052, loss = 0.028397932648658752
In grad_steps = 3053, loss = 0.04934604465961456
In grad_steps = 3054, loss = 0.1524035930633545
In grad_steps = 3055, loss = 0.04306325316429138
In grad_steps = 3056, loss = 0.14435279369354248
In grad_steps = 3057, loss = 0.08992443978786469
In grad_steps = 3058, loss = 0.38756096363067627
In grad_steps = 3059, loss = 0.13860373198986053
In grad_steps = 3060, loss = 0.021081579849123955
In grad_steps = 3061, loss = 0.04599300026893616
In grad_steps = 3062, loss = 1.2601516246795654
In grad_steps = 3063, loss = 0.037481412291526794
In grad_steps = 3064, loss = 0.02511604130268097
In grad_steps = 3065, loss = 0.10538049042224884
In grad_steps = 3066, loss = 1.226983904838562
In grad_steps = 3067, loss = 0.08016989380121231
In grad_steps = 3068, loss = 0.04761344939470291
In grad_steps = 3069, loss = 0.00477847782894969
In grad_steps = 3070, loss = 0.018829941749572754
In grad_steps = 3071, loss = 0.465695321559906
In grad_steps = 3072, loss = 0.8581104874610901
In grad_steps = 3073, loss = 0.8717621564865112
In grad_steps = 3074, loss = 0.40402427315711975
In grad_steps = 3075, loss = 0.23109567165374756
In grad_steps = 3076, loss = 0.11054018139839172
In grad_steps = 3077, loss = 0.2533782720565796
In grad_steps = 3078, loss = 0.936324954032898
In grad_steps = 3079, loss = 0.12991127371788025
In grad_steps = 3080, loss = 0.5130325555801392
In grad_steps = 3081, loss = 0.1829531192779541
In grad_steps = 3082, loss = 0.29171091318130493
In grad_steps = 3083, loss = 0.3008531928062439
In grad_steps = 3084, loss = 0.3692508339881897
In grad_steps = 3085, loss = 0.581028163433075
In grad_steps = 3086, loss = 0.12378952652215958
In grad_steps = 3087, loss = 0.13308589160442352
In grad_steps = 3088, loss = 0.628746747970581
In grad_steps = 3089, loss = 0.7423751950263977
In grad_steps = 3090, loss = 0.16860809922218323
In grad_steps = 3091, loss = 0.2374064028263092
In grad_steps = 3092, loss = 0.3997342586517334
In grad_steps = 3093, loss = 0.512308657169342
In grad_steps = 3094, loss = 0.3756261467933655
In grad_steps = 3095, loss = 0.359707772731781
In grad_steps = 3096, loss = 0.7296453714370728
In grad_steps = 3097, loss = 0.21431216597557068
In grad_steps = 3098, loss = 0.70538330078125
In grad_steps = 3099, loss = 0.03150475025177002
In grad_steps = 3100, loss = 0.053228966891765594
In grad_steps = 3101, loss = 0.35259518027305603
In grad_steps = 3102, loss = 0.1639762967824936
In grad_steps = 3103, loss = 0.6982741355895996
In grad_steps = 3104, loss = 0.08775901049375534
In grad_steps = 3105, loss = 0.1973186582326889
In grad_steps = 3106, loss = 0.40945279598236084
In grad_steps = 3107, loss = 0.061427198350429535
In grad_steps = 3108, loss = 0.11840155720710754
In grad_steps = 3109, loss = 0.4592943787574768
In grad_steps = 3110, loss = 0.3604692816734314
In grad_steps = 3111, loss = 0.17121228575706482
In grad_steps = 3112, loss = 0.07285891473293304
In grad_steps = 3113, loss = 0.48239853978157043
In grad_steps = 3114, loss = 0.12102903425693512
In grad_steps = 3115, loss = 0.2633228302001953
In grad_steps = 3116, loss = 0.0926157757639885
In grad_steps = 3117, loss = 0.22916992008686066
In grad_steps = 3118, loss = 0.027484778314828873
In grad_steps = 3119, loss = 0.4194096028804779
In grad_steps = 3120, loss = 0.3377947211265564
In grad_steps = 3121, loss = 0.2092694491147995
In grad_steps = 3122, loss = 0.07574565708637238
In grad_steps = 3123, loss = 0.6159955263137817
In grad_steps = 3124, loss = 0.08093489706516266
In grad_steps = 3125, loss = 0.02028178982436657
In grad_steps = 3126, loss = 0.16680851578712463
In grad_steps = 3127, loss = 0.03927639126777649
In grad_steps = 3128, loss = 0.3388272523880005
In grad_steps = 3129, loss = 0.041121967136859894
In grad_steps = 3130, loss = 0.04304274171590805
In grad_steps = 3131, loss = 0.34570661187171936
In grad_steps = 3132, loss = 0.05815199017524719
In grad_steps = 3133, loss = 0.1965765655040741
In grad_steps = 3134, loss = 0.6034219861030579
In grad_steps = 3135, loss = 0.02008846029639244
In grad_steps = 3136, loss = 0.031171901151537895
In grad_steps = 3137, loss = 0.1785029172897339
In grad_steps = 3138, loss = 0.6332293748855591
In grad_steps = 3139, loss = 0.037387531250715256
In grad_steps = 3140, loss = 0.542366087436676
In grad_steps = 3141, loss = 0.2032909244298935
In grad_steps = 3142, loss = 0.09678559750318527
In grad_steps = 3143, loss = 0.1352977156639099
In grad_steps = 3144, loss = 0.13963429629802704
In grad_steps = 3145, loss = 0.07266997545957565
In grad_steps = 3146, loss = 0.05493296682834625
In grad_steps = 3147, loss = 0.007254843134433031
In grad_steps = 3148, loss = 0.09479721635580063
In grad_steps = 3149, loss = 0.01193312369287014
In grad_steps = 3150, loss = 0.019324753433465958
In grad_steps = 3151, loss = 0.20477591454982758
In grad_steps = 3152, loss = 0.06732187420129776
In grad_steps = 3153, loss = 0.04379071295261383
In grad_steps = 3154, loss = 0.523262083530426
In grad_steps = 3155, loss = 1.117995262145996
In grad_steps = 3156, loss = 0.0025494175497442484
In grad_steps = 3157, loss = 0.9827568531036377
In grad_steps = 3158, loss = 0.30291345715522766
In grad_steps = 3159, loss = 0.19047550857067108
In grad_steps = 3160, loss = 0.13975022733211517
In grad_steps = 3161, loss = 0.469259113073349
In grad_steps = 3162, loss = 0.16563020646572113
In grad_steps = 3163, loss = 0.25473126769065857
In grad_steps = 3164, loss = 0.5686091184616089
In grad_steps = 3165, loss = 0.5841823220252991
In grad_steps = 3166, loss = 0.23904827237129211
In grad_steps = 3167, loss = 0.07607220113277435
In grad_steps = 3168, loss = 0.06718092411756516
In grad_steps = 3169, loss = 0.2745831608772278
In grad_steps = 3170, loss = 0.07766389846801758
In grad_steps = 3171, loss = 0.40618523955345154
In grad_steps = 3172, loss = 0.6475340723991394
In grad_steps = 3173, loss = 0.03145803511142731
In grad_steps = 3174, loss = 0.3083837032318115
In grad_steps = 3175, loss = 0.055252332240343094
In grad_steps = 3176, loss = 0.07627822458744049
In grad_steps = 3177, loss = 0.4653003215789795
In grad_steps = 3178, loss = 0.07493455708026886
In grad_steps = 3179, loss = 0.28561753034591675
In grad_steps = 3180, loss = 0.1779797524213791
In grad_steps = 3181, loss = 0.2375800609588623
In grad_steps = 3182, loss = 0.16513489186763763
In grad_steps = 3183, loss = 0.05495521053671837
In grad_steps = 3184, loss = 0.08657575398683548
In grad_steps = 3185, loss = 0.15948836505413055
In grad_steps = 3186, loss = 0.5074256658554077
In grad_steps = 3187, loss = 0.06166278198361397
In grad_steps = 3188, loss = 0.05101706460118294
In grad_steps = 3189, loss = 0.37243232131004333
In grad_steps = 3190, loss = 0.0022979751229286194
In grad_steps = 3191, loss = 0.07700008153915405
In grad_steps = 3192, loss = 0.0864064171910286
In grad_steps = 3193, loss = 0.02486143447458744
In grad_steps = 3194, loss = 0.9897832870483398
In grad_steps = 3195, loss = 0.19243022799491882
In grad_steps = 3196, loss = 0.2697509825229645
In grad_steps = 3197, loss = 0.006875944789499044
In grad_steps = 3198, loss = 0.10252177715301514
In grad_steps = 3199, loss = 2.1688036918640137
In grad_steps = 3200, loss = 0.04172524809837341
In grad_steps = 3201, loss = 0.3593986928462982
In grad_steps = 3202, loss = 0.060219358652830124
In grad_steps = 3203, loss = 0.4757830500602722
In grad_steps = 3204, loss = 0.10539202392101288
In grad_steps = 3205, loss = 0.24669307470321655
In grad_steps = 3206, loss = 0.7074254751205444
In grad_steps = 3207, loss = 0.1573568880558014
In grad_steps = 3208, loss = 0.17634882032871246
In grad_steps = 3209, loss = 0.14159990847110748
In grad_steps = 3210, loss = 0.1970510631799698
In grad_steps = 3211, loss = 0.06453055143356323
In grad_steps = 3212, loss = 0.19996009767055511
In grad_steps = 3213, loss = 0.1906062364578247
In grad_steps = 3214, loss = 0.42901942133903503
In grad_steps = 3215, loss = 0.0583638921380043
In grad_steps = 3216, loss = 0.2893170118331909
In grad_steps = 3217, loss = 0.2686319649219513
In grad_steps = 3218, loss = 0.3045896887779236
In grad_steps = 3219, loss = 0.9254236221313477
In grad_steps = 3220, loss = 0.8138049840927124
In grad_steps = 3221, loss = 0.1385844498872757
In grad_steps = 3222, loss = 0.021192699670791626
In grad_steps = 3223, loss = 0.152402862906456
In grad_steps = 3224, loss = 0.11733762919902802
In grad_steps = 3225, loss = 0.08710403740406036
In grad_steps = 3226, loss = 0.32470834255218506
In grad_steps = 3227, loss = 0.055126503109931946
In grad_steps = 3228, loss = 0.6496878862380981
In grad_steps = 3229, loss = 0.10053227841854095
In grad_steps = 3230, loss = 0.03204810246825218
In grad_steps = 3231, loss = 0.8824720978736877
In grad_steps = 3232, loss = 0.016552019864320755
In grad_steps = 3233, loss = 0.058330804109573364
In grad_steps = 3234, loss = 0.13469626009464264
In grad_steps = 3235, loss = 1.1325223445892334
In grad_steps = 3236, loss = 0.7956402897834778
In grad_steps = 3237, loss = 0.8560150861740112
In grad_steps = 3238, loss = 0.2967592775821686
In grad_steps = 3239, loss = 0.1239636018872261
In grad_steps = 3240, loss = 0.23402893543243408
In grad_steps = 3241, loss = 0.47836193442344666
In grad_steps = 3242, loss = 0.11651267856359482
In grad_steps = 3243, loss = 0.08755098283290863
In grad_steps = 3244, loss = 0.08054223656654358
In grad_steps = 3245, loss = 0.18734128773212433
In grad_steps = 3246, loss = 0.18103134632110596
In grad_steps = 3247, loss = 0.13322800397872925
In grad_steps = 3248, loss = 0.5318331718444824
In grad_steps = 3249, loss = 0.06638038903474808
In grad_steps = 3250, loss = 0.5665355920791626
In grad_steps = 3251, loss = 0.16823238134384155
In grad_steps = 3252, loss = 0.79189532995224
In grad_steps = 3253, loss = 0.08686383813619614
In grad_steps = 3254, loss = 0.05817068740725517
In grad_steps = 3255, loss = 0.4342772364616394
In grad_steps = 3256, loss = 0.024307098239660263
In grad_steps = 3257, loss = 0.051671139895915985
In grad_steps = 3258, loss = 0.9839221835136414
In grad_steps = 3259, loss = 0.8927868604660034
In grad_steps = 3260, loss = 0.08013471961021423
In grad_steps = 3261, loss = 0.21727141737937927
In grad_steps = 3262, loss = 0.1746811866760254
In grad_steps = 3263, loss = 0.24401381611824036
In grad_steps = 3264, loss = 0.7041712999343872
In grad_steps = 3265, loss = 0.02422104775905609
In grad_steps = 3266, loss = 0.6241809725761414
In grad_steps = 3267, loss = 0.27281785011291504
In grad_steps = 3268, loss = 0.08256535232067108
In grad_steps = 3269, loss = 0.8660321235656738
In grad_steps = 3270, loss = 0.031205907464027405
In grad_steps = 3271, loss = 0.08578436076641083
In grad_steps = 3272, loss = 0.10470730066299438
In grad_steps = 3273, loss = 0.10266271233558655
In grad_steps = 3274, loss = 0.10354582220315933
In grad_steps = 3275, loss = 0.10803455114364624
In grad_steps = 3276, loss = 0.4986768364906311
In grad_steps = 3277, loss = 1.2636102437973022
In grad_steps = 3278, loss = 0.037819184362888336
In grad_steps = 3279, loss = 0.08269298076629639
In grad_steps = 3280, loss = 0.07038047164678574
In grad_steps = 3281, loss = 0.16068165004253387
In grad_steps = 3282, loss = 0.9225718379020691
In grad_steps = 3283, loss = 0.13209080696105957
In grad_steps = 3284, loss = 0.09816943854093552
In grad_steps = 3285, loss = 0.0344039611518383
In grad_steps = 3286, loss = 0.4748106300830841
In grad_steps = 3287, loss = 0.11158082634210587
In grad_steps = 3288, loss = 0.09973186999559402
In grad_steps = 3289, loss = 0.2586241662502289
In grad_steps = 3290, loss = 0.4291000962257385
In grad_steps = 3291, loss = 0.09562332928180695
In grad_steps = 3292, loss = 0.19935119152069092
In grad_steps = 3293, loss = 0.024293523281812668
In grad_steps = 3294, loss = 0.08255356550216675
In grad_steps = 3295, loss = 0.10631914436817169
In grad_steps = 3296, loss = 0.0945611223578453
In grad_steps = 3297, loss = 0.591833233833313
In grad_steps = 3298, loss = 0.644530177116394
In grad_steps = 3299, loss = 0.06056787073612213
In grad_steps = 3300, loss = 0.09707611054182053
In grad_steps = 3301, loss = 0.6144087314605713
In grad_steps = 3302, loss = 0.2623252272605896
In grad_steps = 3303, loss = 0.07316295802593231
In grad_steps = 3304, loss = 0.2473878413438797
In grad_steps = 3305, loss = 0.2823796272277832
In grad_steps = 3306, loss = 0.06173126772046089
In grad_steps = 3307, loss = 0.055596351623535156
In grad_steps = 3308, loss = 0.8855804800987244
In grad_steps = 3309, loss = 0.6090649962425232
In grad_steps = 3310, loss = 0.12212375551462173
In grad_steps = 3311, loss = 0.6885181069374084
In grad_steps = 3312, loss = 0.03694659471511841
In grad_steps = 3313, loss = 0.09599168598651886
In grad_steps = 3314, loss = 1.0764992237091064
In grad_steps = 3315, loss = 0.20539207756519318
In grad_steps = 3316, loss = 0.30701711773872375
In grad_steps = 3317, loss = 0.02325962297618389
In grad_steps = 3318, loss = 0.06905311346054077
In grad_steps = 3319, loss = 0.2892480492591858
In grad_steps = 3320, loss = 0.35210728645324707
In grad_steps = 3321, loss = 0.502903938293457
In grad_steps = 3322, loss = 0.04619874805212021
In grad_steps = 3323, loss = 0.03548724576830864
In grad_steps = 3324, loss = 0.2618542015552521
In grad_steps = 3325, loss = 0.22830326855182648
In grad_steps = 3326, loss = 0.06026839092373848
In grad_steps = 3327, loss = 0.12446063756942749
In grad_steps = 3328, loss = 0.08966924250125885
In grad_steps = 3329, loss = 0.19861435890197754
In grad_steps = 3330, loss = 0.2055949568748474
In grad_steps = 3331, loss = 0.040126338601112366
In grad_steps = 3332, loss = 0.04388909041881561
In grad_steps = 3333, loss = 0.039435822516679764
In grad_steps = 3334, loss = 0.14311635494232178
In grad_steps = 3335, loss = 0.0688849464058876
In grad_steps = 3336, loss = 0.04134900122880936
In grad_steps = 3337, loss = 0.22474154829978943
In grad_steps = 3338, loss = 0.0830744132399559
In grad_steps = 3339, loss = 0.06421808153390884
In grad_steps = 3340, loss = 0.03346654400229454
In grad_steps = 3341, loss = 0.399657666683197
In grad_steps = 3342, loss = 0.024959567934274673
In grad_steps = 3343, loss = 0.008057803846895695
In grad_steps = 3344, loss = 0.010956885293126106
In grad_steps = 3345, loss = 0.38559022545814514
In grad_steps = 3346, loss = 0.0647667795419693
In grad_steps = 3347, loss = 0.10829292982816696
In grad_steps = 3348, loss = 0.058164484798908234
In grad_steps = 3349, loss = 0.031161131337285042
In grad_steps = 3350, loss = 0.012238939292728901
In grad_steps = 3351, loss = 0.009822183288633823
In grad_steps = 3352, loss = 0.3676707446575165
In grad_steps = 3353, loss = 0.034083880484104156
In grad_steps = 3354, loss = 0.005464473739266396
In grad_steps = 3355, loss = 0.005593264941126108
In grad_steps = 3356, loss = 0.03269156441092491
In grad_steps = 3357, loss = 0.06545642018318176
In grad_steps = 3358, loss = 1.5554887056350708
In grad_steps = 3359, loss = 1.1622049808502197
In grad_steps = 3360, loss = 0.006788985338062048
In grad_steps = 3361, loss = 0.6144133806228638
In grad_steps = 3362, loss = 0.02367684431374073
In grad_steps = 3363, loss = 0.10200786590576172
In grad_steps = 3364, loss = 0.04769464209675789
In grad_steps = 3365, loss = 0.039392415434122086
In grad_steps = 3366, loss = 0.07951273024082184
In grad_steps = 3367, loss = 0.053676921874284744
In grad_steps = 3368, loss = 0.14735735952854156
In grad_steps = 3369, loss = 1.1001622676849365
In grad_steps = 3370, loss = 0.4768947660923004
In grad_steps = 3371, loss = 0.08943035453557968
In grad_steps = 3372, loss = 0.03396819531917572
In grad_steps = 3373, loss = 0.3792603015899658
In grad_steps = 3374, loss = 0.031415291130542755
In grad_steps = 3375, loss = 0.541010856628418
In grad_steps = 3376, loss = 0.09295493364334106
In grad_steps = 3377, loss = 0.027153488248586655
In grad_steps = 3378, loss = 0.10414060950279236
In grad_steps = 3379, loss = 0.14905521273612976
In grad_steps = 3380, loss = 0.9554359316825867
In grad_steps = 3381, loss = 0.021742161363363266
In grad_steps = 3382, loss = 0.8715791702270508
In grad_steps = 3383, loss = 0.6579524874687195
In grad_steps = 3384, loss = 0.14871200919151306
In grad_steps = 3385, loss = 0.03123745322227478
In grad_steps = 3386, loss = 0.24316012859344482
In grad_steps = 3387, loss = 0.0748690515756607
In grad_steps = 3388, loss = 0.5566976070404053
In grad_steps = 3389, loss = 0.24436168372631073
In grad_steps = 3390, loss = 0.08185039460659027
In grad_steps = 3391, loss = 0.04089870676398277
In grad_steps = 3392, loss = 0.4914155900478363
In grad_steps = 3393, loss = 0.8786072731018066
In grad_steps = 3394, loss = 0.1895028054714203
In grad_steps = 3395, loss = 0.19680243730545044
In grad_steps = 3396, loss = 0.4994843304157257
In grad_steps = 3397, loss = 0.2030399739742279
In grad_steps = 3398, loss = 1.1577982902526855
In grad_steps = 3399, loss = 0.19247540831565857
In grad_steps = 3400, loss = 0.07414107769727707
In grad_steps = 3401, loss = 0.31736016273498535
In grad_steps = 3402, loss = 0.014879802241921425
In grad_steps = 3403, loss = 0.3673211634159088
In grad_steps = 3404, loss = 0.24491487443447113
In grad_steps = 3405, loss = 0.19186091423034668
In grad_steps = 3406, loss = 0.1639566719532013
In grad_steps = 3407, loss = 0.048867367208004
In grad_steps = 3408, loss = 0.48897847533226013
In grad_steps = 3409, loss = 0.1588265299797058
In grad_steps = 3410, loss = 0.3604772984981537
In grad_steps = 3411, loss = 0.07374609261751175
In grad_steps = 3412, loss = 0.25630250573158264
In grad_steps = 3413, loss = 0.12505538761615753
In grad_steps = 3414, loss = 0.7730278372764587
In grad_steps = 3415, loss = 0.04253368452191353
In grad_steps = 3416, loss = 0.5829235315322876
In grad_steps = 3417, loss = 0.43501800298690796
In grad_steps = 3418, loss = 0.029716456308960915
In grad_steps = 3419, loss = 0.04421860724687576
In grad_steps = 3420, loss = 0.16108059883117676
In grad_steps = 3421, loss = 0.22969192266464233
In grad_steps = 3422, loss = 0.07087919861078262
In grad_steps = 3423, loss = 0.10928282886743546
In grad_steps = 3424, loss = 0.27685919404029846
In grad_steps = 3425, loss = 0.9082130193710327
In grad_steps = 3426, loss = 0.06092001870274544
In grad_steps = 3427, loss = 0.8087468147277832
In grad_steps = 3428, loss = 0.0183354951441288
In grad_steps = 3429, loss = 0.3516741991043091
In grad_steps = 3430, loss = 0.03264258801937103
In grad_steps = 3431, loss = 0.6925092339515686
In grad_steps = 3432, loss = 0.2898155152797699
In grad_steps = 3433, loss = 0.1488855630159378
In grad_steps = 3434, loss = 0.04266458377242088
In grad_steps = 3435, loss = 0.07390547543764114
In grad_steps = 3436, loss = 0.544547438621521
In grad_steps = 3437, loss = 0.059521012008190155
In grad_steps = 3438, loss = 1.27190363407135
In grad_steps = 3439, loss = 0.19066689908504486
In grad_steps = 3440, loss = 0.10343199968338013
In grad_steps = 3441, loss = 0.04325262829661369
In grad_steps = 3442, loss = 0.07051519304513931
In grad_steps = 3443, loss = 0.011797238141298294
In grad_steps = 3444, loss = 0.5918133854866028
In grad_steps = 3445, loss = 0.16647878289222717
In grad_steps = 3446, loss = 0.16058233380317688
In grad_steps = 3447, loss = 0.13487395644187927
In grad_steps = 3448, loss = 0.2632982134819031
In grad_steps = 3449, loss = 0.5172362327575684
In grad_steps = 3450, loss = 0.03568902611732483
In grad_steps = 3451, loss = 0.19734829664230347
In grad_steps = 3452, loss = 0.07560347765684128
In grad_steps = 3453, loss = 0.700262188911438
In grad_steps = 3454, loss = 0.06495143473148346
In grad_steps = 3455, loss = 0.184814453125
In grad_steps = 3456, loss = 0.6284153461456299
In grad_steps = 3457, loss = 0.543115496635437
In grad_steps = 3458, loss = 0.024069026112556458
In grad_steps = 3459, loss = 0.13465505838394165
In grad_steps = 3460, loss = 0.9568465948104858
In grad_steps = 3461, loss = 0.7761287093162537
In grad_steps = 3462, loss = 0.09493216872215271
In grad_steps = 3463, loss = 0.17728987336158752
In grad_steps = 3464, loss = 0.21282850205898285
In grad_steps = 3465, loss = 0.14520107209682465
In grad_steps = 3466, loss = 0.8889151811599731
In grad_steps = 3467, loss = 0.28373757004737854
In grad_steps = 3468, loss = 0.20236235857009888
In grad_steps = 3469, loss = 0.13599810004234314
In grad_steps = 3470, loss = 0.5669589042663574
In grad_steps = 3471, loss = 0.05838482081890106
In grad_steps = 3472, loss = 0.07104472070932388
In grad_steps = 3473, loss = 0.4562210738658905
In grad_steps = 3474, loss = 0.08414912968873978
In grad_steps = 3475, loss = 0.09904997050762177
In grad_steps = 3476, loss = 0.4071129560470581
In grad_steps = 3477, loss = 0.04507901519536972
In grad_steps = 3478, loss = 0.24235855042934418
In grad_steps = 3479, loss = 0.0718802809715271
In grad_steps = 3480, loss = 0.3178507089614868
In grad_steps = 3481, loss = 0.4084089696407318
In grad_steps = 3482, loss = 0.17256635427474976
In grad_steps = 3483, loss = 0.03946584090590477
In grad_steps = 3484, loss = 0.9153870940208435
In grad_steps = 3485, loss = 0.02481330931186676
In grad_steps = 3486, loss = 0.03374336287379265
In grad_steps = 3487, loss = 0.3970699608325958
In grad_steps = 3488, loss = 0.11239925771951675
In grad_steps = 3489, loss = 0.06493069976568222
In grad_steps = 3490, loss = 0.684370219707489
In grad_steps = 3491, loss = 0.38353443145751953
In grad_steps = 3492, loss = 0.024555614218115807
In grad_steps = 3493, loss = 0.0697779506444931
In grad_steps = 3494, loss = 0.2972375750541687
In grad_steps = 3495, loss = 0.1101471558213234
In grad_steps = 3496, loss = 0.9922153353691101
In grad_steps = 3497, loss = 0.4663942754268646
In grad_steps = 3498, loss = 0.06639562547206879
In grad_steps = 3499, loss = 0.33459290862083435
In grad_steps = 3500, loss = 0.01275303028523922
In grad_steps = 3501, loss = 0.6840212941169739
In grad_steps = 3502, loss = 0.04180103540420532
In grad_steps = 3503, loss = 0.025361407548189163
In grad_steps = 3504, loss = 0.28931084275245667
In grad_steps = 3505, loss = 0.14588826894760132
In grad_steps = 3506, loss = 1.0704838037490845
In grad_steps = 3507, loss = 0.10144853591918945
In grad_steps = 3508, loss = 0.21229352056980133
In grad_steps = 3509, loss = 0.587557852268219
In grad_steps = 3510, loss = 0.1973337084054947
In grad_steps = 3511, loss = 0.1201254203915596
In grad_steps = 3512, loss = 0.0853370651602745
In grad_steps = 3513, loss = 0.05856078118085861
In grad_steps = 3514, loss = 0.06521588563919067
In grad_steps = 3515, loss = 0.06560702621936798
In grad_steps = 3516, loss = 0.8915548920631409
In grad_steps = 3517, loss = 0.39311453700065613
In grad_steps = 3518, loss = 0.44738438725471497
In grad_steps = 3519, loss = 0.08223409205675125
In grad_steps = 3520, loss = 0.6565640568733215
In grad_steps = 3521, loss = 0.04074503481388092
In grad_steps = 3522, loss = 0.06146078556776047
In grad_steps = 3523, loss = 0.08680173754692078
In grad_steps = 3524, loss = 0.06658560782670975
In grad_steps = 3525, loss = 0.0839194655418396
In grad_steps = 3526, loss = 0.06069260835647583
In grad_steps = 3527, loss = 0.7951936721801758
In grad_steps = 3528, loss = 0.36313170194625854
In grad_steps = 3529, loss = 0.2202940285205841
In grad_steps = 3530, loss = 0.1693829894065857
In grad_steps = 3531, loss = 0.083138607442379
In grad_steps = 3532, loss = 0.46047741174697876
In grad_steps = 3533, loss = 0.17265978455543518
In grad_steps = 3534, loss = 0.055601589381694794
In grad_steps = 3535, loss = 0.04902306944131851
In grad_steps = 3536, loss = 0.11211730539798737
In grad_steps = 3537, loss = 0.18189308047294617
In grad_steps = 3538, loss = 0.15377072989940643
In grad_steps = 3539, loss = 0.7001499533653259
In grad_steps = 3540, loss = 0.05360276997089386
In grad_steps = 3541, loss = 0.4456906318664551
In grad_steps = 3542, loss = 0.2611826956272125
In grad_steps = 3543, loss = 0.06822246313095093
In grad_steps = 3544, loss = 0.24629461765289307
In grad_steps = 3545, loss = 0.04549324885010719
In grad_steps = 3546, loss = 0.1234012246131897
In grad_steps = 3547, loss = 0.1752365678548813
In grad_steps = 3548, loss = 0.03466588631272316
In grad_steps = 3549, loss = 0.023253928869962692
In grad_steps = 3550, loss = 0.02786020003259182
In grad_steps = 3551, loss = 0.8166808485984802
In grad_steps = 3552, loss = 0.47447115182876587
In grad_steps = 3553, loss = 0.15149874985218048
In grad_steps = 3554, loss = 0.9375708699226379
In grad_steps = 3555, loss = 0.04979346692562103
In grad_steps = 3556, loss = 0.11621291935443878
In grad_steps = 3557, loss = 0.14309550821781158
In grad_steps = 3558, loss = 0.36087191104888916
In grad_steps = 3559, loss = 0.22941532731056213
In grad_steps = 3560, loss = 0.6768676042556763
In grad_steps = 3561, loss = 0.0839175134897232
In grad_steps = 3562, loss = 1.174037218093872
In grad_steps = 3563, loss = 0.7032692432403564
In grad_steps = 3564, loss = 0.6964567303657532
In grad_steps = 3565, loss = 1.2892796993255615
In grad_steps = 3566, loss = 0.6113438010215759
In grad_steps = 3567, loss = 0.15929733216762543
In grad_steps = 3568, loss = 0.29437607526779175
In grad_steps = 3569, loss = 0.0978589877486229
In grad_steps = 3570, loss = 0.0672302097082138
In grad_steps = 3571, loss = 0.038724858313798904
In grad_steps = 3572, loss = 0.13378119468688965
In grad_steps = 3573, loss = 0.23208098113536835
In grad_steps = 3574, loss = 1.2798843383789062
In grad_steps = 3575, loss = 0.10700041055679321
In grad_steps = 3576, loss = 1.0015136003494263
In grad_steps = 3577, loss = 0.6210977435112
In grad_steps = 3578, loss = 0.24854837357997894
In grad_steps = 3579, loss = 0.2960149645805359
In grad_steps = 3580, loss = 0.46856430172920227
In grad_steps = 3581, loss = 0.7584072351455688
In grad_steps = 3582, loss = 0.10926873236894608
In grad_steps = 3583, loss = 0.47069424390792847
In grad_steps = 3584, loss = 0.30819451808929443
In grad_steps = 3585, loss = 0.15488353371620178
In grad_steps = 3586, loss = 0.17062439024448395
In grad_steps = 3587, loss = 0.1419745236635208
In grad_steps = 3588, loss = 0.2961796224117279
In grad_steps = 3589, loss = 0.05614183098077774
In grad_steps = 3590, loss = 0.7850767374038696
In grad_steps = 3591, loss = 0.14001397788524628
In grad_steps = 3592, loss = 0.0252990759909153
In grad_steps = 3593, loss = 0.07540028542280197
In grad_steps = 3594, loss = 0.19595766067504883
In grad_steps = 3595, loss = 0.23157146573066711
In grad_steps = 3596, loss = 0.2395351529121399
In grad_steps = 3597, loss = 0.10042042285203934
In grad_steps = 3598, loss = 0.023687807843089104
In grad_steps = 3599, loss = 0.1715884655714035
In grad_steps = 3600, loss = 1.5234363079071045
In grad_steps = 3601, loss = 0.01672382839024067
In grad_steps = 3602, loss = 0.01138855330646038
In grad_steps = 3603, loss = 0.08949865400791168
In grad_steps = 3604, loss = 0.2861006557941437
In grad_steps = 3605, loss = 0.19578878581523895
In grad_steps = 3606, loss = 0.025673408061265945
In grad_steps = 3607, loss = 0.16034340858459473
In grad_steps = 3608, loss = 0.20136983692646027
In grad_steps = 3609, loss = 0.4830697476863861
In grad_steps = 3610, loss = 0.05879382789134979
In grad_steps = 3611, loss = 0.4214114248752594
In grad_steps = 3612, loss = 0.5968317985534668
In grad_steps = 3613, loss = 0.044301558285951614
In grad_steps = 3614, loss = 0.13227353990077972
In grad_steps = 3615, loss = 0.4397507905960083
In grad_steps = 3616, loss = 0.11124930530786514
In grad_steps = 3617, loss = 0.8131873607635498
In grad_steps = 3618, loss = 0.5976725220680237
In grad_steps = 3619, loss = 0.014129836112260818
In grad_steps = 3620, loss = 0.11967620253562927
In grad_steps = 3621, loss = 0.16586662828922272
In grad_steps = 3622, loss = 0.041498519480228424
In grad_steps = 3623, loss = 0.5339086651802063
In grad_steps = 3624, loss = 0.7532616853713989
In grad_steps = 3625, loss = 0.12053947895765305
In grad_steps = 3626, loss = 0.19664855301380157
In grad_steps = 3627, loss = 0.30195870995521545
In grad_steps = 3628, loss = 0.9646024107933044
In grad_steps = 3629, loss = 0.10250862687826157
In grad_steps = 3630, loss = 0.4906516671180725
In grad_steps = 3631, loss = 0.01566358096897602
In grad_steps = 3632, loss = 0.5122795701026917
In grad_steps = 3633, loss = 0.05986551195383072
In grad_steps = 3634, loss = 0.12725995481014252
In grad_steps = 3635, loss = 0.05525538697838783
In grad_steps = 3636, loss = 0.05018309876322746
In grad_steps = 3637, loss = 0.12305772304534912
In grad_steps = 3638, loss = 0.26792946457862854
In grad_steps = 3639, loss = 0.05054391548037529
In grad_steps = 3640, loss = 0.29108789563179016
In grad_steps = 3641, loss = 1.006531000137329
In grad_steps = 3642, loss = 0.33459973335266113
In grad_steps = 3643, loss = 0.16287565231323242
In grad_steps = 3644, loss = 0.7648170590400696
In grad_steps = 3645, loss = 0.052457503974437714
In grad_steps = 3646, loss = 0.03721146285533905
In grad_steps = 3647, loss = 0.042947594076395035
In grad_steps = 3648, loss = 0.2560880482196808
In grad_steps = 3649, loss = 0.039887554943561554
In grad_steps = 3650, loss = 0.07566122710704803
In grad_steps = 3651, loss = 0.08748883754014969
In grad_steps = 3652, loss = 0.28098973631858826
In grad_steps = 3653, loss = 0.3290955722332001
In grad_steps = 3654, loss = 0.05083183944225311
In grad_steps = 3655, loss = 0.09467898309230804
In grad_steps = 3656, loss = 0.3997478485107422
In grad_steps = 3657, loss = 0.04797489941120148
In grad_steps = 3658, loss = 0.14802660048007965
In grad_steps = 3659, loss = 0.33773910999298096
In grad_steps = 3660, loss = 0.02813023515045643
In grad_steps = 3661, loss = 0.42171719670295715
In grad_steps = 3662, loss = 0.19686190783977509
In grad_steps = 3663, loss = 0.3932952880859375
In grad_steps = 3664, loss = 0.23026905953884125
In grad_steps = 3665, loss = 0.03671569749712944
In grad_steps = 3666, loss = 0.1685623824596405
In grad_steps = 3667, loss = 0.03216110169887543
In grad_steps = 3668, loss = 0.01978188566863537
In grad_steps = 3669, loss = 0.00913438480347395
In grad_steps = 3670, loss = 0.6834840774536133
In grad_steps = 3671, loss = 0.2651253044605255
In grad_steps = 3672, loss = 0.034039206802845
In grad_steps = 3673, loss = 0.26383352279663086
In grad_steps = 3674, loss = 0.029624735936522484
In grad_steps = 3675, loss = 1.497567892074585
In grad_steps = 3676, loss = 0.2900470197200775
In grad_steps = 3677, loss = 0.021394027397036552
In grad_steps = 3678, loss = 0.5092225670814514
In grad_steps = 3679, loss = 0.06947141885757446
In grad_steps = 3680, loss = 0.08321014791727066
In grad_steps = 3681, loss = 0.1570282131433487
In grad_steps = 3682, loss = 0.0281686894595623
In grad_steps = 3683, loss = 0.3856287896633148
In grad_steps = 3684, loss = 0.031664904206991196
In grad_steps = 3685, loss = 0.4467773139476776
In grad_steps = 3686, loss = 0.34738844633102417
In grad_steps = 3687, loss = 0.05479511618614197
In grad_steps = 3688, loss = 0.04693889245390892
In grad_steps = 3689, loss = 0.21365107595920563
In grad_steps = 3690, loss = 0.2353563904762268
In grad_steps = 3691, loss = 0.03297578543424606
In grad_steps = 3692, loss = 0.7202205061912537
In grad_steps = 3693, loss = 0.07693550735712051
In grad_steps = 3694, loss = 0.16763488948345184
In grad_steps = 3695, loss = 0.022622784599661827
In grad_steps = 3696, loss = 0.3249014914035797
In grad_steps = 3697, loss = 0.0495402067899704
In grad_steps = 3698, loss = 0.45824137330055237
In grad_steps = 3699, loss = 0.10334683954715729
In grad_steps = 3700, loss = 0.4722345471382141
In grad_steps = 3701, loss = 0.026528142392635345
In grad_steps = 3702, loss = 1.3052388429641724
In grad_steps = 3703, loss = 0.21425797045230865
In grad_steps = 3704, loss = 0.9260534644126892
In grad_steps = 3705, loss = 0.047367725521326065
In grad_steps = 3706, loss = 0.07961936295032501
In grad_steps = 3707, loss = 0.10148430615663528
In grad_steps = 3708, loss = 0.4903697669506073
In grad_steps = 3709, loss = 0.06453898549079895
In grad_steps = 3710, loss = 0.5384836792945862
In grad_steps = 3711, loss = 0.20304006338119507
In grad_steps = 3712, loss = 0.04597737267613411
In grad_steps = 3713, loss = 0.6738873720169067
In grad_steps = 3714, loss = 0.5567009449005127
In grad_steps = 3715, loss = 0.0868123471736908
In grad_steps = 3716, loss = 0.26356062293052673
In grad_steps = 3717, loss = 0.06429238617420197
In grad_steps = 3718, loss = 0.26509401202201843
In grad_steps = 3719, loss = 0.34821295738220215
In grad_steps = 3720, loss = 0.06437594443559647
In grad_steps = 3721, loss = 0.24569252133369446
In grad_steps = 3722, loss = 0.47934189438819885
In grad_steps = 3723, loss = 0.293925940990448
In grad_steps = 3724, loss = 0.027451129630208015
In grad_steps = 3725, loss = 0.15419699251651764
In grad_steps = 3726, loss = 0.059998027980327606
In grad_steps = 3727, loss = 0.06477640569210052
In grad_steps = 3728, loss = 0.05011465772986412
In grad_steps = 3729, loss = 0.10429011285305023
In grad_steps = 3730, loss = 0.07286082208156586
In grad_steps = 3731, loss = 0.12941427528858185
In grad_steps = 3732, loss = 2.251418113708496
In grad_steps = 3733, loss = 0.11688660085201263
In grad_steps = 3734, loss = 0.18898671865463257
In grad_steps = 3735, loss = 0.040769774466753006
In grad_steps = 3736, loss = 0.09962392598390579
In grad_steps = 3737, loss = 0.3542637825012207
In grad_steps = 3738, loss = 0.0824463963508606
In grad_steps = 3739, loss = 0.05279877036809921
In grad_steps = 3740, loss = 0.052051953971385956
In grad_steps = 3741, loss = 0.2058957815170288
In grad_steps = 3742, loss = 0.8044120073318481
In grad_steps = 3743, loss = 0.18365085124969482
In grad_steps = 3744, loss = 0.10895448923110962
In grad_steps = 3745, loss = 0.0390692837536335
In grad_steps = 3746, loss = 0.11906608194112778
In grad_steps = 3747, loss = 0.8991987109184265
In grad_steps = 3748, loss = 0.3215804100036621
In grad_steps = 3749, loss = 0.04999977722764015
In grad_steps = 3750, loss = 0.09700693190097809
In grad_steps = 3751, loss = 0.054551854729652405
In grad_steps = 3752, loss = 0.3736843466758728
In grad_steps = 3753, loss = 0.25244230031967163
In grad_steps = 3754, loss = 0.7636691927909851
In grad_steps = 3755, loss = 0.048946358263492584
In grad_steps = 3756, loss = 0.22264254093170166
In grad_steps = 3757, loss = 0.04131421446800232
In grad_steps = 3758, loss = 0.1441291868686676
In grad_steps = 3759, loss = 0.09795593470335007
In grad_steps = 3760, loss = 0.7494136095046997
In grad_steps = 3761, loss = 0.03240453824400902
In grad_steps = 3762, loss = 0.024841144680976868
In grad_steps = 3763, loss = 1.1215064525604248
In grad_steps = 3764, loss = 0.1982703059911728
In grad_steps = 3765, loss = 0.10121390223503113
In grad_steps = 3766, loss = 0.0986483097076416
In grad_steps = 3767, loss = 0.18470370769500732
In grad_steps = 3768, loss = 0.15205679833889008
In grad_steps = 3769, loss = 0.05674178898334503
In grad_steps = 3770, loss = 0.0339684784412384
In grad_steps = 3771, loss = 0.8898385167121887
In grad_steps = 3772, loss = 0.031219374388456345
In grad_steps = 3773, loss = 0.04799480736255646
In grad_steps = 3774, loss = 0.1892443746328354
In grad_steps = 3775, loss = 0.02812933176755905
In grad_steps = 3776, loss = 0.06488508731126785
In grad_steps = 3777, loss = 0.042923539876937866
In grad_steps = 3778, loss = 0.03460988774895668
In grad_steps = 3779, loss = 0.03179778903722763
In grad_steps = 3780, loss = 0.12891022861003876
In grad_steps = 3781, loss = 0.19186723232269287
In grad_steps = 3782, loss = 0.018585240468382835
In grad_steps = 3783, loss = 0.28813284635543823
In grad_steps = 3784, loss = 0.7749586701393127
In grad_steps = 3785, loss = 1.3574671745300293
In grad_steps = 3786, loss = 0.12282327562570572
In grad_steps = 3787, loss = 1.00961172580719
In grad_steps = 3788, loss = 0.07374778389930725
In grad_steps = 3789, loss = 0.5593451857566833
In grad_steps = 3790, loss = 0.13488227128982544
In grad_steps = 3791, loss = 0.9143813252449036
In grad_steps = 3792, loss = 0.6656229496002197
In grad_steps = 3793, loss = 0.5057104825973511
In grad_steps = 3794, loss = 0.08421818166971207
In grad_steps = 3795, loss = 0.09018769860267639
In grad_steps = 3796, loss = 0.23426303267478943
In grad_steps = 3797, loss = 0.7038894891738892
In grad_steps = 3798, loss = 0.5156757235527039
In grad_steps = 3799, loss = 0.07086307555437088
In grad_steps = 3800, loss = 0.06591059267520905
In grad_steps = 3801, loss = 0.1609707474708557
In grad_steps = 3802, loss = 0.588620662689209
In grad_steps = 3803, loss = 0.5041533708572388
In grad_steps = 3804, loss = 0.09737679362297058
In grad_steps = 3805, loss = 0.6264743804931641
In grad_steps = 3806, loss = 0.05968283861875534
In grad_steps = 3807, loss = 0.16607314348220825
In grad_steps = 3808, loss = 0.36086568236351013
In grad_steps = 3809, loss = 0.06983448565006256
In grad_steps = 3810, loss = 0.15167048573493958
In grad_steps = 3811, loss = 0.6877436637878418
In grad_steps = 3812, loss = 0.2656812071800232
In grad_steps = 3813, loss = 0.19514411687850952
In grad_steps = 3814, loss = 0.15817368030548096
In grad_steps = 3815, loss = 0.23756885528564453
In grad_steps = 3816, loss = 0.5035000443458557
In grad_steps = 3817, loss = 0.3977105915546417
In grad_steps = 3818, loss = 0.570600152015686
In grad_steps = 3819, loss = 0.09770189225673676
In grad_steps = 3820, loss = 0.12347281724214554
In grad_steps = 3821, loss = 0.3521563410758972
In grad_steps = 3822, loss = 0.25342726707458496
In grad_steps = 3823, loss = 0.3818591833114624
In grad_steps = 3824, loss = 0.4961336851119995
In grad_steps = 3825, loss = 0.0873623937368393
In grad_steps = 3826, loss = 0.10012820363044739
In grad_steps = 3827, loss = 0.1041983887553215
In grad_steps = 3828, loss = 0.11090869456529617
In grad_steps = 3829, loss = 0.010930454358458519
In grad_steps = 3830, loss = 0.15638363361358643
In grad_steps = 3831, loss = 0.3738047182559967
In grad_steps = 3832, loss = 0.08845476806163788
In grad_steps = 3833, loss = 0.23196585476398468
In grad_steps = 3834, loss = 0.013112742453813553
In grad_steps = 3835, loss = 0.05496716499328613
In grad_steps = 3836, loss = 0.08293919265270233
In grad_steps = 3837, loss = 0.12651295959949493
In grad_steps = 3838, loss = 0.6164984107017517
In grad_steps = 3839, loss = 0.35676559805870056
In grad_steps = 3840, loss = 0.014815658330917358
In grad_steps = 3841, loss = 0.08333923667669296
In grad_steps = 3842, loss = 0.023708602413535118
In grad_steps = 3843, loss = 0.18493206799030304
In grad_steps = 3844, loss = 0.10308701545000076
In grad_steps = 3845, loss = 0.2129453420639038
In grad_steps = 3846, loss = 0.14632993936538696
In grad_steps = 3847, loss = 0.08739451318979263
In grad_steps = 3848, loss = 1.4802467823028564
In grad_steps = 3849, loss = 0.008176430128514767
In grad_steps = 3850, loss = 0.5956887602806091
In grad_steps = 3851, loss = 0.005739221349358559
In grad_steps = 3852, loss = 0.16824886202812195
In grad_steps = 3853, loss = 0.0037811854854226112
In grad_steps = 3854, loss = 0.13083720207214355
In grad_steps = 3855, loss = 0.0060043539851903915
In grad_steps = 3856, loss = 0.2966320514678955
In grad_steps = 3857, loss = 0.03901475667953491
In grad_steps = 3858, loss = 0.11290179193019867
In grad_steps = 3859, loss = 0.2460504025220871
In grad_steps = 3860, loss = 0.18114733695983887
In grad_steps = 3861, loss = 0.6517090797424316
In grad_steps = 3862, loss = 0.27047768235206604
In grad_steps = 3863, loss = 0.1405947208404541
In grad_steps = 3864, loss = 0.6262052655220032
In grad_steps = 3865, loss = 0.06268315762281418
In grad_steps = 3866, loss = 0.7579797506332397
In grad_steps = 3867, loss = 0.07615435123443604
In grad_steps = 3868, loss = 0.28503748774528503
In grad_steps = 3869, loss = 0.2872871458530426
In grad_steps = 3870, loss = 0.04335706681013107
In grad_steps = 3871, loss = 0.02046600915491581
In grad_steps = 3872, loss = 0.07327964901924133
In grad_steps = 3873, loss = 0.6418933272361755
In grad_steps = 3874, loss = 0.04955490306019783
In grad_steps = 3875, loss = 0.19028498232364655
In grad_steps = 3876, loss = 0.06256847828626633
In grad_steps = 3877, loss = 0.2587619423866272
In grad_steps = 3878, loss = 0.5966894626617432
In grad_steps = 3879, loss = 0.669663667678833
In grad_steps = 3880, loss = 0.24064093828201294
In grad_steps = 3881, loss = 0.5517601370811462
In grad_steps = 3882, loss = 0.0950532779097557
In grad_steps = 3883, loss = 0.6600256562232971
In grad_steps = 3884, loss = 0.07932566851377487
In grad_steps = 3885, loss = 0.41089192032814026
In grad_steps = 3886, loss = 0.08358778059482574
In grad_steps = 3887, loss = 0.07009529322385788
In grad_steps = 3888, loss = 0.03639708459377289
In grad_steps = 3889, loss = 0.14473840594291687
In grad_steps = 3890, loss = 0.6587840914726257
In grad_steps = 3891, loss = 0.5612982511520386
In grad_steps = 3892, loss = 0.2362213432788849
In grad_steps = 3893, loss = 0.24069549143314362
In grad_steps = 3894, loss = 0.1742413192987442
In grad_steps = 3895, loss = 0.12532371282577515
In grad_steps = 3896, loss = 0.01960938423871994
In grad_steps = 3897, loss = 0.2505004405975342
In grad_steps = 3898, loss = 0.06135864928364754
In grad_steps = 3899, loss = 0.3498792052268982
In grad_steps = 3900, loss = 0.18631163239479065
In grad_steps = 3901, loss = 0.05069027096033096
In grad_steps = 3902, loss = 0.30132856965065
In grad_steps = 3903, loss = 0.024863949045538902
In grad_steps = 3904, loss = 0.030774492770433426
In grad_steps = 3905, loss = 0.6704850196838379
In grad_steps = 3906, loss = 0.9723283052444458
In grad_steps = 3907, loss = 0.14395608007907867
In grad_steps = 3908, loss = 0.24975188076496124
In grad_steps = 3909, loss = 0.03305501490831375
In grad_steps = 3910, loss = 0.2073054015636444
In grad_steps = 3911, loss = 0.06935128569602966
In grad_steps = 3912, loss = 0.0702524185180664
In grad_steps = 3913, loss = 0.09041737020015717
In grad_steps = 3914, loss = 0.0102516058832407
In grad_steps = 3915, loss = 0.008905372582376003
In grad_steps = 3916, loss = 1.3304482698440552
In grad_steps = 3917, loss = 0.020106568932533264
In grad_steps = 3918, loss = 0.4248098134994507
In grad_steps = 3919, loss = 1.1252208948135376
In grad_steps = 3920, loss = 0.016556650400161743
In grad_steps = 3921, loss = 0.1677185595035553
In grad_steps = 3922, loss = 0.14843563735485077
In grad_steps = 3923, loss = 0.7273165583610535
In grad_steps = 3924, loss = 0.11479134112596512
In grad_steps = 3925, loss = 0.10728897154331207
In grad_steps = 3926, loss = 0.12207774072885513
In grad_steps = 3927, loss = 0.300315797328949
In grad_steps = 3928, loss = 0.9664015173912048
In grad_steps = 3929, loss = 0.23426946997642517
In grad_steps = 3930, loss = 0.19306336343288422
In grad_steps = 3931, loss = 0.8999793529510498
In grad_steps = 3932, loss = 0.01902138814330101
In grad_steps = 3933, loss = 0.6826333999633789
In grad_steps = 3934, loss = 0.1789790391921997
In grad_steps = 3935, loss = 0.21293890476226807
In grad_steps = 3936, loss = 0.25822508335113525
In grad_steps = 3937, loss = 0.05299947410821915
In grad_steps = 3938, loss = 0.08223569393157959
In grad_steps = 3939, loss = 0.1838625967502594
In grad_steps = 3940, loss = 0.3407941162586212
In grad_steps = 3941, loss = 0.19885900616645813
In grad_steps = 3942, loss = 0.30002227425575256
In grad_steps = 3943, loss = 0.1468028873205185
In grad_steps = 3944, loss = 0.30273380875587463
In grad_steps = 3945, loss = 0.2750348448753357
In grad_steps = 3946, loss = 0.051422715187072754
In grad_steps = 3947, loss = 0.07791010290384293
In grad_steps = 3948, loss = 0.06075473502278328
In grad_steps = 3949, loss = 0.1655464470386505
In grad_steps = 3950, loss = 0.045457128435373306
In grad_steps = 3951, loss = 0.08716410398483276
In grad_steps = 3952, loss = 0.045628782361745834
In grad_steps = 3953, loss = 0.0252649188041687
In grad_steps = 3954, loss = 0.04845461994409561
In grad_steps = 3955, loss = 0.02562517300248146
In grad_steps = 3956, loss = 0.2682867646217346
In grad_steps = 3957, loss = 0.20735853910446167
In grad_steps = 3958, loss = 0.260428249835968
In grad_steps = 3959, loss = 0.007581694982945919
In grad_steps = 3960, loss = 0.036354824900627136
In grad_steps = 3961, loss = 0.005218139849603176
In grad_steps = 3962, loss = 0.015265272930264473
In grad_steps = 3963, loss = 0.010138120502233505
In grad_steps = 3964, loss = 0.0067712245509028435
In grad_steps = 3965, loss = 0.0031950247939676046
In grad_steps = 3966, loss = 1.2325830459594727
In grad_steps = 3967, loss = 0.9983384013175964
In grad_steps = 3968, loss = 0.3362204134464264
In grad_steps = 3969, loss = 0.05402793362736702
In grad_steps = 3970, loss = 0.014028185978531837
In grad_steps = 3971, loss = 0.45561668276786804
In grad_steps = 3972, loss = 0.07863981276750565
In grad_steps = 3973, loss = 0.020771600306034088
In grad_steps = 3974, loss = 0.24159471690654755
In grad_steps = 3975, loss = 0.008042523637413979
In grad_steps = 3976, loss = 0.07957091182470322
In grad_steps = 3977, loss = 0.5414512753486633
In grad_steps = 3978, loss = 0.05419111251831055
In grad_steps = 3979, loss = 0.06410486996173859
In grad_steps = 3980, loss = 0.02903570979833603
In grad_steps = 3981, loss = 0.17483298480510712
In grad_steps = 3982, loss = 0.032039325684309006
In grad_steps = 3983, loss = 1.1215018033981323
In grad_steps = 3984, loss = 0.5565598011016846
In grad_steps = 3985, loss = 0.09037479758262634
In grad_steps = 3986, loss = 0.29110512137413025
In grad_steps = 3987, loss = 0.0519932359457016
In grad_steps = 3988, loss = 0.018268665298819542
In grad_steps = 3989, loss = 0.05877956375479698
In grad_steps = 3990, loss = 0.10454343259334564
In grad_steps = 3991, loss = 0.20269529521465302
In grad_steps = 3992, loss = 0.7637120485305786
In grad_steps = 3993, loss = 0.19967201352119446
In grad_steps = 3994, loss = 0.023310912773013115
In grad_steps = 3995, loss = 0.11612028628587723
In grad_steps = 3996, loss = 0.10791457444429398
In grad_steps = 3997, loss = 0.9898450970649719
In grad_steps = 3998, loss = 1.1135902404785156
In grad_steps = 3999, loss = 0.5698233842849731
In grad_steps = 4000, loss = 0.21153849363327026
In grad_steps = 4001, loss = 0.255312442779541
In grad_steps = 4002, loss = 0.298210084438324
In grad_steps = 4003, loss = 0.14765392243862152
In grad_steps = 4004, loss = 0.3218896985054016
In grad_steps = 4005, loss = 0.2858034670352936
In grad_steps = 4006, loss = 0.15947429835796356
In grad_steps = 4007, loss = 0.2580564320087433
In grad_steps = 4008, loss = 0.39298978447914124
In grad_steps = 4009, loss = 0.5053697228431702
In grad_steps = 4010, loss = 0.16696731746196747
In grad_steps = 4011, loss = 0.30274006724357605
In grad_steps = 4012, loss = 0.41720521450042725
In grad_steps = 4013, loss = 0.02943173423409462
In grad_steps = 4014, loss = 0.08584024012088776
In grad_steps = 4015, loss = 0.09732306003570557
In grad_steps = 4016, loss = 0.3940420150756836
In grad_steps = 4017, loss = 0.08722390234470367
In grad_steps = 4018, loss = 0.26835232973098755
In grad_steps = 4019, loss = 0.063409723341465
In grad_steps = 4020, loss = 0.011189490556716919
In grad_steps = 4021, loss = 0.35362333059310913
In grad_steps = 4022, loss = 0.7191658616065979
In grad_steps = 4023, loss = 0.021596934646368027
In grad_steps = 4024, loss = 0.6838083267211914
In grad_steps = 4025, loss = 0.07567901909351349
In grad_steps = 4026, loss = 0.02238854393362999
In grad_steps = 4027, loss = 0.028412220999598503
In grad_steps = 4028, loss = 0.298076331615448
In grad_steps = 4029, loss = 0.8652909398078918
In grad_steps = 4030, loss = 0.04976219683885574
In grad_steps = 4031, loss = 0.07133010029792786
In grad_steps = 4032, loss = 0.35349899530410767
In grad_steps = 4033, loss = 0.5282654762268066
In grad_steps = 4034, loss = 0.10236036777496338
In grad_steps = 4035, loss = 0.4474889934062958
In grad_steps = 4036, loss = 0.04142974317073822
In grad_steps = 4037, loss = 0.04029995575547218
In grad_steps = 4038, loss = 0.514018177986145
In grad_steps = 4039, loss = 0.2047300785779953
In grad_steps = 4040, loss = 0.0212220698595047
In grad_steps = 4041, loss = 2.806347370147705
In grad_steps = 4042, loss = 0.4788455367088318
In grad_steps = 4043, loss = 0.015418268740177155
In grad_steps = 4044, loss = 0.7242478728294373
In grad_steps = 4045, loss = 0.20147490501403809
In grad_steps = 4046, loss = 0.17864887416362762
In grad_steps = 4047, loss = 0.6677387356758118
In grad_steps = 4048, loss = 0.06953689455986023
In grad_steps = 4049, loss = 0.272309273481369
In grad_steps = 4050, loss = 0.13717854022979736
In grad_steps = 4051, loss = 0.09317942708730698
In grad_steps = 4052, loss = 0.13542672991752625
In grad_steps = 4053, loss = 0.1197158694267273
In grad_steps = 4054, loss = 0.422320157289505
In grad_steps = 4055, loss = 0.23360209167003632
In grad_steps = 4056, loss = 0.05161034315824509
In grad_steps = 4057, loss = 0.17354990541934967
In grad_steps = 4058, loss = 0.18296700716018677
In grad_steps = 4059, loss = 0.30656278133392334
In grad_steps = 4060, loss = 0.05235828086733818
In grad_steps = 4061, loss = 0.07501235604286194
In grad_steps = 4062, loss = 0.14510440826416016
In grad_steps = 4063, loss = 0.21263566613197327
In grad_steps = 4064, loss = 0.04873545095324516
In grad_steps = 4065, loss = 0.06402642279863358
In grad_steps = 4066, loss = 0.26225870847702026
In grad_steps = 4067, loss = 0.08492343872785568
In grad_steps = 4068, loss = 0.03252500295639038
In grad_steps = 4069, loss = 0.023055870085954666
In grad_steps = 4070, loss = 0.560423731803894
In grad_steps = 4071, loss = 0.2508432865142822
In grad_steps = 4072, loss = 0.5604803562164307
In grad_steps = 4073, loss = 0.18015418946743011
In grad_steps = 4074, loss = 0.007592229638248682
In grad_steps = 4075, loss = 0.037830088287591934
In grad_steps = 4076, loss = 0.01482483558356762
In grad_steps = 4077, loss = 0.010876213200390339
In grad_steps = 4078, loss = 0.4846421182155609
In grad_steps = 4079, loss = 0.015552675351500511
In grad_steps = 4080, loss = 0.013000129722058773
In grad_steps = 4081, loss = 0.1363617330789566
In grad_steps = 4082, loss = 0.1388586014509201
In grad_steps = 4083, loss = 0.02578289806842804
In grad_steps = 4084, loss = 0.025597548112273216
In grad_steps = 4085, loss = 0.4039924740791321
In grad_steps = 4086, loss = 0.0759279653429985
In grad_steps = 4087, loss = 0.011991378851234913
In grad_steps = 4088, loss = 0.01218086015433073
In grad_steps = 4089, loss = 0.4891554117202759
In grad_steps = 4090, loss = 0.7512692213058472
In grad_steps = 4091, loss = 0.8844757080078125
In grad_steps = 4092, loss = 0.3557513356208801
In grad_steps = 4093, loss = 0.013211201876401901
In grad_steps = 4094, loss = 0.2731592655181885
In grad_steps = 4095, loss = 0.026645386591553688
In grad_steps = 4096, loss = 0.1388779878616333
In grad_steps = 4097, loss = 0.2616550922393799
In grad_steps = 4098, loss = 0.3108859062194824
In grad_steps = 4099, loss = 0.023774566128849983
In grad_steps = 4100, loss = 0.16859695315361023
In grad_steps = 4101, loss = 0.19453392922878265
In grad_steps = 4102, loss = 0.5581488609313965
In grad_steps = 4103, loss = 0.12176650762557983
In grad_steps = 4104, loss = 0.3069014847278595
In grad_steps = 4105, loss = 0.016331808641552925
Elapsed time: 2341.0529959201813 seconds for ensemble 4 with 2 epochs
LoRA instance 4 evaluation complete. Data saved to /pscratch/sd/t/tianle/lucid/other_source/SURP_2024/results/tmp_lora_ensemble/Llama3-8B-set-5/test_data_instance_4_seed_40373.npz.
lora instance i = 4 Successfully finished.
Final, Test average ensemble probabilities = 
[[0.94502145 0.05497859]
 [0.9559644  0.04403568]
 [0.00712686 0.9928731 ]
 ...
 [0.00440912 0.99559087]
 [0.20728545 0.7927145 ]
 [0.009046   0.99095404]]
Accuracy: 0.8719
MCC: 0.7476
AUC: 0.9498
Confusion Matrix:
tensor([[959,  78],
        [185, 831]])
Specificity: 0.9248
Precision (Macro): 0.8762
F1 Score (Macro): 0.8714
Expected Calibration Error (ECE): 0.0366
NLL loss: 0.2925
Ensemble evaluation complete.
