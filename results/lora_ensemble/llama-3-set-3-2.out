Job is running on partition: gpu_ss11
Job is running under account: m2616_g
/pscratch/sd/t/tianle/conda/envs/lucid_surp_2024/lib/python3.11/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:10<00:30, 10.07s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:20<00:20, 10.23s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:30<00:10, 10.12s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:32<00:00,  7.04s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:32<00:00,  8.18s/it]
Llama3 has been loaded successfully.
Model:  LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(128256, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)
          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)
          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)
          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)
)
Size of dataset:  154
In grad_steps = 0, loss = 1.16717529296875
In grad_steps = 1, loss = 1.1693611145019531
In grad_steps = 2, loss = 1.210344672203064
In grad_steps = 3, loss = 0.7037745714187622
In grad_steps = 4, loss = 0.4072515070438385
In grad_steps = 5, loss = 1.6316769123077393
In grad_steps = 6, loss = 0.3348316550254822
In grad_steps = 7, loss = 0.25698360800743103
In grad_steps = 8, loss = 1.3589305877685547
In grad_steps = 9, loss = 1.3242995738983154
In grad_steps = 10, loss = 0.44682055711746216
In grad_steps = 11, loss = 1.0583703517913818
In grad_steps = 12, loss = 0.547451376914978
In grad_steps = 13, loss = 0.7534626126289368
In grad_steps = 14, loss = 0.5042758584022522
In grad_steps = 15, loss = 0.383688747882843
In grad_steps = 16, loss = 1.7497648000717163
In grad_steps = 17, loss = 1.6365902423858643
In grad_steps = 18, loss = 0.30571436882019043
In grad_steps = 19, loss = 0.3281194269657135
In grad_steps = 20, loss = 0.3168221414089203
In grad_steps = 21, loss = 0.3989340364933014
In grad_steps = 22, loss = 1.1324045658111572
In grad_steps = 23, loss = 0.39347368478775024
In grad_steps = 24, loss = 0.9335219264030457
In grad_steps = 25, loss = 1.069944143295288
In grad_steps = 26, loss = 0.5760931372642517
In grad_steps = 27, loss = 0.5725230574607849
In grad_steps = 28, loss = 0.7955935001373291
In grad_steps = 29, loss = 0.7006394267082214
In grad_steps = 30, loss = 0.6918291449546814
In grad_steps = 31, loss = 0.6561817526817322
In grad_steps = 32, loss = 0.7517085671424866
In grad_steps = 33, loss = 0.4709576964378357
In grad_steps = 34, loss = 0.48354214429855347
In grad_steps = 35, loss = 1.1573864221572876
In grad_steps = 36, loss = 1.4680711030960083
In grad_steps = 37, loss = 0.29645270109176636
In grad_steps = 38, loss = 1.827886700630188
In grad_steps = 39, loss = 0.34757962822914124
In grad_steps = 40, loss = 0.38045746088027954
In grad_steps = 41, loss = 1.2709629535675049
In grad_steps = 42, loss = 1.1873233318328857
In grad_steps = 43, loss = 0.5199940204620361
In grad_steps = 44, loss = 0.9631799459457397
In grad_steps = 45, loss = 0.9098832607269287
In grad_steps = 46, loss = 0.5741568803787231
In grad_steps = 47, loss = 0.6400278210639954
In grad_steps = 48, loss = 0.7247496843338013
In grad_steps = 49, loss = 0.6964862942695618
In grad_steps = 50, loss = 0.7254928350448608
In grad_steps = 51, loss = 0.8165243268013
In grad_steps = 52, loss = 0.6937200427055359
In grad_steps = 53, loss = 0.7089591026306152
In grad_steps = 54, loss = 0.6883251070976257
In grad_steps = 55, loss = 0.7102729678153992
In grad_steps = 56, loss = 0.736531138420105
In grad_steps = 57, loss = 0.6248099207878113
In grad_steps = 58, loss = 0.8413203358650208
In grad_steps = 59, loss = 0.7931194305419922
In grad_steps = 60, loss = 0.6075631976127625
In grad_steps = 61, loss = 0.8095842003822327
In grad_steps = 62, loss = 0.6209916472434998
In grad_steps = 63, loss = 0.7673700451850891
In grad_steps = 64, loss = 0.8006241321563721
In grad_steps = 65, loss = 0.7027437686920166
In grad_steps = 66, loss = 0.7617440223693848
In grad_steps = 67, loss = 0.7366598844528198
In grad_steps = 68, loss = 0.7115336060523987
In grad_steps = 69, loss = 0.6938799023628235
In grad_steps = 70, loss = 0.6790698766708374
In grad_steps = 71, loss = 0.7861037850379944
In grad_steps = 72, loss = 0.6464800834655762
In grad_steps = 73, loss = 0.6441220045089722
In grad_steps = 74, loss = 0.7559931874275208
In grad_steps = 75, loss = 0.6731247305870056
In grad_steps = 76, loss = 0.6517030000686646
In grad_steps = 77, loss = 0.6452154517173767
In grad_steps = 78, loss = 0.8190403580665588
In grad_steps = 79, loss = 0.5378676056861877
In grad_steps = 80, loss = 0.8947455883026123
In grad_steps = 81, loss = 0.9471601843833923
In grad_steps = 82, loss = 0.5149435997009277
In grad_steps = 83, loss = 0.4939999282360077
In grad_steps = 84, loss = 0.8486852645874023
In grad_steps = 85, loss = 0.9274412393569946
In grad_steps = 86, loss = 0.5374523401260376
In grad_steps = 87, loss = 0.8438176512718201
In grad_steps = 88, loss = 0.8269917368888855
In grad_steps = 89, loss = 0.5881356000900269
In grad_steps = 90, loss = 0.7747917175292969
In grad_steps = 91, loss = 0.6907591819763184
In grad_steps = 92, loss = 0.7836086750030518
In grad_steps = 93, loss = 0.6397785544395447
In grad_steps = 94, loss = 0.7886831164360046
In grad_steps = 95, loss = 0.5542833805084229
In grad_steps = 96, loss = 0.5078864097595215
In grad_steps = 97, loss = 0.8973751068115234
In grad_steps = 98, loss = 0.4885624051094055
In grad_steps = 99, loss = 0.4812777638435364
In grad_steps = 100, loss = 1.0860878229141235
In grad_steps = 101, loss = 0.4717956781387329
In grad_steps = 102, loss = 1.1359676122665405
In grad_steps = 103, loss = 1.2395479679107666
In grad_steps = 104, loss = 0.9936684370040894
In grad_steps = 105, loss = 0.9627200961112976
In grad_steps = 106, loss = 0.5576705932617188
In grad_steps = 107, loss = 0.8187934756278992
In grad_steps = 108, loss = 0.7986259460449219
In grad_steps = 109, loss = 0.6527155041694641
In grad_steps = 110, loss = 0.694928765296936
In grad_steps = 111, loss = 0.6275193691253662
In grad_steps = 112, loss = 0.8017417192459106
In grad_steps = 113, loss = 0.5667152404785156
In grad_steps = 114, loss = 0.5263941287994385
In grad_steps = 115, loss = 0.9586522579193115
In grad_steps = 116, loss = 0.9799774289131165
In grad_steps = 117, loss = 0.4452248215675354
In grad_steps = 118, loss = 0.9664660096168518
In grad_steps = 119, loss = 1.02330482006073
In grad_steps = 120, loss = 0.88932204246521
In grad_steps = 121, loss = 0.8192963600158691
In grad_steps = 122, loss = 0.6110516786575317
In grad_steps = 123, loss = 0.6989349722862244
In grad_steps = 124, loss = 0.7218424677848816
In grad_steps = 125, loss = 0.7596104741096497
In grad_steps = 126, loss = 0.7558606863021851
In grad_steps = 127, loss = 0.6959295272827148
In grad_steps = 128, loss = 0.7039576768875122
In grad_steps = 129, loss = 0.7009700536727905
In grad_steps = 130, loss = 0.6380382180213928
In grad_steps = 131, loss = 0.7268674373626709
In grad_steps = 132, loss = 0.7037321329116821
In grad_steps = 133, loss = 0.6157858967781067
In grad_steps = 134, loss = 0.7907254099845886
In grad_steps = 135, loss = 0.5613300800323486
In grad_steps = 136, loss = 0.8378798961639404
In grad_steps = 137, loss = 0.7845519185066223
In grad_steps = 138, loss = 0.7879003286361694
In grad_steps = 139, loss = 0.6275891065597534
In grad_steps = 140, loss = 0.592971920967102
In grad_steps = 141, loss = 0.7351576089859009
In grad_steps = 142, loss = 0.6970064640045166
In grad_steps = 143, loss = 0.6593756079673767
In grad_steps = 144, loss = 0.6742405891418457
In grad_steps = 145, loss = 0.7124890685081482
In grad_steps = 146, loss = 0.6025214791297913
In grad_steps = 147, loss = 0.7366042137145996
In grad_steps = 148, loss = 0.8407058715820312
In grad_steps = 149, loss = 0.5697147250175476
In grad_steps = 150, loss = 0.5411784052848816
In grad_steps = 151, loss = 0.830366849899292
In grad_steps = 152, loss = 0.5352428555488586
In grad_steps = 153, loss = 0.8716195821762085
In grad_steps = 154, loss = 0.8760002851486206
In grad_steps = 155, loss = 0.9699647426605225
In grad_steps = 156, loss = 0.7509296536445618
In grad_steps = 157, loss = 0.8018059730529785
In grad_steps = 158, loss = 0.5941804647445679
In grad_steps = 159, loss = 0.7642009854316711
In grad_steps = 160, loss = 0.5952286720275879
In grad_steps = 161, loss = 0.8877447843551636
In grad_steps = 162, loss = 0.6230251789093018
In grad_steps = 163, loss = 0.5902917981147766
In grad_steps = 164, loss = 0.8929653167724609
In grad_steps = 165, loss = 0.8435198068618774
In grad_steps = 166, loss = 0.7044995427131653
In grad_steps = 167, loss = 0.7843964099884033
In grad_steps = 168, loss = 0.799540102481842
In grad_steps = 169, loss = 0.6044031977653503
In grad_steps = 170, loss = 0.6347529888153076
In grad_steps = 171, loss = 0.7094871401786804
In grad_steps = 172, loss = 0.6776761412620544
In grad_steps = 173, loss = 0.7527214288711548
In grad_steps = 174, loss = 0.7454647421836853
In grad_steps = 175, loss = 0.6350891590118408
In grad_steps = 176, loss = 0.7686333656311035
In grad_steps = 177, loss = 0.6593300104141235
In grad_steps = 178, loss = 0.6409366726875305
In grad_steps = 179, loss = 0.7138657569885254
In grad_steps = 180, loss = 0.6370604038238525
In grad_steps = 181, loss = 0.812495768070221
In grad_steps = 182, loss = 0.7322632074356079
In grad_steps = 183, loss = 0.6343501806259155
In grad_steps = 184, loss = 0.7296748161315918
In grad_steps = 185, loss = 0.6552058458328247
In grad_steps = 186, loss = 0.7289928197860718
In grad_steps = 187, loss = 0.7896674275398254
In grad_steps = 188, loss = 0.6938703656196594
In grad_steps = 189, loss = 0.7511346340179443
In grad_steps = 190, loss = 0.7124613523483276
In grad_steps = 191, loss = 0.7361892461776733
In grad_steps = 192, loss = 0.6547223329544067
In grad_steps = 193, loss = 0.6810824871063232
In grad_steps = 194, loss = 0.7550784945487976
In grad_steps = 195, loss = 0.6384364366531372
In grad_steps = 196, loss = 0.641925036907196
In grad_steps = 197, loss = 0.7228677868843079
In grad_steps = 198, loss = 0.6520678997039795
In grad_steps = 199, loss = 0.6602089405059814
In grad_steps = 200, loss = 0.6185796856880188
In grad_steps = 201, loss = 0.772373378276825
In grad_steps = 202, loss = 0.5669887661933899
In grad_steps = 203, loss = 0.8125079870223999
In grad_steps = 204, loss = 0.8717232942581177
In grad_steps = 205, loss = 0.5427478551864624
In grad_steps = 206, loss = 0.5542099475860596
In grad_steps = 207, loss = 0.8026418685913086
In grad_steps = 208, loss = 0.900733470916748
In grad_steps = 209, loss = 0.5494546890258789
In grad_steps = 210, loss = 0.8498689532279968
In grad_steps = 211, loss = 0.8281292915344238
In grad_steps = 212, loss = 0.5689989924430847
In grad_steps = 213, loss = 0.7888200283050537
In grad_steps = 214, loss = 0.7550284266471863
In grad_steps = 215, loss = 0.6974474191665649
In grad_steps = 216, loss = 0.7182227373123169
In grad_steps = 217, loss = 0.7360853552818298
In grad_steps = 218, loss = 0.6452993750572205
In grad_steps = 219, loss = 0.6031703948974609
In grad_steps = 220, loss = 0.795650064945221
In grad_steps = 221, loss = 0.5866422653198242
In grad_steps = 222, loss = 0.5662813782691956
In grad_steps = 223, loss = 0.9031691551208496
In grad_steps = 224, loss = 0.5344977378845215
In grad_steps = 225, loss = 0.940683901309967
In grad_steps = 226, loss = 1.032416582107544
In grad_steps = 227, loss = 0.8974575996398926
In grad_steps = 228, loss = 0.874473512172699
In grad_steps = 229, loss = 0.5886582136154175
In grad_steps = 230, loss = 0.8040995597839355
In grad_steps = 231, loss = 0.815479040145874
In grad_steps = 232, loss = 0.6280533671379089
In grad_steps = 233, loss = 0.7288042902946472
In grad_steps = 234, loss = 0.6710307598114014
In grad_steps = 235, loss = 0.7269871830940247
In grad_steps = 236, loss = 0.6218245029449463
In grad_steps = 237, loss = 0.5876345038414001
In grad_steps = 238, loss = 0.8434383869171143
In grad_steps = 239, loss = 0.8443009853363037
In grad_steps = 240, loss = 0.5220930576324463
In grad_steps = 241, loss = 0.8403587937355042
In grad_steps = 242, loss = 0.9065356850624084
In grad_steps = 243, loss = 0.8152610063552856
In grad_steps = 244, loss = 0.7798218727111816
In grad_steps = 245, loss = 0.6388973593711853
In grad_steps = 246, loss = 0.7015304565429688
In grad_steps = 247, loss = 0.6774560213088989
In grad_steps = 248, loss = 0.7338725924491882
In grad_steps = 249, loss = 0.7089838981628418
In grad_steps = 250, loss = 0.6698608994483948
In grad_steps = 251, loss = 0.7147907614707947
In grad_steps = 252, loss = 0.6788188815116882
In grad_steps = 253, loss = 0.6361918449401855
In grad_steps = 254, loss = 0.7402900457382202
In grad_steps = 255, loss = 0.6513372659683228
In grad_steps = 256, loss = 0.6204628348350525
In grad_steps = 257, loss = 0.7460547685623169
In grad_steps = 258, loss = 0.5737774968147278
In grad_steps = 259, loss = 0.8212301135063171
In grad_steps = 260, loss = 0.7693741321563721
In grad_steps = 261, loss = 0.7568647265434265
In grad_steps = 262, loss = 0.6078441739082336
In grad_steps = 263, loss = 0.5795046091079712
In grad_steps = 264, loss = 0.7266802191734314
In grad_steps = 265, loss = 0.683271586894989
In grad_steps = 266, loss = 0.6725283861160278
In grad_steps = 267, loss = 0.6658257246017456
In grad_steps = 268, loss = 0.6972501277923584
In grad_steps = 269, loss = 0.6034759283065796
In grad_steps = 270, loss = 0.6651440262794495
In grad_steps = 271, loss = 0.766313374042511
In grad_steps = 272, loss = 0.5641171336174011
In grad_steps = 273, loss = 0.5591538548469543
In grad_steps = 274, loss = 0.7078739404678345
In grad_steps = 275, loss = 0.4992085099220276
In grad_steps = 276, loss = 0.8580182194709778
In grad_steps = 277, loss = 0.7575585246086121
In grad_steps = 278, loss = 0.8823287487030029
In grad_steps = 279, loss = 0.5567857027053833
In grad_steps = 280, loss = 0.6120643615722656
In grad_steps = 281, loss = 0.585153341293335
In grad_steps = 282, loss = 1.4249608516693115
In grad_steps = 283, loss = 0.15608099102973938
In grad_steps = 284, loss = 1.0263516902923584
In grad_steps = 285, loss = 0.48989182710647583
In grad_steps = 286, loss = 0.29633331298828125
In grad_steps = 287, loss = 0.7743424773216248
In grad_steps = 288, loss = 0.5679101347923279
In grad_steps = 289, loss = 1.1278804540634155
In grad_steps = 290, loss = 0.5709599852561951
In grad_steps = 291, loss = 0.6946331858634949
In grad_steps = 292, loss = 0.7019113898277283
In grad_steps = 293, loss = 0.592131495475769
In grad_steps = 294, loss = 0.45702648162841797
In grad_steps = 295, loss = 0.870952844619751
In grad_steps = 296, loss = 0.6673141717910767
In grad_steps = 297, loss = 0.8255545496940613
In grad_steps = 298, loss = 0.6315847635269165
In grad_steps = 299, loss = 0.6390963792800903
In grad_steps = 300, loss = 0.47372424602508545
In grad_steps = 301, loss = 0.386642187833786
In grad_steps = 302, loss = 0.6771393418312073
In grad_steps = 303, loss = 0.6078793406486511
In grad_steps = 304, loss = 1.023621916770935
In grad_steps = 305, loss = 0.6090635061264038
In grad_steps = 306, loss = 0.5047450661659241
In grad_steps = 307, loss = 0.5634028911590576
In grad_steps = 308, loss = 0.5190533399581909
In grad_steps = 309, loss = 0.711201012134552
In grad_steps = 310, loss = 1.1638456583023071
In grad_steps = 311, loss = 0.891335129737854
In grad_steps = 312, loss = 1.278641700744629
In grad_steps = 313, loss = 0.8739705085754395
In grad_steps = 314, loss = 0.8710504770278931
In grad_steps = 315, loss = 0.3254651427268982
In grad_steps = 316, loss = 0.7091954946517944
In grad_steps = 317, loss = 0.7433064579963684
In grad_steps = 318, loss = 0.6171950697898865
In grad_steps = 319, loss = 0.645666778087616
In grad_steps = 320, loss = 0.7896310091018677
In grad_steps = 321, loss = 0.5827953219413757
In grad_steps = 322, loss = 0.5971872806549072
In grad_steps = 323, loss = 0.6071451902389526
In grad_steps = 324, loss = 0.7426398992538452
In grad_steps = 325, loss = 0.5140032172203064
In grad_steps = 326, loss = 0.7995491027832031
In grad_steps = 327, loss = 0.8399869799613953
In grad_steps = 328, loss = 0.560206413269043
In grad_steps = 329, loss = 0.634231448173523
In grad_steps = 330, loss = 0.858025848865509
In grad_steps = 331, loss = 0.8716695308685303
In grad_steps = 332, loss = 0.4915221333503723
In grad_steps = 333, loss = 0.8286331295967102
In grad_steps = 334, loss = 0.7842316031455994
In grad_steps = 335, loss = 0.6104214191436768
In grad_steps = 336, loss = 0.7976155281066895
In grad_steps = 337, loss = 0.7377740144729614
In grad_steps = 338, loss = 0.7440199255943298
In grad_steps = 339, loss = 0.6939261555671692
In grad_steps = 340, loss = 0.8666383028030396
In grad_steps = 341, loss = 0.5650827884674072
In grad_steps = 342, loss = 0.5401735901832581
In grad_steps = 343, loss = 0.8658894300460815
In grad_steps = 344, loss = 0.560718297958374
In grad_steps = 345, loss = 0.5183455944061279
In grad_steps = 346, loss = 0.965610682964325
In grad_steps = 347, loss = 0.535148024559021
In grad_steps = 348, loss = 0.9967464804649353
In grad_steps = 349, loss = 1.0874509811401367
In grad_steps = 350, loss = 0.9580267667770386
In grad_steps = 351, loss = 0.8975909352302551
In grad_steps = 352, loss = 0.524196982383728
In grad_steps = 353, loss = 0.8247583508491516
In grad_steps = 354, loss = 0.8340578079223633
In grad_steps = 355, loss = 0.6353030800819397
In grad_steps = 356, loss = 0.7001798152923584
In grad_steps = 357, loss = 0.6495801210403442
In grad_steps = 358, loss = 0.767281174659729
In grad_steps = 359, loss = 0.5731298923492432
In grad_steps = 360, loss = 0.5049723386764526
In grad_steps = 361, loss = 0.965909481048584
In grad_steps = 362, loss = 1.0101251602172852
In grad_steps = 363, loss = 0.4323652684688568
In grad_steps = 364, loss = 1.0164762735366821
In grad_steps = 365, loss = 0.9634352922439575
In grad_steps = 366, loss = 0.8626677989959717
In grad_steps = 367, loss = 0.7793401479721069
In grad_steps = 368, loss = 0.6564764976501465
In grad_steps = 369, loss = 0.6821664571762085
In grad_steps = 370, loss = 0.7426319122314453
In grad_steps = 371, loss = 0.8343378901481628
In grad_steps = 372, loss = 0.8275697827339172
In grad_steps = 373, loss = 0.7311877608299255
In grad_steps = 374, loss = 0.6588149070739746
In grad_steps = 375, loss = 0.7569705247879028
In grad_steps = 376, loss = 0.6532235741615295
In grad_steps = 377, loss = 0.7300813794136047
In grad_steps = 378, loss = 0.7227033376693726
In grad_steps = 379, loss = 0.6427015066146851
In grad_steps = 380, loss = 0.7499017715454102
In grad_steps = 381, loss = 0.5946748852729797
In grad_steps = 382, loss = 0.7802215814590454
In grad_steps = 383, loss = 0.7564815282821655
In grad_steps = 384, loss = 0.8075271248817444
In grad_steps = 385, loss = 0.599955677986145
In grad_steps = 386, loss = 0.5787135362625122
In grad_steps = 387, loss = 0.7569664716720581
In grad_steps = 388, loss = 0.7131861448287964
In grad_steps = 389, loss = 0.7179813385009766
In grad_steps = 390, loss = 0.6382389068603516
In grad_steps = 391, loss = 0.7117817401885986
In grad_steps = 392, loss = 0.6366094350814819
In grad_steps = 393, loss = 0.7563388347625732
In grad_steps = 394, loss = 0.7571613788604736
In grad_steps = 395, loss = 0.578579843044281
In grad_steps = 396, loss = 0.6043140888214111
In grad_steps = 397, loss = 0.7327727675437927
In grad_steps = 398, loss = 0.5843528509140015
In grad_steps = 399, loss = 0.8030030131340027
In grad_steps = 400, loss = 0.8043440580368042
In grad_steps = 401, loss = 0.4835461676120758
In grad_steps = 402, loss = 0.7189247012138367
In grad_steps = 403, loss = 0.6351944804191589
In grad_steps = 404, loss = 0.5009949803352356
In grad_steps = 405, loss = 0.5822092294692993
In grad_steps = 406, loss = 0.18108652532100677
In grad_steps = 407, loss = 0.6175398826599121
In grad_steps = 408, loss = 0.5644444823265076
In grad_steps = 409, loss = 0.19326521456241608
In grad_steps = 410, loss = 0.42257165908813477
In grad_steps = 411, loss = 0.35285434126853943
In grad_steps = 412, loss = 0.18942324817180634
In grad_steps = 413, loss = 0.20928142964839935
In grad_steps = 414, loss = 0.488822340965271
In grad_steps = 415, loss = 0.0014900782844051719
In grad_steps = 416, loss = 0.0024352199397981167
In grad_steps = 417, loss = 0.005551990587264299
In grad_steps = 418, loss = 3.762537956237793
In grad_steps = 419, loss = 0.3271552324295044
In grad_steps = 420, loss = 0.0010355116100981832
In grad_steps = 421, loss = 0.14962151646614075
In grad_steps = 422, loss = 0.19627000391483307
In grad_steps = 423, loss = 0.006808415986597538
In grad_steps = 424, loss = 0.0036145609337836504
In grad_steps = 425, loss = 0.2881060838699341
In grad_steps = 426, loss = 0.4900436997413635
In grad_steps = 427, loss = 0.40479621291160583
In grad_steps = 428, loss = 0.37489065527915955
In grad_steps = 429, loss = 0.05455365404486656
In grad_steps = 430, loss = 0.050605785101652145
In grad_steps = 431, loss = 1.3134520053863525
In grad_steps = 432, loss = 0.1287909895181656
In grad_steps = 433, loss = 2.1526341438293457
In grad_steps = 434, loss = 0.5476861596107483
In grad_steps = 435, loss = 0.24426808953285217
In grad_steps = 436, loss = 0.0920899510383606
In grad_steps = 437, loss = 0.6103208065032959
In grad_steps = 438, loss = 0.0059714484959840775
In grad_steps = 439, loss = 0.6775447130203247
In grad_steps = 440, loss = 0.6316830515861511
In grad_steps = 441, loss = 0.23573993146419525
In grad_steps = 442, loss = 0.3137059211730957
In grad_steps = 443, loss = 0.23187701404094696
In grad_steps = 444, loss = 0.3087524175643921
In grad_steps = 445, loss = 0.08519787341356277
In grad_steps = 446, loss = 1.2022156715393066
In grad_steps = 447, loss = 0.28016671538352966
In grad_steps = 448, loss = 0.007813130505383015
In grad_steps = 449, loss = 0.7838055491447449
In grad_steps = 450, loss = 2.159721851348877
In grad_steps = 451, loss = 1.4148988723754883
In grad_steps = 452, loss = 0.4952770471572876
In grad_steps = 453, loss = 0.03516634553670883
In grad_steps = 454, loss = 2.498616933822632
In grad_steps = 455, loss = 0.48557257652282715
In grad_steps = 456, loss = 0.8799343109130859
In grad_steps = 457, loss = 0.27731409668922424
In grad_steps = 458, loss = 0.5813040137290955
In grad_steps = 459, loss = 0.43894654512405396
In grad_steps = 460, loss = 1.324115514755249
In grad_steps = 461, loss = 1.2528327703475952
In grad_steps = 462, loss = 0.5231092572212219
In grad_steps = 463, loss = 1.6627557277679443
In grad_steps = 464, loss = 0.39499950408935547
In grad_steps = 465, loss = 0.42998266220092773
In grad_steps = 466, loss = 0.995967447757721
In grad_steps = 467, loss = 0.45832765102386475
In grad_steps = 468, loss = 0.4090450704097748
In grad_steps = 469, loss = 1.0270010232925415
In grad_steps = 470, loss = 0.43606629967689514
In grad_steps = 471, loss = 1.079442024230957
In grad_steps = 472, loss = 1.026097059249878
In grad_steps = 473, loss = 0.8605468273162842
In grad_steps = 474, loss = 0.7172532081604004
In grad_steps = 475, loss = 0.7685213088989258
In grad_steps = 476, loss = 0.5533197522163391
In grad_steps = 477, loss = 0.5042399168014526
In grad_steps = 478, loss = 1.0354321002960205
In grad_steps = 479, loss = 0.35511547327041626
In grad_steps = 480, loss = 0.3382920026779175
In grad_steps = 481, loss = 1.2905125617980957
In grad_steps = 482, loss = 0.31137701869010925
In grad_steps = 483, loss = 0.30291566252708435
In grad_steps = 484, loss = 1.4172766208648682
In grad_steps = 485, loss = 1.3736693859100342
In grad_steps = 486, loss = 0.31924235820770264
In grad_steps = 487, loss = 1.1760103702545166
In grad_steps = 488, loss = 0.9952470660209656
In grad_steps = 489, loss = 0.8617416024208069
In grad_steps = 490, loss = 0.6747669577598572
In grad_steps = 491, loss = 0.8017096519470215
i = 0, Test ensemble probabilities = 
[array([[0.6284508 , 0.37154916],
       [0.6362697 , 0.36373034],
       [0.6395782 , 0.36042175],
       [0.6325759 , 0.3674241 ],
       [0.6236578 , 0.37634215],
       [0.64280176, 0.35719827],
       [0.6226229 , 0.37737706],
       [0.63043296, 0.36956698],
       [0.61667144, 0.38332856],
       [0.6516359 , 0.34836406],
       [0.629431  , 0.37056902],
       [0.6349214 , 0.36507857],
       [0.6210475 , 0.37895253],
       [0.64654285, 0.35345718],
       [0.6052796 , 0.39472032],
       [0.6282811 , 0.37171885]], dtype=float32)]
i = 0, Test true class= 
[1 0 0 0 1 0 1 0 0 0 1 1 0 1 1 0]
In grad_steps = 0, loss = 1.16717529296875
In grad_steps = 1, loss = 1.1362868547439575
In grad_steps = 2, loss = 1.2066805362701416
In grad_steps = 3, loss = 0.7030346989631653
In grad_steps = 4, loss = 0.39959144592285156
In grad_steps = 5, loss = 1.6640294790267944
In grad_steps = 6, loss = 0.3283422291278839
In grad_steps = 7, loss = 0.25185897946357727
In grad_steps = 8, loss = 1.3606276512145996
In grad_steps = 9, loss = 1.313143253326416
In grad_steps = 10, loss = 0.452059268951416
In grad_steps = 11, loss = 1.0651452541351318
In grad_steps = 12, loss = 0.5400521755218506
In grad_steps = 13, loss = 0.7447624206542969
In grad_steps = 14, loss = 0.49745845794677734
In grad_steps = 15, loss = 0.3823169469833374
In grad_steps = 16, loss = 1.7589913606643677
In grad_steps = 17, loss = 1.6439601182937622
In grad_steps = 18, loss = 0.28873932361602783
In grad_steps = 19, loss = 0.3235330879688263
In grad_steps = 20, loss = 0.3100128769874573
In grad_steps = 21, loss = 0.395047128200531
In grad_steps = 22, loss = 1.1443088054656982
In grad_steps = 23, loss = 0.386488139629364
In grad_steps = 24, loss = 0.9610814452171326
In grad_steps = 25, loss = 1.0760066509246826
In grad_steps = 26, loss = 0.5801982879638672
In grad_steps = 27, loss = 0.572715163230896
In grad_steps = 28, loss = 0.7673133611679077
In grad_steps = 29, loss = 0.7055551409721375
In grad_steps = 30, loss = 0.6802067160606384
In grad_steps = 31, loss = 0.6415099501609802
In grad_steps = 32, loss = 0.7342405319213867
In grad_steps = 33, loss = 0.4599332809448242
In grad_steps = 34, loss = 0.4731603264808655
In grad_steps = 35, loss = 1.1826939582824707
In grad_steps = 36, loss = 1.4645013809204102
In grad_steps = 37, loss = 0.2878835201263428
In grad_steps = 38, loss = 1.863997220993042
In grad_steps = 39, loss = 0.3469623327255249
In grad_steps = 40, loss = 0.3771190643310547
In grad_steps = 41, loss = 1.2666195631027222
In grad_steps = 42, loss = 1.1675903797149658
In grad_steps = 43, loss = 0.5282130241394043
In grad_steps = 44, loss = 0.979060173034668
In grad_steps = 45, loss = 0.9273685216903687
In grad_steps = 46, loss = 0.5579780340194702
In grad_steps = 47, loss = 0.6233493089675903
In grad_steps = 48, loss = 0.7521873116493225
In grad_steps = 49, loss = 0.671180009841919
In grad_steps = 50, loss = 0.7436782121658325
In grad_steps = 51, loss = 0.8042961359024048
In grad_steps = 52, loss = 0.6641581058502197
In grad_steps = 53, loss = 0.7249704599380493
In grad_steps = 54, loss = 0.6687785983085632
In grad_steps = 55, loss = 0.7058255672454834
In grad_steps = 56, loss = 0.7196720838546753
In grad_steps = 57, loss = 0.6395054459571838
In grad_steps = 58, loss = 0.8477745652198792
In grad_steps = 59, loss = 0.7768190503120422
In grad_steps = 60, loss = 0.6166383624076843
In grad_steps = 61, loss = 0.7755175828933716
In grad_steps = 62, loss = 0.636696994304657
In grad_steps = 63, loss = 0.7462939620018005
In grad_steps = 64, loss = 0.7907634377479553
In grad_steps = 65, loss = 0.7174724340438843
In grad_steps = 66, loss = 0.7855842113494873
In grad_steps = 67, loss = 0.762170672416687
In grad_steps = 68, loss = 0.6992146968841553
In grad_steps = 69, loss = 0.6995174288749695
In grad_steps = 70, loss = 0.682977020740509
In grad_steps = 71, loss = 0.7991868257522583
In grad_steps = 72, loss = 0.6334815621376038
In grad_steps = 73, loss = 0.6343402862548828
In grad_steps = 74, loss = 0.7690855860710144
In grad_steps = 75, loss = 0.6567642688751221
In grad_steps = 76, loss = 0.6410418152809143
In grad_steps = 77, loss = 0.6332130432128906
In grad_steps = 78, loss = 0.8502807021141052
In grad_steps = 79, loss = 0.5273815393447876
In grad_steps = 80, loss = 0.9278868436813354
In grad_steps = 81, loss = 0.9718097448348999
In grad_steps = 82, loss = 0.4964638352394104
In grad_steps = 83, loss = 0.484772264957428
In grad_steps = 84, loss = 0.8659314513206482
In grad_steps = 85, loss = 0.936838686466217
In grad_steps = 86, loss = 0.5369454026222229
In grad_steps = 87, loss = 0.8367781639099121
In grad_steps = 88, loss = 0.8248289823532104
In grad_steps = 89, loss = 0.5977590680122375
In grad_steps = 90, loss = 0.7499560713768005
In grad_steps = 91, loss = 0.6886426210403442
In grad_steps = 92, loss = 0.8264016509056091
In grad_steps = 93, loss = 0.6133208274841309
In grad_steps = 94, loss = 0.8247711658477783
In grad_steps = 95, loss = 0.5286420583724976
In grad_steps = 96, loss = 0.48637792468070984
In grad_steps = 97, loss = 0.9514130353927612
In grad_steps = 98, loss = 0.4659503102302551
In grad_steps = 99, loss = 0.469545841217041
In grad_steps = 100, loss = 1.1347761154174805
In grad_steps = 101, loss = 0.4622548818588257
In grad_steps = 102, loss = 1.1716924905776978
In grad_steps = 103, loss = 1.3013770580291748
In grad_steps = 104, loss = 1.0025324821472168
In grad_steps = 105, loss = 0.9514726400375366
In grad_steps = 106, loss = 0.5679613351821899
In grad_steps = 107, loss = 0.8238630294799805
In grad_steps = 108, loss = 0.8202828764915466
In grad_steps = 109, loss = 0.6560542583465576
In grad_steps = 110, loss = 0.6957875490188599
In grad_steps = 111, loss = 0.6034015417098999
In grad_steps = 112, loss = 0.8204953670501709
In grad_steps = 113, loss = 0.5582162141799927
In grad_steps = 114, loss = 0.5238029956817627
In grad_steps = 115, loss = 0.9606238603591919
In grad_steps = 116, loss = 0.9891513586044312
In grad_steps = 117, loss = 0.4508053660392761
In grad_steps = 118, loss = 0.96506267786026
In grad_steps = 119, loss = 1.0180585384368896
In grad_steps = 120, loss = 0.8922383189201355
In grad_steps = 121, loss = 0.8146576881408691
In grad_steps = 122, loss = 0.6131940484046936
In grad_steps = 123, loss = 0.704248309135437
In grad_steps = 124, loss = 0.709455668926239
In grad_steps = 125, loss = 0.7745896577835083
In grad_steps = 126, loss = 0.7575463652610779
In grad_steps = 127, loss = 0.6924207210540771
In grad_steps = 128, loss = 0.7182237505912781
In grad_steps = 129, loss = 0.6923635601997375
In grad_steps = 130, loss = 0.645902693271637
In grad_steps = 131, loss = 0.7365176677703857
In grad_steps = 132, loss = 0.7315306663513184
In grad_steps = 133, loss = 0.6176269054412842
In grad_steps = 134, loss = 0.7931126356124878
In grad_steps = 135, loss = 0.5611879825592041
In grad_steps = 136, loss = 0.833611786365509
In grad_steps = 137, loss = 0.7791274785995483
In grad_steps = 138, loss = 0.7775719165802002
In grad_steps = 139, loss = 0.6276220083236694
In grad_steps = 140, loss = 0.6067151427268982
In grad_steps = 141, loss = 0.7287545204162598
In grad_steps = 142, loss = 0.701492965221405
In grad_steps = 143, loss = 0.6631340384483337
In grad_steps = 144, loss = 0.6689557433128357
In grad_steps = 145, loss = 0.7387667894363403
In grad_steps = 146, loss = 0.5950533151626587
In grad_steps = 147, loss = 0.7356263995170593
In grad_steps = 148, loss = 0.8547120690345764
In grad_steps = 149, loss = 0.5810257196426392
In grad_steps = 150, loss = 0.5442661046981812
In grad_steps = 151, loss = 0.833261251449585
In grad_steps = 152, loss = 0.5293659567832947
In grad_steps = 153, loss = 0.8859425187110901
In grad_steps = 154, loss = 0.8805968761444092
In grad_steps = 155, loss = 0.9557235836982727
In grad_steps = 156, loss = 0.776192307472229
In grad_steps = 157, loss = 0.8101257681846619
In grad_steps = 158, loss = 0.6112919449806213
In grad_steps = 159, loss = 0.7548930048942566
In grad_steps = 160, loss = 0.6159911155700684
In grad_steps = 161, loss = 0.8677150011062622
In grad_steps = 162, loss = 0.6226829290390015
In grad_steps = 163, loss = 0.5952911972999573
In grad_steps = 164, loss = 0.836849570274353
In grad_steps = 165, loss = 0.8222689628601074
In grad_steps = 166, loss = 0.6943514347076416
In grad_steps = 167, loss = 0.7860212326049805
In grad_steps = 168, loss = 0.8130772709846497
In grad_steps = 169, loss = 0.5963202714920044
In grad_steps = 170, loss = 0.6269545555114746
In grad_steps = 171, loss = 0.7282521724700928
In grad_steps = 172, loss = 0.6650044918060303
In grad_steps = 173, loss = 0.7769679427146912
In grad_steps = 174, loss = 0.7256872653961182
In grad_steps = 175, loss = 0.6168424487113953
In grad_steps = 176, loss = 0.7959944605827332
In grad_steps = 177, loss = 0.645430862903595
In grad_steps = 178, loss = 0.6235196590423584
In grad_steps = 179, loss = 0.7314648032188416
In grad_steps = 180, loss = 0.6265127062797546
In grad_steps = 181, loss = 0.8286207914352417
In grad_steps = 182, loss = 0.7522780895233154
In grad_steps = 183, loss = 0.6268999576568604
In grad_steps = 184, loss = 0.7508561015129089
In grad_steps = 185, loss = 0.6524590849876404
In grad_steps = 186, loss = 0.7318146824836731
In grad_steps = 187, loss = 0.8062580823898315
In grad_steps = 188, loss = 0.6890922784805298
In grad_steps = 189, loss = 0.7427068948745728
In grad_steps = 190, loss = 0.7144688367843628
In grad_steps = 191, loss = 0.7339494228363037
In grad_steps = 192, loss = 0.646874725818634
In grad_steps = 193, loss = 0.6806237697601318
In grad_steps = 194, loss = 0.7588018178939819
In grad_steps = 195, loss = 0.6423705816268921
In grad_steps = 196, loss = 0.6486775875091553
In grad_steps = 197, loss = 0.7294500470161438
In grad_steps = 198, loss = 0.6589921116828918
In grad_steps = 199, loss = 0.6669715642929077
In grad_steps = 200, loss = 0.6345051527023315
In grad_steps = 201, loss = 0.7651807069778442
In grad_steps = 202, loss = 0.5847057104110718
In grad_steps = 203, loss = 0.8088037371635437
In grad_steps = 204, loss = 0.8500633239746094
In grad_steps = 205, loss = 0.5506643056869507
In grad_steps = 206, loss = 0.5570243000984192
In grad_steps = 207, loss = 0.8026497960090637
In grad_steps = 208, loss = 0.8703285455703735
In grad_steps = 209, loss = 0.557275652885437
In grad_steps = 210, loss = 0.8456506729125977
In grad_steps = 211, loss = 0.8185637593269348
In grad_steps = 212, loss = 0.5739223957061768
In grad_steps = 213, loss = 0.8021851778030396
In grad_steps = 214, loss = 0.7426854372024536
In grad_steps = 215, loss = 0.6997156143188477
In grad_steps = 216, loss = 0.7201610207557678
In grad_steps = 217, loss = 0.7154659628868103
In grad_steps = 218, loss = 0.6462734937667847
In grad_steps = 219, loss = 0.6066799759864807
In grad_steps = 220, loss = 0.7828695774078369
In grad_steps = 221, loss = 0.5955954194068909
In grad_steps = 222, loss = 0.5761703252792358
In grad_steps = 223, loss = 0.8935004472732544
In grad_steps = 224, loss = 0.5495429039001465
In grad_steps = 225, loss = 0.9246761798858643
In grad_steps = 226, loss = 1.0222153663635254
In grad_steps = 227, loss = 0.8872091174125671
In grad_steps = 228, loss = 0.8658508658409119
In grad_steps = 229, loss = 0.5796598196029663
In grad_steps = 230, loss = 0.7998594045639038
In grad_steps = 231, loss = 0.8174272179603577
In grad_steps = 232, loss = 0.6252828240394592
In grad_steps = 233, loss = 0.7280341386795044
In grad_steps = 234, loss = 0.6765047311782837
In grad_steps = 235, loss = 0.729453980922699
In grad_steps = 236, loss = 0.6367409825325012
In grad_steps = 237, loss = 0.5970226526260376
In grad_steps = 238, loss = 0.8346654176712036
In grad_steps = 239, loss = 0.8322021961212158
In grad_steps = 240, loss = 0.534236490726471
In grad_steps = 241, loss = 0.8251076936721802
In grad_steps = 242, loss = 0.8835026621818542
In grad_steps = 243, loss = 0.7995191812515259
In grad_steps = 244, loss = 0.7669450640678406
In grad_steps = 245, loss = 0.6372352838516235
In grad_steps = 246, loss = 0.6972401142120361
In grad_steps = 247, loss = 0.6827127933502197
In grad_steps = 248, loss = 0.7442831993103027
In grad_steps = 249, loss = 0.7150591015815735
In grad_steps = 250, loss = 0.6683042645454407
In grad_steps = 251, loss = 0.7112306952476501
In grad_steps = 252, loss = 0.6766942143440247
In grad_steps = 253, loss = 0.6397104263305664
In grad_steps = 254, loss = 0.7333872318267822
In grad_steps = 255, loss = 0.6636819839477539
In grad_steps = 256, loss = 0.6324855089187622
In grad_steps = 257, loss = 0.7519171237945557
In grad_steps = 258, loss = 0.5751414895057678
In grad_steps = 259, loss = 0.8122931718826294
In grad_steps = 260, loss = 0.771309494972229
In grad_steps = 261, loss = 0.7539528012275696
In grad_steps = 262, loss = 0.6086925864219666
In grad_steps = 263, loss = 0.5914584398269653
In grad_steps = 264, loss = 0.7374775409698486
In grad_steps = 265, loss = 0.6810202598571777
In grad_steps = 266, loss = 0.6904461979866028
In grad_steps = 267, loss = 0.6739441156387329
In grad_steps = 268, loss = 0.6850444674491882
In grad_steps = 269, loss = 0.6015913486480713
In grad_steps = 270, loss = 0.6601476073265076
In grad_steps = 271, loss = 0.7859143018722534
In grad_steps = 272, loss = 0.5780960917472839
In grad_steps = 273, loss = 0.5674706101417542
In grad_steps = 274, loss = 0.7186791896820068
In grad_steps = 275, loss = 0.5090590715408325
In grad_steps = 276, loss = 0.8802953958511353
In grad_steps = 277, loss = 0.8002259731292725
In grad_steps = 278, loss = 0.9180009365081787
In grad_steps = 279, loss = 0.6710625290870667
In grad_steps = 280, loss = 0.7169831991195679
In grad_steps = 281, loss = 0.5620371103286743
In grad_steps = 282, loss = 0.9319775104522705
In grad_steps = 283, loss = 0.28850385546684265
In grad_steps = 284, loss = 1.0467305183410645
In grad_steps = 285, loss = 0.4668055772781372
In grad_steps = 286, loss = 0.30909430980682373
In grad_steps = 287, loss = 0.9781043529510498
In grad_steps = 288, loss = 0.8697246313095093
In grad_steps = 289, loss = 0.8308888673782349
In grad_steps = 290, loss = 0.7126426696777344
In grad_steps = 291, loss = 0.778612494468689
In grad_steps = 292, loss = 0.6598121523857117
In grad_steps = 293, loss = 0.6484245657920837
In grad_steps = 294, loss = 0.5486568212509155
In grad_steps = 295, loss = 0.7866194248199463
In grad_steps = 296, loss = 0.6197784543037415
In grad_steps = 297, loss = 0.8499951362609863
In grad_steps = 298, loss = 0.6706122159957886
In grad_steps = 299, loss = 0.6528064012527466
In grad_steps = 300, loss = 0.6648327112197876
In grad_steps = 301, loss = 0.576755702495575
In grad_steps = 302, loss = 0.6127626895904541
In grad_steps = 303, loss = 0.673830509185791
In grad_steps = 304, loss = 0.7920880317687988
In grad_steps = 305, loss = 0.6356120705604553
In grad_steps = 306, loss = 0.6149374842643738
In grad_steps = 307, loss = 0.6224610805511475
In grad_steps = 308, loss = 0.6717521548271179
In grad_steps = 309, loss = 0.6984214186668396
In grad_steps = 310, loss = 0.9448639154434204
In grad_steps = 311, loss = 0.6814056634902954
In grad_steps = 312, loss = 0.8549827337265015
In grad_steps = 313, loss = 0.7018950581550598
In grad_steps = 314, loss = 0.8985698223114014
In grad_steps = 315, loss = 0.43809738755226135
In grad_steps = 316, loss = 0.6899588704109192
In grad_steps = 317, loss = 0.8176323175430298
In grad_steps = 318, loss = 0.5565788745880127
In grad_steps = 319, loss = 0.5765777230262756
In grad_steps = 320, loss = 0.7904857397079468
In grad_steps = 321, loss = 0.5526008009910583
In grad_steps = 322, loss = 0.5765008330345154
In grad_steps = 323, loss = 0.5746878385543823
In grad_steps = 324, loss = 0.7240616083145142
In grad_steps = 325, loss = 0.4850826859474182
In grad_steps = 326, loss = 0.8895748257637024
In grad_steps = 327, loss = 0.9751012325286865
In grad_steps = 328, loss = 0.5295768976211548
In grad_steps = 329, loss = 0.5853397250175476
In grad_steps = 330, loss = 0.7735490798950195
In grad_steps = 331, loss = 1.142775297164917
In grad_steps = 332, loss = 0.4935067892074585
In grad_steps = 333, loss = 0.7845270037651062
In grad_steps = 334, loss = 0.8031624555587769
In grad_steps = 335, loss = 0.552546501159668
In grad_steps = 336, loss = 0.680035412311554
In grad_steps = 337, loss = 0.711639940738678
In grad_steps = 338, loss = 0.7732224464416504
In grad_steps = 339, loss = 0.7043663859367371
In grad_steps = 340, loss = 0.888927698135376
In grad_steps = 341, loss = 0.5635265111923218
In grad_steps = 342, loss = 0.5066472291946411
In grad_steps = 343, loss = 0.939210832118988
In grad_steps = 344, loss = 0.5613218545913696
In grad_steps = 345, loss = 0.49218904972076416
In grad_steps = 346, loss = 1.0176913738250732
In grad_steps = 347, loss = 0.5279003381729126
In grad_steps = 348, loss = 0.9964672327041626
In grad_steps = 349, loss = 1.2413887977600098
In grad_steps = 350, loss = 0.9407269954681396
In grad_steps = 351, loss = 0.8824936747550964
In grad_steps = 352, loss = 0.575864315032959
In grad_steps = 353, loss = 0.778445839881897
In grad_steps = 354, loss = 0.7652857303619385
In grad_steps = 355, loss = 0.6625291109085083
In grad_steps = 356, loss = 0.6796388626098633
In grad_steps = 357, loss = 0.6167510747909546
In grad_steps = 358, loss = 0.8016701936721802
In grad_steps = 359, loss = 0.5345081090927124
In grad_steps = 360, loss = 0.48977959156036377
In grad_steps = 361, loss = 0.9874929189682007
In grad_steps = 362, loss = 1.01449716091156
In grad_steps = 363, loss = 0.4343540668487549
In grad_steps = 364, loss = 0.9886291027069092
In grad_steps = 365, loss = 0.9886153340339661
In grad_steps = 366, loss = 0.8760865926742554
In grad_steps = 367, loss = 0.7836822271347046
In grad_steps = 368, loss = 0.6535250544548035
In grad_steps = 369, loss = 0.6779699921607971
In grad_steps = 370, loss = 0.7374441623687744
In grad_steps = 371, loss = 0.8156933188438416
In grad_steps = 372, loss = 0.8180558085441589
In grad_steps = 373, loss = 0.7384849786758423
In grad_steps = 374, loss = 0.6366767883300781
In grad_steps = 375, loss = 0.7335294485092163
In grad_steps = 376, loss = 0.6642275452613831
In grad_steps = 377, loss = 0.7043134570121765
In grad_steps = 378, loss = 0.674849808216095
In grad_steps = 379, loss = 0.6450513005256653
In grad_steps = 380, loss = 0.7456392049789429
In grad_steps = 381, loss = 0.5858017206192017
In grad_steps = 382, loss = 0.8233214616775513
In grad_steps = 383, loss = 0.7727962732315063
In grad_steps = 384, loss = 0.7988483905792236
In grad_steps = 385, loss = 0.5974442362785339
In grad_steps = 386, loss = 0.5728657245635986
In grad_steps = 387, loss = 0.7459931373596191
In grad_steps = 388, loss = 0.6965500116348267
In grad_steps = 389, loss = 0.7301639318466187
In grad_steps = 390, loss = 0.6389797329902649
In grad_steps = 391, loss = 0.6948609352111816
In grad_steps = 392, loss = 0.6001265048980713
In grad_steps = 393, loss = 0.7422506213188171
In grad_steps = 394, loss = 0.7928515672683716
In grad_steps = 395, loss = 0.5484156608581543
In grad_steps = 396, loss = 0.5846123695373535
In grad_steps = 397, loss = 0.7680749297142029
In grad_steps = 398, loss = 0.586500883102417
In grad_steps = 399, loss = 0.857488751411438
In grad_steps = 400, loss = 0.8527364134788513
In grad_steps = 401, loss = 0.7246120572090149
In grad_steps = 402, loss = 0.7264280319213867
In grad_steps = 403, loss = 0.6716756820678711
In grad_steps = 404, loss = 0.45574647188186646
In grad_steps = 405, loss = 0.6167125105857849
In grad_steps = 406, loss = 0.246290385723114
In grad_steps = 407, loss = 0.5384407043457031
In grad_steps = 408, loss = 0.5857175588607788
In grad_steps = 409, loss = 0.17869384586811066
In grad_steps = 410, loss = 0.2735210955142975
In grad_steps = 411, loss = 0.28350335359573364
In grad_steps = 412, loss = 0.11798148602247238
In grad_steps = 413, loss = 0.09744296222925186
In grad_steps = 414, loss = 0.7462027072906494
In grad_steps = 415, loss = 0.012563489377498627
In grad_steps = 416, loss = 0.19980010390281677
In grad_steps = 417, loss = 0.0007288183551281691
In grad_steps = 418, loss = 6.57035493850708
In grad_steps = 419, loss = 0.009217919781804085
In grad_steps = 420, loss = 0.0021121830213814974
In grad_steps = 421, loss = 0.3355681896209717
In grad_steps = 422, loss = 1.546330451965332
In grad_steps = 423, loss = 0.024053772911429405
In grad_steps = 424, loss = 0.010653998702764511
In grad_steps = 425, loss = 0.2275722324848175
In grad_steps = 426, loss = 0.9707815647125244
In grad_steps = 427, loss = 0.42025870084762573
In grad_steps = 428, loss = 0.35691025853157043
In grad_steps = 429, loss = 0.6468955278396606
In grad_steps = 430, loss = 0.2341078817844391
In grad_steps = 431, loss = 0.7580799460411072
In grad_steps = 432, loss = 0.3274475038051605
In grad_steps = 433, loss = 1.1175059080123901
In grad_steps = 434, loss = 0.7408698797225952
In grad_steps = 435, loss = 1.0070910453796387
In grad_steps = 436, loss = 0.4998966455459595
In grad_steps = 437, loss = 1.0853701829910278
In grad_steps = 438, loss = 0.33902454376220703
In grad_steps = 439, loss = 0.9215631484985352
In grad_steps = 440, loss = 1.0154809951782227
In grad_steps = 441, loss = 0.437093585729599
In grad_steps = 442, loss = 0.5414925813674927
In grad_steps = 443, loss = 0.8378453254699707
In grad_steps = 444, loss = 0.53239506483078
In grad_steps = 445, loss = 0.5681692361831665
In grad_steps = 446, loss = 0.587655782699585
In grad_steps = 447, loss = 0.8007749319076538
In grad_steps = 448, loss = 0.47831618785858154
In grad_steps = 449, loss = 0.8834825158119202
In grad_steps = 450, loss = 0.8325216770172119
In grad_steps = 451, loss = 0.4702838957309723
In grad_steps = 452, loss = 0.5232557654380798
In grad_steps = 453, loss = 0.7100688219070435
In grad_steps = 454, loss = 0.901627242565155
In grad_steps = 455, loss = 0.4873730540275574
In grad_steps = 456, loss = 0.8097635507583618
In grad_steps = 457, loss = 0.6688989996910095
In grad_steps = 458, loss = 0.6537922620773315
In grad_steps = 459, loss = 0.5632537603378296
In grad_steps = 460, loss = 0.5812467932701111
In grad_steps = 461, loss = 0.903856098651886
In grad_steps = 462, loss = 0.5511850118637085
In grad_steps = 463, loss = 1.2030342817306519
In grad_steps = 464, loss = 0.4163118898868561
In grad_steps = 465, loss = 0.30680498480796814
In grad_steps = 466, loss = 1.1457586288452148
In grad_steps = 467, loss = 0.4151214063167572
In grad_steps = 468, loss = 0.39591068029403687
In grad_steps = 469, loss = 1.2257322072982788
In grad_steps = 470, loss = 0.33954980969429016
In grad_steps = 471, loss = 1.0449161529541016
In grad_steps = 472, loss = 1.5103662014007568
In grad_steps = 473, loss = 1.0057926177978516
In grad_steps = 474, loss = 0.8909853100776672
In grad_steps = 475, loss = 0.551032304763794
In grad_steps = 476, loss = 0.7445695400238037
In grad_steps = 477, loss = 0.8525881171226501
In grad_steps = 478, loss = 0.6924879550933838
In grad_steps = 479, loss = 0.6400255560874939
In grad_steps = 480, loss = 0.5492168664932251
In grad_steps = 481, loss = 0.8060368895530701
In grad_steps = 482, loss = 0.5929462909698486
In grad_steps = 483, loss = 0.441150963306427
In grad_steps = 484, loss = 1.112575650215149
In grad_steps = 485, loss = 0.9720361828804016
In grad_steps = 486, loss = 0.3915101885795593
In grad_steps = 487, loss = 1.0137100219726562
In grad_steps = 488, loss = 0.9861344695091248
In grad_steps = 489, loss = 0.7815996408462524
In grad_steps = 490, loss = 0.7794826030731201
In grad_steps = 491, loss = 0.6504644751548767
i = 1, Test ensemble probabilities = 
[array([[0.6284508 , 0.37154916],
       [0.6362697 , 0.36373034],
       [0.6395782 , 0.36042175],
       [0.6325759 , 0.3674241 ],
       [0.6236578 , 0.37634215],
       [0.64280176, 0.35719827],
       [0.6226229 , 0.37737706],
       [0.63043296, 0.36956698],
       [0.61667144, 0.38332856],
       [0.6516359 , 0.34836406],
       [0.629431  , 0.37056902],
       [0.6349214 , 0.36507857],
       [0.6210475 , 0.37895253],
       [0.64654285, 0.35345718],
       [0.6052796 , 0.39472032],
       [0.6282811 , 0.37171885]], dtype=float32), array([[0.53180707, 0.46819288],
       [0.43626124, 0.5637387 ],
       [0.32801697, 0.67198306],
       [0.4875767 , 0.51242334],
       [0.48902902, 0.51097095],
       [0.4543582 , 0.54564184],
       [0.45123112, 0.54876894],
       [0.5060453 , 0.49395472],
       [0.50209475, 0.49790528],
       [0.4963399 , 0.50366014],
       [0.46167827, 0.5383218 ],
       [0.5426025 , 0.45739752],
       [0.49294376, 0.50705624],
       [0.5642455 , 0.4357545 ],
       [0.51248384, 0.48751613],
       [0.47801602, 0.521984  ]], dtype=float32)]
i = 1, Test true class= 
[1 0 0 0 1 0 1 0 0 0 1 1 0 1 1 0]
In grad_steps = 0, loss = 1.16717529296875
In grad_steps = 1, loss = 1.1758217811584473
In grad_steps = 2, loss = 1.2235751152038574
In grad_steps = 3, loss = 0.6913006901741028
In grad_steps = 4, loss = 0.39438191056251526
In grad_steps = 5, loss = 1.6578261852264404
In grad_steps = 6, loss = 0.32588791847229004
In grad_steps = 7, loss = 0.25140202045440674
In grad_steps = 8, loss = 1.3696409463882446
In grad_steps = 9, loss = 1.3327587842941284
In grad_steps = 10, loss = 0.44360291957855225
In grad_steps = 11, loss = 1.0725892782211304
In grad_steps = 12, loss = 0.5442084670066833
In grad_steps = 13, loss = 0.7482717037200928
In grad_steps = 14, loss = 0.5016198754310608
In grad_steps = 15, loss = 0.3787674903869629
In grad_steps = 16, loss = 1.8061718940734863
In grad_steps = 17, loss = 1.6495875120162964
In grad_steps = 18, loss = 0.28843140602111816
In grad_steps = 19, loss = 0.326842337846756
In grad_steps = 20, loss = 0.311009019613266
In grad_steps = 21, loss = 0.40011540055274963
In grad_steps = 22, loss = 1.1144598722457886
In grad_steps = 23, loss = 0.403078556060791
In grad_steps = 24, loss = 0.9158490896224976
In grad_steps = 25, loss = 1.0638771057128906
In grad_steps = 26, loss = 0.5878956317901611
In grad_steps = 27, loss = 0.5810971856117249
In grad_steps = 28, loss = 0.7806851267814636
In grad_steps = 29, loss = 0.6925856471061707
In grad_steps = 30, loss = 0.681029736995697
In grad_steps = 31, loss = 0.6462712287902832
In grad_steps = 32, loss = 0.7619600892066956
In grad_steps = 33, loss = 0.4647749662399292
In grad_steps = 34, loss = 0.4743492603302002
In grad_steps = 35, loss = 1.178853154182434
In grad_steps = 36, loss = 1.476576805114746
In grad_steps = 37, loss = 0.2901032567024231
In grad_steps = 38, loss = 1.8371245861053467
In grad_steps = 39, loss = 0.34505388140678406
In grad_steps = 40, loss = 0.37293362617492676
In grad_steps = 41, loss = 1.2747915983200073
In grad_steps = 42, loss = 1.2117842435836792
In grad_steps = 43, loss = 0.5121808052062988
In grad_steps = 44, loss = 0.9768098592758179
In grad_steps = 45, loss = 0.9257974624633789
In grad_steps = 46, loss = 0.5573217868804932
In grad_steps = 47, loss = 0.6269221305847168
In grad_steps = 48, loss = 0.7388988733291626
In grad_steps = 49, loss = 0.6834329962730408
In grad_steps = 50, loss = 0.736984133720398
In grad_steps = 51, loss = 0.793312668800354
In grad_steps = 52, loss = 0.6846761703491211
In grad_steps = 53, loss = 0.7207967042922974
In grad_steps = 54, loss = 0.6862545013427734
In grad_steps = 55, loss = 0.7048472166061401
In grad_steps = 56, loss = 0.7125332951545715
In grad_steps = 57, loss = 0.6314548254013062
In grad_steps = 58, loss = 0.8389706611633301
In grad_steps = 59, loss = 0.7946761250495911
In grad_steps = 60, loss = 0.6135692000389099
In grad_steps = 61, loss = 0.7941475510597229
In grad_steps = 62, loss = 0.6241145133972168
In grad_steps = 63, loss = 0.7608088254928589
In grad_steps = 64, loss = 0.7973021268844604
In grad_steps = 65, loss = 0.7047932744026184
In grad_steps = 66, loss = 0.7795476913452148
In grad_steps = 67, loss = 0.7431213855743408
In grad_steps = 68, loss = 0.7058071494102478
In grad_steps = 69, loss = 0.6936575174331665
In grad_steps = 70, loss = 0.6847976446151733
In grad_steps = 71, loss = 0.785346508026123
In grad_steps = 72, loss = 0.6469764709472656
In grad_steps = 73, loss = 0.6384733319282532
In grad_steps = 74, loss = 0.7542311549186707
In grad_steps = 75, loss = 0.666942834854126
In grad_steps = 76, loss = 0.653549313545227
In grad_steps = 77, loss = 0.6416106820106506
In grad_steps = 78, loss = 0.8313792943954468
In grad_steps = 79, loss = 0.5374626517295837
In grad_steps = 80, loss = 0.9123666286468506
In grad_steps = 81, loss = 0.9674104452133179
In grad_steps = 82, loss = 0.500974714756012
In grad_steps = 83, loss = 0.4883151650428772
In grad_steps = 84, loss = 0.8492755889892578
In grad_steps = 85, loss = 0.9267721176147461
In grad_steps = 86, loss = 0.5442861318588257
In grad_steps = 87, loss = 0.838295578956604
In grad_steps = 88, loss = 0.8237099647521973
In grad_steps = 89, loss = 0.5956720113754272
In grad_steps = 90, loss = 0.750186562538147
In grad_steps = 91, loss = 0.6876501441001892
In grad_steps = 92, loss = 0.8056944608688354
In grad_steps = 93, loss = 0.618211567401886
In grad_steps = 94, loss = 0.8264713883399963
In grad_steps = 95, loss = 0.5366910696029663
In grad_steps = 96, loss = 0.5012776255607605
In grad_steps = 97, loss = 0.9318121671676636
In grad_steps = 98, loss = 0.47909438610076904
In grad_steps = 99, loss = 0.4760034382343292
In grad_steps = 100, loss = 1.0969221591949463
In grad_steps = 101, loss = 0.4652736485004425
In grad_steps = 102, loss = 1.1286139488220215
In grad_steps = 103, loss = 1.252958059310913
In grad_steps = 104, loss = 0.9975486993789673
In grad_steps = 105, loss = 0.9508235454559326
In grad_steps = 106, loss = 0.5699650049209595
In grad_steps = 107, loss = 0.826810359954834
In grad_steps = 108, loss = 0.81739741563797
In grad_steps = 109, loss = 0.653389573097229
In grad_steps = 110, loss = 0.7021749019622803
In grad_steps = 111, loss = 0.62784343957901
In grad_steps = 112, loss = 0.7987445592880249
In grad_steps = 113, loss = 0.5678237676620483
In grad_steps = 114, loss = 0.537614107131958
In grad_steps = 115, loss = 0.9574859738349915
In grad_steps = 116, loss = 0.9757940173149109
In grad_steps = 117, loss = 0.45023301243782043
In grad_steps = 118, loss = 0.962143063545227
In grad_steps = 119, loss = 1.0211739540100098
In grad_steps = 120, loss = 0.9005268216133118
In grad_steps = 121, loss = 0.8244176506996155
In grad_steps = 122, loss = 0.6004419922828674
In grad_steps = 123, loss = 0.7123194932937622
In grad_steps = 124, loss = 0.6980931758880615
In grad_steps = 125, loss = 0.748887836933136
In grad_steps = 126, loss = 0.7324151992797852
In grad_steps = 127, loss = 0.685500979423523
In grad_steps = 128, loss = 0.7210116386413574
In grad_steps = 129, loss = 0.687531590461731
In grad_steps = 130, loss = 0.6344743371009827
In grad_steps = 131, loss = 0.7302923798561096
In grad_steps = 132, loss = 0.7175876498222351
In grad_steps = 133, loss = 0.622772216796875
In grad_steps = 134, loss = 0.7812837362289429
In grad_steps = 135, loss = 0.5639370679855347
In grad_steps = 136, loss = 0.8240193128585815
In grad_steps = 137, loss = 0.7796354293823242
In grad_steps = 138, loss = 0.7755952477455139
In grad_steps = 139, loss = 0.6352711915969849
In grad_steps = 140, loss = 0.6047125458717346
In grad_steps = 141, loss = 0.7267782688140869
In grad_steps = 142, loss = 0.6976510286331177
In grad_steps = 143, loss = 0.6578159928321838
In grad_steps = 144, loss = 0.6685539484024048
In grad_steps = 145, loss = 0.7271448373794556
In grad_steps = 146, loss = 0.5990380644798279
In grad_steps = 147, loss = 0.7276170253753662
In grad_steps = 148, loss = 0.8358745574951172
In grad_steps = 149, loss = 0.5845764875411987
In grad_steps = 150, loss = 0.5468987226486206
In grad_steps = 151, loss = 0.850691556930542
In grad_steps = 152, loss = 0.5321727395057678
In grad_steps = 153, loss = 0.871013879776001
In grad_steps = 154, loss = 0.881531834602356
In grad_steps = 155, loss = 0.957897424697876
In grad_steps = 156, loss = 0.7611995935440063
In grad_steps = 157, loss = 0.7998404502868652
In grad_steps = 158, loss = 0.5978999137878418
In grad_steps = 159, loss = 0.7710265517234802
In grad_steps = 160, loss = 0.6038492918014526
In grad_steps = 161, loss = 0.8727595806121826
In grad_steps = 162, loss = 0.6231940984725952
In grad_steps = 163, loss = 0.5939804911613464
In grad_steps = 164, loss = 0.8545882105827332
In grad_steps = 165, loss = 0.8335027694702148
In grad_steps = 166, loss = 0.7065393924713135
In grad_steps = 167, loss = 0.7761768698692322
In grad_steps = 168, loss = 0.7991758584976196
In grad_steps = 169, loss = 0.6038795113563538
In grad_steps = 170, loss = 0.6339466571807861
In grad_steps = 171, loss = 0.7174568176269531
In grad_steps = 172, loss = 0.6779099106788635
In grad_steps = 173, loss = 0.7570935487747192
In grad_steps = 174, loss = 0.7453916072845459
In grad_steps = 175, loss = 0.625440239906311
In grad_steps = 176, loss = 0.7941271662712097
In grad_steps = 177, loss = 0.6533104777336121
In grad_steps = 178, loss = 0.640727698802948
In grad_steps = 179, loss = 0.7281372547149658
In grad_steps = 180, loss = 0.6274961829185486
In grad_steps = 181, loss = 0.8186991214752197
In grad_steps = 182, loss = 0.750724732875824
In grad_steps = 183, loss = 0.6313620209693909
In grad_steps = 184, loss = 0.7465152144432068
In grad_steps = 185, loss = 0.6499388217926025
In grad_steps = 186, loss = 0.7261395454406738
In grad_steps = 187, loss = 0.7984791398048401
In grad_steps = 188, loss = 0.6828095316886902
In grad_steps = 189, loss = 0.7533091902732849
In grad_steps = 190, loss = 0.7138895392417908
In grad_steps = 191, loss = 0.7392426133155823
In grad_steps = 192, loss = 0.6612251996994019
In grad_steps = 193, loss = 0.6784989237785339
In grad_steps = 194, loss = 0.755383312702179
In grad_steps = 195, loss = 0.6525134444236755
In grad_steps = 196, loss = 0.6511889696121216
In grad_steps = 197, loss = 0.7276436686515808
In grad_steps = 198, loss = 0.6574726104736328
In grad_steps = 199, loss = 0.6723570227622986
In grad_steps = 200, loss = 0.6332667469978333
In grad_steps = 201, loss = 0.7680431604385376
In grad_steps = 202, loss = 0.5805199146270752
In grad_steps = 203, loss = 0.8071995377540588
In grad_steps = 204, loss = 0.8561057448387146
In grad_steps = 205, loss = 0.5502460598945618
In grad_steps = 206, loss = 0.5600407719612122
In grad_steps = 207, loss = 0.7989612221717834
In grad_steps = 208, loss = 0.870711624622345
In grad_steps = 209, loss = 0.5625826120376587
In grad_steps = 210, loss = 0.839092493057251
In grad_steps = 211, loss = 0.8221026659011841
In grad_steps = 212, loss = 0.5749067068099976
In grad_steps = 213, loss = 0.7935842275619507
In grad_steps = 214, loss = 0.7438167333602905
In grad_steps = 215, loss = 0.7041846513748169
In grad_steps = 216, loss = 0.7128283381462097
In grad_steps = 217, loss = 0.7247880101203918
In grad_steps = 218, loss = 0.6506354808807373
In grad_steps = 219, loss = 0.606906533241272
In grad_steps = 220, loss = 0.7832927107810974
In grad_steps = 221, loss = 0.5917808413505554
In grad_steps = 222, loss = 0.5686253905296326
In grad_steps = 223, loss = 0.8908413648605347
In grad_steps = 224, loss = 0.5436519384384155
In grad_steps = 225, loss = 0.9332880973815918
In grad_steps = 226, loss = 1.0271081924438477
In grad_steps = 227, loss = 0.8943649530410767
In grad_steps = 228, loss = 0.8687050938606262
In grad_steps = 229, loss = 0.5781232714653015
In grad_steps = 230, loss = 0.7974321842193604
In grad_steps = 231, loss = 0.7989234924316406
In grad_steps = 232, loss = 0.6338334083557129
In grad_steps = 233, loss = 0.7268359065055847
In grad_steps = 234, loss = 0.6744384765625
In grad_steps = 235, loss = 0.7340103983879089
In grad_steps = 236, loss = 0.6283740401268005
In grad_steps = 237, loss = 0.5936974287033081
In grad_steps = 238, loss = 0.8284429311752319
In grad_steps = 239, loss = 0.8403024673461914
In grad_steps = 240, loss = 0.5205812454223633
In grad_steps = 241, loss = 0.8466531038284302
In grad_steps = 242, loss = 0.9017977714538574
In grad_steps = 243, loss = 0.8143713474273682
In grad_steps = 244, loss = 0.775492787361145
In grad_steps = 245, loss = 0.6327032446861267
In grad_steps = 246, loss = 0.6923993229866028
In grad_steps = 247, loss = 0.6780184507369995
In grad_steps = 248, loss = 0.7344384789466858
In grad_steps = 249, loss = 0.7069453597068787
In grad_steps = 250, loss = 0.6680436134338379
In grad_steps = 251, loss = 0.7066776752471924
In grad_steps = 252, loss = 0.6817477941513062
In grad_steps = 253, loss = 0.6331907510757446
In grad_steps = 254, loss = 0.7385307550430298
In grad_steps = 255, loss = 0.6619682908058167
In grad_steps = 256, loss = 0.6215949058532715
In grad_steps = 257, loss = 0.7503345608711243
In grad_steps = 258, loss = 0.5671790838241577
In grad_steps = 259, loss = 0.8110674619674683
In grad_steps = 260, loss = 0.7831481695175171
In grad_steps = 261, loss = 0.7570667266845703
In grad_steps = 262, loss = 0.6064440608024597
In grad_steps = 263, loss = 0.5924277305603027
In grad_steps = 264, loss = 0.7264758348464966
In grad_steps = 265, loss = 0.6655652523040771
In grad_steps = 266, loss = 0.6633448600769043
In grad_steps = 267, loss = 0.6673755049705505
In grad_steps = 268, loss = 0.6897341012954712
In grad_steps = 269, loss = 0.5790311694145203
In grad_steps = 270, loss = 0.6717788577079773
In grad_steps = 271, loss = 0.7865498065948486
In grad_steps = 272, loss = 0.5326719284057617
In grad_steps = 273, loss = 0.5266041159629822
In grad_steps = 274, loss = 0.7146353721618652
In grad_steps = 275, loss = 0.41547277569770813
In grad_steps = 276, loss = 0.9064066410064697
In grad_steps = 277, loss = 0.7887799739837646
In grad_steps = 278, loss = 0.8910516500473022
In grad_steps = 279, loss = 0.4355717897415161
In grad_steps = 280, loss = 0.39255407452583313
In grad_steps = 281, loss = 1.1643959283828735
In grad_steps = 282, loss = 1.9110920429229736
In grad_steps = 283, loss = 0.13263627886772156
In grad_steps = 284, loss = 0.771324872970581
In grad_steps = 285, loss = 0.5705021023750305
In grad_steps = 286, loss = 0.45380091667175293
In grad_steps = 287, loss = 0.7204082012176514
In grad_steps = 288, loss = 0.5957096219062805
In grad_steps = 289, loss = 0.9497941136360168
In grad_steps = 290, loss = 0.6010953783988953
In grad_steps = 291, loss = 0.6889389753341675
In grad_steps = 292, loss = 0.6607543230056763
In grad_steps = 293, loss = 0.6799799203872681
In grad_steps = 294, loss = 0.5795800685882568
In grad_steps = 295, loss = 0.7676265835762024
In grad_steps = 296, loss = 0.6012448668479919
In grad_steps = 297, loss = 0.8323320150375366
In grad_steps = 298, loss = 0.6596003174781799
In grad_steps = 299, loss = 0.7115617990493774
In grad_steps = 300, loss = 0.6473249793052673
In grad_steps = 301, loss = 0.6083160638809204
In grad_steps = 302, loss = 0.7135719060897827
In grad_steps = 303, loss = 0.5786811709403992
In grad_steps = 304, loss = 0.9064646363258362
In grad_steps = 305, loss = 0.7386074662208557
In grad_steps = 306, loss = 0.5538849830627441
In grad_steps = 307, loss = 0.7102686166763306
In grad_steps = 308, loss = 0.6107373833656311
In grad_steps = 309, loss = 0.736449658870697
In grad_steps = 310, loss = 0.8750489354133606
In grad_steps = 311, loss = 0.6630333065986633
In grad_steps = 312, loss = 0.7926756739616394
In grad_steps = 313, loss = 0.6635164618492126
In grad_steps = 314, loss = 0.8872528076171875
In grad_steps = 315, loss = 0.41035494208335876
In grad_steps = 316, loss = 0.7431459426879883
In grad_steps = 317, loss = 0.7777976989746094
In grad_steps = 318, loss = 0.5859255194664001
In grad_steps = 319, loss = 0.6355156898498535
In grad_steps = 320, loss = 0.729198694229126
In grad_steps = 321, loss = 0.5906535983085632
In grad_steps = 322, loss = 0.6011298894882202
In grad_steps = 323, loss = 0.6395360827445984
In grad_steps = 324, loss = 0.7220091819763184
In grad_steps = 325, loss = 0.5163379311561584
In grad_steps = 326, loss = 0.8199679851531982
In grad_steps = 327, loss = 0.8791113495826721
In grad_steps = 328, loss = 0.5389586091041565
In grad_steps = 329, loss = 0.6301186084747314
In grad_steps = 330, loss = 0.8190867900848389
In grad_steps = 331, loss = 0.8530138731002808
In grad_steps = 332, loss = 0.576932966709137
In grad_steps = 333, loss = 0.7733322978019714
In grad_steps = 334, loss = 0.8190099596977234
In grad_steps = 335, loss = 0.5433583855628967
In grad_steps = 336, loss = 0.77158123254776
In grad_steps = 337, loss = 0.6263453960418701
In grad_steps = 338, loss = 0.8728822469711304
In grad_steps = 339, loss = 0.6696621179580688
In grad_steps = 340, loss = 0.9252666234970093
In grad_steps = 341, loss = 0.5022645592689514
In grad_steps = 342, loss = 0.4406578838825226
In grad_steps = 343, loss = 0.937035322189331
In grad_steps = 344, loss = 0.45973432064056396
In grad_steps = 345, loss = 0.46762925386428833
In grad_steps = 346, loss = 1.2094310522079468
In grad_steps = 347, loss = 0.4276805520057678
In grad_steps = 348, loss = 1.1516544818878174
In grad_steps = 349, loss = 1.1599618196487427
In grad_steps = 350, loss = 0.9479607343673706
In grad_steps = 351, loss = 0.8296021223068237
In grad_steps = 352, loss = 0.743889331817627
In grad_steps = 353, loss = 0.6647215485572815
In grad_steps = 354, loss = 0.672981858253479
In grad_steps = 355, loss = 0.8212979435920715
In grad_steps = 356, loss = 0.5366470217704773
In grad_steps = 357, loss = 0.48067039251327515
In grad_steps = 358, loss = 1.0104210376739502
In grad_steps = 359, loss = 0.4156479239463806
In grad_steps = 360, loss = 0.4202331602573395
In grad_steps = 361, loss = 1.1925008296966553
In grad_steps = 362, loss = 1.158811092376709
In grad_steps = 363, loss = 0.432717889547348
In grad_steps = 364, loss = 1.0410500764846802
In grad_steps = 365, loss = 0.9594798684120178
In grad_steps = 366, loss = 0.834843635559082
In grad_steps = 367, loss = 0.7176218032836914
In grad_steps = 368, loss = 0.7608505487442017
In grad_steps = 369, loss = 0.5937567949295044
In grad_steps = 370, loss = 0.8562543988227844
In grad_steps = 371, loss = 0.9262104034423828
In grad_steps = 372, loss = 0.8656148910522461
In grad_steps = 373, loss = 0.8235291242599487
In grad_steps = 374, loss = 0.6250740885734558
In grad_steps = 375, loss = 0.7226577401161194
In grad_steps = 376, loss = 0.6940221786499023
In grad_steps = 377, loss = 0.7142089009284973
In grad_steps = 378, loss = 0.7509518265724182
In grad_steps = 379, loss = 0.6126870512962341
In grad_steps = 380, loss = 0.7879830002784729
In grad_steps = 381, loss = 0.587041437625885
In grad_steps = 382, loss = 0.8149324655532837
In grad_steps = 383, loss = 0.7856324911117554
In grad_steps = 384, loss = 0.7837619781494141
In grad_steps = 385, loss = 0.6480960845947266
In grad_steps = 386, loss = 0.640060305595398
In grad_steps = 387, loss = 0.6973751783370972
In grad_steps = 388, loss = 0.6873793601989746
In grad_steps = 389, loss = 0.6901267170906067
In grad_steps = 390, loss = 0.6247053742408752
In grad_steps = 391, loss = 0.8094862699508667
In grad_steps = 392, loss = 0.5587989687919617
In grad_steps = 393, loss = 0.8777231574058533
In grad_steps = 394, loss = 0.9114189743995667
In grad_steps = 395, loss = 0.526759147644043
In grad_steps = 396, loss = 0.5377223491668701
In grad_steps = 397, loss = 0.9309181571006775
In grad_steps = 398, loss = 0.5041176080703735
In grad_steps = 399, loss = 0.9106475114822388
In grad_steps = 400, loss = 0.9290231466293335
In grad_steps = 401, loss = 0.8887850642204285
In grad_steps = 402, loss = 0.8642942309379578
In grad_steps = 403, loss = 0.81178879737854
In grad_steps = 404, loss = 0.6370434165000916
In grad_steps = 405, loss = 0.6743678450584412
In grad_steps = 406, loss = 0.7080543637275696
In grad_steps = 407, loss = 0.7064746618270874
In grad_steps = 408, loss = 0.6663798689842224
In grad_steps = 409, loss = 0.6433266401290894
In grad_steps = 410, loss = 0.7759513854980469
In grad_steps = 411, loss = 0.7757281064987183
In grad_steps = 412, loss = 0.6224022507667542
In grad_steps = 413, loss = 0.7690533399581909
In grad_steps = 414, loss = 0.7653408050537109
In grad_steps = 415, loss = 0.6324846148490906
In grad_steps = 416, loss = 0.6492895483970642
In grad_steps = 417, loss = 0.731876015663147
In grad_steps = 418, loss = 0.6421979665756226
In grad_steps = 419, loss = 0.7349153757095337
In grad_steps = 420, loss = 0.6577562093734741
In grad_steps = 421, loss = 0.6442664861679077
In grad_steps = 422, loss = 0.769576907157898
In grad_steps = 423, loss = 0.6396572589874268
In grad_steps = 424, loss = 0.6165360808372498
In grad_steps = 425, loss = 0.7839682698249817
In grad_steps = 426, loss = 0.5933610796928406
In grad_steps = 427, loss = 0.8238887190818787
In grad_steps = 428, loss = 0.8461373448371887
In grad_steps = 429, loss = 0.5839679837226868
In grad_steps = 430, loss = 0.8158565759658813
In grad_steps = 431, loss = 0.5958569049835205
In grad_steps = 432, loss = 0.81160968542099
In grad_steps = 433, loss = 0.7833123207092285
In grad_steps = 434, loss = 0.64783775806427
In grad_steps = 435, loss = 0.6642669439315796
In grad_steps = 436, loss = 0.6402910947799683
In grad_steps = 437, loss = 0.7558475136756897
In grad_steps = 438, loss = 0.6254655122756958
In grad_steps = 439, loss = 0.7562658786773682
In grad_steps = 440, loss = 0.7516374588012695
In grad_steps = 441, loss = 0.6331585049629211
In grad_steps = 442, loss = 0.6369068622589111
In grad_steps = 443, loss = 0.7465909123420715
In grad_steps = 444, loss = 0.6280550956726074
In grad_steps = 445, loss = 0.6277700662612915
In grad_steps = 446, loss = 0.6305868029594421
In grad_steps = 447, loss = 0.7746996879577637
In grad_steps = 448, loss = 0.5812653303146362
In grad_steps = 449, loss = 0.8175545334815979
In grad_steps = 450, loss = 0.8246244192123413
In grad_steps = 451, loss = 0.5800915956497192
In grad_steps = 452, loss = 0.5802620649337769
In grad_steps = 453, loss = 0.8021515011787415
In grad_steps = 454, loss = 0.8390264511108398
In grad_steps = 455, loss = 0.5915968418121338
In grad_steps = 456, loss = 0.8322619795799255
In grad_steps = 457, loss = 0.7874293923377991
In grad_steps = 458, loss = 0.6071091294288635
In grad_steps = 459, loss = 0.7833972573280334
In grad_steps = 460, loss = 0.7209044098854065
In grad_steps = 461, loss = 0.6864686608314514
In grad_steps = 462, loss = 0.7056150436401367
In grad_steps = 463, loss = 0.7196260690689087
In grad_steps = 464, loss = 0.6996393203735352
In grad_steps = 465, loss = 0.6548675894737244
In grad_steps = 466, loss = 0.7583896517753601
In grad_steps = 467, loss = 0.6160587072372437
In grad_steps = 468, loss = 0.587794303894043
In grad_steps = 469, loss = 0.8530680537223816
In grad_steps = 470, loss = 0.5560727119445801
In grad_steps = 471, loss = 0.8390145897865295
In grad_steps = 472, loss = 0.9085363745689392
In grad_steps = 473, loss = 0.8882314562797546
In grad_steps = 474, loss = 0.8233043551445007
In grad_steps = 475, loss = 0.5770153999328613
In grad_steps = 476, loss = 0.7963300347328186
In grad_steps = 477, loss = 0.7894461154937744
In grad_steps = 478, loss = 0.651181161403656
In grad_steps = 479, loss = 0.7172859311103821
In grad_steps = 480, loss = 0.683469831943512
In grad_steps = 481, loss = 0.7186704277992249
In grad_steps = 482, loss = 0.6357888579368591
In grad_steps = 483, loss = 0.6335343718528748
In grad_steps = 484, loss = 0.8100435733795166
In grad_steps = 485, loss = 0.7977360486984253
In grad_steps = 486, loss = 0.5976647138595581
In grad_steps = 487, loss = 0.8106489777565002
In grad_steps = 488, loss = 0.8237410187721252
In grad_steps = 489, loss = 0.8131638765335083
In grad_steps = 490, loss = 0.7349739074707031
In grad_steps = 491, loss = 0.6607136726379395
i = 2, Test ensemble probabilities = 
[array([[0.6284508 , 0.37154916],
       [0.6362697 , 0.36373034],
       [0.6395782 , 0.36042175],
       [0.6325759 , 0.3674241 ],
       [0.6236578 , 0.37634215],
       [0.64280176, 0.35719827],
       [0.6226229 , 0.37737706],
       [0.63043296, 0.36956698],
       [0.61667144, 0.38332856],
       [0.6516359 , 0.34836406],
       [0.629431  , 0.37056902],
       [0.6349214 , 0.36507857],
       [0.6210475 , 0.37895253],
       [0.64654285, 0.35345718],
       [0.6052796 , 0.39472032],
       [0.6282811 , 0.37171885]], dtype=float32), array([[0.53180707, 0.46819288],
       [0.43626124, 0.5637387 ],
       [0.32801697, 0.67198306],
       [0.4875767 , 0.51242334],
       [0.48902902, 0.51097095],
       [0.4543582 , 0.54564184],
       [0.45123112, 0.54876894],
       [0.5060453 , 0.49395472],
       [0.50209475, 0.49790528],
       [0.4963399 , 0.50366014],
       [0.46167827, 0.5383218 ],
       [0.5426025 , 0.45739752],
       [0.49294376, 0.50705624],
       [0.5642455 , 0.4357545 ],
       [0.51248384, 0.48751613],
       [0.47801602, 0.521984  ]], dtype=float32), array([[0.48643658, 0.5135634 ],
       [0.48687044, 0.51312953],
       [0.4646091 , 0.5353909 ],
       [0.48622504, 0.513775  ],
       [0.47988525, 0.5201148 ],
       [0.47998756, 0.5200124 ],
       [0.48647708, 0.51352286],
       [0.48873806, 0.51126194],
       [0.4840995 , 0.5159005 ],
       [0.48697886, 0.5130212 ],
       [0.48528168, 0.51471835],
       [0.48911697, 0.5108831 ],
       [0.47824326, 0.5217567 ],
       [0.49554408, 0.5044559 ],
       [0.48103884, 0.51896113],
       [0.48362648, 0.51637346]], dtype=float32)]
i = 2, Test true class= 
[1 0 0 0 1 0 1 0 0 0 1 1 0 1 1 0]
In grad_steps = 0, loss = 1.16717529296875
In grad_steps = 1, loss = 1.1276689767837524
In grad_steps = 2, loss = 1.186908483505249
In grad_steps = 3, loss = 0.6822391748428345
In grad_steps = 4, loss = 0.385046124458313
In grad_steps = 5, loss = 1.6777423620224
In grad_steps = 6, loss = 0.3304421007633209
In grad_steps = 7, loss = 0.23998773097991943
In grad_steps = 8, loss = 1.380407452583313
In grad_steps = 9, loss = 1.3533505201339722
In grad_steps = 10, loss = 0.44688308238983154
In grad_steps = 11, loss = 1.0612421035766602
In grad_steps = 12, loss = 0.5569485425949097
In grad_steps = 13, loss = 0.74494868516922
In grad_steps = 14, loss = 0.4918762445449829
In grad_steps = 15, loss = 0.37737321853637695
In grad_steps = 16, loss = 1.732548713684082
In grad_steps = 17, loss = 1.6768584251403809
In grad_steps = 18, loss = 0.28582292795181274
In grad_steps = 19, loss = 0.3181643784046173
In grad_steps = 20, loss = 0.30402034521102905
In grad_steps = 21, loss = 0.3908982574939728
In grad_steps = 22, loss = 1.133939504623413
In grad_steps = 23, loss = 0.3763922154903412
In grad_steps = 24, loss = 0.9610850214958191
In grad_steps = 25, loss = 1.069556474685669
In grad_steps = 26, loss = 0.5713110566139221
In grad_steps = 27, loss = 0.5700005888938904
In grad_steps = 28, loss = 0.7772029042243958
In grad_steps = 29, loss = 0.7131833434104919
In grad_steps = 30, loss = 0.666162371635437
In grad_steps = 31, loss = 0.6522977948188782
In grad_steps = 32, loss = 0.7476469278335571
In grad_steps = 33, loss = 0.46532613039016724
In grad_steps = 34, loss = 0.47658663988113403
In grad_steps = 35, loss = 1.1731159687042236
In grad_steps = 36, loss = 1.4901787042617798
In grad_steps = 37, loss = 0.28631502389907837
In grad_steps = 38, loss = 1.8642067909240723
In grad_steps = 39, loss = 0.3430538475513458
In grad_steps = 40, loss = 0.3809412121772766
In grad_steps = 41, loss = 1.2723355293273926
In grad_steps = 42, loss = 1.190338373184204
In grad_steps = 43, loss = 0.5232661962509155
In grad_steps = 44, loss = 0.9680662751197815
In grad_steps = 45, loss = 0.9201804399490356
In grad_steps = 46, loss = 0.5555863380432129
In grad_steps = 47, loss = 0.6244834065437317
In grad_steps = 48, loss = 0.7430969476699829
In grad_steps = 49, loss = 0.683420717716217
In grad_steps = 50, loss = 0.7606300711631775
In grad_steps = 51, loss = 0.806044340133667
In grad_steps = 52, loss = 0.6897711753845215
In grad_steps = 53, loss = 0.7309697270393372
In grad_steps = 54, loss = 0.6922125816345215
In grad_steps = 55, loss = 0.710875928401947
In grad_steps = 56, loss = 0.7239347696304321
In grad_steps = 57, loss = 0.6326740980148315
In grad_steps = 58, loss = 0.8384904861450195
In grad_steps = 59, loss = 0.7862112522125244
In grad_steps = 60, loss = 0.6124908924102783
In grad_steps = 61, loss = 0.7909537553787231
In grad_steps = 62, loss = 0.6335169076919556
In grad_steps = 63, loss = 0.7608063220977783
In grad_steps = 64, loss = 0.7971769571304321
In grad_steps = 65, loss = 0.7187774777412415
In grad_steps = 66, loss = 0.7854244112968445
In grad_steps = 67, loss = 0.7553125023841858
In grad_steps = 68, loss = 0.7003877758979797
In grad_steps = 69, loss = 0.6971409916877747
In grad_steps = 70, loss = 0.6806078553199768
In grad_steps = 71, loss = 0.7801705002784729
In grad_steps = 72, loss = 0.6401085257530212
In grad_steps = 73, loss = 0.6412950158119202
In grad_steps = 74, loss = 0.7575027942657471
In grad_steps = 75, loss = 0.663862943649292
In grad_steps = 76, loss = 0.6592273116111755
In grad_steps = 77, loss = 0.6372743844985962
In grad_steps = 78, loss = 0.8206544518470764
In grad_steps = 79, loss = 0.5300326943397522
In grad_steps = 80, loss = 0.91816246509552
In grad_steps = 81, loss = 0.9675695896148682
In grad_steps = 82, loss = 0.49165838956832886
In grad_steps = 83, loss = 0.48229801654815674
In grad_steps = 84, loss = 0.8813658952713013
In grad_steps = 85, loss = 0.9394344091415405
In grad_steps = 86, loss = 0.5374642610549927
In grad_steps = 87, loss = 0.843284010887146
In grad_steps = 88, loss = 0.8255053162574768
In grad_steps = 89, loss = 0.599138617515564
In grad_steps = 90, loss = 0.7519604563713074
In grad_steps = 91, loss = 0.6800053119659424
In grad_steps = 92, loss = 0.8039578199386597
In grad_steps = 93, loss = 0.6260160207748413
In grad_steps = 94, loss = 0.8242527842521667
In grad_steps = 95, loss = 0.5294177532196045
In grad_steps = 96, loss = 0.4943951964378357
In grad_steps = 97, loss = 0.945164144039154
In grad_steps = 98, loss = 0.4676954746246338
In grad_steps = 99, loss = 0.46398359537124634
In grad_steps = 100, loss = 1.1262075901031494
In grad_steps = 101, loss = 0.4644942879676819
In grad_steps = 102, loss = 1.1442158222198486
In grad_steps = 103, loss = 1.2823259830474854
In grad_steps = 104, loss = 1.0100609064102173
In grad_steps = 105, loss = 0.9604049921035767
In grad_steps = 106, loss = 0.5729646682739258
In grad_steps = 107, loss = 0.8211681842803955
In grad_steps = 108, loss = 0.8172004222869873
In grad_steps = 109, loss = 0.6607275605201721
In grad_steps = 110, loss = 0.6876387596130371
In grad_steps = 111, loss = 0.6133671998977661
In grad_steps = 112, loss = 0.8106001615524292
In grad_steps = 113, loss = 0.5606532692909241
In grad_steps = 114, loss = 0.5256127119064331
In grad_steps = 115, loss = 0.9684803485870361
In grad_steps = 116, loss = 0.9890458583831787
In grad_steps = 117, loss = 0.442015677690506
In grad_steps = 118, loss = 0.967445969581604
In grad_steps = 119, loss = 1.0351676940917969
In grad_steps = 120, loss = 0.8978373408317566
In grad_steps = 121, loss = 0.8265529274940491
In grad_steps = 122, loss = 0.6020175218582153
In grad_steps = 123, loss = 0.7004324197769165
In grad_steps = 124, loss = 0.7110559344291687
In grad_steps = 125, loss = 0.7619001269340515
In grad_steps = 126, loss = 0.7423560619354248
In grad_steps = 127, loss = 0.6845622062683105
In grad_steps = 128, loss = 0.716334342956543
In grad_steps = 129, loss = 0.6882592439651489
In grad_steps = 130, loss = 0.6379914879798889
In grad_steps = 131, loss = 0.7364988327026367
In grad_steps = 132, loss = 0.7243348360061646
In grad_steps = 133, loss = 0.6133418083190918
In grad_steps = 134, loss = 0.792768120765686
In grad_steps = 135, loss = 0.5555683970451355
In grad_steps = 136, loss = 0.8371253609657288
In grad_steps = 137, loss = 0.7862382531166077
In grad_steps = 138, loss = 0.7809001207351685
In grad_steps = 139, loss = 0.6271687150001526
In grad_steps = 140, loss = 0.6032131910324097
In grad_steps = 141, loss = 0.7316989302635193
In grad_steps = 142, loss = 0.699277937412262
In grad_steps = 143, loss = 0.6597946882247925
In grad_steps = 144, loss = 0.6705460548400879
In grad_steps = 145, loss = 0.7287505269050598
In grad_steps = 146, loss = 0.5929048657417297
In grad_steps = 147, loss = 0.7312990427017212
In grad_steps = 148, loss = 0.8457964658737183
In grad_steps = 149, loss = 0.5771780014038086
In grad_steps = 150, loss = 0.5374321341514587
In grad_steps = 151, loss = 0.8526171445846558
In grad_steps = 152, loss = 0.5222264528274536
In grad_steps = 153, loss = 0.8837628960609436
In grad_steps = 154, loss = 0.8795409202575684
In grad_steps = 155, loss = 0.9717481136322021
In grad_steps = 156, loss = 0.7652561664581299
In grad_steps = 157, loss = 0.8112751841545105
In grad_steps = 158, loss = 0.6127570271492004
In grad_steps = 159, loss = 0.776561975479126
In grad_steps = 160, loss = 0.6029919981956482
In grad_steps = 161, loss = 0.8826038241386414
In grad_steps = 162, loss = 0.6275330781936646
In grad_steps = 163, loss = 0.6043621897697449
In grad_steps = 164, loss = 0.863907516002655
In grad_steps = 165, loss = 0.8364711403846741
In grad_steps = 166, loss = 0.691888153553009
In grad_steps = 167, loss = 0.7858073711395264
In grad_steps = 168, loss = 0.7994298934936523
In grad_steps = 169, loss = 0.6019243001937866
In grad_steps = 170, loss = 0.628969132900238
In grad_steps = 171, loss = 0.7145048975944519
In grad_steps = 172, loss = 0.6688218116760254
In grad_steps = 173, loss = 0.760259747505188
In grad_steps = 174, loss = 0.7380876541137695
In grad_steps = 175, loss = 0.627922534942627
In grad_steps = 176, loss = 0.7873530387878418
In grad_steps = 177, loss = 0.6557601690292358
In grad_steps = 178, loss = 0.6352523565292358
In grad_steps = 179, loss = 0.7250890135765076
In grad_steps = 180, loss = 0.6282891035079956
In grad_steps = 181, loss = 0.8179317116737366
In grad_steps = 182, loss = 0.7570586204528809
In grad_steps = 183, loss = 0.6321319341659546
In grad_steps = 184, loss = 0.7487958669662476
In grad_steps = 185, loss = 0.6520208120346069
In grad_steps = 186, loss = 0.7280874252319336
In grad_steps = 187, loss = 0.7983081936836243
In grad_steps = 188, loss = 0.6928392052650452
In grad_steps = 189, loss = 0.7413704991340637
In grad_steps = 190, loss = 0.7038945555686951
In grad_steps = 191, loss = 0.7321869730949402
In grad_steps = 192, loss = 0.6617767214775085
In grad_steps = 193, loss = 0.6773584485054016
In grad_steps = 194, loss = 0.7592177391052246
In grad_steps = 195, loss = 0.6456581354141235
In grad_steps = 196, loss = 0.6454998254776001
In grad_steps = 197, loss = 0.7318483591079712
In grad_steps = 198, loss = 0.6495715379714966
In grad_steps = 199, loss = 0.6604523062705994
In grad_steps = 200, loss = 0.6297272443771362
In grad_steps = 201, loss = 0.7786697745323181
In grad_steps = 202, loss = 0.5734361410140991
In grad_steps = 203, loss = 0.820216715335846
In grad_steps = 204, loss = 0.8661712408065796
In grad_steps = 205, loss = 0.5436115264892578
In grad_steps = 206, loss = 0.5522497892379761
In grad_steps = 207, loss = 0.8081549406051636
In grad_steps = 208, loss = 0.8845419883728027
In grad_steps = 209, loss = 0.5531831979751587
In grad_steps = 210, loss = 0.8446576595306396
In grad_steps = 211, loss = 0.8250364065170288
In grad_steps = 212, loss = 0.5753488540649414
In grad_steps = 213, loss = 0.786700963973999
In grad_steps = 214, loss = 0.7419294118881226
In grad_steps = 215, loss = 0.717789351940155
In grad_steps = 216, loss = 0.7034478783607483
In grad_steps = 217, loss = 0.7364471554756165
In grad_steps = 218, loss = 0.6419426202774048
In grad_steps = 219, loss = 0.5957931280136108
In grad_steps = 220, loss = 0.8086832165718079
In grad_steps = 221, loss = 0.5810631513595581
In grad_steps = 222, loss = 0.5581227540969849
In grad_steps = 223, loss = 0.9187967777252197
In grad_steps = 224, loss = 0.5387946367263794
In grad_steps = 225, loss = 0.9485613107681274
In grad_steps = 226, loss = 1.036167860031128
In grad_steps = 227, loss = 0.9068517684936523
In grad_steps = 228, loss = 0.878689706325531
In grad_steps = 229, loss = 0.5816290378570557
In grad_steps = 230, loss = 0.8085277080535889
In grad_steps = 231, loss = 0.8128544092178345
In grad_steps = 232, loss = 0.629966139793396
In grad_steps = 233, loss = 0.7242822051048279
In grad_steps = 234, loss = 0.668807864189148
In grad_steps = 235, loss = 0.7350180745124817
In grad_steps = 236, loss = 0.6256362795829773
In grad_steps = 237, loss = 0.5858346223831177
In grad_steps = 238, loss = 0.8375698328018188
In grad_steps = 239, loss = 0.8442493677139282
In grad_steps = 240, loss = 0.5178091526031494
In grad_steps = 241, loss = 0.8362970948219299
In grad_steps = 242, loss = 0.9080082178115845
In grad_steps = 243, loss = 0.8150653839111328
In grad_steps = 244, loss = 0.7771358489990234
In grad_steps = 245, loss = 0.626441240310669
In grad_steps = 246, loss = 0.7043485641479492
In grad_steps = 247, loss = 0.677677571773529
In grad_steps = 248, loss = 0.7290886044502258
In grad_steps = 249, loss = 0.704905092716217
In grad_steps = 250, loss = 0.6674363613128662
In grad_steps = 251, loss = 0.7220553159713745
In grad_steps = 252, loss = 0.6780039072036743
In grad_steps = 253, loss = 0.6271811127662659
In grad_steps = 254, loss = 0.7402421832084656
In grad_steps = 255, loss = 0.673895537853241
In grad_steps = 256, loss = 0.6224893927574158
In grad_steps = 257, loss = 0.7522760629653931
In grad_steps = 258, loss = 0.5668960213661194
In grad_steps = 259, loss = 0.8190957903862
In grad_steps = 260, loss = 0.7759658098220825
In grad_steps = 261, loss = 0.7625700235366821
In grad_steps = 262, loss = 0.6057128310203552
In grad_steps = 263, loss = 0.5936098694801331
In grad_steps = 264, loss = 0.7253143191337585
In grad_steps = 265, loss = 0.6759212613105774
In grad_steps = 266, loss = 0.6709869503974915
In grad_steps = 267, loss = 0.6808780431747437
In grad_steps = 268, loss = 0.6952331066131592
In grad_steps = 269, loss = 0.5888729095458984
In grad_steps = 270, loss = 0.6675900816917419
In grad_steps = 271, loss = 0.7852407097816467
In grad_steps = 272, loss = 0.5624069571495056
In grad_steps = 273, loss = 0.5579959750175476
In grad_steps = 274, loss = 0.714277982711792
In grad_steps = 275, loss = 0.48046374320983887
In grad_steps = 276, loss = 0.8739538192749023
In grad_steps = 277, loss = 0.7888256907463074
In grad_steps = 278, loss = 0.9125834703445435
In grad_steps = 279, loss = 0.6086599230766296
In grad_steps = 280, loss = 0.6401796936988831
In grad_steps = 281, loss = 0.630993664264679
In grad_steps = 282, loss = 1.1703803539276123
In grad_steps = 283, loss = 0.19871965050697327
In grad_steps = 284, loss = 1.0037137269973755
In grad_steps = 285, loss = 0.4722628593444824
In grad_steps = 286, loss = 0.32006847858428955
In grad_steps = 287, loss = 0.9537456035614014
In grad_steps = 288, loss = 0.7742118835449219
In grad_steps = 289, loss = 1.0154664516448975
In grad_steps = 290, loss = 0.6257241368293762
In grad_steps = 291, loss = 0.719758152961731
In grad_steps = 292, loss = 0.6948155760765076
In grad_steps = 293, loss = 0.6381646990776062
In grad_steps = 294, loss = 0.5187129378318787
In grad_steps = 295, loss = 0.8306832313537598
In grad_steps = 296, loss = 0.5947352051734924
In grad_steps = 297, loss = 0.8954912424087524
In grad_steps = 298, loss = 0.6705385446548462
In grad_steps = 299, loss = 0.6659119725227356
In grad_steps = 300, loss = 0.6437737941741943
In grad_steps = 301, loss = 0.6083943247795105
In grad_steps = 302, loss = 0.693461000919342
In grad_steps = 303, loss = 0.6630300879478455
In grad_steps = 304, loss = 0.825007975101471
In grad_steps = 305, loss = 0.6416802406311035
In grad_steps = 306, loss = 0.5929680466651917
In grad_steps = 307, loss = 0.6662695407867432
In grad_steps = 308, loss = 0.6271109580993652
In grad_steps = 309, loss = 0.7272931337356567
In grad_steps = 310, loss = 0.8958292007446289
In grad_steps = 311, loss = 0.6661661267280579
In grad_steps = 312, loss = 0.806086003780365
In grad_steps = 313, loss = 0.689710795879364
In grad_steps = 314, loss = 0.8538674116134644
In grad_steps = 315, loss = 0.4085269570350647
In grad_steps = 316, loss = 0.7447308897972107
In grad_steps = 317, loss = 0.815473735332489
In grad_steps = 318, loss = 0.5787892937660217
In grad_steps = 319, loss = 0.5929304957389832
In grad_steps = 320, loss = 0.8241643905639648
In grad_steps = 321, loss = 0.5608177781105042
In grad_steps = 322, loss = 0.5669413208961487
In grad_steps = 323, loss = 0.5677945017814636
In grad_steps = 324, loss = 0.7710050344467163
In grad_steps = 325, loss = 0.49791908264160156
In grad_steps = 326, loss = 0.8836772441864014
In grad_steps = 327, loss = 0.9418291449546814
In grad_steps = 328, loss = 0.5259407162666321
In grad_steps = 329, loss = 0.5993273258209229
In grad_steps = 330, loss = 0.7797474265098572
In grad_steps = 331, loss = 1.0476038455963135
In grad_steps = 332, loss = 0.5583308935165405
In grad_steps = 333, loss = 0.7627547383308411
In grad_steps = 334, loss = 0.7972308993339539
In grad_steps = 335, loss = 0.6008179783821106
In grad_steps = 336, loss = 0.7323864698410034
In grad_steps = 337, loss = 0.7227867841720581
In grad_steps = 338, loss = 0.7837904691696167
In grad_steps = 339, loss = 0.7080683708190918
In grad_steps = 340, loss = 0.8818084001541138
In grad_steps = 341, loss = 0.585236132144928
In grad_steps = 342, loss = 0.5291183590888977
In grad_steps = 343, loss = 0.8656916618347168
In grad_steps = 344, loss = 0.5487974286079407
In grad_steps = 345, loss = 0.5053202509880066
In grad_steps = 346, loss = 0.9841522574424744
In grad_steps = 347, loss = 0.5507588386535645
In grad_steps = 348, loss = 0.9715186953544617
In grad_steps = 349, loss = 1.1011006832122803
In grad_steps = 350, loss = 0.9505404233932495
In grad_steps = 351, loss = 0.9012613296508789
In grad_steps = 352, loss = 0.5464164614677429
In grad_steps = 353, loss = 0.7894560098648071
In grad_steps = 354, loss = 0.7801333069801331
In grad_steps = 355, loss = 0.6727272868156433
In grad_steps = 356, loss = 0.6671340465545654
In grad_steps = 357, loss = 0.6237949132919312
In grad_steps = 358, loss = 0.809511125087738
In grad_steps = 359, loss = 0.5251182317733765
In grad_steps = 360, loss = 0.4838207960128784
In grad_steps = 361, loss = 1.010637640953064
In grad_steps = 362, loss = 1.023563265800476
In grad_steps = 363, loss = 0.42033353447914124
In grad_steps = 364, loss = 1.018079400062561
In grad_steps = 365, loss = 0.9724874496459961
In grad_steps = 366, loss = 0.86609947681427
In grad_steps = 367, loss = 0.7550501227378845
In grad_steps = 368, loss = 0.6693711876869202
In grad_steps = 369, loss = 0.6664137840270996
In grad_steps = 370, loss = 0.7633721828460693
In grad_steps = 371, loss = 0.8454432487487793
In grad_steps = 372, loss = 0.8466007709503174
In grad_steps = 373, loss = 0.7401230931282043
In grad_steps = 374, loss = 0.6337841153144836
In grad_steps = 375, loss = 0.7544760704040527
In grad_steps = 376, loss = 0.6615450382232666
In grad_steps = 377, loss = 0.7040087580680847
In grad_steps = 378, loss = 0.6909099817276001
In grad_steps = 379, loss = 0.6358346343040466
In grad_steps = 380, loss = 0.7224335074424744
In grad_steps = 381, loss = 0.5872481465339661
In grad_steps = 382, loss = 0.8007045984268188
In grad_steps = 383, loss = 0.7461450695991516
In grad_steps = 384, loss = 0.7965290546417236
In grad_steps = 385, loss = 0.5855610966682434
In grad_steps = 386, loss = 0.5641767382621765
In grad_steps = 387, loss = 0.7225669026374817
In grad_steps = 388, loss = 0.68833327293396
In grad_steps = 389, loss = 0.7231254577636719
In grad_steps = 390, loss = 0.6522964239120483
In grad_steps = 391, loss = 0.6886023283004761
In grad_steps = 392, loss = 0.5916492342948914
In grad_steps = 393, loss = 0.7606020569801331
In grad_steps = 394, loss = 0.7718541026115417
In grad_steps = 395, loss = 0.5338903665542603
In grad_steps = 396, loss = 0.5831378102302551
In grad_steps = 397, loss = 0.6823711395263672
In grad_steps = 398, loss = 0.5590432286262512
In grad_steps = 399, loss = 0.8234530687332153
In grad_steps = 400, loss = 0.8360015749931335
In grad_steps = 401, loss = 0.4649675786495209
In grad_steps = 402, loss = 0.5597072243690491
In grad_steps = 403, loss = 0.43679279088974
In grad_steps = 404, loss = 0.31511202454566956
In grad_steps = 405, loss = 0.5092113018035889
In grad_steps = 406, loss = 0.041655998677015305
In grad_steps = 407, loss = 0.17753812670707703
In grad_steps = 408, loss = 0.18833042681217194
In grad_steps = 409, loss = 0.005808616988360882
In grad_steps = 410, loss = 0.04814334213733673
In grad_steps = 411, loss = 0.008523277007043362
In grad_steps = 412, loss = 0.0007962394156493247
In grad_steps = 413, loss = 0.011001642793416977
In grad_steps = 414, loss = 7.387948989868164
In grad_steps = 415, loss = 0.19576279819011688
In grad_steps = 416, loss = 0.016727427020668983
In grad_steps = 417, loss = 0.007345573045313358
In grad_steps = 418, loss = 2.8718109130859375
In grad_steps = 419, loss = 0.10031161457300186
In grad_steps = 420, loss = 0.7041736245155334
In grad_steps = 421, loss = 0.48953184485435486
In grad_steps = 422, loss = 0.8818855285644531
In grad_steps = 423, loss = 0.33089908957481384
In grad_steps = 424, loss = 0.18889974057674408
In grad_steps = 425, loss = 1.0805209875106812
In grad_steps = 426, loss = 0.31314215064048767
In grad_steps = 427, loss = 1.308660626411438
In grad_steps = 428, loss = 0.5359645485877991
In grad_steps = 429, loss = 0.4712320864200592
In grad_steps = 430, loss = 0.677476704120636
In grad_steps = 431, loss = 0.4946128726005554
In grad_steps = 432, loss = 0.5929557085037231
In grad_steps = 433, loss = 0.9559537172317505
In grad_steps = 434, loss = 0.8820861577987671
In grad_steps = 435, loss = 1.2894129753112793
In grad_steps = 436, loss = 1.019652247428894
In grad_steps = 437, loss = 0.6615081429481506
In grad_steps = 438, loss = 0.3818504512310028
In grad_steps = 439, loss = 0.5218266248703003
In grad_steps = 440, loss = 0.6283820867538452
In grad_steps = 441, loss = 0.6682150363922119
In grad_steps = 442, loss = 0.6307166814804077
In grad_steps = 443, loss = 0.7995086312294006
In grad_steps = 444, loss = 0.5727522373199463
In grad_steps = 445, loss = 0.49832427501678467
In grad_steps = 446, loss = 0.2742014229297638
In grad_steps = 447, loss = 0.9716188907623291
In grad_steps = 448, loss = 0.06599874794483185
In grad_steps = 449, loss = 1.320317268371582
In grad_steps = 450, loss = 0.5885520577430725
In grad_steps = 451, loss = 0.41078847646713257
In grad_steps = 452, loss = 0.6084095239639282
In grad_steps = 453, loss = 0.4567348062992096
In grad_steps = 454, loss = 0.5503821969032288
In grad_steps = 455, loss = 0.4358943998813629
In grad_steps = 456, loss = 0.40637052059173584
In grad_steps = 457, loss = 0.4219130873680115
In grad_steps = 458, loss = 0.7329511046409607
In grad_steps = 459, loss = 0.41656461358070374
In grad_steps = 460, loss = 2.169933319091797
In grad_steps = 461, loss = 1.3787212371826172
In grad_steps = 462, loss = 0.502423882484436
In grad_steps = 463, loss = 1.2294567823410034
In grad_steps = 464, loss = 0.3719111382961273
In grad_steps = 465, loss = 0.3386246860027313
In grad_steps = 466, loss = 1.0438729524612427
In grad_steps = 467, loss = 0.47336483001708984
In grad_steps = 468, loss = 0.4127694070339203
In grad_steps = 469, loss = 1.064036250114441
In grad_steps = 470, loss = 0.4791674017906189
In grad_steps = 471, loss = 1.0164263248443604
In grad_steps = 472, loss = 1.116747498512268
In grad_steps = 473, loss = 0.8424106240272522
In grad_steps = 474, loss = 0.7430539131164551
In grad_steps = 475, loss = 0.7935320138931274
In grad_steps = 476, loss = 0.518032431602478
In grad_steps = 477, loss = 0.5340497493743896
In grad_steps = 478, loss = 1.027151107788086
In grad_steps = 479, loss = 0.40999558568000793
In grad_steps = 480, loss = 0.37881141901016235
In grad_steps = 481, loss = 1.2298076152801514
In grad_steps = 482, loss = 0.3384250998497009
In grad_steps = 483, loss = 0.37870463728904724
In grad_steps = 484, loss = 1.272780418395996
In grad_steps = 485, loss = 1.2131543159484863
In grad_steps = 486, loss = 0.47769612073898315
In grad_steps = 487, loss = 0.8379924893379211
In grad_steps = 488, loss = 0.6403636932373047
In grad_steps = 489, loss = 0.38949576020240784
In grad_steps = 490, loss = 0.22799845039844513
In grad_steps = 491, loss = 2.1036040782928467
i = 3, Test ensemble probabilities = 
[array([[0.6284508 , 0.37154916],
       [0.6362697 , 0.36373034],
       [0.6395782 , 0.36042175],
       [0.6325759 , 0.3674241 ],
       [0.6236578 , 0.37634215],
       [0.64280176, 0.35719827],
       [0.6226229 , 0.37737706],
       [0.63043296, 0.36956698],
       [0.61667144, 0.38332856],
       [0.6516359 , 0.34836406],
       [0.629431  , 0.37056902],
       [0.6349214 , 0.36507857],
       [0.6210475 , 0.37895253],
       [0.64654285, 0.35345718],
       [0.6052796 , 0.39472032],
       [0.6282811 , 0.37171885]], dtype=float32), array([[0.53180707, 0.46819288],
       [0.43626124, 0.5637387 ],
       [0.32801697, 0.67198306],
       [0.4875767 , 0.51242334],
       [0.48902902, 0.51097095],
       [0.4543582 , 0.54564184],
       [0.45123112, 0.54876894],
       [0.5060453 , 0.49395472],
       [0.50209475, 0.49790528],
       [0.4963399 , 0.50366014],
       [0.46167827, 0.5383218 ],
       [0.5426025 , 0.45739752],
       [0.49294376, 0.50705624],
       [0.5642455 , 0.4357545 ],
       [0.51248384, 0.48751613],
       [0.47801602, 0.521984  ]], dtype=float32), array([[0.48643658, 0.5135634 ],
       [0.48687044, 0.51312953],
       [0.4646091 , 0.5353909 ],
       [0.48622504, 0.513775  ],
       [0.47988525, 0.5201148 ],
       [0.47998756, 0.5200124 ],
       [0.48647708, 0.51352286],
       [0.48873806, 0.51126194],
       [0.4840995 , 0.5159005 ],
       [0.48697886, 0.5130212 ],
       [0.48528168, 0.51471835],
       [0.48911697, 0.5108831 ],
       [0.47824326, 0.5217567 ],
       [0.49554408, 0.5044559 ],
       [0.48103884, 0.51896113],
       [0.48362648, 0.51637346]], dtype=float32), array([[0.87576663, 0.12423334],
       [0.8847664 , 0.11523368],
       [0.88147444, 0.1185255 ],
       [0.88156   , 0.11843994],
       [0.86536527, 0.13463475],
       [0.8823913 , 0.11760876],
       [0.8840158 , 0.11598418],
       [0.8863739 , 0.11362606],
       [0.8862254 , 0.11377464],
       [0.87569225, 0.12430774],
       [0.8827979 , 0.11720211],
       [0.890293  , 0.10970698],
       [0.8857197 , 0.11428026],
       [0.89008754, 0.10991244],
       [0.8815627 , 0.11843727],
       [0.8878101 , 0.11218985]], dtype=float32)]
i = 3, Test true class= 
[1 0 0 0 1 0 1 0 0 0 1 1 0 1 1 0]
In grad_steps = 0, loss = 1.16717529296875
In grad_steps = 1, loss = 1.1310341358184814
In grad_steps = 2, loss = 1.2213413715362549
In grad_steps = 3, loss = 0.708564043045044
In grad_steps = 4, loss = 0.398021399974823
In grad_steps = 5, loss = 1.660946011543274
In grad_steps = 6, loss = 0.3360883891582489
In grad_steps = 7, loss = 0.26051193475723267
In grad_steps = 8, loss = 1.3425637483596802
In grad_steps = 9, loss = 1.3205416202545166
In grad_steps = 10, loss = 0.45714128017425537
In grad_steps = 11, loss = 1.0546525716781616
In grad_steps = 12, loss = 0.5422067642211914
In grad_steps = 13, loss = 0.7529176473617554
In grad_steps = 14, loss = 0.5050641894340515
In grad_steps = 15, loss = 0.38753512501716614
In grad_steps = 16, loss = 1.753478765487671
In grad_steps = 17, loss = 1.6438632011413574
In grad_steps = 18, loss = 0.2988535463809967
In grad_steps = 19, loss = 0.31474539637565613
In grad_steps = 20, loss = 0.3091813027858734
In grad_steps = 21, loss = 0.3930305540561676
In grad_steps = 22, loss = 1.1443074941635132
In grad_steps = 23, loss = 0.3907014727592468
In grad_steps = 24, loss = 0.933587908744812
In grad_steps = 25, loss = 1.0512511730194092
In grad_steps = 26, loss = 0.5696343779563904
In grad_steps = 27, loss = 0.5686874389648438
In grad_steps = 28, loss = 0.7761366963386536
In grad_steps = 29, loss = 0.7006144523620605
In grad_steps = 30, loss = 0.6675705313682556
In grad_steps = 31, loss = 0.6404259204864502
In grad_steps = 32, loss = 0.7551800608634949
In grad_steps = 33, loss = 0.46397221088409424
In grad_steps = 34, loss = 0.4688872992992401
In grad_steps = 35, loss = 1.201271891593933
In grad_steps = 36, loss = 1.492160677909851
In grad_steps = 37, loss = 0.2890463173389435
In grad_steps = 38, loss = 1.8561922311782837
In grad_steps = 39, loss = 0.341290682554245
In grad_steps = 40, loss = 0.379801869392395
In grad_steps = 41, loss = 1.2582345008850098
In grad_steps = 42, loss = 1.1534689664840698
In grad_steps = 43, loss = 0.5320338010787964
In grad_steps = 44, loss = 0.9499993324279785
In grad_steps = 45, loss = 0.8844597339630127
In grad_steps = 46, loss = 0.602219820022583
In grad_steps = 47, loss = 0.6848099827766418
In grad_steps = 48, loss = 0.7027134895324707
In grad_steps = 49, loss = 0.7235055565834045
In grad_steps = 50, loss = 0.6999076008796692
In grad_steps = 51, loss = 0.8466671705245972
In grad_steps = 52, loss = 0.7044479250907898
In grad_steps = 53, loss = 0.716857373714447
In grad_steps = 54, loss = 0.6880262494087219
In grad_steps = 55, loss = 0.6937186121940613
In grad_steps = 56, loss = 0.7350588440895081
In grad_steps = 57, loss = 0.6210904121398926
In grad_steps = 58, loss = 0.86012202501297
In grad_steps = 59, loss = 0.813560962677002
In grad_steps = 60, loss = 0.5906349420547485
In grad_steps = 61, loss = 0.820805013179779
In grad_steps = 62, loss = 0.6129960417747498
In grad_steps = 63, loss = 0.7734412550926208
In grad_steps = 64, loss = 0.8131839632987976
In grad_steps = 65, loss = 0.6865226626396179
In grad_steps = 66, loss = 0.7599251866340637
In grad_steps = 67, loss = 0.7393881678581238
In grad_steps = 68, loss = 0.7201712727546692
In grad_steps = 69, loss = 0.6835665702819824
In grad_steps = 70, loss = 0.6801238656044006
In grad_steps = 71, loss = 0.8025882244110107
In grad_steps = 72, loss = 0.6349677443504333
In grad_steps = 73, loss = 0.6303161382675171
In grad_steps = 74, loss = 0.7512661218643188
In grad_steps = 75, loss = 0.6720461249351501
In grad_steps = 76, loss = 0.6499100923538208
In grad_steps = 77, loss = 0.647174060344696
In grad_steps = 78, loss = 0.8197833895683289
In grad_steps = 79, loss = 0.5446652770042419
In grad_steps = 80, loss = 0.899545431137085
In grad_steps = 81, loss = 0.9530118107795715
In grad_steps = 82, loss = 0.5115512013435364
In grad_steps = 83, loss = 0.49121707677841187
In grad_steps = 84, loss = 0.8620164394378662
In grad_steps = 85, loss = 0.9290410876274109
In grad_steps = 86, loss = 0.5382906198501587
In grad_steps = 87, loss = 0.8508379459381104
In grad_steps = 88, loss = 0.8340303897857666
In grad_steps = 89, loss = 0.5928093194961548
In grad_steps = 90, loss = 0.7594668865203857
In grad_steps = 91, loss = 0.6999210119247437
In grad_steps = 92, loss = 0.7916993498802185
In grad_steps = 93, loss = 0.6232861280441284
In grad_steps = 94, loss = 0.8076671361923218
In grad_steps = 95, loss = 0.5507224798202515
In grad_steps = 96, loss = 0.5009664297103882
In grad_steps = 97, loss = 0.9357666969299316
In grad_steps = 98, loss = 0.48442327976226807
In grad_steps = 99, loss = 0.4756380319595337
In grad_steps = 100, loss = 1.100597858428955
In grad_steps = 101, loss = 0.467171311378479
In grad_steps = 102, loss = 1.1236436367034912
In grad_steps = 103, loss = 1.2515865564346313
In grad_steps = 104, loss = 1.0015993118286133
In grad_steps = 105, loss = 0.9541759490966797
In grad_steps = 106, loss = 0.5637521743774414
In grad_steps = 107, loss = 0.8184160590171814
In grad_steps = 108, loss = 0.8066719770431519
In grad_steps = 109, loss = 0.6616317629814148
In grad_steps = 110, loss = 0.6877297759056091
In grad_steps = 111, loss = 0.6177974343299866
In grad_steps = 112, loss = 0.8169608116149902
In grad_steps = 113, loss = 0.5607801079750061
In grad_steps = 114, loss = 0.5281116366386414
In grad_steps = 115, loss = 0.9571828842163086
In grad_steps = 116, loss = 0.977403998374939
In grad_steps = 117, loss = 0.4533102512359619
In grad_steps = 118, loss = 0.9608287811279297
In grad_steps = 119, loss = 1.0316252708435059
In grad_steps = 120, loss = 0.8942928314208984
In grad_steps = 121, loss = 0.805808424949646
In grad_steps = 122, loss = 0.599289059638977
In grad_steps = 123, loss = 0.7027602195739746
In grad_steps = 124, loss = 0.6994626522064209
In grad_steps = 125, loss = 0.7632597088813782
In grad_steps = 126, loss = 0.7398163676261902
In grad_steps = 127, loss = 0.6766262650489807
In grad_steps = 128, loss = 0.7219557166099548
In grad_steps = 129, loss = 0.6801577806472778
In grad_steps = 130, loss = 0.6238656640052795
In grad_steps = 131, loss = 0.747612714767456
In grad_steps = 132, loss = 0.7168759107589722
In grad_steps = 133, loss = 0.6174344420433044
In grad_steps = 134, loss = 0.7958081364631653
In grad_steps = 135, loss = 0.5538963079452515
In grad_steps = 136, loss = 0.8455591797828674
In grad_steps = 137, loss = 0.7859391570091248
In grad_steps = 138, loss = 0.777265727519989
In grad_steps = 139, loss = 0.6292363405227661
In grad_steps = 140, loss = 0.5975376963615417
In grad_steps = 141, loss = 0.7103966474533081
In grad_steps = 142, loss = 0.6930193901062012
In grad_steps = 143, loss = 0.6442587375640869
In grad_steps = 144, loss = 0.6586300134658813
In grad_steps = 145, loss = 0.7067211270332336
In grad_steps = 146, loss = 0.5790286064147949
In grad_steps = 147, loss = 0.7274928092956543
In grad_steps = 148, loss = 0.8528173565864563
In grad_steps = 149, loss = 0.5524084568023682
In grad_steps = 150, loss = 0.5161134004592896
In grad_steps = 151, loss = 0.858345627784729
In grad_steps = 152, loss = 0.4783419668674469
In grad_steps = 153, loss = 0.8802406787872314
In grad_steps = 154, loss = 0.9104457497596741
In grad_steps = 155, loss = 1.0316689014434814
In grad_steps = 156, loss = 0.6911131143569946
In grad_steps = 157, loss = 0.803376317024231
In grad_steps = 158, loss = 0.6556817889213562
In grad_steps = 159, loss = 0.8883678317070007
In grad_steps = 160, loss = 0.4671862721443176
In grad_steps = 161, loss = 1.044154167175293
In grad_steps = 162, loss = 0.5673285722732544
In grad_steps = 163, loss = 0.5358818769454956
In grad_steps = 164, loss = 0.9484794735908508
In grad_steps = 165, loss = 0.9186185598373413
In grad_steps = 166, loss = 0.7127135992050171
In grad_steps = 167, loss = 0.7822055220603943
In grad_steps = 168, loss = 0.7810189723968506
In grad_steps = 169, loss = 0.6386956572532654
In grad_steps = 170, loss = 0.6804684400558472
In grad_steps = 171, loss = 0.654002845287323
In grad_steps = 172, loss = 0.7281209826469421
In grad_steps = 173, loss = 0.6865070462226868
In grad_steps = 174, loss = 0.7926892042160034
In grad_steps = 175, loss = 0.6870736479759216
In grad_steps = 176, loss = 0.7139343619346619
In grad_steps = 177, loss = 0.7053466439247131
In grad_steps = 178, loss = 0.7019897103309631
In grad_steps = 179, loss = 0.6919110417366028
In grad_steps = 180, loss = 0.6697338223457336
In grad_steps = 181, loss = 0.7730082273483276
In grad_steps = 182, loss = 0.7329971790313721
In grad_steps = 183, loss = 0.6541356444358826
In grad_steps = 184, loss = 0.7215017676353455
In grad_steps = 185, loss = 0.6594483256340027
In grad_steps = 186, loss = 0.7313094139099121
In grad_steps = 187, loss = 0.7767438292503357
In grad_steps = 188, loss = 0.6995187997817993
In grad_steps = 189, loss = 0.742041826248169
In grad_steps = 190, loss = 0.7083069682121277
In grad_steps = 191, loss = 0.7297547459602356
In grad_steps = 192, loss = 0.644866406917572
In grad_steps = 193, loss = 0.7072117328643799
In grad_steps = 194, loss = 0.7684834599494934
In grad_steps = 195, loss = 0.6326826810836792
In grad_steps = 196, loss = 0.6371893882751465
In grad_steps = 197, loss = 0.7475675940513611
In grad_steps = 198, loss = 0.6468928456306458
In grad_steps = 199, loss = 0.6486161947250366
In grad_steps = 200, loss = 0.614189624786377
In grad_steps = 201, loss = 0.7972015738487244
In grad_steps = 202, loss = 0.5551717281341553
In grad_steps = 203, loss = 0.8459315299987793
In grad_steps = 204, loss = 0.8958969116210938
In grad_steps = 205, loss = 0.5297953486442566
In grad_steps = 206, loss = 0.5374737977981567
In grad_steps = 207, loss = 0.8382689952850342
In grad_steps = 208, loss = 0.893547773361206
In grad_steps = 209, loss = 0.5438555479049683
In grad_steps = 210, loss = 0.860836923122406
In grad_steps = 211, loss = 0.8411798477172852
In grad_steps = 212, loss = 0.5805971622467041
In grad_steps = 213, loss = 0.7791037559509277
In grad_steps = 214, loss = 0.7299094200134277
In grad_steps = 215, loss = 0.7201576232910156
In grad_steps = 216, loss = 0.6924183964729309
In grad_steps = 217, loss = 0.7463050484657288
In grad_steps = 218, loss = 0.6294407844543457
In grad_steps = 219, loss = 0.5909217000007629
In grad_steps = 220, loss = 0.8050467371940613
In grad_steps = 221, loss = 0.5684648752212524
In grad_steps = 222, loss = 0.5438663363456726
In grad_steps = 223, loss = 0.9375615119934082
In grad_steps = 224, loss = 0.5117085576057434
In grad_steps = 225, loss = 0.9708484411239624
In grad_steps = 226, loss = 1.0560775995254517
In grad_steps = 227, loss = 0.9196521639823914
In grad_steps = 228, loss = 0.899639368057251
In grad_steps = 229, loss = 0.5514041185379028
In grad_steps = 230, loss = 0.8147081732749939
In grad_steps = 231, loss = 0.8238930106163025
In grad_steps = 232, loss = 0.6226944327354431
In grad_steps = 233, loss = 0.7360585331916809
In grad_steps = 234, loss = 0.679961085319519
In grad_steps = 235, loss = 0.725523829460144
In grad_steps = 236, loss = 0.6277176737785339
In grad_steps = 237, loss = 0.6105896234512329
In grad_steps = 238, loss = 0.8328040242195129
In grad_steps = 239, loss = 0.8332240581512451
In grad_steps = 240, loss = 0.5304923057556152
In grad_steps = 241, loss = 0.8399073481559753
In grad_steps = 242, loss = 0.9008382558822632
In grad_steps = 243, loss = 0.8164690136909485
In grad_steps = 244, loss = 0.782253623008728
In grad_steps = 245, loss = 0.6207009553909302
In grad_steps = 246, loss = 0.7235855460166931
In grad_steps = 247, loss = 0.6572727560997009
In grad_steps = 248, loss = 0.7106522917747498
In grad_steps = 249, loss = 0.6713259220123291
In grad_steps = 250, loss = 0.6511707305908203
In grad_steps = 251, loss = 0.7409871816635132
In grad_steps = 252, loss = 0.6672673225402832
In grad_steps = 253, loss = 0.630486011505127
In grad_steps = 254, loss = 0.7510248422622681
In grad_steps = 255, loss = 0.6748067140579224
In grad_steps = 256, loss = 0.6402081251144409
In grad_steps = 257, loss = 0.759868323802948
In grad_steps = 258, loss = 0.5673194527626038
In grad_steps = 259, loss = 0.8536083698272705
In grad_steps = 260, loss = 0.7780332565307617
In grad_steps = 261, loss = 0.7576436400413513
In grad_steps = 262, loss = 0.6057947278022766
In grad_steps = 263, loss = 0.591131865978241
In grad_steps = 264, loss = 0.72061687707901
In grad_steps = 265, loss = 0.6912940144538879
In grad_steps = 266, loss = 0.6711100339889526
In grad_steps = 267, loss = 0.6763955354690552
In grad_steps = 268, loss = 0.6995701193809509
In grad_steps = 269, loss = 0.6080509424209595
In grad_steps = 270, loss = 0.665215790271759
In grad_steps = 271, loss = 0.7508677244186401
In grad_steps = 272, loss = 0.5802658796310425
In grad_steps = 273, loss = 0.5639407634735107
In grad_steps = 274, loss = 0.6578627824783325
In grad_steps = 275, loss = 0.5163145065307617
In grad_steps = 276, loss = 0.8374844193458557
In grad_steps = 277, loss = 0.7295947670936584
In grad_steps = 278, loss = 0.8302453756332397
In grad_steps = 279, loss = 0.638851523399353
In grad_steps = 280, loss = 0.6618016362190247
In grad_steps = 281, loss = 0.4585159420967102
In grad_steps = 282, loss = 1.004575490951538
In grad_steps = 283, loss = 0.14968515932559967
In grad_steps = 284, loss = 0.828239381313324
In grad_steps = 285, loss = 0.41601255536079407
In grad_steps = 286, loss = 0.14081144332885742
In grad_steps = 287, loss = 0.9434218406677246
In grad_steps = 288, loss = 0.48771417140960693
In grad_steps = 289, loss = 1.7086515426635742
In grad_steps = 290, loss = 0.347770094871521
In grad_steps = 291, loss = 0.6146510243415833
In grad_steps = 292, loss = 0.8380893468856812
In grad_steps = 293, loss = 0.6447780132293701
In grad_steps = 294, loss = 0.3913659155368805
In grad_steps = 295, loss = 0.8600851893424988
In grad_steps = 296, loss = 1.1934860944747925
In grad_steps = 297, loss = 0.6620298027992249
In grad_steps = 298, loss = 0.5818725824356079
In grad_steps = 299, loss = 0.6517870426177979
In grad_steps = 300, loss = 0.4053991436958313
In grad_steps = 301, loss = 0.4586566686630249
In grad_steps = 302, loss = 0.7697622776031494
In grad_steps = 303, loss = 0.6684563159942627
In grad_steps = 304, loss = 0.933459997177124
In grad_steps = 305, loss = 0.8334013819694519
In grad_steps = 306, loss = 0.5015329122543335
In grad_steps = 307, loss = 0.6361227035522461
In grad_steps = 308, loss = 0.4901445209980011
In grad_steps = 309, loss = 0.6813331246376038
In grad_steps = 310, loss = 0.9812319874763489
In grad_steps = 311, loss = 0.8784070611000061
In grad_steps = 312, loss = 1.0039843320846558
In grad_steps = 313, loss = 0.9199866056442261
In grad_steps = 314, loss = 0.8287932872772217
In grad_steps = 315, loss = 0.5670218467712402
In grad_steps = 316, loss = 0.637141764163971
In grad_steps = 317, loss = 0.7495661973953247
In grad_steps = 318, loss = 0.641891598701477
In grad_steps = 319, loss = 0.6458255052566528
In grad_steps = 320, loss = 0.7203460931777954
In grad_steps = 321, loss = 0.615759551525116
In grad_steps = 322, loss = 0.5925223231315613
In grad_steps = 323, loss = 0.5911655426025391
In grad_steps = 324, loss = 0.7064644694328308
In grad_steps = 325, loss = 0.5068666934967041
In grad_steps = 326, loss = 0.8486289978027344
In grad_steps = 327, loss = 0.9063708782196045
In grad_steps = 328, loss = 0.5189986228942871
In grad_steps = 329, loss = 0.6033164262771606
In grad_steps = 330, loss = 0.8410799503326416
In grad_steps = 331, loss = 0.8467581868171692
In grad_steps = 332, loss = 0.44127240777015686
In grad_steps = 333, loss = 0.8435161113739014
In grad_steps = 334, loss = 0.8115825653076172
In grad_steps = 335, loss = 0.5852192640304565
In grad_steps = 336, loss = 0.7274829149246216
In grad_steps = 337, loss = 0.8848516941070557
In grad_steps = 338, loss = 0.704889714717865
In grad_steps = 339, loss = 0.8035121560096741
In grad_steps = 340, loss = 0.9404618740081787
In grad_steps = 341, loss = 0.6437457799911499
In grad_steps = 342, loss = 0.5453540682792664
In grad_steps = 343, loss = 0.8665869235992432
In grad_steps = 344, loss = 0.5705158114433289
In grad_steps = 345, loss = 0.5019152164459229
In grad_steps = 346, loss = 0.9724969267845154
In grad_steps = 347, loss = 0.53657066822052
In grad_steps = 348, loss = 0.9843093156814575
In grad_steps = 349, loss = 1.0449464321136475
In grad_steps = 350, loss = 0.9453210234642029
In grad_steps = 351, loss = 0.9019910097122192
In grad_steps = 352, loss = 0.5338872075080872
In grad_steps = 353, loss = 0.7754424214363098
In grad_steps = 354, loss = 0.8164870738983154
In grad_steps = 355, loss = 0.6446982622146606
In grad_steps = 356, loss = 0.7055614590644836
In grad_steps = 357, loss = 0.6580743193626404
In grad_steps = 358, loss = 0.8050066828727722
In grad_steps = 359, loss = 0.5497954487800598
In grad_steps = 360, loss = 0.5122048854827881
In grad_steps = 361, loss = 0.9603224992752075
In grad_steps = 362, loss = 0.9868557453155518
In grad_steps = 363, loss = 0.43973585963249207
In grad_steps = 364, loss = 1.0369623899459839
In grad_steps = 365, loss = 0.9795681834220886
In grad_steps = 366, loss = 0.8675529360771179
In grad_steps = 367, loss = 0.7648746967315674
In grad_steps = 368, loss = 0.6583911776542664
In grad_steps = 369, loss = 0.6904500126838684
In grad_steps = 370, loss = 0.7487822771072388
In grad_steps = 371, loss = 0.8245965242385864
In grad_steps = 372, loss = 0.8206101059913635
In grad_steps = 373, loss = 0.7329347133636475
In grad_steps = 374, loss = 0.6295605897903442
In grad_steps = 375, loss = 0.7540179491043091
In grad_steps = 376, loss = 0.6647145748138428
In grad_steps = 377, loss = 0.7023977637290955
In grad_steps = 378, loss = 0.6896680593490601
In grad_steps = 379, loss = 0.6551925539970398
In grad_steps = 380, loss = 0.721423327922821
In grad_steps = 381, loss = 0.6137922406196594
In grad_steps = 382, loss = 0.7849330902099609
In grad_steps = 383, loss = 0.7397395968437195
In grad_steps = 384, loss = 0.7996026277542114
In grad_steps = 385, loss = 0.617789089679718
In grad_steps = 386, loss = 0.5725622177124023
In grad_steps = 387, loss = 0.7364665865898132
In grad_steps = 388, loss = 0.6905964612960815
In grad_steps = 389, loss = 0.7156941294670105
In grad_steps = 390, loss = 0.6338884830474854
In grad_steps = 391, loss = 0.7123165726661682
In grad_steps = 392, loss = 0.6354371309280396
In grad_steps = 393, loss = 0.7548398971557617
In grad_steps = 394, loss = 0.7534949779510498
In grad_steps = 395, loss = 0.5869347453117371
In grad_steps = 396, loss = 0.5983081459999084
In grad_steps = 397, loss = 0.791934609413147
In grad_steps = 398, loss = 0.6152205467224121
In grad_steps = 399, loss = 0.8601077795028687
In grad_steps = 400, loss = 0.8276422023773193
In grad_steps = 401, loss = 0.8222385048866272
In grad_steps = 402, loss = 0.7608646750450134
In grad_steps = 403, loss = 0.6492704153060913
In grad_steps = 404, loss = 0.6010880470275879
In grad_steps = 405, loss = 0.6211910843849182
In grad_steps = 406, loss = 0.41928863525390625
In grad_steps = 407, loss = 0.7409048080444336
In grad_steps = 408, loss = 0.7086037397384644
In grad_steps = 409, loss = 0.5639457106590271
In grad_steps = 410, loss = 0.6135433912277222
In grad_steps = 411, loss = 0.5415548086166382
In grad_steps = 412, loss = 0.4270421266555786
In grad_steps = 413, loss = 0.5478849411010742
In grad_steps = 414, loss = 0.6372887492179871
In grad_steps = 415, loss = 0.05829786881804466
In grad_steps = 416, loss = 0.07629339396953583
In grad_steps = 417, loss = 0.34298354387283325
In grad_steps = 418, loss = 0.30511751770973206
In grad_steps = 419, loss = 0.5400769114494324
In grad_steps = 420, loss = 0.00993367936462164
In grad_steps = 421, loss = 1.211979866027832
In grad_steps = 422, loss = 0.06457467377185822
In grad_steps = 423, loss = 0.005933764856308699
In grad_steps = 424, loss = 0.0017296605510637164
In grad_steps = 425, loss = 0.40654948353767395
In grad_steps = 426, loss = 0.046429943293333054
In grad_steps = 427, loss = 0.10457887500524521
In grad_steps = 428, loss = 0.01396291796118021
In grad_steps = 429, loss = 0.00817914493381977
In grad_steps = 430, loss = 0.002561147790402174
In grad_steps = 431, loss = 0.001212576637044549
In grad_steps = 432, loss = 0.0029948167502880096
In grad_steps = 433, loss = 5.722663402557373
In grad_steps = 434, loss = 3.4454150199890137
In grad_steps = 435, loss = 3.2341699600219727
In grad_steps = 436, loss = 0.29096341133117676
In grad_steps = 437, loss = 0.5854398608207703
In grad_steps = 438, loss = 0.10941651463508606
In grad_steps = 439, loss = 0.7130290865898132
In grad_steps = 440, loss = 0.7606310248374939
In grad_steps = 441, loss = 0.37920042872428894
In grad_steps = 442, loss = 0.40904608368873596
In grad_steps = 443, loss = 0.7249158620834351
In grad_steps = 444, loss = 0.5250146389007568
In grad_steps = 445, loss = 0.36187416315078735
In grad_steps = 446, loss = 0.41263750195503235
In grad_steps = 447, loss = 0.8033825755119324
In grad_steps = 448, loss = 0.10815751552581787
In grad_steps = 449, loss = 1.2324132919311523
In grad_steps = 450, loss = 0.6797460317611694
In grad_steps = 451, loss = 1.412934422492981
In grad_steps = 452, loss = 1.0025746822357178
In grad_steps = 453, loss = 0.3869163691997528
In grad_steps = 454, loss = 0.6973674893379211
In grad_steps = 455, loss = 0.7322067022323608
In grad_steps = 456, loss = 0.54442298412323
In grad_steps = 457, loss = 0.52774977684021
In grad_steps = 458, loss = 0.8254130482673645
In grad_steps = 459, loss = 0.47397473454475403
In grad_steps = 460, loss = 0.4794062674045563
In grad_steps = 461, loss = 1.102561593055725
In grad_steps = 462, loss = 0.478658527135849
In grad_steps = 463, loss = 1.1475260257720947
In grad_steps = 464, loss = 0.42225295305252075
In grad_steps = 465, loss = 0.4001946747303009
In grad_steps = 466, loss = 1.1171170473098755
In grad_steps = 467, loss = 0.4508877992630005
In grad_steps = 468, loss = 0.4115108847618103
In grad_steps = 469, loss = 1.125045895576477
In grad_steps = 470, loss = 0.45009469985961914
In grad_steps = 471, loss = 1.0070760250091553
In grad_steps = 472, loss = 1.0464668273925781
In grad_steps = 473, loss = 0.8633023500442505
In grad_steps = 474, loss = 0.7353854775428772
In grad_steps = 475, loss = 0.7847743630409241
In grad_steps = 476, loss = 0.5130311250686646
In grad_steps = 477, loss = 0.4508368968963623
In grad_steps = 478, loss = 1.122762680053711
In grad_steps = 479, loss = 0.3476409316062927
In grad_steps = 480, loss = 0.31288307905197144
In grad_steps = 481, loss = 1.4078142642974854
In grad_steps = 482, loss = 0.27203723788261414
In grad_steps = 483, loss = 0.2647816240787506
In grad_steps = 484, loss = 1.5283054113388062
In grad_steps = 485, loss = 1.3974848985671997
In grad_steps = 486, loss = 0.29815757274627686
In grad_steps = 487, loss = 1.1478019952774048
In grad_steps = 488, loss = 1.080864429473877
In grad_steps = 489, loss = 0.8958529233932495
In grad_steps = 490, loss = 0.749871551990509
In grad_steps = 491, loss = 0.6880831718444824
i = 4, Test ensemble probabilities = 
[array([[0.6284508 , 0.37154916],
       [0.6362697 , 0.36373034],
       [0.6395782 , 0.36042175],
       [0.6325759 , 0.3674241 ],
       [0.6236578 , 0.37634215],
       [0.64280176, 0.35719827],
       [0.6226229 , 0.37737706],
       [0.63043296, 0.36956698],
       [0.61667144, 0.38332856],
       [0.6516359 , 0.34836406],
       [0.629431  , 0.37056902],
       [0.6349214 , 0.36507857],
       [0.6210475 , 0.37895253],
       [0.64654285, 0.35345718],
       [0.6052796 , 0.39472032],
       [0.6282811 , 0.37171885]], dtype=float32), array([[0.53180707, 0.46819288],
       [0.43626124, 0.5637387 ],
       [0.32801697, 0.67198306],
       [0.4875767 , 0.51242334],
       [0.48902902, 0.51097095],
       [0.4543582 , 0.54564184],
       [0.45123112, 0.54876894],
       [0.5060453 , 0.49395472],
       [0.50209475, 0.49790528],
       [0.4963399 , 0.50366014],
       [0.46167827, 0.5383218 ],
       [0.5426025 , 0.45739752],
       [0.49294376, 0.50705624],
       [0.5642455 , 0.4357545 ],
       [0.51248384, 0.48751613],
       [0.47801602, 0.521984  ]], dtype=float32), array([[0.48643658, 0.5135634 ],
       [0.48687044, 0.51312953],
       [0.4646091 , 0.5353909 ],
       [0.48622504, 0.513775  ],
       [0.47988525, 0.5201148 ],
       [0.47998756, 0.5200124 ],
       [0.48647708, 0.51352286],
       [0.48873806, 0.51126194],
       [0.4840995 , 0.5159005 ],
       [0.48697886, 0.5130212 ],
       [0.48528168, 0.51471835],
       [0.48911697, 0.5108831 ],
       [0.47824326, 0.5217567 ],
       [0.49554408, 0.5044559 ],
       [0.48103884, 0.51896113],
       [0.48362648, 0.51637346]], dtype=float32), array([[0.87576663, 0.12423334],
       [0.8847664 , 0.11523368],
       [0.88147444, 0.1185255 ],
       [0.88156   , 0.11843994],
       [0.86536527, 0.13463475],
       [0.8823913 , 0.11760876],
       [0.8840158 , 0.11598418],
       [0.8863739 , 0.11362606],
       [0.8862254 , 0.11377464],
       [0.87569225, 0.12430774],
       [0.8827979 , 0.11720211],
       [0.890293  , 0.10970698],
       [0.8857197 , 0.11428026],
       [0.89008754, 0.10991244],
       [0.8815627 , 0.11843727],
       [0.8878101 , 0.11218985]], dtype=float32), array([[0.5259112 , 0.47408876],
       [0.52974117, 0.4702588 ],
       [0.51496774, 0.48503226],
       [0.5220147 , 0.47798535],
       [0.5183659 , 0.48163405],
       [0.51667666, 0.48332334],
       [0.53143644, 0.46856362],
       [0.5326735 , 0.46732655],
       [0.52914697, 0.47085303],
       [0.5357942 , 0.4642058 ],
       [0.5162356 , 0.4837644 ],
       [0.5523013 , 0.44769868],
       [0.5395926 , 0.46040738],
       [0.5515756 , 0.44842437],
       [0.52506465, 0.47493532],
       [0.53122455, 0.46877542]], dtype=float32)]
i = 4, Test true class= 
[1 0 0 0 1 0 1 0 0 0 1 1 0 1 1 0]
Final, Test average ensemble probabilities = 
[[0.60967445 0.39032552]
 [0.59478176 0.40521818]
 [0.56572926 0.4342707 ]
 [0.60199046 0.39800954]
 [0.5952606  0.40473932]
 [0.5952431  0.40475693]
 [0.59515667 0.40484333]
 [0.6088527  0.39114726]
 [0.6036476  0.39635238]
 [0.6092882  0.39071178]
 [0.5950849  0.40491515]
 [0.62184703 0.378153  ]
 [0.6035093  0.3964906 ]
 [0.62959915 0.37040085]
 [0.60108596 0.39891404]
 [0.6017916  0.39820832]]
Accuracy: 0.5625
MCC: 0.0630
AUC: 0.4127
Confusion Matrix:
tensor([[9, 0],
        [7, 0]])
Specificity: 1.0000
Precision (Macro): 0.2812
F1 Score (Macro): 0.3600
Expected Calibration Error (ECE): 0.0395
NLL loss: 0.6977
Main task is done! Can finish
