Job is running on partition: gpu_ss11
Job is running under account: m2616_g
/pscratch/sd/t/tianle/conda/envs/lucid_surp_2024/lib/python3.11/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:11<00:34, 11.34s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:22<00:22, 11.16s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:33<00:11, 11.11s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:35<00:00,  7.67s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:35<00:00,  8.96s/it]
Llama3 has been loaded successfully.
Namespace(model_name='Llama3', config='/pscratch/sd/t/tianle/lucid/other_source/SURP_2024/src/config.json', repo_dir='/pscratch/sd/t/tianle/lucid/other_source/SURP_2024/', dataset='3c', n_ensemble=5, seed=1, use_model_snapshot=False)
self.num_epochs = 8, self.batch_size = 16, self.max_length = 120
Model:  LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(128256, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)
          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)
          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)
          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)
)
Train dataset looks like:  Dataset({
    features: ['question', 'answer'],
    num_rows: 1999
})
Test dataset looks like:  Dataset({
    features: ['question', 'answer'],
    num_rows: 223
})
Training lora instance 0
DEBUG: Total parameters: 8037076992
DEBUG: Trainable parameters: 6815744
Beginning epoch 1
In grad_steps = 0, loss = 0.7359974980354309
In grad_steps = 1, loss = 0.809502899646759
In grad_steps = 2, loss = 0.7594748139381409
In grad_steps = 3, loss = 0.6531755924224854
In grad_steps = 4, loss = 0.90959233045578
In grad_steps = 5, loss = 0.7146127820014954
In grad_steps = 6, loss = 0.6416398882865906
In grad_steps = 7, loss = 0.7691930532455444
In grad_steps = 8, loss = 0.5925799608230591
In grad_steps = 9, loss = 0.8689308762550354
In grad_steps = 10, loss = 0.8566645979881287
In grad_steps = 11, loss = 0.7607731819152832
In grad_steps = 12, loss = 0.7083812355995178
In grad_steps = 13, loss = 0.6483562588691711
In grad_steps = 14, loss = 0.7896094918251038
In grad_steps = 15, loss = 0.8083934187889099
In grad_steps = 16, loss = 0.7144002914428711
In grad_steps = 17, loss = 0.8092383742332458
In grad_steps = 18, loss = 0.701874852180481
In grad_steps = 19, loss = 0.7217331528663635
In grad_steps = 20, loss = 0.7255706787109375
In grad_steps = 21, loss = 0.7690385580062866
In grad_steps = 22, loss = 0.7298033237457275
In grad_steps = 23, loss = 0.7710447907447815
In grad_steps = 24, loss = 0.7529963850975037
In grad_steps = 25, loss = 0.6993989944458008
In grad_steps = 26, loss = 0.6984331011772156
In grad_steps = 27, loss = 0.6328371167182922
In grad_steps = 28, loss = 0.7351199388504028
In grad_steps = 29, loss = 0.7444514036178589
In grad_steps = 30, loss = 0.7847611904144287
In grad_steps = 31, loss = 0.5702963471412659
In grad_steps = 32, loss = 0.7694626450538635
In grad_steps = 33, loss = 0.750909149646759
In grad_steps = 34, loss = 0.644061803817749
In grad_steps = 35, loss = 0.6697342395782471
In grad_steps = 36, loss = 0.6511440277099609
In grad_steps = 37, loss = 0.6858751177787781
In grad_steps = 38, loss = 0.7765644192695618
In grad_steps = 39, loss = 0.726331353187561
In grad_steps = 40, loss = 0.7020547986030579
In grad_steps = 41, loss = 0.714479386806488
In grad_steps = 42, loss = 0.6632052659988403
In grad_steps = 43, loss = 0.7069534063339233
In grad_steps = 44, loss = 0.7304897308349609
In grad_steps = 45, loss = 0.7631579637527466
In grad_steps = 46, loss = 0.7820580005645752
In grad_steps = 47, loss = 0.7452714443206787
In grad_steps = 48, loss = 0.703375518321991
In grad_steps = 49, loss = 0.7030802965164185
In grad_steps = 50, loss = 0.6851218938827515
In grad_steps = 51, loss = 0.6864044070243835
In grad_steps = 52, loss = 0.7359482645988464
In grad_steps = 53, loss = 0.6635847687721252
In grad_steps = 54, loss = 0.7709187865257263
In grad_steps = 55, loss = 0.7404735684394836
In grad_steps = 56, loss = 0.7092030644416809
In grad_steps = 57, loss = 0.7175034880638123
In grad_steps = 58, loss = 0.7510772347450256
In grad_steps = 59, loss = 0.6847658157348633
In grad_steps = 60, loss = 0.6863969564437866
In grad_steps = 61, loss = 0.7042648792266846
In grad_steps = 62, loss = 0.7144479751586914
In grad_steps = 63, loss = 0.7111033201217651
In grad_steps = 64, loss = 0.6691768169403076
In grad_steps = 65, loss = 0.6835505962371826
In grad_steps = 66, loss = 0.7410129308700562
In grad_steps = 67, loss = 0.6900362968444824
In grad_steps = 68, loss = 0.7104638814926147
In grad_steps = 69, loss = 0.6931900382041931
In grad_steps = 70, loss = 0.6960692405700684
In grad_steps = 71, loss = 0.6794483661651611
In grad_steps = 72, loss = 0.7028449773788452
In grad_steps = 73, loss = 0.7003819942474365
In grad_steps = 74, loss = 0.6834512948989868
In grad_steps = 75, loss = 0.68172687292099
In grad_steps = 76, loss = 0.7111992835998535
In grad_steps = 77, loss = 0.7198532819747925
In grad_steps = 78, loss = 0.6932160258293152
In grad_steps = 79, loss = 0.6824145317077637
In grad_steps = 80, loss = 0.7229809761047363
In grad_steps = 81, loss = 0.7070185542106628
In grad_steps = 82, loss = 0.703079879283905
In grad_steps = 83, loss = 0.6986123323440552
In grad_steps = 84, loss = 0.6708034873008728
In grad_steps = 85, loss = 0.6643413305282593
In grad_steps = 86, loss = 0.7660834789276123
In grad_steps = 87, loss = 0.687114953994751
In grad_steps = 88, loss = 0.6913396120071411
In grad_steps = 89, loss = 0.7243891954421997
In grad_steps = 90, loss = 0.7188201546669006
In grad_steps = 91, loss = 0.7225371599197388
In grad_steps = 92, loss = 0.684645414352417
In grad_steps = 93, loss = 0.7239383459091187
In grad_steps = 94, loss = 0.7014009952545166
In grad_steps = 95, loss = 0.6609421372413635
In grad_steps = 96, loss = 0.6667898893356323
In grad_steps = 97, loss = 0.6791088581085205
In grad_steps = 98, loss = 0.6981287002563477
In grad_steps = 99, loss = 0.7019060254096985
In grad_steps = 100, loss = 0.7115759253501892
In grad_steps = 101, loss = 0.6770327091217041
In grad_steps = 102, loss = 0.7044470906257629
In grad_steps = 103, loss = 0.7181980609893799
In grad_steps = 104, loss = 0.7275883555412292
In grad_steps = 105, loss = 0.6937642693519592
In grad_steps = 106, loss = 0.6972524523735046
In grad_steps = 107, loss = 0.6968033313751221
In grad_steps = 108, loss = 0.7000954747200012
In grad_steps = 109, loss = 0.7093273401260376
In grad_steps = 110, loss = 0.6933387517929077
In grad_steps = 111, loss = 0.677290141582489
In grad_steps = 112, loss = 0.6894152164459229
In grad_steps = 113, loss = 0.645163893699646
In grad_steps = 114, loss = 0.7269982099533081
In grad_steps = 115, loss = 0.7362468242645264
In grad_steps = 116, loss = 0.6478110551834106
In grad_steps = 117, loss = 0.7075824737548828
In grad_steps = 118, loss = 0.7312228083610535
In grad_steps = 119, loss = 0.6706342101097107
In grad_steps = 120, loss = 0.701867401599884
In grad_steps = 121, loss = 0.7073248028755188
In grad_steps = 122, loss = 0.6803480982780457
In grad_steps = 123, loss = 0.7463594675064087
In grad_steps = 124, loss = 0.7183048129081726
Beginning epoch 2
In grad_steps = 125, loss = 0.6969835758209229
In grad_steps = 126, loss = 0.6863070130348206
In grad_steps = 127, loss = 0.688564658164978
In grad_steps = 128, loss = 0.6999258995056152
In grad_steps = 129, loss = 0.6814457774162292
In grad_steps = 130, loss = 0.6690332293510437
In grad_steps = 131, loss = 0.6859912872314453
In grad_steps = 132, loss = 0.6778799891471863
In grad_steps = 133, loss = 0.6380083560943604
In grad_steps = 134, loss = 0.7273335456848145
In grad_steps = 135, loss = 0.7533692121505737
In grad_steps = 136, loss = 0.7313387989997864
In grad_steps = 137, loss = 0.7215119004249573
In grad_steps = 138, loss = 0.734652578830719
In grad_steps = 139, loss = 0.6758750677108765
In grad_steps = 140, loss = 0.6829811334609985
In grad_steps = 141, loss = 0.7121381759643555
In grad_steps = 142, loss = 0.6858237385749817
In grad_steps = 143, loss = 0.6847894787788391
In grad_steps = 144, loss = 0.6841781735420227
In grad_steps = 145, loss = 0.7022197246551514
In grad_steps = 146, loss = 0.6906573176383972
In grad_steps = 147, loss = 0.6930769681930542
In grad_steps = 148, loss = 0.6805413365364075
In grad_steps = 149, loss = 0.6489946246147156
In grad_steps = 150, loss = 0.6272227764129639
In grad_steps = 151, loss = 0.707194983959198
In grad_steps = 152, loss = 0.6233054995536804
In grad_steps = 153, loss = 0.7282799482345581
In grad_steps = 154, loss = 0.7485160827636719
In grad_steps = 155, loss = 0.7526099681854248
In grad_steps = 156, loss = 0.5755154490470886
In grad_steps = 157, loss = 0.7233496308326721
In grad_steps = 158, loss = 0.7143219113349915
In grad_steps = 159, loss = 0.6416118144989014
In grad_steps = 160, loss = 0.6630296111106873
In grad_steps = 161, loss = 0.6596964001655579
In grad_steps = 162, loss = 0.681176483631134
In grad_steps = 163, loss = 0.7407307624816895
In grad_steps = 164, loss = 0.7178865671157837
In grad_steps = 165, loss = 0.7001364231109619
In grad_steps = 166, loss = 0.6886577010154724
In grad_steps = 167, loss = 0.6976989507675171
In grad_steps = 168, loss = 0.6587210893630981
In grad_steps = 169, loss = 0.7029889225959778
In grad_steps = 170, loss = 0.7198212742805481
In grad_steps = 171, loss = 0.7179582118988037
In grad_steps = 172, loss = 0.7270066738128662
In grad_steps = 173, loss = 0.6998957991600037
In grad_steps = 174, loss = 0.6934437155723572
In grad_steps = 175, loss = 0.6920546889305115
In grad_steps = 176, loss = 0.6898322105407715
In grad_steps = 177, loss = 0.6917105317115784
In grad_steps = 178, loss = 0.6906651258468628
In grad_steps = 179, loss = 0.6961040496826172
In grad_steps = 180, loss = 0.6906095147132874
In grad_steps = 181, loss = 0.6920983791351318
In grad_steps = 182, loss = 0.6958937644958496
In grad_steps = 183, loss = 0.7030001878738403
In grad_steps = 184, loss = 0.6751303672790527
In grad_steps = 185, loss = 0.6856216788291931
In grad_steps = 186, loss = 0.6900648474693298
In grad_steps = 187, loss = 0.6929975748062134
In grad_steps = 188, loss = 0.6903559565544128
In grad_steps = 189, loss = 0.6799942851066589
In grad_steps = 190, loss = 0.6845983862876892
In grad_steps = 191, loss = 0.7028542757034302
In grad_steps = 192, loss = 0.6840642690658569
In grad_steps = 193, loss = 0.7007062435150146
In grad_steps = 194, loss = 0.6859766244888306
In grad_steps = 195, loss = 0.6991031169891357
In grad_steps = 196, loss = 0.6922364830970764
In grad_steps = 197, loss = 0.6886303424835205
In grad_steps = 198, loss = 0.6713013052940369
In grad_steps = 199, loss = 0.6847456693649292
In grad_steps = 200, loss = 0.6953368186950684
In grad_steps = 201, loss = 0.6950949430465698
In grad_steps = 202, loss = 0.6923558712005615
In grad_steps = 203, loss = 0.6763790249824524
In grad_steps = 204, loss = 0.6884818077087402
In grad_steps = 205, loss = 0.7140655517578125
In grad_steps = 206, loss = 0.6856265664100647
In grad_steps = 207, loss = 0.6983110308647156
In grad_steps = 208, loss = 0.7004126906394958
In grad_steps = 209, loss = 0.6629592776298523
In grad_steps = 210, loss = 0.6447882652282715
In grad_steps = 211, loss = 0.7743711471557617
In grad_steps = 212, loss = 0.6836724281311035
In grad_steps = 213, loss = 0.6839465498924255
In grad_steps = 214, loss = 0.7258096933364868
In grad_steps = 215, loss = 0.7134803533554077
In grad_steps = 216, loss = 0.7300392389297485
In grad_steps = 217, loss = 0.6843252778053284
In grad_steps = 218, loss = 0.7002944946289062
In grad_steps = 219, loss = 0.6887149214744568
In grad_steps = 220, loss = 0.677765965461731
In grad_steps = 221, loss = 0.6697332262992859
In grad_steps = 222, loss = 0.6931963562965393
In grad_steps = 223, loss = 0.7024970650672913
In grad_steps = 224, loss = 0.6939965486526489
In grad_steps = 225, loss = 0.707266628742218
In grad_steps = 226, loss = 0.6816941499710083
In grad_steps = 227, loss = 0.6988188028335571
In grad_steps = 228, loss = 0.7111511826515198
In grad_steps = 229, loss = 0.7237476110458374
In grad_steps = 230, loss = 0.6912290453910828
In grad_steps = 231, loss = 0.7012511491775513
In grad_steps = 232, loss = 0.7018017768859863
In grad_steps = 233, loss = 0.6916854977607727
In grad_steps = 234, loss = 0.6980559229850769
In grad_steps = 235, loss = 0.6903899908065796
In grad_steps = 236, loss = 0.6801558136940002
In grad_steps = 237, loss = 0.6875292062759399
In grad_steps = 238, loss = 0.6543503403663635
In grad_steps = 239, loss = 0.7202088236808777
In grad_steps = 240, loss = 0.7315506935119629
In grad_steps = 241, loss = 0.6495418548583984
In grad_steps = 242, loss = 0.7048877477645874
In grad_steps = 243, loss = 0.7186369895935059
In grad_steps = 244, loss = 0.668948769569397
In grad_steps = 245, loss = 0.7008939981460571
In grad_steps = 246, loss = 0.7045551538467407
In grad_steps = 247, loss = 0.6775145530700684
In grad_steps = 248, loss = 0.7463710904121399
In grad_steps = 249, loss = 0.7192798852920532
Beginning epoch 3
In grad_steps = 250, loss = 0.7002164721488953
In grad_steps = 251, loss = 0.6735397577285767
In grad_steps = 252, loss = 0.6857308745384216
In grad_steps = 253, loss = 0.6934192180633545
In grad_steps = 254, loss = 0.6819244623184204
In grad_steps = 255, loss = 0.6699531078338623
In grad_steps = 256, loss = 0.6827539801597595
In grad_steps = 257, loss = 0.6760537624359131
In grad_steps = 258, loss = 0.6318772435188293
In grad_steps = 259, loss = 0.7295012474060059
In grad_steps = 260, loss = 0.7553293704986572
In grad_steps = 261, loss = 0.7275682091712952
In grad_steps = 262, loss = 0.7095311880111694
In grad_steps = 263, loss = 0.7335689067840576
In grad_steps = 264, loss = 0.6704479455947876
In grad_steps = 265, loss = 0.6772686243057251
In grad_steps = 266, loss = 0.6949108242988586
In grad_steps = 267, loss = 0.6851444244384766
In grad_steps = 268, loss = 0.6744980216026306
In grad_steps = 269, loss = 0.6767990589141846
In grad_steps = 270, loss = 0.6865383982658386
In grad_steps = 271, loss = 0.6851608157157898
In grad_steps = 272, loss = 0.6891934871673584
In grad_steps = 273, loss = 0.6743847131729126
In grad_steps = 274, loss = 0.6381087899208069
In grad_steps = 275, loss = 0.6218523979187012
In grad_steps = 276, loss = 0.6978833675384521
In grad_steps = 277, loss = 0.5973856449127197
In grad_steps = 278, loss = 0.7055588960647583
In grad_steps = 279, loss = 0.7722501754760742
In grad_steps = 280, loss = 0.7056139707565308
In grad_steps = 281, loss = 0.5395249128341675
In grad_steps = 282, loss = 0.6664521098136902
In grad_steps = 283, loss = 0.6886926889419556
In grad_steps = 284, loss = 0.6220273375511169
In grad_steps = 285, loss = 0.6637158989906311
In grad_steps = 286, loss = 0.6682978868484497
In grad_steps = 287, loss = 0.6772006750106812
In grad_steps = 288, loss = 0.6872938275337219
In grad_steps = 289, loss = 0.6960837841033936
In grad_steps = 290, loss = 0.6540876626968384
In grad_steps = 291, loss = 0.6378041505813599
In grad_steps = 292, loss = 0.6302099823951721
In grad_steps = 293, loss = 0.5283905863761902
In grad_steps = 294, loss = 0.8588554263114929
In grad_steps = 295, loss = 0.7055830359458923
In grad_steps = 296, loss = 0.7566516399383545
In grad_steps = 297, loss = 0.7562929391860962
In grad_steps = 298, loss = 0.6969689130783081
In grad_steps = 299, loss = 0.636719286441803
In grad_steps = 300, loss = 0.6591553092002869
In grad_steps = 301, loss = 0.6402236819267273
In grad_steps = 302, loss = 0.7177873849868774
In grad_steps = 303, loss = 0.6692322492599487
In grad_steps = 304, loss = 0.6958420872688293
In grad_steps = 305, loss = 0.6767749786376953
In grad_steps = 306, loss = 0.7046586871147156
In grad_steps = 307, loss = 0.6871883273124695
In grad_steps = 308, loss = 0.6350147724151611
In grad_steps = 309, loss = 0.7260474562644958
In grad_steps = 310, loss = 0.656072735786438
In grad_steps = 311, loss = 0.7359654903411865
In grad_steps = 312, loss = 0.7048225402832031
In grad_steps = 313, loss = 0.6905880570411682
In grad_steps = 314, loss = 0.6678979396820068
In grad_steps = 315, loss = 0.6746301054954529
In grad_steps = 316, loss = 0.6831328272819519
In grad_steps = 317, loss = 0.6860893964767456
In grad_steps = 318, loss = 0.6656078100204468
In grad_steps = 319, loss = 0.6349735260009766
In grad_steps = 320, loss = 0.7536863088607788
In grad_steps = 321, loss = 0.6114848256111145
In grad_steps = 322, loss = 0.7063921093940735
In grad_steps = 323, loss = 0.5645999312400818
In grad_steps = 324, loss = 0.6414067149162292
In grad_steps = 325, loss = 0.6740085482597351
In grad_steps = 326, loss = 0.6840636730194092
In grad_steps = 327, loss = 0.6526188850402832
In grad_steps = 328, loss = 0.6516406536102295
In grad_steps = 329, loss = 0.6839942336082458
In grad_steps = 330, loss = 0.7755166292190552
In grad_steps = 331, loss = 0.5658577084541321
In grad_steps = 332, loss = 0.7075421810150146
In grad_steps = 333, loss = 0.752362847328186
In grad_steps = 334, loss = 0.6714199185371399
In grad_steps = 335, loss = 0.6431349515914917
In grad_steps = 336, loss = 0.7727001905441284
In grad_steps = 337, loss = 0.7019246816635132
In grad_steps = 338, loss = 0.665458083152771
In grad_steps = 339, loss = 0.7261084318161011
In grad_steps = 340, loss = 0.6263205409049988
In grad_steps = 341, loss = 0.6783306002616882
In grad_steps = 342, loss = 0.679839551448822
In grad_steps = 343, loss = 0.6342675685882568
In grad_steps = 344, loss = 0.6405990719795227
In grad_steps = 345, loss = 0.704724907875061
In grad_steps = 346, loss = 0.6635356545448303
In grad_steps = 347, loss = 0.7240935564041138
In grad_steps = 348, loss = 0.6898206472396851
In grad_steps = 349, loss = 0.6896321177482605
In grad_steps = 350, loss = 0.7221766114234924
In grad_steps = 351, loss = 0.65633225440979
In grad_steps = 352, loss = 0.7077648639678955
In grad_steps = 353, loss = 0.747310221195221
In grad_steps = 354, loss = 0.7385384440422058
In grad_steps = 355, loss = 0.6817384958267212
In grad_steps = 356, loss = 0.7167108654975891
In grad_steps = 357, loss = 0.7134107351303101
In grad_steps = 358, loss = 0.6842563152313232
In grad_steps = 359, loss = 0.7003962993621826
In grad_steps = 360, loss = 0.6606737971305847
In grad_steps = 361, loss = 0.6429716348648071
In grad_steps = 362, loss = 0.6952449083328247
In grad_steps = 363, loss = 0.6269813179969788
In grad_steps = 364, loss = 0.7558363676071167
In grad_steps = 365, loss = 0.7503273487091064
In grad_steps = 366, loss = 0.6482114195823669
In grad_steps = 367, loss = 0.7036328911781311
In grad_steps = 368, loss = 0.6891418695449829
In grad_steps = 369, loss = 0.6643555760383606
In grad_steps = 370, loss = 0.6963332891464233
In grad_steps = 371, loss = 0.6852066516876221
In grad_steps = 372, loss = 0.6742871999740601
In grad_steps = 373, loss = 0.684177041053772
In grad_steps = 374, loss = 0.6751832365989685
Beginning epoch 4
In grad_steps = 375, loss = 0.667252242565155
In grad_steps = 376, loss = 0.7382274866104126
In grad_steps = 377, loss = 0.6708983182907104
In grad_steps = 378, loss = 0.7267228960990906
In grad_steps = 379, loss = 0.6303462982177734
In grad_steps = 380, loss = 0.6292451620101929
In grad_steps = 381, loss = 0.6585763096809387
In grad_steps = 382, loss = 0.6536481380462646
In grad_steps = 383, loss = 0.6007986068725586
In grad_steps = 384, loss = 0.756013810634613
In grad_steps = 385, loss = 0.7701157331466675
In grad_steps = 386, loss = 0.6719786524772644
In grad_steps = 387, loss = 0.6166235208511353
In grad_steps = 388, loss = 0.6810919046401978
In grad_steps = 389, loss = 0.680158793926239
In grad_steps = 390, loss = 0.6809744834899902
In grad_steps = 391, loss = 0.6448111534118652
In grad_steps = 392, loss = 0.7628040909767151
In grad_steps = 393, loss = 0.7098277807235718
In grad_steps = 394, loss = 0.6304327845573425
In grad_steps = 395, loss = 0.6256957054138184
In grad_steps = 396, loss = 0.6706190705299377
In grad_steps = 397, loss = 0.667427659034729
In grad_steps = 398, loss = 0.6804946660995483
In grad_steps = 399, loss = 0.6364431381225586
In grad_steps = 400, loss = 0.6313495635986328
In grad_steps = 401, loss = 0.6373040676116943
In grad_steps = 402, loss = 0.5507323741912842
In grad_steps = 403, loss = 0.6146841645240784
In grad_steps = 404, loss = 0.7068862318992615
In grad_steps = 405, loss = 0.5853670835494995
In grad_steps = 406, loss = 0.3995423913002014
In grad_steps = 407, loss = 0.5812221169471741
In grad_steps = 408, loss = 0.552534282207489
In grad_steps = 409, loss = 0.48389148712158203
In grad_steps = 410, loss = 0.42197203636169434
In grad_steps = 411, loss = 0.42145177721977234
In grad_steps = 412, loss = 0.43040528893470764
In grad_steps = 413, loss = 0.8875855207443237
In grad_steps = 414, loss = 0.5681990385055542
In grad_steps = 415, loss = 0.6517279744148254
In grad_steps = 416, loss = 0.4783712327480316
In grad_steps = 417, loss = 0.4942372739315033
In grad_steps = 418, loss = 0.4746094346046448
In grad_steps = 419, loss = 0.5921725630760193
In grad_steps = 420, loss = 0.5490879416465759
In grad_steps = 421, loss = 0.4869723618030548
In grad_steps = 422, loss = 0.563827395439148
In grad_steps = 423, loss = 0.5115070939064026
In grad_steps = 424, loss = 0.3842414617538452
In grad_steps = 425, loss = 0.4978582262992859
In grad_steps = 426, loss = 0.5240508317947388
In grad_steps = 427, loss = 0.43117183446884155
In grad_steps = 428, loss = 0.8112208247184753
In grad_steps = 429, loss = 0.5668851137161255
In grad_steps = 430, loss = 0.695120096206665
In grad_steps = 431, loss = 0.7246654033660889
In grad_steps = 432, loss = 0.6303150653839111
In grad_steps = 433, loss = 0.559566080570221
In grad_steps = 434, loss = 0.5885400176048279
In grad_steps = 435, loss = 0.5353927612304688
In grad_steps = 436, loss = 0.689915657043457
In grad_steps = 437, loss = 0.5892000794410706
In grad_steps = 438, loss = 0.6020803451538086
In grad_steps = 439, loss = 0.6936817765235901
In grad_steps = 440, loss = 0.7307846546173096
In grad_steps = 441, loss = 0.6557104587554932
In grad_steps = 442, loss = 0.7055044770240784
In grad_steps = 443, loss = 0.5941229462623596
In grad_steps = 444, loss = 0.5825852155685425
In grad_steps = 445, loss = 0.7176311612129211
In grad_steps = 446, loss = 0.5483273267745972
In grad_steps = 447, loss = 0.5351542234420776
In grad_steps = 448, loss = 0.38143056631088257
In grad_steps = 449, loss = 0.6047462821006775
In grad_steps = 450, loss = 0.6772893071174622
In grad_steps = 451, loss = 0.5757942199707031
In grad_steps = 452, loss = 0.522291898727417
In grad_steps = 453, loss = 0.6302811503410339
In grad_steps = 454, loss = 0.4300130307674408
In grad_steps = 455, loss = 0.7301600575447083
In grad_steps = 456, loss = 0.4519285261631012
In grad_steps = 457, loss = 0.6780010461807251
In grad_steps = 458, loss = 0.7212781310081482
In grad_steps = 459, loss = 0.5301011204719543
In grad_steps = 460, loss = 0.4736243486404419
In grad_steps = 461, loss = 0.7603878378868103
In grad_steps = 462, loss = 0.7264212965965271
In grad_steps = 463, loss = 0.5219223499298096
In grad_steps = 464, loss = 0.7506528496742249
In grad_steps = 465, loss = 0.3938720226287842
In grad_steps = 466, loss = 0.7307566404342651
In grad_steps = 467, loss = 0.6510825157165527
In grad_steps = 468, loss = 0.47805076837539673
In grad_steps = 469, loss = 0.5335085391998291
In grad_steps = 470, loss = 0.7156522274017334
In grad_steps = 471, loss = 0.6672153472900391
In grad_steps = 472, loss = 0.6588274836540222
In grad_steps = 473, loss = 0.662474513053894
In grad_steps = 474, loss = 0.7187802791595459
In grad_steps = 475, loss = 0.753531277179718
In grad_steps = 476, loss = 0.6831815242767334
In grad_steps = 477, loss = 0.6361320614814758
In grad_steps = 478, loss = 0.6991373300552368
In grad_steps = 479, loss = 0.6008795499801636
In grad_steps = 480, loss = 0.6507834196090698
In grad_steps = 481, loss = 0.5781955718994141
In grad_steps = 482, loss = 0.7134506702423096
In grad_steps = 483, loss = 0.7240066528320312
In grad_steps = 484, loss = 0.7493973970413208
In grad_steps = 485, loss = 0.6279817819595337
In grad_steps = 486, loss = 0.628176748752594
In grad_steps = 487, loss = 0.6539837121963501
In grad_steps = 488, loss = 0.5792445540428162
In grad_steps = 489, loss = 0.6971404552459717
In grad_steps = 490, loss = 0.6925158500671387
In grad_steps = 491, loss = 0.6950899362564087
In grad_steps = 492, loss = 0.6728670001029968
In grad_steps = 493, loss = 0.6320298910140991
In grad_steps = 494, loss = 0.6616380214691162
In grad_steps = 495, loss = 0.6505791544914246
In grad_steps = 496, loss = 0.6384338140487671
In grad_steps = 497, loss = 0.702176034450531
In grad_steps = 498, loss = 0.6433942317962646
In grad_steps = 499, loss = 0.6715447902679443
Beginning epoch 5
In grad_steps = 500, loss = 0.6612876057624817
In grad_steps = 501, loss = 0.6159437894821167
In grad_steps = 502, loss = 0.6062933206558228
In grad_steps = 503, loss = 0.6792181134223938
In grad_steps = 504, loss = 0.6412252187728882
In grad_steps = 505, loss = 0.5593408942222595
In grad_steps = 506, loss = 0.5292682647705078
In grad_steps = 507, loss = 0.6220983266830444
In grad_steps = 508, loss = 0.45399731397628784
In grad_steps = 509, loss = 0.7939057350158691
In grad_steps = 510, loss = 0.7887568473815918
In grad_steps = 511, loss = 0.5180851817131042
In grad_steps = 512, loss = 0.473774790763855
In grad_steps = 513, loss = 0.7542416453361511
In grad_steps = 514, loss = 0.705024778842926
In grad_steps = 515, loss = 0.6173279881477356
In grad_steps = 516, loss = 0.5002148151397705
In grad_steps = 517, loss = 0.541006326675415
In grad_steps = 518, loss = 0.6151185035705566
In grad_steps = 519, loss = 0.6368362307548523
In grad_steps = 520, loss = 0.5213261246681213
In grad_steps = 521, loss = 0.6464347839355469
In grad_steps = 522, loss = 0.7275424003601074
In grad_steps = 523, loss = 0.6477187871932983
In grad_steps = 524, loss = 0.6770561933517456
In grad_steps = 525, loss = 0.5867776870727539
In grad_steps = 526, loss = 0.6254545450210571
In grad_steps = 527, loss = 0.5502567887306213
In grad_steps = 528, loss = 0.6813103556632996
In grad_steps = 529, loss = 0.643879234790802
In grad_steps = 530, loss = 0.5261839628219604
In grad_steps = 531, loss = 0.3361612558364868
In grad_steps = 532, loss = 0.4716649353504181
In grad_steps = 533, loss = 0.519243061542511
In grad_steps = 534, loss = 0.4549850821495056
In grad_steps = 535, loss = 0.4377777874469757
In grad_steps = 536, loss = 0.5014368295669556
In grad_steps = 537, loss = 0.31162023544311523
In grad_steps = 538, loss = 0.3858717978000641
In grad_steps = 539, loss = 0.3103513717651367
In grad_steps = 540, loss = 0.3817393183708191
In grad_steps = 541, loss = 0.3828659951686859
In grad_steps = 542, loss = 0.4267013669013977
In grad_steps = 543, loss = 0.24187298119068146
In grad_steps = 544, loss = 0.6947664618492126
In grad_steps = 545, loss = 0.3629955053329468
In grad_steps = 546, loss = 0.4267435669898987
In grad_steps = 547, loss = 0.49297165870666504
In grad_steps = 548, loss = 0.44782689213752747
In grad_steps = 549, loss = 0.3108195960521698
In grad_steps = 550, loss = 0.39456719160079956
In grad_steps = 551, loss = 0.35994890332221985
In grad_steps = 552, loss = 0.23507776856422424
In grad_steps = 553, loss = 0.388449490070343
In grad_steps = 554, loss = 0.2265268862247467
In grad_steps = 555, loss = 0.3857046067714691
In grad_steps = 556, loss = 0.45187756419181824
In grad_steps = 557, loss = 0.5695111751556396
In grad_steps = 558, loss = 0.6179128885269165
In grad_steps = 559, loss = 0.4351620376110077
In grad_steps = 560, loss = 0.35386183857917786
In grad_steps = 561, loss = 0.7177965641021729
In grad_steps = 562, loss = 0.36201196908950806
In grad_steps = 563, loss = 0.5967256426811218
In grad_steps = 564, loss = 0.5723149180412292
In grad_steps = 565, loss = 0.6389936208724976
In grad_steps = 566, loss = 0.5783340930938721
In grad_steps = 567, loss = 0.47046932578086853
In grad_steps = 568, loss = 0.5564687252044678
In grad_steps = 569, loss = 0.5090863108634949
In grad_steps = 570, loss = 0.4979430139064789
In grad_steps = 571, loss = 0.5738900303840637
In grad_steps = 572, loss = 0.4420011341571808
In grad_steps = 573, loss = 0.27817559242248535
In grad_steps = 574, loss = 0.48760098218917847
In grad_steps = 575, loss = 0.4116832911968231
In grad_steps = 576, loss = 0.5119826197624207
In grad_steps = 577, loss = 0.41895759105682373
In grad_steps = 578, loss = 0.427601158618927
In grad_steps = 579, loss = 0.18727564811706543
In grad_steps = 580, loss = 0.4526432156562805
In grad_steps = 581, loss = 0.227594256401062
In grad_steps = 582, loss = 0.4503787159919739
In grad_steps = 583, loss = 0.4206050634384155
In grad_steps = 584, loss = 0.5217350721359253
In grad_steps = 585, loss = 0.5352367758750916
In grad_steps = 586, loss = 0.738755464553833
In grad_steps = 587, loss = 0.2584751844406128
In grad_steps = 588, loss = 0.4568425416946411
In grad_steps = 589, loss = 0.4427211582660675
In grad_steps = 590, loss = 0.182336688041687
In grad_steps = 591, loss = 0.3704228103160858
In grad_steps = 592, loss = 0.36523181200027466
In grad_steps = 593, loss = 0.3756648004055023
In grad_steps = 594, loss = 0.23632070422172546
In grad_steps = 595, loss = 0.6162828803062439
In grad_steps = 596, loss = 0.370645672082901
In grad_steps = 597, loss = 0.469543993473053
In grad_steps = 598, loss = 0.3245978355407715
In grad_steps = 599, loss = 0.4461878538131714
In grad_steps = 600, loss = 0.8197638988494873
In grad_steps = 601, loss = 0.5045828819274902
In grad_steps = 602, loss = 0.7594330310821533
In grad_steps = 603, loss = 0.5308650732040405
In grad_steps = 604, loss = 0.43856632709503174
In grad_steps = 605, loss = 0.5505352020263672
In grad_steps = 606, loss = 0.38164183497428894
In grad_steps = 607, loss = 0.7287737131118774
In grad_steps = 608, loss = 0.6402296423912048
In grad_steps = 609, loss = 0.6255144476890564
In grad_steps = 610, loss = 0.5052343606948853
In grad_steps = 611, loss = 0.5506779551506042
In grad_steps = 612, loss = 0.5939739942550659
In grad_steps = 613, loss = 0.44712501764297485
In grad_steps = 614, loss = 0.47668135166168213
In grad_steps = 615, loss = 0.5128553509712219
In grad_steps = 616, loss = 0.5551019906997681
In grad_steps = 617, loss = 0.5737144947052002
In grad_steps = 618, loss = 0.539482057094574
In grad_steps = 619, loss = 0.5683776140213013
In grad_steps = 620, loss = 0.5110519528388977
In grad_steps = 621, loss = 0.5162779688835144
In grad_steps = 622, loss = 0.5234030485153198
In grad_steps = 623, loss = 0.48972955346107483
In grad_steps = 624, loss = 0.3706009089946747
Beginning epoch 6
In grad_steps = 625, loss = 0.4405602514743805
In grad_steps = 626, loss = 0.5144124031066895
In grad_steps = 627, loss = 0.40503770112991333
In grad_steps = 628, loss = 0.4609900712966919
In grad_steps = 629, loss = 0.5593682527542114
In grad_steps = 630, loss = 0.36274173855781555
In grad_steps = 631, loss = 0.5174642205238342
In grad_steps = 632, loss = 0.5023072957992554
In grad_steps = 633, loss = 0.2064904272556305
In grad_steps = 634, loss = 0.4150390028953552
In grad_steps = 635, loss = 0.6038997173309326
In grad_steps = 636, loss = 0.37393999099731445
In grad_steps = 637, loss = 0.1525251567363739
In grad_steps = 638, loss = 0.4675900340080261
In grad_steps = 639, loss = 0.3480325937271118
In grad_steps = 640, loss = 0.45839449763298035
In grad_steps = 641, loss = 0.23089353740215302
In grad_steps = 642, loss = 0.481600821018219
In grad_steps = 643, loss = 0.8021897673606873
In grad_steps = 644, loss = 0.4418518543243408
In grad_steps = 645, loss = 0.3251338005065918
In grad_steps = 646, loss = 0.5110208988189697
In grad_steps = 647, loss = 0.7882278561592102
In grad_steps = 648, loss = 0.543563187122345
In grad_steps = 649, loss = 0.43973222374916077
In grad_steps = 650, loss = 0.3631506860256195
In grad_steps = 651, loss = 0.4937754273414612
In grad_steps = 652, loss = 0.46087756752967834
In grad_steps = 653, loss = 0.608961820602417
In grad_steps = 654, loss = 0.595676064491272
In grad_steps = 655, loss = 0.4421043395996094
In grad_steps = 656, loss = 0.34955915808677673
In grad_steps = 657, loss = 0.554641842842102
In grad_steps = 658, loss = 0.4399975836277008
In grad_steps = 659, loss = 0.42510098218917847
In grad_steps = 660, loss = 0.4146842658519745
In grad_steps = 661, loss = 0.402776300907135
In grad_steps = 662, loss = 0.2778245806694031
In grad_steps = 663, loss = 0.35263049602508545
In grad_steps = 664, loss = 0.10216676443815231
In grad_steps = 665, loss = 0.21706309914588928
In grad_steps = 666, loss = 0.21742522716522217
In grad_steps = 667, loss = 0.20197142660617828
In grad_steps = 668, loss = 0.15169265866279602
In grad_steps = 669, loss = 0.6223526000976562
In grad_steps = 670, loss = 0.6191056966781616
In grad_steps = 671, loss = 0.911009669303894
In grad_steps = 672, loss = 0.607373833656311
In grad_steps = 673, loss = 0.6495863199234009
In grad_steps = 674, loss = 0.2797953486442566
In grad_steps = 675, loss = 0.6273777484893799
In grad_steps = 676, loss = 0.4017266035079956
In grad_steps = 677, loss = 0.8910695314407349
In grad_steps = 678, loss = 0.625163197517395
In grad_steps = 679, loss = 0.5421018600463867
In grad_steps = 680, loss = 0.5131658911705017
In grad_steps = 681, loss = 0.6165120601654053
In grad_steps = 682, loss = 0.5642101168632507
In grad_steps = 683, loss = 0.39612504839897156
In grad_steps = 684, loss = 0.7995349168777466
In grad_steps = 685, loss = 0.3987058997154236
In grad_steps = 686, loss = 0.5617080330848694
In grad_steps = 687, loss = 0.3131083548069
In grad_steps = 688, loss = 0.414948970079422
In grad_steps = 689, loss = 0.4296610355377197
In grad_steps = 690, loss = 0.46461230516433716
In grad_steps = 691, loss = 0.4445558786392212
In grad_steps = 692, loss = 0.33471256494522095
In grad_steps = 693, loss = 0.3307318389415741
In grad_steps = 694, loss = 0.6150195002555847
In grad_steps = 695, loss = 0.35623854398727417
In grad_steps = 696, loss = 0.21734286844730377
In grad_steps = 697, loss = 0.301200807094574
In grad_steps = 698, loss = 0.1352083683013916
In grad_steps = 699, loss = 0.1495514214038849
In grad_steps = 700, loss = 0.08382180333137512
In grad_steps = 701, loss = 0.2967037260532379
In grad_steps = 702, loss = 0.2577836811542511
In grad_steps = 703, loss = 0.17924702167510986
In grad_steps = 704, loss = 0.20390155911445618
In grad_steps = 705, loss = 0.15652431547641754
In grad_steps = 706, loss = 0.2557011544704437
In grad_steps = 707, loss = 0.3350994288921356
In grad_steps = 708, loss = 0.15154296159744263
In grad_steps = 709, loss = 0.4789751470088959
In grad_steps = 710, loss = 0.17023873329162598
In grad_steps = 711, loss = 0.12502720952033997
In grad_steps = 712, loss = 0.22523252665996552
In grad_steps = 713, loss = 0.08171414583921432
In grad_steps = 714, loss = 0.8056037425994873
In grad_steps = 715, loss = 0.704195499420166
In grad_steps = 716, loss = 0.6061218976974487
In grad_steps = 717, loss = 0.24054041504859924
In grad_steps = 718, loss = 0.3127906322479248
In grad_steps = 719, loss = 0.38539552688598633
In grad_steps = 720, loss = 0.7029951810836792
In grad_steps = 721, loss = 0.7521376609802246
In grad_steps = 722, loss = 0.6621319055557251
In grad_steps = 723, loss = 0.41432663798332214
In grad_steps = 724, loss = 0.3348493278026581
In grad_steps = 725, loss = 0.5400279760360718
In grad_steps = 726, loss = 0.3966643810272217
In grad_steps = 727, loss = 0.6031642556190491
In grad_steps = 728, loss = 0.6918383240699768
In grad_steps = 729, loss = 0.6621218919754028
In grad_steps = 730, loss = 0.5352159738540649
In grad_steps = 731, loss = 0.7496881484985352
In grad_steps = 732, loss = 0.7070485353469849
In grad_steps = 733, loss = 0.4887247681617737
In grad_steps = 734, loss = 0.4926871061325073
In grad_steps = 735, loss = 0.4270997643470764
In grad_steps = 736, loss = 0.5017778277397156
In grad_steps = 737, loss = 0.6121039986610413
In grad_steps = 738, loss = 0.5101157426834106
In grad_steps = 739, loss = 0.6314196586608887
In grad_steps = 740, loss = 0.5158640742301941
In grad_steps = 741, loss = 0.4114144444465637
In grad_steps = 742, loss = 0.4249815344810486
In grad_steps = 743, loss = 0.38757315278053284
In grad_steps = 744, loss = 0.5815542340278625
In grad_steps = 745, loss = 0.349242627620697
In grad_steps = 746, loss = 0.4015076458454132
In grad_steps = 747, loss = 0.42190858721733093
In grad_steps = 748, loss = 0.18062978982925415
In grad_steps = 749, loss = 0.4022026062011719
Beginning epoch 7
In grad_steps = 750, loss = 0.6070244908332825
In grad_steps = 751, loss = 0.22059792280197144
In grad_steps = 752, loss = 0.2861455976963043
In grad_steps = 753, loss = 0.29174065589904785
In grad_steps = 754, loss = 0.2478724867105484
In grad_steps = 755, loss = 0.05051017180085182
In grad_steps = 756, loss = 0.5305156707763672
In grad_steps = 757, loss = 0.6543154716491699
In grad_steps = 758, loss = 0.3021768033504486
In grad_steps = 759, loss = 0.6367555856704712
In grad_steps = 760, loss = 0.3439612090587616
In grad_steps = 761, loss = 0.5315033197402954
In grad_steps = 762, loss = 0.30420389771461487
In grad_steps = 763, loss = 0.3230843245983124
In grad_steps = 764, loss = 0.4957672953605652
In grad_steps = 765, loss = 0.4494723379611969
In grad_steps = 766, loss = 0.2625501751899719
In grad_steps = 767, loss = 0.38605353236198425
In grad_steps = 768, loss = 0.4400343596935272
In grad_steps = 769, loss = 0.4143657386302948
In grad_steps = 770, loss = 0.3255871534347534
In grad_steps = 771, loss = 0.29259204864501953
In grad_steps = 772, loss = 0.384296178817749
In grad_steps = 773, loss = 0.28570958971977234
In grad_steps = 774, loss = 0.5375728607177734
In grad_steps = 775, loss = 0.3977433741092682
In grad_steps = 776, loss = 0.41433772444725037
In grad_steps = 777, loss = 0.3408183157444
In grad_steps = 778, loss = 0.6415888071060181
In grad_steps = 779, loss = 0.6201717257499695
In grad_steps = 780, loss = 0.2813865542411804
In grad_steps = 781, loss = 0.13582774996757507
In grad_steps = 782, loss = 0.216475248336792
In grad_steps = 783, loss = 0.12129238247871399
In grad_steps = 784, loss = 0.20144644379615784
In grad_steps = 785, loss = 0.21263070404529572
In grad_steps = 786, loss = 0.32849574089050293
In grad_steps = 787, loss = 0.09836256504058838
In grad_steps = 788, loss = 0.11310220509767532
In grad_steps = 789, loss = 0.10967042297124863
In grad_steps = 790, loss = 0.1027650237083435
In grad_steps = 791, loss = 0.13520219922065735
In grad_steps = 792, loss = 0.6534189581871033
In grad_steps = 793, loss = 0.25213536620140076
In grad_steps = 794, loss = 0.28751140832901
In grad_steps = 795, loss = 0.27896004915237427
In grad_steps = 796, loss = 0.09936223179101944
In grad_steps = 797, loss = 0.4166066348552704
In grad_steps = 798, loss = 0.5822554230690002
In grad_steps = 799, loss = 0.13678880035877228
In grad_steps = 800, loss = 0.3509305417537689
In grad_steps = 801, loss = 0.342862993478775
In grad_steps = 802, loss = 0.05526715889573097
In grad_steps = 803, loss = 0.30312255024909973
In grad_steps = 804, loss = 0.19703787565231323
In grad_steps = 805, loss = 0.7570626735687256
In grad_steps = 806, loss = 0.2555254101753235
In grad_steps = 807, loss = 0.29936257004737854
In grad_steps = 808, loss = 0.1485445201396942
In grad_steps = 809, loss = 0.37470120191574097
In grad_steps = 810, loss = 0.10721519589424133
In grad_steps = 811, loss = 0.4225405752658844
In grad_steps = 812, loss = 0.2456660121679306
In grad_steps = 813, loss = 0.7044604420661926
In grad_steps = 814, loss = 0.40404778718948364
In grad_steps = 815, loss = 0.5450820922851562
In grad_steps = 816, loss = 0.27270135283470154
In grad_steps = 817, loss = 0.22759823501110077
In grad_steps = 818, loss = 0.41436293721199036
In grad_steps = 819, loss = 0.26023802161216736
In grad_steps = 820, loss = 0.4001402258872986
In grad_steps = 821, loss = 0.1649801880121231
In grad_steps = 822, loss = 0.35484549403190613
In grad_steps = 823, loss = 0.3034802973270416
In grad_steps = 824, loss = 0.3215253949165344
In grad_steps = 825, loss = 0.2778560221195221
In grad_steps = 826, loss = 0.3254750967025757
In grad_steps = 827, loss = 0.3110918402671814
In grad_steps = 828, loss = 0.2573806941509247
In grad_steps = 829, loss = 0.20689596235752106
In grad_steps = 830, loss = 0.18505102396011353
In grad_steps = 831, loss = 0.16619780659675598
In grad_steps = 832, loss = 0.2881634831428528
In grad_steps = 833, loss = 0.15177330374717712
In grad_steps = 834, loss = 0.1030346155166626
In grad_steps = 835, loss = 0.06302278488874435
In grad_steps = 836, loss = 0.07680213451385498
In grad_steps = 837, loss = 0.22095619142055511
In grad_steps = 838, loss = 0.12453195452690125
In grad_steps = 839, loss = 0.11536575853824615
In grad_steps = 840, loss = 0.2578212022781372
In grad_steps = 841, loss = 0.19650126993656158
In grad_steps = 842, loss = 0.13672247529029846
In grad_steps = 843, loss = 0.452317476272583
In grad_steps = 844, loss = 0.3474976420402527
In grad_steps = 845, loss = 0.27865609526634216
In grad_steps = 846, loss = 0.0738515555858612
In grad_steps = 847, loss = 0.4170725643634796
In grad_steps = 848, loss = 0.15827694535255432
In grad_steps = 849, loss = 0.20206065475940704
In grad_steps = 850, loss = 0.29175305366516113
In grad_steps = 851, loss = 0.2873833179473877
In grad_steps = 852, loss = 0.37436264753341675
In grad_steps = 853, loss = 0.2868998646736145
In grad_steps = 854, loss = 0.5591575503349304
In grad_steps = 855, loss = 0.5214431881904602
In grad_steps = 856, loss = 0.8717384338378906
In grad_steps = 857, loss = 0.5465165376663208
In grad_steps = 858, loss = 0.33791089057922363
In grad_steps = 859, loss = 0.3752325475215912
In grad_steps = 860, loss = 0.224416121840477
In grad_steps = 861, loss = 0.43907368183135986
In grad_steps = 862, loss = 0.2855342626571655
In grad_steps = 863, loss = 0.424299955368042
In grad_steps = 864, loss = 0.4538881480693817
In grad_steps = 865, loss = 0.8282473683357239
In grad_steps = 866, loss = 0.42196789383888245
In grad_steps = 867, loss = 0.6098389625549316
In grad_steps = 868, loss = 0.31650328636169434
In grad_steps = 869, loss = 0.33469918370246887
In grad_steps = 870, loss = 0.27661535143852234
In grad_steps = 871, loss = 0.32869401574134827
In grad_steps = 872, loss = 0.3198851943016052
In grad_steps = 873, loss = 0.20182166993618011
In grad_steps = 874, loss = 0.28532078862190247
Beginning epoch 8
In grad_steps = 875, loss = 0.200179785490036
In grad_steps = 876, loss = 0.283812016248703
In grad_steps = 877, loss = 0.17167511582374573
In grad_steps = 878, loss = 0.3030202388763428
In grad_steps = 879, loss = 0.18386214971542358
In grad_steps = 880, loss = 0.19162458181381226
In grad_steps = 881, loss = 0.21157382428646088
In grad_steps = 882, loss = 0.4016939103603363
In grad_steps = 883, loss = 0.13396061956882477
In grad_steps = 884, loss = 0.4176657497882843
In grad_steps = 885, loss = 0.3534826338291168
In grad_steps = 886, loss = 0.2981303036212921
In grad_steps = 887, loss = 0.2012975811958313
In grad_steps = 888, loss = 0.2673322558403015
In grad_steps = 889, loss = 0.1830957978963852
In grad_steps = 890, loss = 0.185098797082901
In grad_steps = 891, loss = 0.26815369725227356
In grad_steps = 892, loss = 0.07902886718511581
In grad_steps = 893, loss = 0.35296863317489624
In grad_steps = 894, loss = 0.10910674929618835
In grad_steps = 895, loss = 0.2515921890735626
In grad_steps = 896, loss = 0.157770574092865
In grad_steps = 897, loss = 0.1362716555595398
In grad_steps = 898, loss = 0.11370103806257248
In grad_steps = 899, loss = 0.3243749141693115
In grad_steps = 900, loss = 0.2572519779205322
In grad_steps = 901, loss = 0.26530128717422485
In grad_steps = 902, loss = 0.28827282786369324
In grad_steps = 903, loss = 0.2857692241668701
In grad_steps = 904, loss = 0.2389000952243805
In grad_steps = 905, loss = 0.2271498143672943
In grad_steps = 906, loss = 0.05184157192707062
In grad_steps = 907, loss = 0.11451546847820282
In grad_steps = 908, loss = 0.30240458250045776
In grad_steps = 909, loss = 0.34315404295921326
In grad_steps = 910, loss = 0.1470910757780075
In grad_steps = 911, loss = 0.17993533611297607
In grad_steps = 912, loss = 0.1177922934293747
In grad_steps = 913, loss = 0.04489058256149292
In grad_steps = 914, loss = 0.03595000505447388
In grad_steps = 915, loss = 0.03541838005185127
In grad_steps = 916, loss = 0.11430996656417847
In grad_steps = 917, loss = 0.24810290336608887
In grad_steps = 918, loss = 0.17153750360012054
In grad_steps = 919, loss = 0.11591267585754395
In grad_steps = 920, loss = 0.5046913027763367
In grad_steps = 921, loss = 0.025919584557414055
In grad_steps = 922, loss = 0.28185880184173584
In grad_steps = 923, loss = 0.27963194251060486
In grad_steps = 924, loss = 0.03788246959447861
In grad_steps = 925, loss = 0.1466987580060959
In grad_steps = 926, loss = 0.11931958049535751
In grad_steps = 927, loss = 0.11829729378223419
In grad_steps = 928, loss = 0.31824594736099243
In grad_steps = 929, loss = 0.2270217388868332
In grad_steps = 930, loss = 0.28494322299957275
In grad_steps = 931, loss = 0.5674092173576355
In grad_steps = 932, loss = 0.4675499200820923
In grad_steps = 933, loss = 0.9855222105979919
In grad_steps = 934, loss = 0.17859606444835663
In grad_steps = 935, loss = 0.2943342328071594
In grad_steps = 936, loss = 0.3677562177181244
In grad_steps = 937, loss = 0.0879424661397934
In grad_steps = 938, loss = 0.29267463088035583
In grad_steps = 939, loss = 0.168566033244133
In grad_steps = 940, loss = 0.2426581084728241
In grad_steps = 941, loss = 0.4916205108165741
In grad_steps = 942, loss = 0.39443811774253845
In grad_steps = 943, loss = 0.6000394225120544
In grad_steps = 944, loss = 0.36123353242874146
In grad_steps = 945, loss = 0.22521308064460754
In grad_steps = 946, loss = 0.14570939540863037
In grad_steps = 947, loss = 0.1883450299501419
In grad_steps = 948, loss = 0.3030630648136139
In grad_steps = 949, loss = 0.17996937036514282
In grad_steps = 950, loss = 0.19254283607006073
In grad_steps = 951, loss = 0.411155641078949
In grad_steps = 952, loss = 0.2545805275440216
In grad_steps = 953, loss = 0.39597055315971375
In grad_steps = 954, loss = 0.1066717654466629
In grad_steps = 955, loss = 0.22943253815174103
In grad_steps = 956, loss = 0.2791762948036194
In grad_steps = 957, loss = 0.11041328310966492
In grad_steps = 958, loss = 0.2415536791086197
In grad_steps = 959, loss = 0.08828245848417282
In grad_steps = 960, loss = 0.06340538710355759
In grad_steps = 961, loss = 0.2095492035150528
In grad_steps = 962, loss = 0.14220501482486725
In grad_steps = 963, loss = 0.26559725403785706
In grad_steps = 964, loss = 0.11490758508443832
In grad_steps = 965, loss = 0.19323353469371796
In grad_steps = 966, loss = 0.4097934663295746
In grad_steps = 967, loss = 0.062331534922122955
In grad_steps = 968, loss = 0.30047133564949036
In grad_steps = 969, loss = 0.06855244189500809
In grad_steps = 970, loss = 0.06084807962179184
In grad_steps = 971, loss = 0.17121945321559906
In grad_steps = 972, loss = 0.23860330879688263
In grad_steps = 973, loss = 0.20839940011501312
In grad_steps = 974, loss = 0.02144456095993519
In grad_steps = 975, loss = 0.07903463393449783
In grad_steps = 976, loss = 0.23301181197166443
In grad_steps = 977, loss = 0.05335970222949982
In grad_steps = 978, loss = 0.19545577466487885
In grad_steps = 979, loss = 0.23359568417072296
In grad_steps = 980, loss = 0.4757195711135864
In grad_steps = 981, loss = 0.3037950098514557
In grad_steps = 982, loss = 0.14985647797584534
In grad_steps = 983, loss = 0.2863236665725708
In grad_steps = 984, loss = 0.3078642785549164
In grad_steps = 985, loss = 0.19483619928359985
In grad_steps = 986, loss = 0.25922542810440063
In grad_steps = 987, loss = 0.11727098375558853
In grad_steps = 988, loss = 0.2981235086917877
In grad_steps = 989, loss = 0.17483480274677277
In grad_steps = 990, loss = 0.29021456837654114
In grad_steps = 991, loss = 0.08026819676160812
In grad_steps = 992, loss = 0.509411096572876
In grad_steps = 993, loss = 0.2815360724925995
In grad_steps = 994, loss = 0.31844305992126465
In grad_steps = 995, loss = 0.24995404481887817
In grad_steps = 996, loss = 0.28421473503112793
In grad_steps = 997, loss = 0.4206337034702301
In grad_steps = 998, loss = 0.18010778725147247
In grad_steps = 999, loss = 0.2145494520664215
Elapsed time: 3356.5132830142975 seconds for ensemble 0 with 8 epochs
LoRA instance 0 evaluation complete. Data saved to /pscratch/sd/t/tianle/lucid/other_source/SURP_2024/results/tmp_lora_ensemble/Llama3-8B-set-3c/test_data_instance_0_seed_1.npz.
lora instance i = 0 Successfully finished.
Training lora instance 1
DEBUG: Total parameters: 8037076992
DEBUG: Trainable parameters: 6815744
Beginning epoch 1
In grad_steps = 0, loss = 0.7359974980354309
In grad_steps = 1, loss = 0.8015907406806946
In grad_steps = 2, loss = 0.7655412554740906
In grad_steps = 3, loss = 0.6513668894767761
In grad_steps = 4, loss = 0.9092102646827698
In grad_steps = 5, loss = 0.7072112560272217
In grad_steps = 6, loss = 0.6408734917640686
In grad_steps = 7, loss = 0.7701166868209839
In grad_steps = 8, loss = 0.5917847156524658
In grad_steps = 9, loss = 0.8633478879928589
In grad_steps = 10, loss = 0.8508691787719727
In grad_steps = 11, loss = 0.7616345882415771
In grad_steps = 12, loss = 0.7041429281234741
In grad_steps = 13, loss = 0.6385524272918701
In grad_steps = 14, loss = 0.8056890964508057
In grad_steps = 15, loss = 0.8204545974731445
In grad_steps = 16, loss = 0.7174454927444458
In grad_steps = 17, loss = 0.8012728691101074
In grad_steps = 18, loss = 0.6979532241821289
In grad_steps = 19, loss = 0.7355899810791016
In grad_steps = 20, loss = 0.7336549758911133
In grad_steps = 21, loss = 0.7771452069282532
In grad_steps = 22, loss = 0.7334275841712952
In grad_steps = 23, loss = 0.7737659215927124
In grad_steps = 24, loss = 0.7540508508682251
In grad_steps = 25, loss = 0.6967831254005432
In grad_steps = 26, loss = 0.6997994184494019
In grad_steps = 27, loss = 0.6312052607536316
In grad_steps = 28, loss = 0.7365081310272217
In grad_steps = 29, loss = 0.749345064163208
In grad_steps = 30, loss = 0.788482129573822
In grad_steps = 31, loss = 0.5721086263656616
In grad_steps = 32, loss = 0.7666628360748291
In grad_steps = 33, loss = 0.7474778890609741
In grad_steps = 34, loss = 0.6414082050323486
In grad_steps = 35, loss = 0.6687329411506653
In grad_steps = 36, loss = 0.6586511135101318
In grad_steps = 37, loss = 0.6883024573326111
In grad_steps = 38, loss = 0.767170786857605
In grad_steps = 39, loss = 0.718533456325531
In grad_steps = 40, loss = 0.6995823979377747
In grad_steps = 41, loss = 0.7174540758132935
In grad_steps = 42, loss = 0.6666930317878723
In grad_steps = 43, loss = 0.707165539264679
In grad_steps = 44, loss = 0.7318004965782166
In grad_steps = 45, loss = 0.7562091946601868
In grad_steps = 46, loss = 0.778733491897583
In grad_steps = 47, loss = 0.7443453669548035
In grad_steps = 48, loss = 0.7015724778175354
In grad_steps = 49, loss = 0.7022713422775269
In grad_steps = 50, loss = 0.6857334971427917
In grad_steps = 51, loss = 0.6845201253890991
In grad_steps = 52, loss = 0.7428938150405884
In grad_steps = 53, loss = 0.6646731495857239
In grad_steps = 54, loss = 0.7756821513175964
In grad_steps = 55, loss = 0.7417046427726746
In grad_steps = 56, loss = 0.7092040181159973
In grad_steps = 57, loss = 0.7149854898452759
In grad_steps = 58, loss = 0.744400143623352
In grad_steps = 59, loss = 0.6904692053794861
In grad_steps = 60, loss = 0.6841046810150146
In grad_steps = 61, loss = 0.7139371037483215
In grad_steps = 62, loss = 0.7240060567855835
In grad_steps = 63, loss = 0.7177900671958923
In grad_steps = 64, loss = 0.6659555435180664
In grad_steps = 65, loss = 0.6842557191848755
In grad_steps = 66, loss = 0.736981987953186
In grad_steps = 67, loss = 0.68979412317276
In grad_steps = 68, loss = 0.7062428593635559
In grad_steps = 69, loss = 0.6925535202026367
In grad_steps = 70, loss = 0.7011311650276184
In grad_steps = 71, loss = 0.6715647578239441
In grad_steps = 72, loss = 0.7089573740959167
In grad_steps = 73, loss = 0.7050608992576599
In grad_steps = 74, loss = 0.6832515597343445
In grad_steps = 75, loss = 0.6828568577766418
In grad_steps = 76, loss = 0.7130324244499207
In grad_steps = 77, loss = 0.7205598950386047
In grad_steps = 78, loss = 0.6950744390487671
In grad_steps = 79, loss = 0.6821703910827637
In grad_steps = 80, loss = 0.7195026278495789
In grad_steps = 81, loss = 0.7064577341079712
In grad_steps = 82, loss = 0.7006270289421082
In grad_steps = 83, loss = 0.7020790576934814
In grad_steps = 84, loss = 0.6649327278137207
In grad_steps = 85, loss = 0.6586161255836487
In grad_steps = 86, loss = 0.7783990502357483
In grad_steps = 87, loss = 0.6849880218505859
In grad_steps = 88, loss = 0.6922615766525269
In grad_steps = 89, loss = 0.7233175039291382
In grad_steps = 90, loss = 0.7229623794555664
In grad_steps = 91, loss = 0.7237378358840942
In grad_steps = 92, loss = 0.6850971579551697
In grad_steps = 93, loss = 0.7241020798683167
In grad_steps = 94, loss = 0.7007521986961365
In grad_steps = 95, loss = 0.6632106900215149
In grad_steps = 96, loss = 0.6715778112411499
In grad_steps = 97, loss = 0.6840696334838867
In grad_steps = 98, loss = 0.697716236114502
In grad_steps = 99, loss = 0.7010332942008972
In grad_steps = 100, loss = 0.7086295485496521
In grad_steps = 101, loss = 0.6801407933235168
In grad_steps = 102, loss = 0.7020491361618042
In grad_steps = 103, loss = 0.713588535785675
In grad_steps = 104, loss = 0.7221128940582275
In grad_steps = 105, loss = 0.6946142911911011
In grad_steps = 106, loss = 0.696456253528595
In grad_steps = 107, loss = 0.6966515183448792
In grad_steps = 108, loss = 0.6993078589439392
In grad_steps = 109, loss = 0.7120658159255981
In grad_steps = 110, loss = 0.6926039457321167
In grad_steps = 111, loss = 0.6770627498626709
In grad_steps = 112, loss = 0.6893975138664246
In grad_steps = 113, loss = 0.644993007183075
In grad_steps = 114, loss = 0.7258659601211548
In grad_steps = 115, loss = 0.7363532781600952
In grad_steps = 116, loss = 0.6490730047225952
In grad_steps = 117, loss = 0.706848680973053
In grad_steps = 118, loss = 0.7299061417579651
In grad_steps = 119, loss = 0.6716302633285522
In grad_steps = 120, loss = 0.7007195353507996
In grad_steps = 121, loss = 0.7056607007980347
In grad_steps = 122, loss = 0.68077552318573
In grad_steps = 123, loss = 0.745107889175415
In grad_steps = 124, loss = 0.717359185218811
Beginning epoch 2
In grad_steps = 125, loss = 0.6961156129837036
In grad_steps = 126, loss = 0.6884618997573853
In grad_steps = 127, loss = 0.6873948574066162
In grad_steps = 128, loss = 0.7024645209312439
In grad_steps = 129, loss = 0.6775009632110596
In grad_steps = 130, loss = 0.6677398681640625
In grad_steps = 131, loss = 0.6874128580093384
In grad_steps = 132, loss = 0.6769097447395325
In grad_steps = 133, loss = 0.636421263217926
In grad_steps = 134, loss = 0.7313119769096375
In grad_steps = 135, loss = 0.7547891139984131
In grad_steps = 136, loss = 0.7342483401298523
In grad_steps = 137, loss = 0.7241561412811279
In grad_steps = 138, loss = 0.7348840236663818
In grad_steps = 139, loss = 0.6760740876197815
In grad_steps = 140, loss = 0.6823770403862
In grad_steps = 141, loss = 0.7122016549110413
In grad_steps = 142, loss = 0.6837213635444641
In grad_steps = 143, loss = 0.6844083666801453
In grad_steps = 144, loss = 0.683228611946106
In grad_steps = 145, loss = 0.7010599374771118
In grad_steps = 146, loss = 0.6923739910125732
In grad_steps = 147, loss = 0.6922490000724792
In grad_steps = 148, loss = 0.6793521642684937
In grad_steps = 149, loss = 0.650051474571228
In grad_steps = 150, loss = 0.6304990649223328
In grad_steps = 151, loss = 0.7058908343315125
In grad_steps = 152, loss = 0.625726580619812
In grad_steps = 153, loss = 0.7255359292030334
In grad_steps = 154, loss = 0.7378149628639221
In grad_steps = 155, loss = 0.7493432760238647
In grad_steps = 156, loss = 0.5750154852867126
In grad_steps = 157, loss = 0.7270598411560059
In grad_steps = 158, loss = 0.7136879563331604
In grad_steps = 159, loss = 0.6368418335914612
In grad_steps = 160, loss = 0.6614495515823364
In grad_steps = 161, loss = 0.6554467082023621
In grad_steps = 162, loss = 0.6801214218139648
In grad_steps = 163, loss = 0.749574601650238
In grad_steps = 164, loss = 0.72174471616745
In grad_steps = 165, loss = 0.7025198936462402
In grad_steps = 166, loss = 0.6881904602050781
In grad_steps = 167, loss = 0.7025958299636841
In grad_steps = 168, loss = 0.659568727016449
In grad_steps = 169, loss = 0.7032838463783264
In grad_steps = 170, loss = 0.721682071685791
In grad_steps = 171, loss = 0.7214627265930176
In grad_steps = 172, loss = 0.7262550592422485
In grad_steps = 173, loss = 0.7012101411819458
In grad_steps = 174, loss = 0.6927579045295715
In grad_steps = 175, loss = 0.6925073266029358
In grad_steps = 176, loss = 0.6876407265663147
In grad_steps = 177, loss = 0.6925008296966553
In grad_steps = 178, loss = 0.6894805431365967
In grad_steps = 179, loss = 0.699062168598175
In grad_steps = 180, loss = 0.6923156976699829
In grad_steps = 181, loss = 0.6929762959480286
In grad_steps = 182, loss = 0.6946227550506592
In grad_steps = 183, loss = 0.7058495283126831
In grad_steps = 184, loss = 0.6758924722671509
In grad_steps = 185, loss = 0.6866008639335632
In grad_steps = 186, loss = 0.690879762172699
In grad_steps = 187, loss = 0.6945724487304688
In grad_steps = 188, loss = 0.690074622631073
In grad_steps = 189, loss = 0.6791377663612366
In grad_steps = 190, loss = 0.6840885281562805
In grad_steps = 191, loss = 0.7025738954544067
In grad_steps = 192, loss = 0.6843858361244202
In grad_steps = 193, loss = 0.7015208005905151
In grad_steps = 194, loss = 0.6865409016609192
In grad_steps = 195, loss = 0.6967988610267639
In grad_steps = 196, loss = 0.6938738822937012
In grad_steps = 197, loss = 0.6915555000305176
In grad_steps = 198, loss = 0.6744662523269653
In grad_steps = 199, loss = 0.6835026741027832
In grad_steps = 200, loss = 0.6952145099639893
In grad_steps = 201, loss = 0.6955868601799011
In grad_steps = 202, loss = 0.6941742897033691
In grad_steps = 203, loss = 0.6771508455276489
In grad_steps = 204, loss = 0.6880336999893188
In grad_steps = 205, loss = 0.7131639719009399
In grad_steps = 206, loss = 0.6882503032684326
In grad_steps = 207, loss = 0.6971430778503418
In grad_steps = 208, loss = 0.7029760479927063
In grad_steps = 209, loss = 0.6653556227684021
In grad_steps = 210, loss = 0.6456524133682251
In grad_steps = 211, loss = 0.7740803956985474
In grad_steps = 212, loss = 0.6838047504425049
In grad_steps = 213, loss = 0.6845527291297913
In grad_steps = 214, loss = 0.7228332161903381
In grad_steps = 215, loss = 0.7119388580322266
In grad_steps = 216, loss = 0.7272034883499146
In grad_steps = 217, loss = 0.6832519173622131
In grad_steps = 218, loss = 0.7040759921073914
In grad_steps = 219, loss = 0.6870202422142029
In grad_steps = 220, loss = 0.6753801703453064
In grad_steps = 221, loss = 0.6699405908584595
In grad_steps = 222, loss = 0.6916852593421936
In grad_steps = 223, loss = 0.7014237642288208
In grad_steps = 224, loss = 0.6936317086219788
In grad_steps = 225, loss = 0.7053751945495605
In grad_steps = 226, loss = 0.6826887726783752
In grad_steps = 227, loss = 0.7008358836174011
In grad_steps = 228, loss = 0.709622323513031
In grad_steps = 229, loss = 0.7228138446807861
In grad_steps = 230, loss = 0.690884530544281
In grad_steps = 231, loss = 0.7047064900398254
In grad_steps = 232, loss = 0.7005096673965454
In grad_steps = 233, loss = 0.6899369955062866
In grad_steps = 234, loss = 0.6986513137817383
In grad_steps = 235, loss = 0.6895948648452759
In grad_steps = 236, loss = 0.6794558763504028
In grad_steps = 237, loss = 0.6869921684265137
In grad_steps = 238, loss = 0.653883159160614
In grad_steps = 239, loss = 0.7201790809631348
In grad_steps = 240, loss = 0.7313889861106873
In grad_steps = 241, loss = 0.6496458649635315
In grad_steps = 242, loss = 0.7049837112426758
In grad_steps = 243, loss = 0.7201694846153259
In grad_steps = 244, loss = 0.6697602272033691
In grad_steps = 245, loss = 0.698725700378418
In grad_steps = 246, loss = 0.7045034766197205
In grad_steps = 247, loss = 0.6786186695098877
In grad_steps = 248, loss = 0.7460876107215881
In grad_steps = 249, loss = 0.7190712094306946
Beginning epoch 3
In grad_steps = 250, loss = 0.7004085779190063
In grad_steps = 251, loss = 0.6753184199333191
In grad_steps = 252, loss = 0.6857526302337646
In grad_steps = 253, loss = 0.6948050856590271
In grad_steps = 254, loss = 0.681301474571228
In grad_steps = 255, loss = 0.6698004007339478
In grad_steps = 256, loss = 0.6822644472122192
In grad_steps = 257, loss = 0.6760773062705994
In grad_steps = 258, loss = 0.6345496773719788
In grad_steps = 259, loss = 0.7275866866111755
In grad_steps = 260, loss = 0.7530218362808228
In grad_steps = 261, loss = 0.7273380756378174
In grad_steps = 262, loss = 0.710455596446991
In grad_steps = 263, loss = 0.7356597781181335
In grad_steps = 264, loss = 0.6711873412132263
In grad_steps = 265, loss = 0.6769519448280334
In grad_steps = 266, loss = 0.6993140578269958
In grad_steps = 267, loss = 0.6811073422431946
In grad_steps = 268, loss = 0.6768587231636047
In grad_steps = 269, loss = 0.6804144978523254
In grad_steps = 270, loss = 0.686912477016449
In grad_steps = 271, loss = 0.6871928572654724
In grad_steps = 272, loss = 0.6902158856391907
In grad_steps = 273, loss = 0.6766340732574463
In grad_steps = 274, loss = 0.6383357048034668
In grad_steps = 275, loss = 0.6273241639137268
In grad_steps = 276, loss = 0.6973149180412292
In grad_steps = 277, loss = 0.6027138233184814
In grad_steps = 278, loss = 0.7022177577018738
In grad_steps = 279, loss = 0.7620894312858582
In grad_steps = 280, loss = 0.7240208983421326
In grad_steps = 281, loss = 0.5371063947677612
In grad_steps = 282, loss = 0.674390435218811
In grad_steps = 283, loss = 0.692542552947998
In grad_steps = 284, loss = 0.611546516418457
In grad_steps = 285, loss = 0.6499618887901306
In grad_steps = 286, loss = 0.6658529043197632
In grad_steps = 287, loss = 0.6666020154953003
In grad_steps = 288, loss = 0.6937965750694275
In grad_steps = 289, loss = 0.6735174655914307
In grad_steps = 290, loss = 0.6509352326393127
In grad_steps = 291, loss = 0.6502498388290405
In grad_steps = 292, loss = 0.6044690608978271
In grad_steps = 293, loss = 0.5427266359329224
In grad_steps = 294, loss = 0.9055121541023254
In grad_steps = 295, loss = 0.7409431338310242
In grad_steps = 296, loss = 0.6640369296073914
In grad_steps = 297, loss = 0.7467174530029297
In grad_steps = 298, loss = 0.6968542337417603
In grad_steps = 299, loss = 0.6380332708358765
In grad_steps = 300, loss = 0.6556951999664307
In grad_steps = 301, loss = 0.6428041458129883
In grad_steps = 302, loss = 0.7006611227989197
In grad_steps = 303, loss = 0.6838613748550415
In grad_steps = 304, loss = 0.6558829545974731
In grad_steps = 305, loss = 0.6635401248931885
In grad_steps = 306, loss = 0.7112244367599487
In grad_steps = 307, loss = 0.6827024221420288
In grad_steps = 308, loss = 0.6063775420188904
In grad_steps = 309, loss = 0.7173717021942139
In grad_steps = 310, loss = 0.646981418132782
In grad_steps = 311, loss = 0.7314339280128479
In grad_steps = 312, loss = 0.7043721675872803
In grad_steps = 313, loss = 0.6721879839897156
In grad_steps = 314, loss = 0.6540393829345703
In grad_steps = 315, loss = 0.6834813952445984
In grad_steps = 316, loss = 0.6530401110649109
In grad_steps = 317, loss = 0.683347761631012
In grad_steps = 318, loss = 0.6347775459289551
In grad_steps = 319, loss = 0.6236084699630737
In grad_steps = 320, loss = 0.8067439794540405
In grad_steps = 321, loss = 0.5920761823654175
In grad_steps = 322, loss = 0.6876309514045715
In grad_steps = 323, loss = 0.5473915338516235
In grad_steps = 324, loss = 0.6530877351760864
In grad_steps = 325, loss = 0.7032198905944824
In grad_steps = 326, loss = 0.6663210391998291
In grad_steps = 327, loss = 0.6357945799827576
In grad_steps = 328, loss = 0.6786751747131348
In grad_steps = 329, loss = 0.6811654567718506
In grad_steps = 330, loss = 0.7300156950950623
In grad_steps = 331, loss = 0.5957181453704834
In grad_steps = 332, loss = 0.7301559448242188
In grad_steps = 333, loss = 0.7461966276168823
In grad_steps = 334, loss = 0.6255635619163513
In grad_steps = 335, loss = 0.6677627563476562
In grad_steps = 336, loss = 0.7872302532196045
In grad_steps = 337, loss = 0.6844936013221741
In grad_steps = 338, loss = 0.6585895419120789
In grad_steps = 339, loss = 0.7465880513191223
In grad_steps = 340, loss = 0.636438250541687
In grad_steps = 341, loss = 0.6847013235092163
In grad_steps = 342, loss = 0.6848530173301697
In grad_steps = 343, loss = 0.6330126523971558
In grad_steps = 344, loss = 0.6524930596351624
In grad_steps = 345, loss = 0.7349227666854858
In grad_steps = 346, loss = 0.7073907852172852
In grad_steps = 347, loss = 0.7201962471008301
In grad_steps = 348, loss = 0.7062668800354004
In grad_steps = 349, loss = 0.6868178844451904
In grad_steps = 350, loss = 0.73899906873703
In grad_steps = 351, loss = 0.6738185286521912
In grad_steps = 352, loss = 0.7303065061569214
In grad_steps = 353, loss = 0.7615146040916443
In grad_steps = 354, loss = 0.7641173601150513
In grad_steps = 355, loss = 0.6844202280044556
In grad_steps = 356, loss = 0.6902561187744141
In grad_steps = 357, loss = 0.7113710045814514
In grad_steps = 358, loss = 0.6820580363273621
In grad_steps = 359, loss = 0.7061803936958313
In grad_steps = 360, loss = 0.6763128638267517
In grad_steps = 361, loss = 0.6687784790992737
In grad_steps = 362, loss = 0.6921745538711548
In grad_steps = 363, loss = 0.6400710940361023
In grad_steps = 364, loss = 0.7301499843597412
In grad_steps = 365, loss = 0.7450951337814331
In grad_steps = 366, loss = 0.6498260498046875
In grad_steps = 367, loss = 0.697481095790863
In grad_steps = 368, loss = 0.7065986394882202
In grad_steps = 369, loss = 0.6738802790641785
In grad_steps = 370, loss = 0.6958703398704529
In grad_steps = 371, loss = 0.6899605393409729
In grad_steps = 372, loss = 0.6748473048210144
In grad_steps = 373, loss = 0.7061684727668762
In grad_steps = 374, loss = 0.687317967414856
Beginning epoch 4
In grad_steps = 375, loss = 0.6633140444755554
In grad_steps = 376, loss = 0.7412818670272827
In grad_steps = 377, loss = 0.6707972288131714
In grad_steps = 378, loss = 0.7339648008346558
In grad_steps = 379, loss = 0.6310745477676392
In grad_steps = 380, loss = 0.6407473087310791
In grad_steps = 381, loss = 0.6814801692962646
In grad_steps = 382, loss = 0.6618693470954895
In grad_steps = 383, loss = 0.6084977984428406
In grad_steps = 384, loss = 0.7508029937744141
In grad_steps = 385, loss = 0.7559235095977783
In grad_steps = 386, loss = 0.7076547145843506
In grad_steps = 387, loss = 0.6557148098945618
In grad_steps = 388, loss = 0.6966468095779419
In grad_steps = 389, loss = 0.6857091188430786
In grad_steps = 390, loss = 0.6937931776046753
In grad_steps = 391, loss = 0.6486246585845947
In grad_steps = 392, loss = 0.7702723741531372
In grad_steps = 393, loss = 0.708554208278656
In grad_steps = 394, loss = 0.6147158741950989
In grad_steps = 395, loss = 0.6451094150543213
In grad_steps = 396, loss = 0.6590095162391663
In grad_steps = 397, loss = 0.6953670382499695
In grad_steps = 398, loss = 0.6795778870582581
In grad_steps = 399, loss = 0.6227811574935913
In grad_steps = 400, loss = 0.6165918111801147
In grad_steps = 401, loss = 0.6499453783035278
In grad_steps = 402, loss = 0.583151638507843
In grad_steps = 403, loss = 0.6588229537010193
In grad_steps = 404, loss = 0.7003310322761536
In grad_steps = 405, loss = 0.6109220385551453
In grad_steps = 406, loss = 0.4648406505584717
In grad_steps = 407, loss = 0.5615399479866028
In grad_steps = 408, loss = 0.5544275045394897
In grad_steps = 409, loss = 0.46351897716522217
In grad_steps = 410, loss = 0.4920956492424011
In grad_steps = 411, loss = 0.41236579418182373
In grad_steps = 412, loss = 0.4611814022064209
In grad_steps = 413, loss = 1.0063505172729492
In grad_steps = 414, loss = 0.4955556094646454
In grad_steps = 415, loss = 0.6570714116096497
In grad_steps = 416, loss = 0.4513409733772278
In grad_steps = 417, loss = 0.5298651456832886
In grad_steps = 418, loss = 0.5079549551010132
In grad_steps = 419, loss = 0.5624171495437622
In grad_steps = 420, loss = 0.5899714827537537
In grad_steps = 421, loss = 0.4474431574344635
In grad_steps = 422, loss = 0.5570741891860962
In grad_steps = 423, loss = 0.5658692717552185
In grad_steps = 424, loss = 0.35447922348976135
In grad_steps = 425, loss = 0.5409955978393555
In grad_steps = 426, loss = 0.3612641394138336
In grad_steps = 427, loss = 0.7932466268539429
In grad_steps = 428, loss = 0.6045637726783752
In grad_steps = 429, loss = 0.46793484687805176
In grad_steps = 430, loss = 0.6594693660736084
In grad_steps = 431, loss = 0.8570535182952881
In grad_steps = 432, loss = 0.5305190086364746
In grad_steps = 433, loss = 0.5711958408355713
In grad_steps = 434, loss = 0.501681387424469
In grad_steps = 435, loss = 0.5361875295639038
In grad_steps = 436, loss = 0.627760648727417
In grad_steps = 437, loss = 0.6160567998886108
In grad_steps = 438, loss = 0.551117479801178
In grad_steps = 439, loss = 0.5832430124282837
In grad_steps = 440, loss = 0.6161437034606934
In grad_steps = 441, loss = 0.5918880105018616
In grad_steps = 442, loss = 0.5500959753990173
In grad_steps = 443, loss = 0.5727940201759338
In grad_steps = 444, loss = 0.649265706539154
In grad_steps = 445, loss = 0.6540842056274414
In grad_steps = 446, loss = 0.48626676201820374
In grad_steps = 447, loss = 0.4200069308280945
In grad_steps = 448, loss = 0.37061265110969543
In grad_steps = 449, loss = 0.4698542356491089
In grad_steps = 450, loss = 0.3799673020839691
In grad_steps = 451, loss = 0.4510513246059418
In grad_steps = 452, loss = 0.5485201478004456
In grad_steps = 453, loss = 0.8081449866294861
In grad_steps = 454, loss = 0.43086689710617065
In grad_steps = 455, loss = 0.4952537715435028
In grad_steps = 456, loss = 0.3049871027469635
In grad_steps = 457, loss = 0.46914681792259216
In grad_steps = 458, loss = 0.42345309257507324
In grad_steps = 459, loss = 0.601707398891449
In grad_steps = 460, loss = 0.68039870262146
In grad_steps = 461, loss = 0.7288074493408203
In grad_steps = 462, loss = 0.6400882601737976
In grad_steps = 463, loss = 0.6074768304824829
In grad_steps = 464, loss = 0.7392246723175049
In grad_steps = 465, loss = 0.415734201669693
In grad_steps = 466, loss = 0.6337936520576477
In grad_steps = 467, loss = 0.7244334816932678
In grad_steps = 468, loss = 0.5703296065330505
In grad_steps = 469, loss = 0.5648024678230286
In grad_steps = 470, loss = 0.7949954271316528
In grad_steps = 471, loss = 0.6158445477485657
In grad_steps = 472, loss = 0.6183562278747559
In grad_steps = 473, loss = 0.6085084676742554
In grad_steps = 474, loss = 0.6711280345916748
In grad_steps = 475, loss = 0.7828466296195984
In grad_steps = 476, loss = 0.7003802061080933
In grad_steps = 477, loss = 0.7722920775413513
In grad_steps = 478, loss = 0.7417457699775696
In grad_steps = 479, loss = 0.6539751887321472
In grad_steps = 480, loss = 0.6618150472640991
In grad_steps = 481, loss = 0.5983446836471558
In grad_steps = 482, loss = 0.7289679646492004
In grad_steps = 483, loss = 0.7460227012634277
In grad_steps = 484, loss = 0.8147526383399963
In grad_steps = 485, loss = 0.646343469619751
In grad_steps = 486, loss = 0.6947011947631836
In grad_steps = 487, loss = 0.6930878758430481
In grad_steps = 488, loss = 0.6184595823287964
In grad_steps = 489, loss = 0.6834449172019958
In grad_steps = 490, loss = 0.7490718364715576
In grad_steps = 491, loss = 0.7010675668716431
In grad_steps = 492, loss = 0.6956874132156372
In grad_steps = 493, loss = 0.6741554141044617
In grad_steps = 494, loss = 0.7149716019630432
In grad_steps = 495, loss = 0.6628407835960388
In grad_steps = 496, loss = 0.6434855461120605
In grad_steps = 497, loss = 0.690987765789032
In grad_steps = 498, loss = 0.714733362197876
In grad_steps = 499, loss = 0.6539680361747742
Beginning epoch 5
In grad_steps = 500, loss = 0.6399626731872559
In grad_steps = 501, loss = 0.7116339206695557
In grad_steps = 502, loss = 0.6330606937408447
In grad_steps = 503, loss = 0.6728574633598328
In grad_steps = 504, loss = 0.6450273990631104
In grad_steps = 505, loss = 0.5967468023300171
In grad_steps = 506, loss = 0.6458932757377625
In grad_steps = 507, loss = 0.6216648817062378
In grad_steps = 508, loss = 0.5889225006103516
In grad_steps = 509, loss = 0.707190752029419
In grad_steps = 510, loss = 0.8132163286209106
In grad_steps = 511, loss = 0.6184818148612976
In grad_steps = 512, loss = 0.6158851385116577
In grad_steps = 513, loss = 0.678752064704895
In grad_steps = 514, loss = 0.6688315272331238
In grad_steps = 515, loss = 0.6862979531288147
In grad_steps = 516, loss = 0.6251712441444397
In grad_steps = 517, loss = 0.798775851726532
In grad_steps = 518, loss = 0.687098503112793
In grad_steps = 519, loss = 0.6554614305496216
In grad_steps = 520, loss = 0.5403152704238892
In grad_steps = 521, loss = 0.6478773951530457
In grad_steps = 522, loss = 0.6492614150047302
In grad_steps = 523, loss = 0.6092318892478943
In grad_steps = 524, loss = 0.6974402070045471
In grad_steps = 525, loss = 0.5731623768806458
In grad_steps = 526, loss = 0.5766412019729614
In grad_steps = 527, loss = 0.5502257347106934
In grad_steps = 528, loss = 0.5787991881370544
In grad_steps = 529, loss = 0.7183782458305359
In grad_steps = 530, loss = 0.5361917018890381
In grad_steps = 531, loss = 0.3693290650844574
In grad_steps = 532, loss = 0.4738599956035614
In grad_steps = 533, loss = 0.4982118010520935
In grad_steps = 534, loss = 0.3796331584453583
In grad_steps = 535, loss = 0.3857291340827942
In grad_steps = 536, loss = 0.3884675204753876
In grad_steps = 537, loss = 0.2655220925807953
In grad_steps = 538, loss = 0.32085609436035156
In grad_steps = 539, loss = 0.31280943751335144
In grad_steps = 540, loss = 0.2987430691719055
In grad_steps = 541, loss = 0.24604573845863342
In grad_steps = 542, loss = 0.37811803817749023
In grad_steps = 543, loss = 0.1363380402326584
In grad_steps = 544, loss = 0.6334239840507507
In grad_steps = 545, loss = 0.26658162474632263
In grad_steps = 546, loss = 0.08486342430114746
In grad_steps = 547, loss = 0.7531223893165588
In grad_steps = 548, loss = 0.47061681747436523
In grad_steps = 549, loss = 0.144303560256958
In grad_steps = 550, loss = 0.18058255314826965
In grad_steps = 551, loss = 0.21240265667438507
In grad_steps = 552, loss = 0.3089616894721985
In grad_steps = 553, loss = 0.37083134055137634
In grad_steps = 554, loss = 0.47547799348831177
In grad_steps = 555, loss = 0.5410268902778625
In grad_steps = 556, loss = 0.6967504620552063
In grad_steps = 557, loss = 0.5279792547225952
In grad_steps = 558, loss = 0.33957338333129883
In grad_steps = 559, loss = 0.5682428479194641
In grad_steps = 560, loss = 0.28500890731811523
In grad_steps = 561, loss = 0.49201500415802
In grad_steps = 562, loss = 0.5964616537094116
In grad_steps = 563, loss = 0.36042118072509766
In grad_steps = 564, loss = 0.7100546360015869
In grad_steps = 565, loss = 0.5732936859130859
In grad_steps = 566, loss = 0.37987813353538513
In grad_steps = 567, loss = 0.46057868003845215
In grad_steps = 568, loss = 0.493737131357193
In grad_steps = 569, loss = 0.3818681240081787
In grad_steps = 570, loss = 0.4857815206050873
In grad_steps = 571, loss = 0.3891453742980957
In grad_steps = 572, loss = 0.3197084665298462
In grad_steps = 573, loss = 0.17551440000534058
In grad_steps = 574, loss = 0.16622743010520935
In grad_steps = 575, loss = 0.3319045901298523
In grad_steps = 576, loss = 0.22766558825969696
In grad_steps = 577, loss = 0.16396819055080414
In grad_steps = 578, loss = 0.08602094650268555
In grad_steps = 579, loss = 0.07500678300857544
In grad_steps = 580, loss = 0.12201693654060364
In grad_steps = 581, loss = 0.31189626455307007
In grad_steps = 582, loss = 0.6536187529563904
In grad_steps = 583, loss = 0.4756162762641907
In grad_steps = 584, loss = 0.23891638219356537
In grad_steps = 585, loss = 0.09303887188434601
In grad_steps = 586, loss = 0.4064500629901886
In grad_steps = 587, loss = 0.5638303756713867
In grad_steps = 588, loss = 0.37195998430252075
In grad_steps = 589, loss = 0.5995596647262573
In grad_steps = 590, loss = 0.5680086612701416
In grad_steps = 591, loss = 1.3763456344604492
In grad_steps = 592, loss = 0.5881029367446899
In grad_steps = 593, loss = 0.5222927927970886
In grad_steps = 594, loss = 0.3962358236312866
In grad_steps = 595, loss = 0.49967315793037415
In grad_steps = 596, loss = 0.6650087237358093
In grad_steps = 597, loss = 0.8324522972106934
In grad_steps = 598, loss = 0.7294426560401917
In grad_steps = 599, loss = 0.6235311031341553
In grad_steps = 600, loss = 0.6230911016464233
In grad_steps = 601, loss = 0.6474676132202148
In grad_steps = 602, loss = 0.6359902024269104
In grad_steps = 603, loss = 0.6790221929550171
In grad_steps = 604, loss = 0.6992715001106262
In grad_steps = 605, loss = 0.6405715942382812
In grad_steps = 606, loss = 0.6923007369041443
In grad_steps = 607, loss = 0.6679331064224243
In grad_steps = 608, loss = 0.604934573173523
In grad_steps = 609, loss = 0.6165713667869568
In grad_steps = 610, loss = 0.561016321182251
In grad_steps = 611, loss = 0.6462108492851257
In grad_steps = 612, loss = 0.6894581317901611
In grad_steps = 613, loss = 0.5826904773712158
In grad_steps = 614, loss = 0.7430406212806702
In grad_steps = 615, loss = 0.7557756304740906
In grad_steps = 616, loss = 0.6834126114845276
In grad_steps = 617, loss = 0.6459461450576782
In grad_steps = 618, loss = 0.5952788591384888
In grad_steps = 619, loss = 0.650073230266571
In grad_steps = 620, loss = 0.661756694316864
In grad_steps = 621, loss = 0.6150939464569092
In grad_steps = 622, loss = 0.7569759488105774
In grad_steps = 623, loss = 0.6676605939865112
In grad_steps = 624, loss = 0.6282755136489868
Beginning epoch 6
In grad_steps = 625, loss = 0.5651937127113342
In grad_steps = 626, loss = 0.6521216034889221
In grad_steps = 627, loss = 0.6104921102523804
In grad_steps = 628, loss = 0.6450114846229553
In grad_steps = 629, loss = 0.5943113565444946
In grad_steps = 630, loss = 0.5573869347572327
In grad_steps = 631, loss = 0.6137426495552063
In grad_steps = 632, loss = 0.6462677121162415
In grad_steps = 633, loss = 0.5136293172836304
In grad_steps = 634, loss = 0.6829507350921631
In grad_steps = 635, loss = 0.6524530649185181
In grad_steps = 636, loss = 0.5719665884971619
In grad_steps = 637, loss = 0.4649861752986908
In grad_steps = 638, loss = 0.6407443284988403
In grad_steps = 639, loss = 0.47305533289909363
In grad_steps = 640, loss = 0.5803617238998413
In grad_steps = 641, loss = 0.5356947779655457
In grad_steps = 642, loss = 0.5925303101539612
In grad_steps = 643, loss = 0.5611183643341064
In grad_steps = 644, loss = 0.4557975232601166
In grad_steps = 645, loss = 0.44936901330947876
In grad_steps = 646, loss = 0.5848031640052795
In grad_steps = 647, loss = 0.5797290802001953
In grad_steps = 648, loss = 0.5705362558364868
In grad_steps = 649, loss = 0.5591559410095215
In grad_steps = 650, loss = 0.37446871399879456
In grad_steps = 651, loss = 0.5847401022911072
In grad_steps = 652, loss = 0.39576202630996704
In grad_steps = 653, loss = 0.5239415764808655
In grad_steps = 654, loss = 0.6084728837013245
In grad_steps = 655, loss = 0.3793060779571533
In grad_steps = 656, loss = 0.30177441239356995
In grad_steps = 657, loss = 0.35124167799949646
In grad_steps = 658, loss = 0.48409926891326904
In grad_steps = 659, loss = 0.22324591875076294
In grad_steps = 660, loss = 0.30742019414901733
In grad_steps = 661, loss = 0.35907068848609924
In grad_steps = 662, loss = 0.20905262231826782
In grad_steps = 663, loss = 0.18679259717464447
In grad_steps = 664, loss = 0.17152489721775055
In grad_steps = 665, loss = 0.3175487220287323
In grad_steps = 666, loss = 0.3205508291721344
In grad_steps = 667, loss = 0.17845241725444794
In grad_steps = 668, loss = 0.06295108795166016
In grad_steps = 669, loss = 0.5343234539031982
In grad_steps = 670, loss = 0.20633694529533386
In grad_steps = 671, loss = 0.40868332982063293
In grad_steps = 672, loss = 0.7481623888015747
In grad_steps = 673, loss = 0.26375094056129456
In grad_steps = 674, loss = 0.40166884660720825
In grad_steps = 675, loss = 0.4364120662212372
In grad_steps = 676, loss = 0.1861150711774826
In grad_steps = 677, loss = 0.2131192982196808
In grad_steps = 678, loss = 0.3886851370334625
In grad_steps = 679, loss = 0.36447447538375854
In grad_steps = 680, loss = 0.5127153992652893
In grad_steps = 681, loss = 0.4371791183948517
In grad_steps = 682, loss = 0.32663100957870483
In grad_steps = 683, loss = 0.2669062614440918
In grad_steps = 684, loss = 0.6395458579063416
In grad_steps = 685, loss = 0.3260544240474701
In grad_steps = 686, loss = 0.6197314262390137
In grad_steps = 687, loss = 0.3520181477069855
In grad_steps = 688, loss = 0.42518338561058044
In grad_steps = 689, loss = 0.4821070432662964
In grad_steps = 690, loss = 0.5699402689933777
In grad_steps = 691, loss = 0.24845777451992035
In grad_steps = 692, loss = 0.5056958198547363
In grad_steps = 693, loss = 0.3519931137561798
In grad_steps = 694, loss = 0.428877592086792
In grad_steps = 695, loss = 0.1755024492740631
In grad_steps = 696, loss = 0.19062627851963043
In grad_steps = 697, loss = 0.2531694769859314
In grad_steps = 698, loss = 0.2922709584236145
In grad_steps = 699, loss = 0.3276013731956482
In grad_steps = 700, loss = 0.17047809064388275
In grad_steps = 701, loss = 0.4090404510498047
In grad_steps = 702, loss = 0.19015105068683624
In grad_steps = 703, loss = 0.29495689272880554
In grad_steps = 704, loss = 0.6026135683059692
In grad_steps = 705, loss = 0.3598043918609619
In grad_steps = 706, loss = 0.21082660555839539
In grad_steps = 707, loss = 0.22708067297935486
In grad_steps = 708, loss = 0.3022615909576416
In grad_steps = 709, loss = 0.1959841251373291
In grad_steps = 710, loss = 0.2192436158657074
In grad_steps = 711, loss = 0.774910032749176
In grad_steps = 712, loss = 0.4715590178966522
In grad_steps = 713, loss = 0.23572848737239838
In grad_steps = 714, loss = 0.283534437417984
In grad_steps = 715, loss = 0.17334891855716705
In grad_steps = 716, loss = 0.3444129526615143
In grad_steps = 717, loss = 0.2679872512817383
In grad_steps = 718, loss = 0.22163164615631104
In grad_steps = 719, loss = 0.3117976486682892
In grad_steps = 720, loss = 0.514880359172821
In grad_steps = 721, loss = 0.5299437642097473
In grad_steps = 722, loss = 0.7682238221168518
In grad_steps = 723, loss = 0.6566794514656067
In grad_steps = 724, loss = 0.5920747518539429
In grad_steps = 725, loss = 0.7862043976783752
In grad_steps = 726, loss = 0.6760544180870056
In grad_steps = 727, loss = 0.5792257189750671
In grad_steps = 728, loss = 0.7232174873352051
In grad_steps = 729, loss = 0.6522538065910339
In grad_steps = 730, loss = 0.46624425053596497
In grad_steps = 731, loss = 0.6839650869369507
In grad_steps = 732, loss = 0.649917721748352
In grad_steps = 733, loss = 0.6155539155006409
In grad_steps = 734, loss = 0.7170966267585754
In grad_steps = 735, loss = 0.6360594034194946
In grad_steps = 736, loss = 0.6286791563034058
In grad_steps = 737, loss = 0.6138334274291992
In grad_steps = 738, loss = 0.5626339316368103
In grad_steps = 739, loss = 0.6795891523361206
In grad_steps = 740, loss = 0.7207898497581482
In grad_steps = 741, loss = 0.6734026074409485
In grad_steps = 742, loss = 0.6399814486503601
In grad_steps = 743, loss = 0.6315954327583313
In grad_steps = 744, loss = 0.5859129428863525
In grad_steps = 745, loss = 0.5939338803291321
In grad_steps = 746, loss = 0.5781550407409668
In grad_steps = 747, loss = 0.5962345600128174
In grad_steps = 748, loss = 0.6025732755661011
In grad_steps = 749, loss = 0.6439796090126038
Beginning epoch 7
In grad_steps = 750, loss = 0.46640339493751526
In grad_steps = 751, loss = 0.6845749020576477
In grad_steps = 752, loss = 0.5494633316993713
In grad_steps = 753, loss = 0.6088283061981201
In grad_steps = 754, loss = 0.5349788069725037
In grad_steps = 755, loss = 0.4928930997848511
In grad_steps = 756, loss = 0.574657678604126
In grad_steps = 757, loss = 0.5327977538108826
In grad_steps = 758, loss = 0.4068375825881958
In grad_steps = 759, loss = 0.6523033976554871
In grad_steps = 760, loss = 0.45157092809677124
In grad_steps = 761, loss = 0.5393635034561157
In grad_steps = 762, loss = 0.25730955600738525
In grad_steps = 763, loss = 0.4299176335334778
In grad_steps = 764, loss = 0.39605575799942017
In grad_steps = 765, loss = 0.3318207859992981
In grad_steps = 766, loss = 0.15564554929733276
In grad_steps = 767, loss = 0.44781365990638733
In grad_steps = 768, loss = 0.37814801931381226
In grad_steps = 769, loss = 0.33705779910087585
In grad_steps = 770, loss = 0.35863491892814636
In grad_steps = 771, loss = 0.2937837243080139
In grad_steps = 772, loss = 0.3850123882293701
In grad_steps = 773, loss = 0.12037396430969238
In grad_steps = 774, loss = 0.030480513349175453
In grad_steps = 775, loss = 0.25856813788414
In grad_steps = 776, loss = 0.3813907504081726
In grad_steps = 777, loss = 0.22037222981452942
In grad_steps = 778, loss = 0.24013826251029968
In grad_steps = 779, loss = 0.58705735206604
In grad_steps = 780, loss = 0.19938881695270538
In grad_steps = 781, loss = 0.11763077974319458
In grad_steps = 782, loss = 0.2704843282699585
In grad_steps = 783, loss = 0.1645398736000061
In grad_steps = 784, loss = 0.18359136581420898
In grad_steps = 785, loss = 0.2974778413772583
In grad_steps = 786, loss = 0.3873731791973114
In grad_steps = 787, loss = 0.17351479828357697
In grad_steps = 788, loss = 0.15967442095279694
In grad_steps = 789, loss = 0.10262665152549744
In grad_steps = 790, loss = 0.17836274206638336
In grad_steps = 791, loss = 0.2011905014514923
In grad_steps = 792, loss = 0.1941709667444229
In grad_steps = 793, loss = 0.22451844811439514
In grad_steps = 794, loss = 0.2953525185585022
In grad_steps = 795, loss = 0.10860699415206909
In grad_steps = 796, loss = 0.2006334513425827
In grad_steps = 797, loss = 0.2223055064678192
In grad_steps = 798, loss = 1.2473207712173462
In grad_steps = 799, loss = 0.47414255142211914
In grad_steps = 800, loss = 0.2424180656671524
In grad_steps = 801, loss = 0.0720626562833786
In grad_steps = 802, loss = 0.04729761928319931
In grad_steps = 803, loss = 0.37597528100013733
In grad_steps = 804, loss = 0.2181086242198944
In grad_steps = 805, loss = 0.46381819248199463
In grad_steps = 806, loss = 0.44191795587539673
In grad_steps = 807, loss = 0.23842363059520721
In grad_steps = 808, loss = 0.6497097015380859
In grad_steps = 809, loss = 0.256727010011673
In grad_steps = 810, loss = 0.1829201877117157
In grad_steps = 811, loss = 0.3172576129436493
In grad_steps = 812, loss = 0.2995966672897339
In grad_steps = 813, loss = 0.3878643214702606
In grad_steps = 814, loss = 0.2789319157600403
In grad_steps = 815, loss = 0.3778955340385437
In grad_steps = 816, loss = 0.2480667382478714
In grad_steps = 817, loss = 0.10455900430679321
In grad_steps = 818, loss = 0.1658165007829666
In grad_steps = 819, loss = 0.2875249683856964
In grad_steps = 820, loss = 0.5108579397201538
In grad_steps = 821, loss = 0.15565918385982513
In grad_steps = 822, loss = 0.25717824697494507
In grad_steps = 823, loss = 0.049766603857278824
In grad_steps = 824, loss = 0.02725113183259964
In grad_steps = 825, loss = 0.20034849643707275
In grad_steps = 826, loss = 0.1030309796333313
In grad_steps = 827, loss = 0.1256342977285385
In grad_steps = 828, loss = 0.15291711688041687
In grad_steps = 829, loss = 0.3274329602718353
In grad_steps = 830, loss = 0.2260039746761322
In grad_steps = 831, loss = 0.035898465663194656
In grad_steps = 832, loss = 0.3410385549068451
In grad_steps = 833, loss = 0.12881416082382202
In grad_steps = 834, loss = 0.15036311745643616
In grad_steps = 835, loss = 0.13840226829051971
In grad_steps = 836, loss = 0.10762501507997513
In grad_steps = 837, loss = 0.2902895510196686
In grad_steps = 838, loss = 0.07823742926120758
In grad_steps = 839, loss = 0.2894595265388489
In grad_steps = 840, loss = 0.29404792189598083
In grad_steps = 841, loss = 0.2260982245206833
In grad_steps = 842, loss = 0.04791977256536484
In grad_steps = 843, loss = 0.13924171030521393
In grad_steps = 844, loss = 0.06291224807500839
In grad_steps = 845, loss = 0.141109898686409
In grad_steps = 846, loss = 0.21526336669921875
In grad_steps = 847, loss = 0.12606416642665863
In grad_steps = 848, loss = 0.1706402599811554
In grad_steps = 849, loss = 0.24624325335025787
In grad_steps = 850, loss = 0.26287493109703064
In grad_steps = 851, loss = 0.6014578938484192
In grad_steps = 852, loss = 0.2976931929588318
In grad_steps = 853, loss = 0.32862603664398193
In grad_steps = 854, loss = 0.40470534563064575
In grad_steps = 855, loss = 0.35541391372680664
In grad_steps = 856, loss = 0.7189791202545166
In grad_steps = 857, loss = 0.7380297183990479
In grad_steps = 858, loss = 0.6289187669754028
In grad_steps = 859, loss = 0.6126605272293091
In grad_steps = 860, loss = 0.5031222701072693
In grad_steps = 861, loss = 0.7868114113807678
In grad_steps = 862, loss = 0.6316400170326233
In grad_steps = 863, loss = 0.46236172318458557
In grad_steps = 864, loss = 0.5227822661399841
In grad_steps = 865, loss = 0.7466366291046143
In grad_steps = 866, loss = 0.6351036429405212
In grad_steps = 867, loss = 0.6752517223358154
In grad_steps = 868, loss = 0.6393439173698425
In grad_steps = 869, loss = 0.5390532612800598
In grad_steps = 870, loss = 0.5150548219680786
In grad_steps = 871, loss = 0.5297636389732361
In grad_steps = 872, loss = 0.6840323805809021
In grad_steps = 873, loss = 0.5746346116065979
In grad_steps = 874, loss = 0.5169597864151001
Beginning epoch 8
In grad_steps = 875, loss = 0.42773517966270447
In grad_steps = 876, loss = 0.6797385215759277
In grad_steps = 877, loss = 0.5177690386772156
In grad_steps = 878, loss = 0.5557904243469238
In grad_steps = 879, loss = 0.4359133839607239
In grad_steps = 880, loss = 0.477230966091156
In grad_steps = 881, loss = 0.34826985001564026
In grad_steps = 882, loss = 0.583802342414856
In grad_steps = 883, loss = 0.2766932547092438
In grad_steps = 884, loss = 0.36355113983154297
In grad_steps = 885, loss = 0.3056122362613678
In grad_steps = 886, loss = 0.3169104754924774
In grad_steps = 887, loss = 0.30860501527786255
In grad_steps = 888, loss = 0.4077795147895813
In grad_steps = 889, loss = 0.513789176940918
In grad_steps = 890, loss = 0.27602025866508484
In grad_steps = 891, loss = 0.1908186972141266
In grad_steps = 892, loss = 0.15213295817375183
In grad_steps = 893, loss = 0.27298563718795776
In grad_steps = 894, loss = 0.24229611456394196
In grad_steps = 895, loss = 0.04568113759160042
In grad_steps = 896, loss = 0.29324060678482056
In grad_steps = 897, loss = 0.3178316652774811
In grad_steps = 898, loss = 0.0582742914557457
In grad_steps = 899, loss = 0.3670572340488434
In grad_steps = 900, loss = 0.1005282774567604
In grad_steps = 901, loss = 0.15025277435779572
In grad_steps = 902, loss = 0.16410720348358154
In grad_steps = 903, loss = 0.10984168946743011
In grad_steps = 904, loss = 0.4818874001502991
In grad_steps = 905, loss = 0.30732759833335876
In grad_steps = 906, loss = 0.058053188025951385
In grad_steps = 907, loss = 0.1258613020181656
In grad_steps = 908, loss = 0.09464156627655029
In grad_steps = 909, loss = 0.0578836128115654
In grad_steps = 910, loss = 0.08042103797197342
In grad_steps = 911, loss = 0.8295502066612244
In grad_steps = 912, loss = 0.22399285435676575
In grad_steps = 913, loss = 0.026824625208973885
In grad_steps = 914, loss = 0.031983718276023865
In grad_steps = 915, loss = 0.11381826549768448
In grad_steps = 916, loss = 0.31699901819229126
In grad_steps = 917, loss = 0.1871851235628128
In grad_steps = 918, loss = 0.3320935368537903
In grad_steps = 919, loss = 0.23206087946891785
In grad_steps = 920, loss = 0.4571395516395569
In grad_steps = 921, loss = 0.05514596030116081
In grad_steps = 922, loss = 0.34550803899765015
In grad_steps = 923, loss = 0.17709705233573914
In grad_steps = 924, loss = 0.18335369229316711
In grad_steps = 925, loss = 0.42190784215927124
In grad_steps = 926, loss = 0.16147878766059875
In grad_steps = 927, loss = 0.2710166275501251
In grad_steps = 928, loss = 0.543884813785553
In grad_steps = 929, loss = 0.42727333307266235
In grad_steps = 930, loss = 0.28059157729148865
In grad_steps = 931, loss = 0.4855295717716217
In grad_steps = 932, loss = 0.700066328048706
In grad_steps = 933, loss = 0.38098791241645813
In grad_steps = 934, loss = 0.37302735447883606
In grad_steps = 935, loss = 0.22328685224056244
In grad_steps = 936, loss = 0.24560749530792236
In grad_steps = 937, loss = 0.27882060408592224
In grad_steps = 938, loss = 0.20851102471351624
In grad_steps = 939, loss = 0.22022655606269836
In grad_steps = 940, loss = 0.1727624535560608
In grad_steps = 941, loss = 0.40058135986328125
In grad_steps = 942, loss = 0.19488978385925293
In grad_steps = 943, loss = 0.37226617336273193
In grad_steps = 944, loss = 0.2194688618183136
In grad_steps = 945, loss = 0.23743653297424316
In grad_steps = 946, loss = 0.10261285305023193
In grad_steps = 947, loss = 0.08137606084346771
In grad_steps = 948, loss = 0.08213263750076294
In grad_steps = 949, loss = 0.10108363628387451
In grad_steps = 950, loss = 0.39742130041122437
In grad_steps = 951, loss = 0.12781715393066406
In grad_steps = 952, loss = 0.22344794869422913
In grad_steps = 953, loss = 0.16288267076015472
In grad_steps = 954, loss = 0.1521492302417755
In grad_steps = 955, loss = 0.0842026025056839
In grad_steps = 956, loss = 0.10354986786842346
In grad_steps = 957, loss = 0.06420007348060608
In grad_steps = 958, loss = 0.06995579600334167
In grad_steps = 959, loss = 0.13060753047466278
In grad_steps = 960, loss = 0.02102905698120594
In grad_steps = 961, loss = 0.10785391926765442
In grad_steps = 962, loss = 0.10786031186580658
In grad_steps = 963, loss = 0.04172142595052719
In grad_steps = 964, loss = 0.047614920884370804
In grad_steps = 965, loss = 0.02806267701089382
In grad_steps = 966, loss = 0.4995865225791931
In grad_steps = 967, loss = 0.12097474932670593
In grad_steps = 968, loss = 0.3122187554836273
In grad_steps = 969, loss = 0.06596270948648453
In grad_steps = 970, loss = 0.12869936227798462
In grad_steps = 971, loss = 0.09312022477388382
In grad_steps = 972, loss = 0.25891706347465515
In grad_steps = 973, loss = 0.14401337504386902
In grad_steps = 974, loss = 0.03511996939778328
In grad_steps = 975, loss = 0.17119266092777252
In grad_steps = 976, loss = 0.11858447641134262
In grad_steps = 977, loss = 0.04392661899328232
In grad_steps = 978, loss = 0.07568715512752533
In grad_steps = 979, loss = 0.2670893371105194
In grad_steps = 980, loss = 0.1426856517791748
In grad_steps = 981, loss = 0.5033383369445801
In grad_steps = 982, loss = 0.42622771859169006
In grad_steps = 983, loss = 0.3961717188358307
In grad_steps = 984, loss = 0.16294802725315094
In grad_steps = 985, loss = 0.17301389575004578
In grad_steps = 986, loss = 0.8154351711273193
In grad_steps = 987, loss = 0.25416064262390137
In grad_steps = 988, loss = 0.4313623905181885
In grad_steps = 989, loss = 0.40809059143066406
In grad_steps = 990, loss = 0.7106719613075256
In grad_steps = 991, loss = 0.47899097204208374
In grad_steps = 992, loss = 1.0102458000183105
In grad_steps = 993, loss = 0.578377902507782
In grad_steps = 994, loss = 0.5582544803619385
In grad_steps = 995, loss = 0.3775923550128937
In grad_steps = 996, loss = 0.5557540059089661
In grad_steps = 997, loss = 0.6214075684547424
In grad_steps = 998, loss = 0.4407401978969574
In grad_steps = 999, loss = 0.5115476250648499
Elapsed time: 3356.238515853882 seconds for ensemble 1 with 8 epochs
LoRA instance 1 evaluation complete. Data saved to /pscratch/sd/t/tianle/lucid/other_source/SURP_2024/results/tmp_lora_ensemble/Llama3-8B-set-3c/test_data_instance_1_seed_10094.npz.
lora instance i = 1 Successfully finished.
Training lora instance 2
DEBUG: Total parameters: 8037076992
DEBUG: Trainable parameters: 6815744
Beginning epoch 1
In grad_steps = 0, loss = 0.7359974980354309
In grad_steps = 1, loss = 0.8009702563285828
In grad_steps = 2, loss = 0.7695653438568115
In grad_steps = 3, loss = 0.6503197550773621
In grad_steps = 4, loss = 0.9082274436950684
In grad_steps = 5, loss = 0.7085379958152771
In grad_steps = 6, loss = 0.6458439230918884
In grad_steps = 7, loss = 0.7706617712974548
In grad_steps = 8, loss = 0.5925168395042419
In grad_steps = 9, loss = 0.8638904094696045
In grad_steps = 10, loss = 0.8521996736526489
In grad_steps = 11, loss = 0.7610728740692139
In grad_steps = 12, loss = 0.7086969017982483
In grad_steps = 13, loss = 0.6487104892730713
In grad_steps = 14, loss = 0.7986992001533508
In grad_steps = 15, loss = 0.8137532472610474
In grad_steps = 16, loss = 0.7148356437683105
In grad_steps = 17, loss = 0.8061079978942871
In grad_steps = 18, loss = 0.6992557048797607
In grad_steps = 19, loss = 0.7273297905921936
In grad_steps = 20, loss = 0.7295624017715454
In grad_steps = 21, loss = 0.7699059247970581
In grad_steps = 22, loss = 0.7271610498428345
In grad_steps = 23, loss = 0.7679815292358398
In grad_steps = 24, loss = 0.7528529167175293
In grad_steps = 25, loss = 0.7011154890060425
In grad_steps = 26, loss = 0.6990393996238708
In grad_steps = 27, loss = 0.6332624554634094
In grad_steps = 28, loss = 0.7377608418464661
In grad_steps = 29, loss = 0.7410069704055786
In grad_steps = 30, loss = 0.7778689861297607
In grad_steps = 31, loss = 0.5691928267478943
In grad_steps = 32, loss = 0.7626011967658997
In grad_steps = 33, loss = 0.7510250806808472
In grad_steps = 34, loss = 0.6447694301605225
In grad_steps = 35, loss = 0.6680260896682739
In grad_steps = 36, loss = 0.653448760509491
In grad_steps = 37, loss = 0.6874132752418518
In grad_steps = 38, loss = 0.7762288451194763
In grad_steps = 39, loss = 0.7238098978996277
In grad_steps = 40, loss = 0.7048447728157043
In grad_steps = 41, loss = 0.7147959470748901
In grad_steps = 42, loss = 0.6669387221336365
In grad_steps = 43, loss = 0.7078213691711426
In grad_steps = 44, loss = 0.7324451208114624
In grad_steps = 45, loss = 0.7562766075134277
In grad_steps = 46, loss = 0.7824593782424927
In grad_steps = 47, loss = 0.7433298826217651
In grad_steps = 48, loss = 0.7005982398986816
In grad_steps = 49, loss = 0.7009392380714417
In grad_steps = 50, loss = 0.6841868162155151
In grad_steps = 51, loss = 0.6868282556533813
In grad_steps = 52, loss = 0.7395806312561035
In grad_steps = 53, loss = 0.6631437540054321
In grad_steps = 54, loss = 0.7756038904190063
In grad_steps = 55, loss = 0.7437053322792053
In grad_steps = 56, loss = 0.7080520391464233
In grad_steps = 57, loss = 0.7159508466720581
In grad_steps = 58, loss = 0.7437395453453064
In grad_steps = 59, loss = 0.6892548203468323
In grad_steps = 60, loss = 0.6857438683509827
In grad_steps = 61, loss = 0.7128727436065674
In grad_steps = 62, loss = 0.7221396565437317
In grad_steps = 63, loss = 0.7167373299598694
In grad_steps = 64, loss = 0.6671173572540283
In grad_steps = 65, loss = 0.6838730573654175
In grad_steps = 66, loss = 0.7374753952026367
In grad_steps = 67, loss = 0.6895479559898376
In grad_steps = 68, loss = 0.7072086334228516
In grad_steps = 69, loss = 0.6931666731834412
In grad_steps = 70, loss = 0.699928879737854
In grad_steps = 71, loss = 0.672986626625061
In grad_steps = 72, loss = 0.7090413570404053
In grad_steps = 73, loss = 0.703201413154602
In grad_steps = 74, loss = 0.6839542984962463
In grad_steps = 75, loss = 0.6818647980690002
In grad_steps = 76, loss = 0.7125365138053894
In grad_steps = 77, loss = 0.7213864326477051
In grad_steps = 78, loss = 0.6937894821166992
In grad_steps = 79, loss = 0.68134605884552
In grad_steps = 80, loss = 0.7216960191726685
In grad_steps = 81, loss = 0.7077859044075012
In grad_steps = 82, loss = 0.7027692794799805
In grad_steps = 83, loss = 0.7002375721931458
In grad_steps = 84, loss = 0.6675170660018921
In grad_steps = 85, loss = 0.6614536046981812
In grad_steps = 86, loss = 0.7724534273147583
In grad_steps = 87, loss = 0.6867674589157104
In grad_steps = 88, loss = 0.6926633715629578
In grad_steps = 89, loss = 0.7240135073661804
In grad_steps = 90, loss = 0.7210721969604492
In grad_steps = 91, loss = 0.7246927618980408
In grad_steps = 92, loss = 0.684935450553894
In grad_steps = 93, loss = 0.7253391146659851
In grad_steps = 94, loss = 0.7019068002700806
In grad_steps = 95, loss = 0.6613345742225647
In grad_steps = 96, loss = 0.6694939732551575
In grad_steps = 97, loss = 0.6808274388313293
In grad_steps = 98, loss = 0.6995460391044617
In grad_steps = 99, loss = 0.7021995782852173
In grad_steps = 100, loss = 0.7108404040336609
In grad_steps = 101, loss = 0.6794633269309998
In grad_steps = 102, loss = 0.703991711139679
In grad_steps = 103, loss = 0.7168952822685242
In grad_steps = 104, loss = 0.7254905104637146
In grad_steps = 105, loss = 0.6926772594451904
In grad_steps = 106, loss = 0.6974071860313416
In grad_steps = 107, loss = 0.6959697008132935
In grad_steps = 108, loss = 0.6996461749076843
In grad_steps = 109, loss = 0.7078786492347717
In grad_steps = 110, loss = 0.6922253370285034
In grad_steps = 111, loss = 0.6770247220993042
In grad_steps = 112, loss = 0.6906329393386841
In grad_steps = 113, loss = 0.6459400057792664
In grad_steps = 114, loss = 0.7258179783821106
In grad_steps = 115, loss = 0.7342004179954529
In grad_steps = 116, loss = 0.6471343636512756
In grad_steps = 117, loss = 0.706600546836853
In grad_steps = 118, loss = 0.7314239144325256
In grad_steps = 119, loss = 0.6711882948875427
In grad_steps = 120, loss = 0.7019614577293396
In grad_steps = 121, loss = 0.705481767654419
In grad_steps = 122, loss = 0.6805427074432373
In grad_steps = 123, loss = 0.7473798394203186
In grad_steps = 124, loss = 0.7188695073127747
Beginning epoch 2
In grad_steps = 125, loss = 0.6980380415916443
In grad_steps = 126, loss = 0.6841525435447693
In grad_steps = 127, loss = 0.6885989308357239
In grad_steps = 128, loss = 0.6991879343986511
In grad_steps = 129, loss = 0.6819396018981934
In grad_steps = 130, loss = 0.6709629893302917
In grad_steps = 131, loss = 0.6849810481071472
In grad_steps = 132, loss = 0.6778533458709717
In grad_steps = 133, loss = 0.6371276378631592
In grad_steps = 134, loss = 0.7264745235443115
In grad_steps = 135, loss = 0.7497996091842651
In grad_steps = 136, loss = 0.7314476370811462
In grad_steps = 137, loss = 0.7218772172927856
In grad_steps = 138, loss = 0.7352705597877502
In grad_steps = 139, loss = 0.676818311214447
In grad_steps = 140, loss = 0.683727502822876
In grad_steps = 141, loss = 0.7125728130340576
In grad_steps = 142, loss = 0.6826974153518677
In grad_steps = 143, loss = 0.6840664148330688
In grad_steps = 144, loss = 0.6846562623977661
In grad_steps = 145, loss = 0.7015886902809143
In grad_steps = 146, loss = 0.6924660205841064
In grad_steps = 147, loss = 0.6931063532829285
In grad_steps = 148, loss = 0.6809346079826355
In grad_steps = 149, loss = 0.6515296101570129
In grad_steps = 150, loss = 0.6295572519302368
In grad_steps = 151, loss = 0.7050300240516663
In grad_steps = 152, loss = 0.6246731281280518
In grad_steps = 153, loss = 0.7208811044692993
In grad_steps = 154, loss = 0.7419552803039551
In grad_steps = 155, loss = 0.750377893447876
In grad_steps = 156, loss = 0.5737302899360657
In grad_steps = 157, loss = 0.7222212553024292
In grad_steps = 158, loss = 0.7167396545410156
In grad_steps = 159, loss = 0.637759268283844
In grad_steps = 160, loss = 0.660193681716919
In grad_steps = 161, loss = 0.6595121622085571
In grad_steps = 162, loss = 0.6797692179679871
In grad_steps = 163, loss = 0.7446944117546082
In grad_steps = 164, loss = 0.7206397652626038
In grad_steps = 165, loss = 0.7009661197662354
In grad_steps = 166, loss = 0.6902579665184021
In grad_steps = 167, loss = 0.6980701088905334
In grad_steps = 168, loss = 0.6543803215026855
In grad_steps = 169, loss = 0.7051056027412415
In grad_steps = 170, loss = 0.723330557346344
In grad_steps = 171, loss = 0.7272174954414368
In grad_steps = 172, loss = 0.7303581833839417
In grad_steps = 173, loss = 0.7007985711097717
In grad_steps = 174, loss = 0.6925135254859924
In grad_steps = 175, loss = 0.6936315894126892
In grad_steps = 176, loss = 0.6899232864379883
In grad_steps = 177, loss = 0.6952981948852539
In grad_steps = 178, loss = 0.6881362795829773
In grad_steps = 179, loss = 0.7009092569351196
In grad_steps = 180, loss = 0.6936025023460388
In grad_steps = 181, loss = 0.6934744119644165
In grad_steps = 182, loss = 0.6979027390480042
In grad_steps = 183, loss = 0.7066246271133423
In grad_steps = 184, loss = 0.6746541857719421
In grad_steps = 185, loss = 0.6864293813705444
In grad_steps = 186, loss = 0.6927288174629211
In grad_steps = 187, loss = 0.6928120851516724
In grad_steps = 188, loss = 0.6886095404624939
In grad_steps = 189, loss = 0.6808539628982544
In grad_steps = 190, loss = 0.6858516931533813
In grad_steps = 191, loss = 0.7034492492675781
In grad_steps = 192, loss = 0.6865336894989014
In grad_steps = 193, loss = 0.699070930480957
In grad_steps = 194, loss = 0.6861606240272522
In grad_steps = 195, loss = 0.6943466663360596
In grad_steps = 196, loss = 0.6947497129440308
In grad_steps = 197, loss = 0.6889764070510864
In grad_steps = 198, loss = 0.6707397699356079
In grad_steps = 199, loss = 0.684886634349823
In grad_steps = 200, loss = 0.6965179443359375
In grad_steps = 201, loss = 0.6945009231567383
In grad_steps = 202, loss = 0.6919659972190857
In grad_steps = 203, loss = 0.6784484386444092
In grad_steps = 204, loss = 0.688839316368103
In grad_steps = 205, loss = 0.7105562090873718
In grad_steps = 206, loss = 0.6848735213279724
In grad_steps = 207, loss = 0.6955730319023132
In grad_steps = 208, loss = 0.700447678565979
In grad_steps = 209, loss = 0.668774425983429
In grad_steps = 210, loss = 0.650197446346283
In grad_steps = 211, loss = 0.7664762735366821
In grad_steps = 212, loss = 0.6848883628845215
In grad_steps = 213, loss = 0.68267822265625
In grad_steps = 214, loss = 0.7204932570457458
In grad_steps = 215, loss = 0.7077016234397888
In grad_steps = 216, loss = 0.7277783751487732
In grad_steps = 217, loss = 0.6820735931396484
In grad_steps = 218, loss = 0.7028949856758118
In grad_steps = 219, loss = 0.6900924444198608
In grad_steps = 220, loss = 0.6696491837501526
In grad_steps = 221, loss = 0.6664382219314575
In grad_steps = 222, loss = 0.6880499124526978
In grad_steps = 223, loss = 0.7028742432594299
In grad_steps = 224, loss = 0.694166362285614
In grad_steps = 225, loss = 0.7091350555419922
In grad_steps = 226, loss = 0.6806768774986267
In grad_steps = 227, loss = 0.7020601034164429
In grad_steps = 228, loss = 0.7144864797592163
In grad_steps = 229, loss = 0.7273349761962891
In grad_steps = 230, loss = 0.6911365985870361
In grad_steps = 231, loss = 0.7047273516654968
In grad_steps = 232, loss = 0.6993220448493958
In grad_steps = 233, loss = 0.6894789934158325
In grad_steps = 234, loss = 0.6993429660797119
In grad_steps = 235, loss = 0.6875091791152954
In grad_steps = 236, loss = 0.6766310930252075
In grad_steps = 237, loss = 0.689000129699707
In grad_steps = 238, loss = 0.6523093581199646
In grad_steps = 239, loss = 0.7217706441879272
In grad_steps = 240, loss = 0.7310041189193726
In grad_steps = 241, loss = 0.6463242769241333
In grad_steps = 242, loss = 0.7062868475914001
In grad_steps = 243, loss = 0.7205008864402771
In grad_steps = 244, loss = 0.6682343482971191
In grad_steps = 245, loss = 0.6998122334480286
In grad_steps = 246, loss = 0.7032203674316406
In grad_steps = 247, loss = 0.6779201626777649
In grad_steps = 248, loss = 0.7455366253852844
In grad_steps = 249, loss = 0.7196261286735535
Beginning epoch 3
In grad_steps = 250, loss = 0.6993721127510071
In grad_steps = 251, loss = 0.6751859188079834
In grad_steps = 252, loss = 0.6863688230514526
In grad_steps = 253, loss = 0.6937945485115051
In grad_steps = 254, loss = 0.6796442866325378
In grad_steps = 255, loss = 0.6678388118743896
In grad_steps = 256, loss = 0.6812839508056641
In grad_steps = 257, loss = 0.6732506155967712
In grad_steps = 258, loss = 0.6312634348869324
In grad_steps = 259, loss = 0.730308473110199
In grad_steps = 260, loss = 0.7571728825569153
In grad_steps = 261, loss = 0.727440357208252
In grad_steps = 262, loss = 0.7106970548629761
In grad_steps = 263, loss = 0.7355941534042358
In grad_steps = 264, loss = 0.6721184253692627
In grad_steps = 265, loss = 0.6802260279655457
In grad_steps = 266, loss = 0.69337397813797
In grad_steps = 267, loss = 0.6817362308502197
In grad_steps = 268, loss = 0.6738084554672241
In grad_steps = 269, loss = 0.6800289154052734
In grad_steps = 270, loss = 0.681035578250885
In grad_steps = 271, loss = 0.6865689158439636
In grad_steps = 272, loss = 0.6891379952430725
In grad_steps = 273, loss = 0.6755474805831909
In grad_steps = 274, loss = 0.6343654990196228
In grad_steps = 275, loss = 0.627402663230896
In grad_steps = 276, loss = 0.6943046450614929
In grad_steps = 277, loss = 0.5965663194656372
In grad_steps = 278, loss = 0.7045338153839111
In grad_steps = 279, loss = 0.7696804404258728
In grad_steps = 280, loss = 0.7020663619041443
In grad_steps = 281, loss = 0.5326352119445801
In grad_steps = 282, loss = 0.6508123874664307
In grad_steps = 283, loss = 0.6867237687110901
In grad_steps = 284, loss = 0.6172759532928467
In grad_steps = 285, loss = 0.6518795490264893
In grad_steps = 286, loss = 0.6558579802513123
In grad_steps = 287, loss = 0.6702427268028259
In grad_steps = 288, loss = 0.6751700043678284
In grad_steps = 289, loss = 0.6927013397216797
In grad_steps = 290, loss = 0.6498370170593262
In grad_steps = 291, loss = 0.6408757567405701
In grad_steps = 292, loss = 0.6208918690681458
In grad_steps = 293, loss = 0.5086484551429749
In grad_steps = 294, loss = 0.9257451295852661
In grad_steps = 295, loss = 0.6675692200660706
In grad_steps = 296, loss = 0.6890540719032288
In grad_steps = 297, loss = 0.7395004034042358
In grad_steps = 298, loss = 0.7016568183898926
In grad_steps = 299, loss = 0.6145961880683899
In grad_steps = 300, loss = 0.6578963994979858
In grad_steps = 301, loss = 0.6601771116256714
In grad_steps = 302, loss = 0.7489975094795227
In grad_steps = 303, loss = 0.6657353043556213
In grad_steps = 304, loss = 0.6887935400009155
In grad_steps = 305, loss = 0.6851314306259155
In grad_steps = 306, loss = 0.7041337490081787
In grad_steps = 307, loss = 0.6746061444282532
In grad_steps = 308, loss = 0.6253209114074707
In grad_steps = 309, loss = 0.746414840221405
In grad_steps = 310, loss = 0.661129891872406
In grad_steps = 311, loss = 0.7476651072502136
In grad_steps = 312, loss = 0.713606059551239
In grad_steps = 313, loss = 0.7028093338012695
In grad_steps = 314, loss = 0.6640021800994873
In grad_steps = 315, loss = 0.6832480430603027
In grad_steps = 316, loss = 0.6840696930885315
In grad_steps = 317, loss = 0.6900865435600281
In grad_steps = 318, loss = 0.6645725965499878
In grad_steps = 319, loss = 0.6435775756835938
In grad_steps = 320, loss = 0.7328917980194092
In grad_steps = 321, loss = 0.6260989904403687
In grad_steps = 322, loss = 0.7135167717933655
In grad_steps = 323, loss = 0.6286145448684692
In grad_steps = 324, loss = 0.6536604166030884
In grad_steps = 325, loss = 0.6792396306991577
In grad_steps = 326, loss = 0.6882888674736023
In grad_steps = 327, loss = 0.6962109804153442
In grad_steps = 328, loss = 0.6576484441757202
In grad_steps = 329, loss = 0.6892820596694946
In grad_steps = 330, loss = 0.705602765083313
In grad_steps = 331, loss = 0.6140251755714417
In grad_steps = 332, loss = 0.6766822934150696
In grad_steps = 333, loss = 0.7748420238494873
In grad_steps = 334, loss = 0.6145391464233398
In grad_steps = 335, loss = 0.5836151242256165
In grad_steps = 336, loss = 0.8690831661224365
In grad_steps = 337, loss = 0.7031153440475464
In grad_steps = 338, loss = 0.6696915626525879
In grad_steps = 339, loss = 0.7463868856430054
In grad_steps = 340, loss = 0.591394305229187
In grad_steps = 341, loss = 0.6703625321388245
In grad_steps = 342, loss = 0.6947411894798279
In grad_steps = 343, loss = 0.6207002401351929
In grad_steps = 344, loss = 0.6392744779586792
In grad_steps = 345, loss = 0.7494708895683289
In grad_steps = 346, loss = 0.6844077706336975
In grad_steps = 347, loss = 0.7357527613639832
In grad_steps = 348, loss = 0.6962981820106506
In grad_steps = 349, loss = 0.6911435723304749
In grad_steps = 350, loss = 0.7404593229293823
In grad_steps = 351, loss = 0.6715086698532104
In grad_steps = 352, loss = 0.7213118076324463
In grad_steps = 353, loss = 0.7697248458862305
In grad_steps = 354, loss = 0.7565984725952148
In grad_steps = 355, loss = 0.676052451133728
In grad_steps = 356, loss = 0.7092999219894409
In grad_steps = 357, loss = 0.7055808901786804
In grad_steps = 358, loss = 0.675309419631958
In grad_steps = 359, loss = 0.6964176297187805
In grad_steps = 360, loss = 0.6690313220024109
In grad_steps = 361, loss = 0.6655325293540955
In grad_steps = 362, loss = 0.6931138634681702
In grad_steps = 363, loss = 0.63351970911026
In grad_steps = 364, loss = 0.7364405989646912
In grad_steps = 365, loss = 0.7457409501075745
In grad_steps = 366, loss = 0.6461756825447083
In grad_steps = 367, loss = 0.7108572721481323
In grad_steps = 368, loss = 0.6963905096054077
In grad_steps = 369, loss = 0.6691544651985168
In grad_steps = 370, loss = 0.7025603652000427
In grad_steps = 371, loss = 0.677781343460083
In grad_steps = 372, loss = 0.677894115447998
In grad_steps = 373, loss = 0.6989122033119202
In grad_steps = 374, loss = 0.6854890584945679
Beginning epoch 4
In grad_steps = 375, loss = 0.6756237149238586
In grad_steps = 376, loss = 0.7127465605735779
In grad_steps = 377, loss = 0.6716291904449463
In grad_steps = 378, loss = 0.7165791988372803
In grad_steps = 379, loss = 0.6237704157829285
In grad_steps = 380, loss = 0.6340155601501465
In grad_steps = 381, loss = 0.666118323802948
In grad_steps = 382, loss = 0.6538158655166626
In grad_steps = 383, loss = 0.5888858437538147
In grad_steps = 384, loss = 0.75977623462677
In grad_steps = 385, loss = 0.7764672040939331
In grad_steps = 386, loss = 0.6679410338401794
In grad_steps = 387, loss = 0.6064140796661377
In grad_steps = 388, loss = 0.6882675886154175
In grad_steps = 389, loss = 0.6825682520866394
In grad_steps = 390, loss = 0.6932079792022705
In grad_steps = 391, loss = 0.6487511396408081
In grad_steps = 392, loss = 0.802967369556427
In grad_steps = 393, loss = 0.6977112293243408
In grad_steps = 394, loss = 0.6406053304672241
In grad_steps = 395, loss = 0.6447072625160217
In grad_steps = 396, loss = 0.653414785861969
In grad_steps = 397, loss = 0.6786230206489563
In grad_steps = 398, loss = 0.6796042323112488
In grad_steps = 399, loss = 0.654231607913971
In grad_steps = 400, loss = 0.6531784534454346
In grad_steps = 401, loss = 0.6382656097412109
In grad_steps = 402, loss = 0.5856695771217346
In grad_steps = 403, loss = 0.6233488321304321
In grad_steps = 404, loss = 0.6770637631416321
In grad_steps = 405, loss = 0.6119400262832642
In grad_steps = 406, loss = 0.42323654890060425
In grad_steps = 407, loss = 0.576576828956604
In grad_steps = 408, loss = 0.5536298751831055
In grad_steps = 409, loss = 0.4444220960140228
In grad_steps = 410, loss = 0.41685405373573303
In grad_steps = 411, loss = 0.3966224193572998
In grad_steps = 412, loss = 0.4262395203113556
In grad_steps = 413, loss = 0.9246051907539368
In grad_steps = 414, loss = 0.31540390849113464
In grad_steps = 415, loss = 0.5316470265388489
In grad_steps = 416, loss = 0.42992591857910156
In grad_steps = 417, loss = 0.43372586369514465
In grad_steps = 418, loss = 0.4422638416290283
In grad_steps = 419, loss = 0.5374661684036255
In grad_steps = 420, loss = 0.5633804202079773
In grad_steps = 421, loss = 0.44989243149757385
In grad_steps = 422, loss = 0.517946183681488
In grad_steps = 423, loss = 0.5173906683921814
In grad_steps = 424, loss = 0.3785184621810913
In grad_steps = 425, loss = 0.3592332899570465
In grad_steps = 426, loss = 0.37138479948043823
In grad_steps = 427, loss = 0.6380962133407593
In grad_steps = 428, loss = 0.8365311622619629
In grad_steps = 429, loss = 0.6148651838302612
In grad_steps = 430, loss = 0.7913559675216675
In grad_steps = 431, loss = 0.8302114009857178
In grad_steps = 432, loss = 0.6670558452606201
In grad_steps = 433, loss = 0.720433235168457
In grad_steps = 434, loss = 0.5196794867515564
In grad_steps = 435, loss = 0.634340226650238
In grad_steps = 436, loss = 0.6720308661460876
In grad_steps = 437, loss = 0.5930356383323669
In grad_steps = 438, loss = 0.580173909664154
In grad_steps = 439, loss = 0.6762607097625732
In grad_steps = 440, loss = 0.6718419790267944
In grad_steps = 441, loss = 0.6756826043128967
In grad_steps = 442, loss = 0.6746668219566345
In grad_steps = 443, loss = 0.6001497507095337
In grad_steps = 444, loss = 0.6701286435127258
In grad_steps = 445, loss = 0.7040601968765259
In grad_steps = 446, loss = 0.6802676916122437
In grad_steps = 447, loss = 0.6144787669181824
In grad_steps = 448, loss = 0.5445506572723389
In grad_steps = 449, loss = 0.6342355608940125
In grad_steps = 450, loss = 0.5728452801704407
In grad_steps = 451, loss = 0.6213654279708862
In grad_steps = 452, loss = 0.7069725394248962
In grad_steps = 453, loss = 0.7034326791763306
In grad_steps = 454, loss = 0.6245025396347046
In grad_steps = 455, loss = 0.6818932294845581
In grad_steps = 456, loss = 0.5367905497550964
In grad_steps = 457, loss = 0.6153625845909119
In grad_steps = 458, loss = 0.7602618932723999
In grad_steps = 459, loss = 0.620816171169281
In grad_steps = 460, loss = 0.46845537424087524
In grad_steps = 461, loss = 0.7668604850769043
In grad_steps = 462, loss = 0.570185661315918
In grad_steps = 463, loss = 0.6348323225975037
In grad_steps = 464, loss = 0.7442566752433777
In grad_steps = 465, loss = 0.48234066367149353
In grad_steps = 466, loss = 0.6377865672111511
In grad_steps = 467, loss = 0.7178148031234741
In grad_steps = 468, loss = 0.6083500385284424
In grad_steps = 469, loss = 0.519481897354126
In grad_steps = 470, loss = 0.7810485363006592
In grad_steps = 471, loss = 0.5029260516166687
In grad_steps = 472, loss = 0.6174232959747314
In grad_steps = 473, loss = 0.6448249816894531
In grad_steps = 474, loss = 0.5814048051834106
In grad_steps = 475, loss = 0.8498014807701111
In grad_steps = 476, loss = 0.6028159260749817
In grad_steps = 477, loss = 0.6445659399032593
In grad_steps = 478, loss = 0.651316225528717
In grad_steps = 479, loss = 0.6118358969688416
In grad_steps = 480, loss = 0.6634671688079834
In grad_steps = 481, loss = 0.661231279373169
In grad_steps = 482, loss = 0.7107481956481934
In grad_steps = 483, loss = 0.6858604550361633
In grad_steps = 484, loss = 0.7836422920227051
In grad_steps = 485, loss = 0.6236339211463928
In grad_steps = 486, loss = 0.6326946020126343
In grad_steps = 487, loss = 0.6639195680618286
In grad_steps = 488, loss = 0.6091176271438599
In grad_steps = 489, loss = 0.7564823627471924
In grad_steps = 490, loss = 0.7651380300521851
In grad_steps = 491, loss = 0.6928789019584656
In grad_steps = 492, loss = 0.6857699155807495
In grad_steps = 493, loss = 0.6510120630264282
In grad_steps = 494, loss = 0.6867802739143372
In grad_steps = 495, loss = 0.687268078327179
In grad_steps = 496, loss = 0.67529296875
In grad_steps = 497, loss = 0.6586041450500488
In grad_steps = 498, loss = 0.6688978672027588
In grad_steps = 499, loss = 0.6325123906135559
Beginning epoch 5
In grad_steps = 500, loss = 0.6709811091423035
In grad_steps = 501, loss = 0.7063881754875183
In grad_steps = 502, loss = 0.6536774635314941
In grad_steps = 503, loss = 0.7151045799255371
In grad_steps = 504, loss = 0.6247055530548096
In grad_steps = 505, loss = 0.5979710221290588
In grad_steps = 506, loss = 0.6346713304519653
In grad_steps = 507, loss = 0.6569799184799194
In grad_steps = 508, loss = 0.5383758544921875
In grad_steps = 509, loss = 0.6900784373283386
In grad_steps = 510, loss = 0.708396315574646
In grad_steps = 511, loss = 0.5878829956054688
In grad_steps = 512, loss = 0.5297889113426208
In grad_steps = 513, loss = 0.6760339736938477
In grad_steps = 514, loss = 0.6277150511741638
In grad_steps = 515, loss = 0.594938337802887
In grad_steps = 516, loss = 0.5434625148773193
In grad_steps = 517, loss = 0.6855354905128479
In grad_steps = 518, loss = 0.6438530683517456
In grad_steps = 519, loss = 0.6132445931434631
In grad_steps = 520, loss = 0.5026747584342957
In grad_steps = 521, loss = 0.6207895874977112
In grad_steps = 522, loss = 0.5708118081092834
In grad_steps = 523, loss = 0.5989480018615723
In grad_steps = 524, loss = 0.5957584381103516
In grad_steps = 525, loss = 0.3613268733024597
In grad_steps = 526, loss = 0.6908035278320312
In grad_steps = 527, loss = 0.3574569821357727
In grad_steps = 528, loss = 0.5102688074111938
In grad_steps = 529, loss = 0.7901267409324646
In grad_steps = 530, loss = 0.44587233662605286
In grad_steps = 531, loss = 0.2845637798309326
In grad_steps = 532, loss = 0.34587186574935913
In grad_steps = 533, loss = 0.4009959399700165
In grad_steps = 534, loss = 0.31417733430862427
In grad_steps = 535, loss = 0.2273702621459961
In grad_steps = 536, loss = 0.2275090366601944
In grad_steps = 537, loss = 0.17711493372917175
In grad_steps = 538, loss = 0.11738155037164688
In grad_steps = 539, loss = 0.17364910244941711
In grad_steps = 540, loss = 0.2988641858100891
In grad_steps = 541, loss = 0.4978240132331848
In grad_steps = 542, loss = 0.7074142694473267
In grad_steps = 543, loss = 0.25393712520599365
In grad_steps = 544, loss = 0.7872124314308167
In grad_steps = 545, loss = 0.41767874360084534
In grad_steps = 546, loss = 0.18420624732971191
In grad_steps = 547, loss = 0.42993372678756714
In grad_steps = 548, loss = 0.5896857380867004
In grad_steps = 549, loss = 0.2887587249279022
In grad_steps = 550, loss = 0.25859129428863525
In grad_steps = 551, loss = 0.2854003608226776
In grad_steps = 552, loss = 0.3043396472930908
In grad_steps = 553, loss = 0.26606014370918274
In grad_steps = 554, loss = 0.504750669002533
In grad_steps = 555, loss = 0.472195565700531
In grad_steps = 556, loss = 0.6614485383033752
In grad_steps = 557, loss = 0.5534756183624268
In grad_steps = 558, loss = 0.5321614742279053
In grad_steps = 559, loss = 0.7021958827972412
In grad_steps = 560, loss = 0.4170132875442505
In grad_steps = 561, loss = 0.6256159543991089
In grad_steps = 562, loss = 0.5350523591041565
In grad_steps = 563, loss = 0.461334764957428
In grad_steps = 564, loss = 0.5533440709114075
In grad_steps = 565, loss = 0.8975420594215393
In grad_steps = 566, loss = 0.5059556365013123
In grad_steps = 567, loss = 0.6659215688705444
In grad_steps = 568, loss = 0.5471447706222534
In grad_steps = 569, loss = 0.6208658814430237
In grad_steps = 570, loss = 0.6281630396842957
In grad_steps = 571, loss = 0.6465526819229126
In grad_steps = 572, loss = 0.5341672301292419
In grad_steps = 573, loss = 0.46107175946235657
In grad_steps = 574, loss = 0.5477757453918457
In grad_steps = 575, loss = 0.5645568370819092
In grad_steps = 576, loss = 0.5691500306129456
In grad_steps = 577, loss = 0.6174818873405457
In grad_steps = 578, loss = 0.5611397624015808
In grad_steps = 579, loss = 0.5508047938346863
In grad_steps = 580, loss = 0.7216954231262207
In grad_steps = 581, loss = 0.42968007922172546
In grad_steps = 582, loss = 0.5048297047615051
In grad_steps = 583, loss = 0.6229652166366577
In grad_steps = 584, loss = 0.4832507073879242
In grad_steps = 585, loss = 0.3999306559562683
In grad_steps = 586, loss = 0.7065610885620117
In grad_steps = 587, loss = 0.46601927280426025
In grad_steps = 588, loss = 0.3820246756076813
In grad_steps = 589, loss = 0.5810332894325256
In grad_steps = 590, loss = 0.3223241865634918
In grad_steps = 591, loss = 0.49847644567489624
In grad_steps = 592, loss = 0.7095209360122681
In grad_steps = 593, loss = 0.4394615888595581
In grad_steps = 594, loss = 0.34097030758857727
In grad_steps = 595, loss = 0.6761564612388611
In grad_steps = 596, loss = 0.31249627470970154
In grad_steps = 597, loss = 0.5919099450111389
In grad_steps = 598, loss = 0.5637536644935608
In grad_steps = 599, loss = 0.5528595447540283
In grad_steps = 600, loss = 0.6603109836578369
In grad_steps = 601, loss = 0.5105315446853638
In grad_steps = 602, loss = 0.6696643233299255
In grad_steps = 603, loss = 0.6624677181243896
In grad_steps = 604, loss = 0.5499878525733948
In grad_steps = 605, loss = 0.6124345660209656
In grad_steps = 606, loss = 0.5839566588401794
In grad_steps = 607, loss = 0.6285739541053772
In grad_steps = 608, loss = 0.6190422177314758
In grad_steps = 609, loss = 0.7437052130699158
In grad_steps = 610, loss = 0.5326401591300964
In grad_steps = 611, loss = 0.5787708163261414
In grad_steps = 612, loss = 0.5811855792999268
In grad_steps = 613, loss = 0.6833149790763855
In grad_steps = 614, loss = 0.7421762347221375
In grad_steps = 615, loss = 0.650011420249939
In grad_steps = 616, loss = 0.6894388198852539
In grad_steps = 617, loss = 0.6934306621551514
In grad_steps = 618, loss = 0.6251188516616821
In grad_steps = 619, loss = 0.6684129238128662
In grad_steps = 620, loss = 0.6634231209754944
In grad_steps = 621, loss = 0.6972759962081909
In grad_steps = 622, loss = 0.7180888652801514
In grad_steps = 623, loss = 0.5493792295455933
In grad_steps = 624, loss = 0.5114205479621887
Beginning epoch 6
In grad_steps = 625, loss = 0.6310034990310669
In grad_steps = 626, loss = 0.6703499555587769
In grad_steps = 627, loss = 0.612829864025116
In grad_steps = 628, loss = 0.6524122357368469
In grad_steps = 629, loss = 0.6578884720802307
In grad_steps = 630, loss = 0.5087502002716064
In grad_steps = 631, loss = 0.6046314239501953
In grad_steps = 632, loss = 0.6170021295547485
In grad_steps = 633, loss = 0.44028857350349426
In grad_steps = 634, loss = 0.6734394431114197
In grad_steps = 635, loss = 0.5768198370933533
In grad_steps = 636, loss = 0.4827980399131775
In grad_steps = 637, loss = 0.43918633460998535
In grad_steps = 638, loss = 0.6172794103622437
In grad_steps = 639, loss = 0.5974922776222229
In grad_steps = 640, loss = 0.4619154930114746
In grad_steps = 641, loss = 0.36904287338256836
In grad_steps = 642, loss = 0.35929012298583984
In grad_steps = 643, loss = 0.45798173546791077
In grad_steps = 644, loss = 0.27999264001846313
In grad_steps = 645, loss = 0.3109711706638336
In grad_steps = 646, loss = 0.2207130491733551
In grad_steps = 647, loss = 0.4758268892765045
In grad_steps = 648, loss = 0.24890199303627014
In grad_steps = 649, loss = 0.21763253211975098
In grad_steps = 650, loss = 0.06418070197105408
In grad_steps = 651, loss = 0.32891279458999634
In grad_steps = 652, loss = 0.4832136631011963
In grad_steps = 653, loss = 0.5439164638519287
In grad_steps = 654, loss = 0.40134087204933167
In grad_steps = 655, loss = 0.1980856955051422
In grad_steps = 656, loss = 0.21009515225887299
In grad_steps = 657, loss = 0.2505565881729126
In grad_steps = 658, loss = 0.26067081093788147
In grad_steps = 659, loss = 0.2875215411186218
In grad_steps = 660, loss = 0.35321012139320374
In grad_steps = 661, loss = 0.21423053741455078
In grad_steps = 662, loss = 0.1489771008491516
In grad_steps = 663, loss = 0.22091174125671387
In grad_steps = 664, loss = 0.14985457062721252
In grad_steps = 665, loss = 0.7389304637908936
In grad_steps = 666, loss = 0.20228318870067596
In grad_steps = 667, loss = 0.3049624264240265
In grad_steps = 668, loss = 0.22224648296833038
In grad_steps = 669, loss = 0.5159619450569153
In grad_steps = 670, loss = 0.6866075396537781
In grad_steps = 671, loss = 0.5999471545219421
In grad_steps = 672, loss = 0.5967170000076294
In grad_steps = 673, loss = 0.41049081087112427
In grad_steps = 674, loss = 0.12766006588935852
In grad_steps = 675, loss = 0.18691188097000122
In grad_steps = 676, loss = 0.2741018533706665
In grad_steps = 677, loss = 0.4376201629638672
In grad_steps = 678, loss = 0.45320188999176025
In grad_steps = 679, loss = 0.419175922870636
In grad_steps = 680, loss = 0.4635612368583679
In grad_steps = 681, loss = 0.37095439434051514
In grad_steps = 682, loss = 0.3512600064277649
In grad_steps = 683, loss = 0.16552096605300903
In grad_steps = 684, loss = 0.4288331866264343
In grad_steps = 685, loss = 0.3598885238170624
In grad_steps = 686, loss = 0.6613264083862305
In grad_steps = 687, loss = 0.3831687271595001
In grad_steps = 688, loss = 0.2784647047519684
In grad_steps = 689, loss = 0.1833440661430359
In grad_steps = 690, loss = 0.47559893131256104
In grad_steps = 691, loss = 0.11994422972202301
In grad_steps = 692, loss = 0.7519251108169556
In grad_steps = 693, loss = 0.2673126757144928
In grad_steps = 694, loss = 0.765387237071991
In grad_steps = 695, loss = 0.5709635019302368
In grad_steps = 696, loss = 0.47431233525276184
In grad_steps = 697, loss = 0.5059720277786255
In grad_steps = 698, loss = 0.30536967515945435
In grad_steps = 699, loss = 0.4877779185771942
In grad_steps = 700, loss = 0.6329290866851807
In grad_steps = 701, loss = 0.4902581572532654
In grad_steps = 702, loss = 0.5077139139175415
In grad_steps = 703, loss = 0.44944462180137634
In grad_steps = 704, loss = 0.49894726276397705
In grad_steps = 705, loss = 0.48397400975227356
In grad_steps = 706, loss = 0.38051652908325195
In grad_steps = 707, loss = 0.5291875600814819
In grad_steps = 708, loss = 0.3451971113681793
In grad_steps = 709, loss = 0.45997342467308044
In grad_steps = 710, loss = 0.24983765184879303
In grad_steps = 711, loss = 0.4971342384815216
In grad_steps = 712, loss = 0.30506619811058044
In grad_steps = 713, loss = 0.1934467852115631
In grad_steps = 714, loss = 0.5315918326377869
In grad_steps = 715, loss = 0.16415970027446747
In grad_steps = 716, loss = 0.4353359341621399
In grad_steps = 717, loss = 0.11801771819591522
In grad_steps = 718, loss = 0.09657566994428635
In grad_steps = 719, loss = 0.046527598053216934
In grad_steps = 720, loss = 0.25841569900512695
In grad_steps = 721, loss = 0.7854688167572021
In grad_steps = 722, loss = 0.1273265779018402
In grad_steps = 723, loss = 0.35369768738746643
In grad_steps = 724, loss = 0.4327285885810852
In grad_steps = 725, loss = 0.737383246421814
In grad_steps = 726, loss = 0.6954253911972046
In grad_steps = 727, loss = 0.4263814091682434
In grad_steps = 728, loss = 0.571925699710846
In grad_steps = 729, loss = 0.5099535584449768
In grad_steps = 730, loss = 0.9404819011688232
In grad_steps = 731, loss = 0.5753722190856934
In grad_steps = 732, loss = 0.9434898495674133
In grad_steps = 733, loss = 0.6833548545837402
In grad_steps = 734, loss = 0.6958729028701782
In grad_steps = 735, loss = 0.5784900188446045
In grad_steps = 736, loss = 0.6072091460227966
In grad_steps = 737, loss = 0.6341265439987183
In grad_steps = 738, loss = 0.6089550256729126
In grad_steps = 739, loss = 0.6362044811248779
In grad_steps = 740, loss = 0.7442744970321655
In grad_steps = 741, loss = 0.7093372344970703
In grad_steps = 742, loss = 0.6551888585090637
In grad_steps = 743, loss = 0.6510823369026184
In grad_steps = 744, loss = 0.7008340358734131
In grad_steps = 745, loss = 0.6375482082366943
In grad_steps = 746, loss = 0.6901217103004456
In grad_steps = 747, loss = 0.6228256225585938
In grad_steps = 748, loss = 0.6875664591789246
In grad_steps = 749, loss = 0.6416770815849304
Beginning epoch 7
In grad_steps = 750, loss = 0.6241941452026367
In grad_steps = 751, loss = 0.6795685291290283
In grad_steps = 752, loss = 0.6160590648651123
In grad_steps = 753, loss = 0.6450846195220947
In grad_steps = 754, loss = 0.6304611563682556
In grad_steps = 755, loss = 0.6172006130218506
In grad_steps = 756, loss = 0.5555007457733154
In grad_steps = 757, loss = 0.6919845342636108
In grad_steps = 758, loss = 0.4653690755367279
In grad_steps = 759, loss = 0.7190331220626831
In grad_steps = 760, loss = 0.8758352994918823
In grad_steps = 761, loss = 0.6287686228752136
In grad_steps = 762, loss = 0.5575588941574097
In grad_steps = 763, loss = 0.5352979898452759
In grad_steps = 764, loss = 0.6856542229652405
In grad_steps = 765, loss = 0.6178940534591675
In grad_steps = 766, loss = 0.40217095613479614
In grad_steps = 767, loss = 0.5327989459037781
In grad_steps = 768, loss = 0.5487630367279053
In grad_steps = 769, loss = 0.5544058084487915
In grad_steps = 770, loss = 0.3441668450832367
In grad_steps = 771, loss = 0.4923960268497467
In grad_steps = 772, loss = 0.6640964150428772
In grad_steps = 773, loss = 0.3142206370830536
In grad_steps = 774, loss = 0.4187700152397156
In grad_steps = 775, loss = 0.24251171946525574
In grad_steps = 776, loss = 0.4046744108200073
In grad_steps = 777, loss = 0.3634147346019745
In grad_steps = 778, loss = 0.5040740370750427
In grad_steps = 779, loss = 0.6024453043937683
In grad_steps = 780, loss = 0.30429303646087646
In grad_steps = 781, loss = 0.14797313511371613
In grad_steps = 782, loss = 0.09529320895671844
In grad_steps = 783, loss = 0.29071420431137085
In grad_steps = 784, loss = 0.48088815808296204
In grad_steps = 785, loss = 0.1331053376197815
In grad_steps = 786, loss = 0.3261636197566986
In grad_steps = 787, loss = 0.15760694444179535
In grad_steps = 788, loss = 0.14674080908298492
In grad_steps = 789, loss = 0.056497178971767426
In grad_steps = 790, loss = 0.22269845008850098
In grad_steps = 791, loss = 0.7617563009262085
In grad_steps = 792, loss = 0.5929558873176575
In grad_steps = 793, loss = 0.25501108169555664
In grad_steps = 794, loss = 0.40705496072769165
In grad_steps = 795, loss = 0.1467374563217163
In grad_steps = 796, loss = 0.10430208593606949
In grad_steps = 797, loss = 0.522367000579834
In grad_steps = 798, loss = 0.6208445429801941
In grad_steps = 799, loss = 0.5423328876495361
In grad_steps = 800, loss = 0.5785537362098694
In grad_steps = 801, loss = 0.4207652807235718
In grad_steps = 802, loss = 0.23791930079460144
In grad_steps = 803, loss = 0.2807771563529968
In grad_steps = 804, loss = 0.3554837703704834
In grad_steps = 805, loss = 0.3956129252910614
In grad_steps = 806, loss = 0.5985311269760132
In grad_steps = 807, loss = 0.6683428883552551
In grad_steps = 808, loss = 0.8365361094474792
In grad_steps = 809, loss = 0.48129981756210327
In grad_steps = 810, loss = 0.21458706259727478
In grad_steps = 811, loss = 0.35860922932624817
In grad_steps = 812, loss = 0.33655938506126404
In grad_steps = 813, loss = 0.39194875955581665
In grad_steps = 814, loss = 0.336495578289032
In grad_steps = 815, loss = 0.4051879942417145
In grad_steps = 816, loss = 0.598945140838623
In grad_steps = 817, loss = 0.35489621758461
In grad_steps = 818, loss = 0.35162821412086487
In grad_steps = 819, loss = 0.4778132438659668
In grad_steps = 820, loss = 0.3626694083213806
In grad_steps = 821, loss = 0.1557331681251526
In grad_steps = 822, loss = 0.4637489914894104
In grad_steps = 823, loss = 0.19553785026073456
In grad_steps = 824, loss = 0.708288311958313
In grad_steps = 825, loss = 0.3336886465549469
In grad_steps = 826, loss = 0.21966762840747833
In grad_steps = 827, loss = 0.32961633801460266
In grad_steps = 828, loss = 0.42442232370376587
In grad_steps = 829, loss = 0.27822110056877136
In grad_steps = 830, loss = 0.4332277178764343
In grad_steps = 831, loss = 0.12079744786024094
In grad_steps = 832, loss = 0.5105242729187012
In grad_steps = 833, loss = 0.3821060061454773
In grad_steps = 834, loss = 0.22575096786022186
In grad_steps = 835, loss = 0.20751194655895233
In grad_steps = 836, loss = 0.2351861447095871
In grad_steps = 837, loss = 0.15720674395561218
In grad_steps = 838, loss = 0.253803551197052
In grad_steps = 839, loss = 0.186044842004776
In grad_steps = 840, loss = 0.1468663215637207
In grad_steps = 841, loss = 0.17046009004116058
In grad_steps = 842, loss = 0.08917497098445892
In grad_steps = 843, loss = 0.14104154706001282
In grad_steps = 844, loss = 0.1919509917497635
In grad_steps = 845, loss = 0.11869524419307709
In grad_steps = 846, loss = 0.04435889422893524
In grad_steps = 847, loss = 0.27379366755485535
In grad_steps = 848, loss = 0.37620192766189575
In grad_steps = 849, loss = 0.1556859016418457
In grad_steps = 850, loss = 0.14948958158493042
In grad_steps = 851, loss = 0.3226110339164734
In grad_steps = 852, loss = 0.4585442543029785
In grad_steps = 853, loss = 0.7894153594970703
In grad_steps = 854, loss = 0.5411897301673889
In grad_steps = 855, loss = 0.8225733041763306
In grad_steps = 856, loss = 1.2019917964935303
In grad_steps = 857, loss = 0.8519588112831116
In grad_steps = 858, loss = 0.45172974467277527
In grad_steps = 859, loss = 0.34330669045448303
In grad_steps = 860, loss = 0.41789329051971436
In grad_steps = 861, loss = 0.4303407669067383
In grad_steps = 862, loss = 0.5982388854026794
In grad_steps = 863, loss = 0.5233182907104492
In grad_steps = 864, loss = 0.7244211435317993
In grad_steps = 865, loss = 0.9057514071464539
In grad_steps = 866, loss = 0.6195772290229797
In grad_steps = 867, loss = 0.6036149263381958
In grad_steps = 868, loss = 0.5392202138900757
In grad_steps = 869, loss = 0.7071064710617065
In grad_steps = 870, loss = 0.5744335055351257
In grad_steps = 871, loss = 0.6278494596481323
In grad_steps = 872, loss = 0.6929203867912292
In grad_steps = 873, loss = 0.6323710083961487
In grad_steps = 874, loss = 0.6306865215301514
Beginning epoch 8
In grad_steps = 875, loss = 0.48253804445266724
In grad_steps = 876, loss = 0.5998344421386719
In grad_steps = 877, loss = 0.5380032062530518
In grad_steps = 878, loss = 0.6445525288581848
In grad_steps = 879, loss = 0.5952819585800171
In grad_steps = 880, loss = 0.5927448272705078
In grad_steps = 881, loss = 0.5332009792327881
In grad_steps = 882, loss = 0.6550156474113464
In grad_steps = 883, loss = 0.3921964168548584
In grad_steps = 884, loss = 0.6567976474761963
In grad_steps = 885, loss = 0.6645103693008423
In grad_steps = 886, loss = 0.6124195456504822
In grad_steps = 887, loss = 0.48411527276039124
In grad_steps = 888, loss = 0.6295723915100098
In grad_steps = 889, loss = 0.5255338549613953
In grad_steps = 890, loss = 0.40499407052993774
In grad_steps = 891, loss = 0.35391128063201904
In grad_steps = 892, loss = 0.40402916073799133
In grad_steps = 893, loss = 0.5952622890472412
In grad_steps = 894, loss = 0.4400753080844879
In grad_steps = 895, loss = 0.3910428583621979
In grad_steps = 896, loss = 0.45626693964004517
In grad_steps = 897, loss = 0.5556095242500305
In grad_steps = 898, loss = 0.4129719138145447
In grad_steps = 899, loss = 0.28528016805648804
In grad_steps = 900, loss = 0.21494540572166443
In grad_steps = 901, loss = 0.13743220269680023
In grad_steps = 902, loss = 0.11682040989398956
In grad_steps = 903, loss = 0.22865179181098938
In grad_steps = 904, loss = 0.5899035334587097
In grad_steps = 905, loss = 0.23647785186767578
In grad_steps = 906, loss = 0.05974465236067772
In grad_steps = 907, loss = 0.2947657108306885
In grad_steps = 908, loss = 0.484829306602478
In grad_steps = 909, loss = 0.05415354296565056
In grad_steps = 910, loss = 0.029362190514802933
In grad_steps = 911, loss = 0.4611879587173462
In grad_steps = 912, loss = 0.48051130771636963
In grad_steps = 913, loss = 0.13146036863327026
In grad_steps = 914, loss = 0.2119516134262085
In grad_steps = 915, loss = 0.03955791890621185
In grad_steps = 916, loss = 0.06000393256545067
In grad_steps = 917, loss = 0.2604142129421234
In grad_steps = 918, loss = 0.28534284234046936
In grad_steps = 919, loss = 0.42990708351135254
In grad_steps = 920, loss = 0.4175584018230438
In grad_steps = 921, loss = 0.325672447681427
In grad_steps = 922, loss = 0.49278247356414795
In grad_steps = 923, loss = 0.5613134503364563
In grad_steps = 924, loss = 0.05802463740110397
In grad_steps = 925, loss = 0.18461066484451294
In grad_steps = 926, loss = 0.2123177945613861
In grad_steps = 927, loss = 0.1843152493238449
In grad_steps = 928, loss = 0.4633156657218933
In grad_steps = 929, loss = 0.438066691160202
In grad_steps = 930, loss = 0.3143272399902344
In grad_steps = 931, loss = 0.4050430953502655
In grad_steps = 932, loss = 0.25856778025627136
In grad_steps = 933, loss = 0.34314343333244324
In grad_steps = 934, loss = 0.32344236969947815
In grad_steps = 935, loss = 0.0988454520702362
In grad_steps = 936, loss = 0.28298085927963257
In grad_steps = 937, loss = 0.19944041967391968
In grad_steps = 938, loss = 0.3146964907646179
In grad_steps = 939, loss = 0.14791525900363922
In grad_steps = 940, loss = 0.3672136962413788
In grad_steps = 941, loss = 0.1598578691482544
In grad_steps = 942, loss = 0.09141681343317032
In grad_steps = 943, loss = 0.299345999956131
In grad_steps = 944, loss = 0.17972013354301453
In grad_steps = 945, loss = 0.2747417092323303
In grad_steps = 946, loss = 0.03794460371136665
In grad_steps = 947, loss = 0.14289666712284088
In grad_steps = 948, loss = 0.009312341921031475
In grad_steps = 949, loss = 0.09341009706258774
In grad_steps = 950, loss = 0.04651524871587753
In grad_steps = 951, loss = 0.18914863467216492
In grad_steps = 952, loss = 0.16742253303527832
In grad_steps = 953, loss = 0.03532572463154793
In grad_steps = 954, loss = 0.27098387479782104
In grad_steps = 955, loss = 0.09306381642818451
In grad_steps = 956, loss = 0.1491730958223343
In grad_steps = 957, loss = 0.22305774688720703
In grad_steps = 958, loss = 0.03126491233706474
In grad_steps = 959, loss = 0.2641553282737732
In grad_steps = 960, loss = 0.03548683226108551
In grad_steps = 961, loss = 0.19693531095981598
In grad_steps = 962, loss = 0.04427681118249893
In grad_steps = 963, loss = 0.006612762808799744
In grad_steps = 964, loss = 0.13061413168907166
In grad_steps = 965, loss = 0.10609428584575653
In grad_steps = 966, loss = 0.17959515750408173
In grad_steps = 967, loss = 0.03448277339339256
In grad_steps = 968, loss = 0.29330480098724365
In grad_steps = 969, loss = 0.10174927115440369
In grad_steps = 970, loss = 0.26690733432769775
In grad_steps = 971, loss = 0.17148473858833313
In grad_steps = 972, loss = 0.38880643248558044
In grad_steps = 973, loss = 0.22146126627922058
In grad_steps = 974, loss = 0.398164302110672
In grad_steps = 975, loss = 0.2828379273414612
In grad_steps = 976, loss = 0.27001631259918213
In grad_steps = 977, loss = 0.09815897047519684
In grad_steps = 978, loss = 0.4355778098106384
In grad_steps = 979, loss = 0.15966130793094635
In grad_steps = 980, loss = 0.16770479083061218
In grad_steps = 981, loss = 0.634148895740509
In grad_steps = 982, loss = 0.8673282861709595
In grad_steps = 983, loss = 0.6784127354621887
In grad_steps = 984, loss = 0.29855048656463623
In grad_steps = 985, loss = 0.36633145809173584
In grad_steps = 986, loss = 0.41235437989234924
In grad_steps = 987, loss = 0.39492490887641907
In grad_steps = 988, loss = 0.5259593725204468
In grad_steps = 989, loss = 0.6240096092224121
In grad_steps = 990, loss = 0.836797297000885
In grad_steps = 991, loss = 0.3817797601222992
In grad_steps = 992, loss = 0.7857720851898193
In grad_steps = 993, loss = 0.5838254690170288
In grad_steps = 994, loss = 0.5716269612312317
In grad_steps = 995, loss = 0.58547443151474
In grad_steps = 996, loss = 0.5011371374130249
In grad_steps = 997, loss = 0.5420002937316895
In grad_steps = 998, loss = 0.5940728187561035
In grad_steps = 999, loss = 0.7269348502159119
Elapsed time: 3355.449422121048 seconds for ensemble 2 with 8 epochs
LoRA instance 2 evaluation complete. Data saved to /pscratch/sd/t/tianle/lucid/other_source/SURP_2024/results/tmp_lora_ensemble/Llama3-8B-set-3c/test_data_instance_2_seed_20187.npz.
lora instance i = 2 Successfully finished.
Training lora instance 3
DEBUG: Total parameters: 8037076992
DEBUG: Trainable parameters: 6815744
Beginning epoch 1
In grad_steps = 0, loss = 0.7359974980354309
In grad_steps = 1, loss = 0.8024043440818787
In grad_steps = 2, loss = 0.7657382488250732
In grad_steps = 3, loss = 0.6494835615158081
In grad_steps = 4, loss = 0.9072285890579224
In grad_steps = 5, loss = 0.7143710255622864
In grad_steps = 6, loss = 0.6396859884262085
In grad_steps = 7, loss = 0.7763288021087646
In grad_steps = 8, loss = 0.5925830006599426
In grad_steps = 9, loss = 0.863892674446106
In grad_steps = 10, loss = 0.8520069718360901
In grad_steps = 11, loss = 0.7596017718315125
In grad_steps = 12, loss = 0.7088144421577454
In grad_steps = 13, loss = 0.646350085735321
In grad_steps = 14, loss = 0.8044261336326599
In grad_steps = 15, loss = 0.8136546611785889
In grad_steps = 16, loss = 0.7124832272529602
In grad_steps = 17, loss = 0.8023949265480042
In grad_steps = 18, loss = 0.6977507472038269
In grad_steps = 19, loss = 0.7318807244300842
In grad_steps = 20, loss = 0.7292636632919312
In grad_steps = 21, loss = 0.7747054696083069
In grad_steps = 22, loss = 0.7311395406723022
In grad_steps = 23, loss = 0.773745596408844
In grad_steps = 24, loss = 0.7566394805908203
In grad_steps = 25, loss = 0.7054548859596252
In grad_steps = 26, loss = 0.7001670598983765
In grad_steps = 27, loss = 0.6327769756317139
In grad_steps = 28, loss = 0.7395936846733093
In grad_steps = 29, loss = 0.7466222643852234
In grad_steps = 30, loss = 0.7772538065910339
In grad_steps = 31, loss = 0.5698246955871582
In grad_steps = 32, loss = 0.7655467391014099
In grad_steps = 33, loss = 0.7502740621566772
In grad_steps = 34, loss = 0.6460198760032654
In grad_steps = 35, loss = 0.6670573353767395
In grad_steps = 36, loss = 0.6577376127243042
In grad_steps = 37, loss = 0.6900984644889832
In grad_steps = 38, loss = 0.7723399996757507
In grad_steps = 39, loss = 0.7243669033050537
In grad_steps = 40, loss = 0.7012805342674255
In grad_steps = 41, loss = 0.7160632610321045
In grad_steps = 42, loss = 0.6601408123970032
In grad_steps = 43, loss = 0.7050686478614807
In grad_steps = 44, loss = 0.732132613658905
In grad_steps = 45, loss = 0.7582352757453918
In grad_steps = 46, loss = 0.7802756428718567
In grad_steps = 47, loss = 0.7454202771186829
In grad_steps = 48, loss = 0.700695812702179
In grad_steps = 49, loss = 0.7010783553123474
In grad_steps = 50, loss = 0.6850093007087708
In grad_steps = 51, loss = 0.6855981945991516
In grad_steps = 52, loss = 0.7398712635040283
In grad_steps = 53, loss = 0.6642394065856934
In grad_steps = 54, loss = 0.7760813236236572
In grad_steps = 55, loss = 0.743685245513916
In grad_steps = 56, loss = 0.7091982364654541
In grad_steps = 57, loss = 0.7170390486717224
In grad_steps = 58, loss = 0.7502321600914001
In grad_steps = 59, loss = 0.6840547919273376
In grad_steps = 60, loss = 0.6875429749488831
In grad_steps = 61, loss = 0.704413652420044
In grad_steps = 62, loss = 0.7157166004180908
In grad_steps = 63, loss = 0.7121489644050598
In grad_steps = 64, loss = 0.667595624923706
In grad_steps = 65, loss = 0.6824668645858765
In grad_steps = 66, loss = 0.7454128265380859
In grad_steps = 67, loss = 0.6892486810684204
In grad_steps = 68, loss = 0.7169452905654907
In grad_steps = 69, loss = 0.6923527717590332
In grad_steps = 70, loss = 0.6930436491966248
In grad_steps = 71, loss = 0.6863601207733154
In grad_steps = 72, loss = 0.7015687823295593
In grad_steps = 73, loss = 0.6985888481140137
In grad_steps = 74, loss = 0.6834331154823303
In grad_steps = 75, loss = 0.6830428838729858
In grad_steps = 76, loss = 0.7119415998458862
In grad_steps = 77, loss = 0.7198269367218018
In grad_steps = 78, loss = 0.6923282146453857
In grad_steps = 79, loss = 0.6824623346328735
In grad_steps = 80, loss = 0.7251827120780945
In grad_steps = 81, loss = 0.7097413539886475
In grad_steps = 82, loss = 0.7048226594924927
In grad_steps = 83, loss = 0.6971204876899719
In grad_steps = 84, loss = 0.6714826822280884
In grad_steps = 85, loss = 0.6675684452056885
In grad_steps = 86, loss = 0.7617748975753784
In grad_steps = 87, loss = 0.6855854988098145
In grad_steps = 88, loss = 0.6903939843177795
In grad_steps = 89, loss = 0.7213638424873352
In grad_steps = 90, loss = 0.7194822430610657
In grad_steps = 91, loss = 0.7228519320487976
In grad_steps = 92, loss = 0.6855401992797852
In grad_steps = 93, loss = 0.7242609262466431
In grad_steps = 94, loss = 0.7031187415122986
In grad_steps = 95, loss = 0.6594083309173584
In grad_steps = 96, loss = 0.6653225421905518
In grad_steps = 97, loss = 0.6784175038337708
In grad_steps = 98, loss = 0.6977550387382507
In grad_steps = 99, loss = 0.702793538570404
In grad_steps = 100, loss = 0.7126964330673218
In grad_steps = 101, loss = 0.6770943403244019
In grad_steps = 102, loss = 0.705618679523468
In grad_steps = 103, loss = 0.7190338373184204
In grad_steps = 104, loss = 0.7296627163887024
In grad_steps = 105, loss = 0.6921015977859497
In grad_steps = 106, loss = 0.6998609304428101
In grad_steps = 107, loss = 0.6972925662994385
In grad_steps = 108, loss = 0.6998552083969116
In grad_steps = 109, loss = 0.7079290151596069
In grad_steps = 110, loss = 0.6916231513023376
In grad_steps = 111, loss = 0.678394079208374
In grad_steps = 112, loss = 0.6905704736709595
In grad_steps = 113, loss = 0.6463583707809448
In grad_steps = 114, loss = 0.7252607345581055
In grad_steps = 115, loss = 0.7334470152854919
In grad_steps = 116, loss = 0.6472885608673096
In grad_steps = 117, loss = 0.7074600458145142
In grad_steps = 118, loss = 0.7322233319282532
In grad_steps = 119, loss = 0.6710754632949829
In grad_steps = 120, loss = 0.7027831077575684
In grad_steps = 121, loss = 0.7050224542617798
In grad_steps = 122, loss = 0.6805354356765747
In grad_steps = 123, loss = 0.7495271563529968
In grad_steps = 124, loss = 0.7199978232383728
Beginning epoch 2
In grad_steps = 125, loss = 0.6993999481201172
In grad_steps = 126, loss = 0.6836133003234863
In grad_steps = 127, loss = 0.6896762847900391
In grad_steps = 128, loss = 0.6976600885391235
In grad_steps = 129, loss = 0.6821711659431458
In grad_steps = 130, loss = 0.6724883913993835
In grad_steps = 131, loss = 0.6859531998634338
In grad_steps = 132, loss = 0.6770774126052856
In grad_steps = 133, loss = 0.6378146409988403
In grad_steps = 134, loss = 0.7290061712265015
In grad_steps = 135, loss = 0.7535535097122192
In grad_steps = 136, loss = 0.7319729924201965
In grad_steps = 137, loss = 0.7239465713500977
In grad_steps = 138, loss = 0.7373126149177551
In grad_steps = 139, loss = 0.676606297492981
In grad_steps = 140, loss = 0.6823258399963379
In grad_steps = 141, loss = 0.7148786783218384
In grad_steps = 142, loss = 0.6764862537384033
In grad_steps = 143, loss = 0.6837038993835449
In grad_steps = 144, loss = 0.6886128783226013
In grad_steps = 145, loss = 0.6993544697761536
In grad_steps = 146, loss = 0.6921515464782715
In grad_steps = 147, loss = 0.6922064423561096
In grad_steps = 148, loss = 0.6817183494567871
In grad_steps = 149, loss = 0.6570221185684204
In grad_steps = 150, loss = 0.6365522146224976
In grad_steps = 151, loss = 0.7032746076583862
In grad_steps = 152, loss = 0.6293799877166748
In grad_steps = 153, loss = 0.7150009870529175
In grad_steps = 154, loss = 0.7348536252975464
In grad_steps = 155, loss = 0.7490886449813843
In grad_steps = 156, loss = 0.5733435750007629
In grad_steps = 157, loss = 0.7273715138435364
In grad_steps = 158, loss = 0.7157187461853027
In grad_steps = 159, loss = 0.638362467288971
In grad_steps = 160, loss = 0.6567437648773193
In grad_steps = 161, loss = 0.6513505578041077
In grad_steps = 162, loss = 0.6789307594299316
In grad_steps = 163, loss = 0.7537202835083008
In grad_steps = 164, loss = 0.7282067537307739
In grad_steps = 165, loss = 0.7044088244438171
In grad_steps = 166, loss = 0.6873345971107483
In grad_steps = 167, loss = 0.6963581442832947
In grad_steps = 168, loss = 0.6574723720550537
In grad_steps = 169, loss = 0.7063831090927124
In grad_steps = 170, loss = 0.7236310243606567
In grad_steps = 171, loss = 0.7272299528121948
In grad_steps = 172, loss = 0.7319809794425964
In grad_steps = 173, loss = 0.7014660239219666
In grad_steps = 174, loss = 0.6930292248725891
In grad_steps = 175, loss = 0.6928163766860962
In grad_steps = 176, loss = 0.6892487406730652
In grad_steps = 177, loss = 0.6927562952041626
In grad_steps = 178, loss = 0.690304160118103
In grad_steps = 179, loss = 0.6999415159225464
In grad_steps = 180, loss = 0.6937330961227417
In grad_steps = 181, loss = 0.6933729648590088
In grad_steps = 182, loss = 0.6953778266906738
In grad_steps = 183, loss = 0.7077803611755371
In grad_steps = 184, loss = 0.6746458411216736
In grad_steps = 185, loss = 0.6884331107139587
In grad_steps = 186, loss = 0.6902088522911072
In grad_steps = 187, loss = 0.6924830079078674
In grad_steps = 188, loss = 0.688393771648407
In grad_steps = 189, loss = 0.6816195249557495
In grad_steps = 190, loss = 0.6853359341621399
In grad_steps = 191, loss = 0.7018836140632629
In grad_steps = 192, loss = 0.6862483024597168
In grad_steps = 193, loss = 0.6986909508705139
In grad_steps = 194, loss = 0.6841734051704407
In grad_steps = 195, loss = 0.6952366232872009
In grad_steps = 196, loss = 0.69565749168396
In grad_steps = 197, loss = 0.6892256736755371
In grad_steps = 198, loss = 0.6729923486709595
In grad_steps = 199, loss = 0.685699462890625
In grad_steps = 200, loss = 0.6959825754165649
In grad_steps = 201, loss = 0.6954261660575867
In grad_steps = 202, loss = 0.6916367411613464
In grad_steps = 203, loss = 0.6795496940612793
In grad_steps = 204, loss = 0.6892105937004089
In grad_steps = 205, loss = 0.7108978033065796
In grad_steps = 206, loss = 0.6869427561759949
In grad_steps = 207, loss = 0.6975201368331909
In grad_steps = 208, loss = 0.7014352083206177
In grad_steps = 209, loss = 0.6672521829605103
In grad_steps = 210, loss = 0.6508961915969849
In grad_steps = 211, loss = 0.7615740299224854
In grad_steps = 212, loss = 0.6819398403167725
In grad_steps = 213, loss = 0.6832255125045776
In grad_steps = 214, loss = 0.7190790176391602
In grad_steps = 215, loss = 0.7103497982025146
In grad_steps = 216, loss = 0.7285773754119873
In grad_steps = 217, loss = 0.6844097375869751
In grad_steps = 218, loss = 0.7038253545761108
In grad_steps = 219, loss = 0.6888700723648071
In grad_steps = 220, loss = 0.666983425617218
In grad_steps = 221, loss = 0.6623349189758301
In grad_steps = 222, loss = 0.684557318687439
In grad_steps = 223, loss = 0.7018156051635742
In grad_steps = 224, loss = 0.6957384943962097
In grad_steps = 225, loss = 0.7114958167076111
In grad_steps = 226, loss = 0.6793422698974609
In grad_steps = 227, loss = 0.7009069323539734
In grad_steps = 228, loss = 0.7136273384094238
In grad_steps = 229, loss = 0.7271696925163269
In grad_steps = 230, loss = 0.6907917261123657
In grad_steps = 231, loss = 0.7038097977638245
In grad_steps = 232, loss = 0.7025174498558044
In grad_steps = 233, loss = 0.6893734931945801
In grad_steps = 234, loss = 0.6979413628578186
In grad_steps = 235, loss = 0.6899408102035522
In grad_steps = 236, loss = 0.6773659586906433
In grad_steps = 237, loss = 0.6882641911506653
In grad_steps = 238, loss = 0.6539438962936401
In grad_steps = 239, loss = 0.7204400897026062
In grad_steps = 240, loss = 0.7316015362739563
In grad_steps = 241, loss = 0.6479072570800781
In grad_steps = 242, loss = 0.7064140439033508
In grad_steps = 243, loss = 0.7190886735916138
In grad_steps = 244, loss = 0.669052243232727
In grad_steps = 245, loss = 0.701337456703186
In grad_steps = 246, loss = 0.7029004096984863
In grad_steps = 247, loss = 0.6776002645492554
In grad_steps = 248, loss = 0.7467917799949646
In grad_steps = 249, loss = 0.7187277674674988
Beginning epoch 3
In grad_steps = 250, loss = 0.70069420337677
In grad_steps = 251, loss = 0.6749048233032227
In grad_steps = 252, loss = 0.6867239475250244
In grad_steps = 253, loss = 0.69314044713974
In grad_steps = 254, loss = 0.6829166412353516
In grad_steps = 255, loss = 0.6731804013252258
In grad_steps = 256, loss = 0.6836482286453247
In grad_steps = 257, loss = 0.676106333732605
In grad_steps = 258, loss = 0.6358321905136108
In grad_steps = 259, loss = 0.7262213826179504
In grad_steps = 260, loss = 0.7533482313156128
In grad_steps = 261, loss = 0.726509153842926
In grad_steps = 262, loss = 0.7123590111732483
In grad_steps = 263, loss = 0.7366245985031128
In grad_steps = 264, loss = 0.6731380224227905
In grad_steps = 265, loss = 0.6788739562034607
In grad_steps = 266, loss = 0.7032016515731812
In grad_steps = 267, loss = 0.6719779968261719
In grad_steps = 268, loss = 0.6753569841384888
In grad_steps = 269, loss = 0.6873571276664734
In grad_steps = 270, loss = 0.6852524280548096
In grad_steps = 271, loss = 0.6882292032241821
In grad_steps = 272, loss = 0.6899562478065491
In grad_steps = 273, loss = 0.6787019371986389
In grad_steps = 274, loss = 0.6506263613700867
In grad_steps = 275, loss = 0.6365759372711182
In grad_steps = 276, loss = 0.6971383094787598
In grad_steps = 277, loss = 0.6113772392272949
In grad_steps = 278, loss = 0.6960158944129944
In grad_steps = 279, loss = 0.7457584142684937
In grad_steps = 280, loss = 0.7230831384658813
In grad_steps = 281, loss = 0.534349799156189
In grad_steps = 282, loss = 0.6912547945976257
In grad_steps = 283, loss = 0.692678689956665
In grad_steps = 284, loss = 0.6091419458389282
In grad_steps = 285, loss = 0.6285120248794556
In grad_steps = 286, loss = 0.6449792981147766
In grad_steps = 287, loss = 0.670279860496521
In grad_steps = 288, loss = 0.6726100444793701
In grad_steps = 289, loss = 0.6463267803192139
In grad_steps = 290, loss = 0.6166958808898926
In grad_steps = 291, loss = 0.6568688154220581
In grad_steps = 292, loss = 0.5755686163902283
In grad_steps = 293, loss = 0.5051918625831604
In grad_steps = 294, loss = 0.8873623609542847
In grad_steps = 295, loss = 0.823459267616272
In grad_steps = 296, loss = 0.646763265132904
In grad_steps = 297, loss = 0.7541236877441406
In grad_steps = 298, loss = 0.7096584439277649
In grad_steps = 299, loss = 0.6186302900314331
In grad_steps = 300, loss = 0.6597088575363159
In grad_steps = 301, loss = 0.6574831604957581
In grad_steps = 302, loss = 0.6994927525520325
In grad_steps = 303, loss = 0.7096233367919922
In grad_steps = 304, loss = 0.654308021068573
In grad_steps = 305, loss = 0.663745105266571
In grad_steps = 306, loss = 0.7092700600624084
In grad_steps = 307, loss = 0.6914469003677368
In grad_steps = 308, loss = 0.6418359279632568
In grad_steps = 309, loss = 0.6808289289474487
In grad_steps = 310, loss = 0.6501855850219727
In grad_steps = 311, loss = 0.7182261943817139
In grad_steps = 312, loss = 0.7044933438301086
In grad_steps = 313, loss = 0.691396951675415
In grad_steps = 314, loss = 0.6666491031646729
In grad_steps = 315, loss = 0.7008891701698303
In grad_steps = 316, loss = 0.6841091513633728
In grad_steps = 317, loss = 0.6802865266799927
In grad_steps = 318, loss = 0.6496081948280334
In grad_steps = 319, loss = 0.6284369230270386
In grad_steps = 320, loss = 0.7899593114852905
In grad_steps = 321, loss = 0.6305451989173889
In grad_steps = 322, loss = 0.7141597270965576
In grad_steps = 323, loss = 0.5898137092590332
In grad_steps = 324, loss = 0.6562926769256592
In grad_steps = 325, loss = 0.68925940990448
In grad_steps = 326, loss = 0.6702641248703003
In grad_steps = 327, loss = 0.6541733145713806
In grad_steps = 328, loss = 0.6590019464492798
In grad_steps = 329, loss = 0.6965558528900146
In grad_steps = 330, loss = 0.7374566197395325
In grad_steps = 331, loss = 0.5973663926124573
In grad_steps = 332, loss = 0.6902700066566467
In grad_steps = 333, loss = 0.7517202496528625
In grad_steps = 334, loss = 0.6413273215293884
In grad_steps = 335, loss = 0.6258862018585205
In grad_steps = 336, loss = 0.8188796043395996
In grad_steps = 337, loss = 0.7138336300849915
In grad_steps = 338, loss = 0.6578391790390015
In grad_steps = 339, loss = 0.7338924407958984
In grad_steps = 340, loss = 0.6344186663627625
In grad_steps = 341, loss = 0.687947154045105
In grad_steps = 342, loss = 0.6905739903450012
In grad_steps = 343, loss = 0.6515596508979797
In grad_steps = 344, loss = 0.6587552428245544
In grad_steps = 345, loss = 0.7309521436691284
In grad_steps = 346, loss = 0.7065801024436951
In grad_steps = 347, loss = 0.7186663150787354
In grad_steps = 348, loss = 0.7039442658424377
In grad_steps = 349, loss = 0.6862373352050781
In grad_steps = 350, loss = 0.712932288646698
In grad_steps = 351, loss = 0.6826494336128235
In grad_steps = 352, loss = 0.7174035310745239
In grad_steps = 353, loss = 0.7485740780830383
In grad_steps = 354, loss = 0.7672346234321594
In grad_steps = 355, loss = 0.6685348153114319
In grad_steps = 356, loss = 0.7315515279769897
In grad_steps = 357, loss = 0.7102404236793518
In grad_steps = 358, loss = 0.6685689687728882
In grad_steps = 359, loss = 0.6879796385765076
In grad_steps = 360, loss = 0.6726015210151672
In grad_steps = 361, loss = 0.6736363768577576
In grad_steps = 362, loss = 0.6879397630691528
In grad_steps = 363, loss = 0.6436005234718323
In grad_steps = 364, loss = 0.7252528667449951
In grad_steps = 365, loss = 0.7398168444633484
In grad_steps = 366, loss = 0.6474857926368713
In grad_steps = 367, loss = 0.7058238387107849
In grad_steps = 368, loss = 0.7141178250312805
In grad_steps = 369, loss = 0.6670844554901123
In grad_steps = 370, loss = 0.6996368169784546
In grad_steps = 371, loss = 0.6892451643943787
In grad_steps = 372, loss = 0.6714704036712646
In grad_steps = 373, loss = 0.7357305288314819
In grad_steps = 374, loss = 0.69699627161026
Beginning epoch 4
In grad_steps = 375, loss = 0.6941473484039307
In grad_steps = 376, loss = 0.6839514374732971
In grad_steps = 377, loss = 0.6678698658943176
In grad_steps = 378, loss = 0.7110511064529419
In grad_steps = 379, loss = 0.6385704278945923
In grad_steps = 380, loss = 0.6414393782615662
In grad_steps = 381, loss = 0.6731507778167725
In grad_steps = 382, loss = 0.6506150960922241
In grad_steps = 383, loss = 0.5961160659790039
In grad_steps = 384, loss = 0.765274167060852
In grad_steps = 385, loss = 0.7877534031867981
In grad_steps = 386, loss = 0.7068352103233337
In grad_steps = 387, loss = 0.6436764001846313
In grad_steps = 388, loss = 0.7013867497444153
In grad_steps = 389, loss = 0.6772596836090088
In grad_steps = 390, loss = 0.6872639060020447
In grad_steps = 391, loss = 0.6552169919013977
In grad_steps = 392, loss = 0.7811159491539001
In grad_steps = 393, loss = 0.70921391248703
In grad_steps = 394, loss = 0.6396508812904358
In grad_steps = 395, loss = 0.6385400891304016
In grad_steps = 396, loss = 0.6686074733734131
In grad_steps = 397, loss = 0.6811448335647583
In grad_steps = 398, loss = 0.686704695224762
In grad_steps = 399, loss = 0.6703020334243774
In grad_steps = 400, loss = 0.6516745090484619
In grad_steps = 401, loss = 0.6623064279556274
In grad_steps = 402, loss = 0.6048530340194702
In grad_steps = 403, loss = 0.648188591003418
In grad_steps = 404, loss = 0.7310650944709778
In grad_steps = 405, loss = 0.6348630785942078
In grad_steps = 406, loss = 0.4716363549232483
In grad_steps = 407, loss = 0.5938978791236877
In grad_steps = 408, loss = 0.6032510995864868
In grad_steps = 409, loss = 0.5386385917663574
In grad_steps = 410, loss = 0.5519224405288696
In grad_steps = 411, loss = 0.464816689491272
In grad_steps = 412, loss = 0.49876171350479126
In grad_steps = 413, loss = 0.5919694900512695
In grad_steps = 414, loss = 0.34939366579055786
In grad_steps = 415, loss = 0.38464972376823425
In grad_steps = 416, loss = 0.37301915884017944
In grad_steps = 417, loss = 0.38961416482925415
In grad_steps = 418, loss = 0.12747123837471008
In grad_steps = 419, loss = 0.6742725968360901
In grad_steps = 420, loss = 0.2786901891231537
In grad_steps = 421, loss = 0.2540920674800873
In grad_steps = 422, loss = 0.8353590369224548
In grad_steps = 423, loss = 0.5293798446655273
In grad_steps = 424, loss = 0.5394353270530701
In grad_steps = 425, loss = 0.872379720211029
In grad_steps = 426, loss = 0.6191065311431885
In grad_steps = 427, loss = 0.5807828307151794
In grad_steps = 428, loss = 0.6745231747627258
In grad_steps = 429, loss = 0.6422086358070374
In grad_steps = 430, loss = 0.6697149276733398
In grad_steps = 431, loss = 0.6782032251358032
In grad_steps = 432, loss = 0.6625796556472778
In grad_steps = 433, loss = 0.7005558609962463
In grad_steps = 434, loss = 0.6102335453033447
In grad_steps = 435, loss = 0.5963367223739624
In grad_steps = 436, loss = 0.7124446034431458
In grad_steps = 437, loss = 0.62944096326828
In grad_steps = 438, loss = 0.6523177623748779
In grad_steps = 439, loss = 0.5775258541107178
In grad_steps = 440, loss = 0.669352114200592
In grad_steps = 441, loss = 0.6778489351272583
In grad_steps = 442, loss = 0.6140521168708801
In grad_steps = 443, loss = 0.5193979740142822
In grad_steps = 444, loss = 0.46630460023880005
In grad_steps = 445, loss = 0.9334548711776733
In grad_steps = 446, loss = 0.5547735691070557
In grad_steps = 447, loss = 0.49278131127357483
In grad_steps = 448, loss = 0.2852509021759033
In grad_steps = 449, loss = 0.7074729204177856
In grad_steps = 450, loss = 0.5171794891357422
In grad_steps = 451, loss = 0.5950701236724854
In grad_steps = 452, loss = 0.49389827251434326
In grad_steps = 453, loss = 0.5981128215789795
In grad_steps = 454, loss = 0.480431467294693
In grad_steps = 455, loss = 0.7714540362358093
In grad_steps = 456, loss = 0.5545334815979004
In grad_steps = 457, loss = 0.518310546875
In grad_steps = 458, loss = 0.6941429376602173
In grad_steps = 459, loss = 0.4408780634403229
In grad_steps = 460, loss = 0.441764235496521
In grad_steps = 461, loss = 0.8914302587509155
In grad_steps = 462, loss = 0.681705117225647
In grad_steps = 463, loss = 0.47743988037109375
In grad_steps = 464, loss = 0.6821660995483398
In grad_steps = 465, loss = 0.4604632556438446
In grad_steps = 466, loss = 0.6785348653793335
In grad_steps = 467, loss = 0.706608235836029
In grad_steps = 468, loss = 0.5555989146232605
In grad_steps = 469, loss = 0.5791099071502686
In grad_steps = 470, loss = 0.8082449436187744
In grad_steps = 471, loss = 0.71010422706604
In grad_steps = 472, loss = 0.7117586731910706
In grad_steps = 473, loss = 0.7504491209983826
In grad_steps = 474, loss = 0.7321726679801941
In grad_steps = 475, loss = 0.739121675491333
In grad_steps = 476, loss = 0.6866447329521179
In grad_steps = 477, loss = 0.7431322336196899
In grad_steps = 478, loss = 0.813149631023407
In grad_steps = 479, loss = 0.7523696422576904
In grad_steps = 480, loss = 0.6676627397537231
In grad_steps = 481, loss = 0.7252188324928284
In grad_steps = 482, loss = 0.7252950668334961
In grad_steps = 483, loss = 0.6703342199325562
In grad_steps = 484, loss = 0.7340035438537598
In grad_steps = 485, loss = 0.6817371249198914
In grad_steps = 486, loss = 0.6354014277458191
In grad_steps = 487, loss = 0.6960940957069397
In grad_steps = 488, loss = 0.6353713274002075
In grad_steps = 489, loss = 0.7694616913795471
In grad_steps = 490, loss = 0.7461049556732178
In grad_steps = 491, loss = 0.6454968452453613
In grad_steps = 492, loss = 0.7048702239990234
In grad_steps = 493, loss = 0.7091690897941589
In grad_steps = 494, loss = 0.6630138754844666
In grad_steps = 495, loss = 0.6936744451522827
In grad_steps = 496, loss = 0.6804291009902954
In grad_steps = 497, loss = 0.6616072654724121
In grad_steps = 498, loss = 0.6686372756958008
In grad_steps = 499, loss = 0.670620322227478
Beginning epoch 5
In grad_steps = 500, loss = 0.6464414000511169
In grad_steps = 501, loss = 0.7421888709068298
In grad_steps = 502, loss = 0.6460766792297363
In grad_steps = 503, loss = 0.7214653491973877
In grad_steps = 504, loss = 0.6059819459915161
In grad_steps = 505, loss = 0.6062331795692444
In grad_steps = 506, loss = 0.6337052583694458
In grad_steps = 507, loss = 0.6578078269958496
In grad_steps = 508, loss = 0.5982083082199097
In grad_steps = 509, loss = 0.7428473830223083
In grad_steps = 510, loss = 0.7327271103858948
In grad_steps = 511, loss = 0.6729180812835693
In grad_steps = 512, loss = 0.5800994038581848
In grad_steps = 513, loss = 0.6709548234939575
In grad_steps = 514, loss = 0.6744469404220581
In grad_steps = 515, loss = 0.6711605787277222
In grad_steps = 516, loss = 0.6518231630325317
In grad_steps = 517, loss = 0.735029399394989
In grad_steps = 518, loss = 0.6936138272285461
In grad_steps = 519, loss = 0.6854798793792725
In grad_steps = 520, loss = 0.572679877281189
In grad_steps = 521, loss = 0.6112719774246216
In grad_steps = 522, loss = 0.6527621746063232
In grad_steps = 523, loss = 0.6216327548027039
In grad_steps = 524, loss = 0.6244776248931885
In grad_steps = 525, loss = 0.5603216886520386
In grad_steps = 526, loss = 0.6066644787788391
In grad_steps = 527, loss = 0.47469279170036316
In grad_steps = 528, loss = 0.6403132677078247
In grad_steps = 529, loss = 0.6226867437362671
In grad_steps = 530, loss = 0.4752000570297241
In grad_steps = 531, loss = 0.35125023126602173
In grad_steps = 532, loss = 0.5034089088439941
In grad_steps = 533, loss = 0.3920234739780426
In grad_steps = 534, loss = 0.42806243896484375
In grad_steps = 535, loss = 0.3738096058368683
In grad_steps = 536, loss = 0.2612331807613373
In grad_steps = 537, loss = 0.3039952218532562
In grad_steps = 538, loss = 0.4680788815021515
In grad_steps = 539, loss = 0.14639556407928467
In grad_steps = 540, loss = 0.21034178137779236
In grad_steps = 541, loss = 0.08368689566850662
In grad_steps = 542, loss = 0.6340554356575012
In grad_steps = 543, loss = 0.10387066751718521
In grad_steps = 544, loss = 0.49484747648239136
In grad_steps = 545, loss = 0.1705751270055771
In grad_steps = 546, loss = 0.27100619673728943
In grad_steps = 547, loss = 0.5247077941894531
In grad_steps = 548, loss = 0.47144100069999695
In grad_steps = 549, loss = 0.742292046546936
In grad_steps = 550, loss = 0.6660548448562622
In grad_steps = 551, loss = 0.39028218388557434
In grad_steps = 552, loss = 0.48645448684692383
In grad_steps = 553, loss = 0.6227381229400635
In grad_steps = 554, loss = 0.5941188335418701
In grad_steps = 555, loss = 0.6888217926025391
In grad_steps = 556, loss = 0.7271574139595032
In grad_steps = 557, loss = 0.589732825756073
In grad_steps = 558, loss = 0.6265124678611755
In grad_steps = 559, loss = 0.620602548122406
In grad_steps = 560, loss = 0.5658982396125793
In grad_steps = 561, loss = 0.6830182075500488
In grad_steps = 562, loss = 0.5664469003677368
In grad_steps = 563, loss = 0.5380643606185913
In grad_steps = 564, loss = 0.6590397357940674
In grad_steps = 565, loss = 0.73126220703125
In grad_steps = 566, loss = 0.47727715969085693
In grad_steps = 567, loss = 0.65360027551651
In grad_steps = 568, loss = 0.5064157843589783
In grad_steps = 569, loss = 0.5617851614952087
In grad_steps = 570, loss = 0.6546427607536316
In grad_steps = 571, loss = 0.34908244013786316
In grad_steps = 572, loss = 0.453008234500885
In grad_steps = 573, loss = 0.24563515186309814
In grad_steps = 574, loss = 0.3193747103214264
In grad_steps = 575, loss = 0.3529563546180725
In grad_steps = 576, loss = 0.3699510991573334
In grad_steps = 577, loss = 0.21608015894889832
In grad_steps = 578, loss = 0.8402605652809143
In grad_steps = 579, loss = 0.5363341569900513
In grad_steps = 580, loss = 0.9329108595848083
In grad_steps = 581, loss = 0.6967905163764954
In grad_steps = 582, loss = 0.6809511780738831
In grad_steps = 583, loss = 0.31663158535957336
In grad_steps = 584, loss = 0.3263743817806244
In grad_steps = 585, loss = 0.34949764609336853
In grad_steps = 586, loss = 0.7309690713882446
In grad_steps = 587, loss = 0.7282781600952148
In grad_steps = 588, loss = 0.4704357087612152
In grad_steps = 589, loss = 0.731372058391571
In grad_steps = 590, loss = 0.36484330892562866
In grad_steps = 591, loss = 0.4473862051963806
In grad_steps = 592, loss = 0.469128280878067
In grad_steps = 593, loss = 0.4535546600818634
In grad_steps = 594, loss = 0.4796988368034363
In grad_steps = 595, loss = 0.7831350564956665
In grad_steps = 596, loss = 0.7216215133666992
In grad_steps = 597, loss = 0.6980353593826294
In grad_steps = 598, loss = 0.6828078627586365
In grad_steps = 599, loss = 0.7174422144889832
In grad_steps = 600, loss = 0.7057217359542847
In grad_steps = 601, loss = 0.6841765642166138
In grad_steps = 602, loss = 0.6489314436912537
In grad_steps = 603, loss = 0.8176360130310059
In grad_steps = 604, loss = 0.6503872871398926
In grad_steps = 605, loss = 0.561636209487915
In grad_steps = 606, loss = 0.6168705224990845
In grad_steps = 607, loss = 0.7658329606056213
In grad_steps = 608, loss = 0.6270391345024109
In grad_steps = 609, loss = 0.6520842909812927
In grad_steps = 610, loss = 0.6010366678237915
In grad_steps = 611, loss = 0.6452706456184387
In grad_steps = 612, loss = 0.6648100018501282
In grad_steps = 613, loss = 0.6433603763580322
In grad_steps = 614, loss = 0.7458338141441345
In grad_steps = 615, loss = 0.7412570714950562
In grad_steps = 616, loss = 0.6589220762252808
In grad_steps = 617, loss = 0.6988598108291626
In grad_steps = 618, loss = 0.7135732173919678
In grad_steps = 619, loss = 0.6263646483421326
In grad_steps = 620, loss = 0.6448376178741455
In grad_steps = 621, loss = 0.6761237382888794
In grad_steps = 622, loss = 0.632213830947876
In grad_steps = 623, loss = 0.6502662301063538
In grad_steps = 624, loss = 0.7236136794090271
Beginning epoch 6
In grad_steps = 625, loss = 0.6503434181213379
In grad_steps = 626, loss = 0.6448286175727844
In grad_steps = 627, loss = 0.6507532596588135
In grad_steps = 628, loss = 0.6424404382705688
In grad_steps = 629, loss = 0.5962175726890564
In grad_steps = 630, loss = 0.5824192762374878
In grad_steps = 631, loss = 0.5437146425247192
In grad_steps = 632, loss = 0.6336732506752014
In grad_steps = 633, loss = 0.49102911353111267
In grad_steps = 634, loss = 0.6773765087127686
In grad_steps = 635, loss = 0.800987184047699
In grad_steps = 636, loss = 0.5730570554733276
In grad_steps = 637, loss = 0.4907994568347931
In grad_steps = 638, loss = 0.6463481783866882
In grad_steps = 639, loss = 0.582770824432373
In grad_steps = 640, loss = 0.6276553273200989
In grad_steps = 641, loss = 0.5681974291801453
In grad_steps = 642, loss = 0.5508314371109009
In grad_steps = 643, loss = 0.6880739331245422
In grad_steps = 644, loss = 0.5469165444374084
In grad_steps = 645, loss = 0.39174285531044006
In grad_steps = 646, loss = 0.5224422216415405
In grad_steps = 647, loss = 0.5514113903045654
In grad_steps = 648, loss = 0.4740160405635834
In grad_steps = 649, loss = 0.5615991353988647
In grad_steps = 650, loss = 0.28937390446662903
In grad_steps = 651, loss = 0.45898234844207764
In grad_steps = 652, loss = 0.2870001196861267
In grad_steps = 653, loss = 0.5353119969367981
In grad_steps = 654, loss = 0.7397922873497009
In grad_steps = 655, loss = 0.17142724990844727
In grad_steps = 656, loss = 0.19526217877864838
In grad_steps = 657, loss = 0.1900440752506256
In grad_steps = 658, loss = 0.32978159189224243
In grad_steps = 659, loss = 0.20542965829372406
In grad_steps = 660, loss = 0.1624634712934494
In grad_steps = 661, loss = 0.2031814306974411
In grad_steps = 662, loss = 0.12505926191806793
In grad_steps = 663, loss = 0.05192110687494278
In grad_steps = 664, loss = 0.32470476627349854
In grad_steps = 665, loss = 0.7072106599807739
In grad_steps = 666, loss = 0.018443332985043526
In grad_steps = 667, loss = 0.3303181529045105
In grad_steps = 668, loss = 0.06666857749223709
In grad_steps = 669, loss = 0.45640674233436584
In grad_steps = 670, loss = 0.489340603351593
In grad_steps = 671, loss = 0.4684513509273529
In grad_steps = 672, loss = 0.22680942714214325
In grad_steps = 673, loss = 0.3392062783241272
In grad_steps = 674, loss = 0.12018705904483795
In grad_steps = 675, loss = 0.25566622614860535
In grad_steps = 676, loss = 0.34856218099594116
In grad_steps = 677, loss = 0.5855746269226074
In grad_steps = 678, loss = 0.713737428188324
In grad_steps = 679, loss = 0.9493284225463867
In grad_steps = 680, loss = 0.8521921038627625
In grad_steps = 681, loss = 0.6581323146820068
In grad_steps = 682, loss = 0.5005614161491394
In grad_steps = 683, loss = 0.5227457880973816
In grad_steps = 684, loss = 0.6843768358230591
In grad_steps = 685, loss = 0.4822230339050293
In grad_steps = 686, loss = 0.7096801400184631
In grad_steps = 687, loss = 0.6342592239379883
In grad_steps = 688, loss = 0.609782338142395
In grad_steps = 689, loss = 0.5695714950561523
In grad_steps = 690, loss = 0.6641676425933838
In grad_steps = 691, loss = 0.528954267501831
In grad_steps = 692, loss = 0.5467705726623535
In grad_steps = 693, loss = 0.48681578040122986
In grad_steps = 694, loss = 0.4901367425918579
In grad_steps = 695, loss = 0.6240485906600952
In grad_steps = 696, loss = 0.4037580192089081
In grad_steps = 697, loss = 0.4159374535083771
In grad_steps = 698, loss = 0.32524582743644714
In grad_steps = 699, loss = 0.31954413652420044
In grad_steps = 700, loss = 0.4425467252731323
In grad_steps = 701, loss = 0.26189887523651123
In grad_steps = 702, loss = 0.22060725092887878
In grad_steps = 703, loss = 0.40905362367630005
In grad_steps = 704, loss = 0.2904777228832245
In grad_steps = 705, loss = 0.6052787899971008
In grad_steps = 706, loss = 0.08786629885435104
In grad_steps = 707, loss = 0.4015750586986542
In grad_steps = 708, loss = 0.14501643180847168
In grad_steps = 709, loss = 0.7581145763397217
In grad_steps = 710, loss = 0.4722979664802551
In grad_steps = 711, loss = 0.49166247248649597
In grad_steps = 712, loss = 0.35658687353134155
In grad_steps = 713, loss = 0.09193798154592514
In grad_steps = 714, loss = 0.6648378968238831
In grad_steps = 715, loss = 0.862916886806488
In grad_steps = 716, loss = 1.2683719396591187
In grad_steps = 717, loss = 0.28844499588012695
In grad_steps = 718, loss = 0.36090028285980225
In grad_steps = 719, loss = 0.5266765356063843
In grad_steps = 720, loss = 0.5023195147514343
In grad_steps = 721, loss = 0.5907891392707825
In grad_steps = 722, loss = 0.7150947451591492
In grad_steps = 723, loss = 0.6475549340248108
In grad_steps = 724, loss = 0.5713984966278076
In grad_steps = 725, loss = 0.58946692943573
In grad_steps = 726, loss = 0.5743648409843445
In grad_steps = 727, loss = 0.5421524047851562
In grad_steps = 728, loss = 0.6076809167861938
In grad_steps = 729, loss = 0.5267520546913147
In grad_steps = 730, loss = 0.5050458908081055
In grad_steps = 731, loss = 0.6363625526428223
In grad_steps = 732, loss = 0.5818618535995483
In grad_steps = 733, loss = 0.5419273376464844
In grad_steps = 734, loss = 0.5214828848838806
In grad_steps = 735, loss = 0.45343613624572754
In grad_steps = 736, loss = 0.4959560036659241
In grad_steps = 737, loss = 0.4936833083629608
In grad_steps = 738, loss = 0.5259081721305847
In grad_steps = 739, loss = 0.8928242325782776
In grad_steps = 740, loss = 0.7467330694198608
In grad_steps = 741, loss = 0.5639272332191467
In grad_steps = 742, loss = 0.7009410858154297
In grad_steps = 743, loss = 0.6392168998718262
In grad_steps = 744, loss = 0.5664371848106384
In grad_steps = 745, loss = 0.3994775712490082
In grad_steps = 746, loss = 0.49509984254837036
In grad_steps = 747, loss = 0.5367350578308105
In grad_steps = 748, loss = 0.5426227450370789
In grad_steps = 749, loss = 0.6125269532203674
Beginning epoch 7
In grad_steps = 750, loss = 0.41074270009994507
In grad_steps = 751, loss = 0.4823978841304779
In grad_steps = 752, loss = 0.4708944261074066
In grad_steps = 753, loss = 0.48853394389152527
In grad_steps = 754, loss = 0.5928354263305664
In grad_steps = 755, loss = 0.536169707775116
In grad_steps = 756, loss = 0.6414743065834045
In grad_steps = 757, loss = 0.546836793422699
In grad_steps = 758, loss = 0.553498387336731
In grad_steps = 759, loss = 0.5872704982757568
In grad_steps = 760, loss = 0.8587803244590759
In grad_steps = 761, loss = 0.44641339778900146
In grad_steps = 762, loss = 0.34577393531799316
In grad_steps = 763, loss = 0.5550662875175476
In grad_steps = 764, loss = 0.4800090193748474
In grad_steps = 765, loss = 0.6190128922462463
In grad_steps = 766, loss = 0.48496773838996887
In grad_steps = 767, loss = 0.41350609064102173
In grad_steps = 768, loss = 0.45859551429748535
In grad_steps = 769, loss = 0.3407008945941925
In grad_steps = 770, loss = 0.2743421494960785
In grad_steps = 771, loss = 0.5473647713661194
In grad_steps = 772, loss = 0.46288150548934937
In grad_steps = 773, loss = 0.34378284215927124
In grad_steps = 774, loss = 0.3819602131843567
In grad_steps = 775, loss = 0.25067654252052307
In grad_steps = 776, loss = 0.361278772354126
In grad_steps = 777, loss = 0.10637850314378738
In grad_steps = 778, loss = 0.3997151255607605
In grad_steps = 779, loss = 0.48611798882484436
In grad_steps = 780, loss = 0.14918813109397888
In grad_steps = 781, loss = 0.11342594027519226
In grad_steps = 782, loss = 0.21724861860275269
In grad_steps = 783, loss = 0.16352388262748718
In grad_steps = 784, loss = 0.06769448518753052
In grad_steps = 785, loss = 0.17122125625610352
In grad_steps = 786, loss = 0.326368510723114
In grad_steps = 787, loss = 0.38104134798049927
In grad_steps = 788, loss = 0.0726931095123291
In grad_steps = 789, loss = 0.011385912075638771
In grad_steps = 790, loss = 0.23201636970043182
In grad_steps = 791, loss = 0.18938030302524567
In grad_steps = 792, loss = 1.0097299814224243
In grad_steps = 793, loss = 0.1909581422805786
In grad_steps = 794, loss = 0.3811652660369873
In grad_steps = 795, loss = 0.1271253526210785
In grad_steps = 796, loss = 0.18393704295158386
In grad_steps = 797, loss = 0.8573153018951416
In grad_steps = 798, loss = 0.8973369002342224
In grad_steps = 799, loss = 0.265264630317688
In grad_steps = 800, loss = 0.4903373718261719
In grad_steps = 801, loss = 0.24896936118602753
In grad_steps = 802, loss = 0.2700323462486267
In grad_steps = 803, loss = 0.5285959839820862
In grad_steps = 804, loss = 0.6024174690246582
In grad_steps = 805, loss = 0.7277921438217163
In grad_steps = 806, loss = 0.6196221113204956
In grad_steps = 807, loss = 0.49512070417404175
In grad_steps = 808, loss = 0.7377864718437195
In grad_steps = 809, loss = 0.49373510479927063
In grad_steps = 810, loss = 0.4090321958065033
In grad_steps = 811, loss = 0.5248441100120544
In grad_steps = 812, loss = 0.4626235365867615
In grad_steps = 813, loss = 0.4820978045463562
In grad_steps = 814, loss = 0.4522039294242859
In grad_steps = 815, loss = 0.6540220975875854
In grad_steps = 816, loss = 0.549366295337677
In grad_steps = 817, loss = 0.41827547550201416
In grad_steps = 818, loss = 0.4218139052391052
In grad_steps = 819, loss = 0.4922657012939453
In grad_steps = 820, loss = 0.4828571677207947
In grad_steps = 821, loss = 0.14917583763599396
In grad_steps = 822, loss = 0.1744736284017563
In grad_steps = 823, loss = 0.17283952236175537
In grad_steps = 824, loss = 0.17840196192264557
In grad_steps = 825, loss = 0.16886526346206665
In grad_steps = 826, loss = 0.17805062234401703
In grad_steps = 827, loss = 0.2005150318145752
In grad_steps = 828, loss = 0.1715354174375534
In grad_steps = 829, loss = 0.2642819583415985
In grad_steps = 830, loss = 0.17926163971424103
In grad_steps = 831, loss = 0.18755893409252167
In grad_steps = 832, loss = 0.265864759683609
In grad_steps = 833, loss = 0.303901344537735
In grad_steps = 834, loss = 0.37937164306640625
In grad_steps = 835, loss = 0.12736646831035614
In grad_steps = 836, loss = 0.3472454845905304
In grad_steps = 837, loss = 0.10371760278940201
In grad_steps = 838, loss = 0.7661633491516113
In grad_steps = 839, loss = 0.48461994528770447
In grad_steps = 840, loss = 0.04628195986151695
In grad_steps = 841, loss = 0.5326130986213684
In grad_steps = 842, loss = 0.5480172634124756
In grad_steps = 843, loss = 0.5662459135055542
In grad_steps = 844, loss = 0.7813518047332764
In grad_steps = 845, loss = 0.41662684082984924
In grad_steps = 846, loss = 0.2764016389846802
In grad_steps = 847, loss = 0.3984558880329132
In grad_steps = 848, loss = 0.5567932724952698
In grad_steps = 849, loss = 0.45140746235847473
In grad_steps = 850, loss = 0.5496615767478943
In grad_steps = 851, loss = 0.4866522550582886
In grad_steps = 852, loss = 0.3530440032482147
In grad_steps = 853, loss = 0.3771289587020874
In grad_steps = 854, loss = 0.3708115816116333
In grad_steps = 855, loss = 0.5753741264343262
In grad_steps = 856, loss = 0.44666391611099243
In grad_steps = 857, loss = 0.6254938244819641
In grad_steps = 858, loss = 0.4461796283721924
In grad_steps = 859, loss = 0.37718144059181213
In grad_steps = 860, loss = 0.2789764404296875
In grad_steps = 861, loss = 0.4539065957069397
In grad_steps = 862, loss = 0.27809396386146545
In grad_steps = 863, loss = 0.41399165987968445
In grad_steps = 864, loss = 0.4798130393028259
In grad_steps = 865, loss = 0.291677325963974
In grad_steps = 866, loss = 0.24291861057281494
In grad_steps = 867, loss = 0.43425777554512024
In grad_steps = 868, loss = 0.5690476298332214
In grad_steps = 869, loss = 0.31338122487068176
In grad_steps = 870, loss = 0.39702659845352173
In grad_steps = 871, loss = 0.4892655909061432
In grad_steps = 872, loss = 0.3588046133518219
In grad_steps = 873, loss = 0.3056703209877014
In grad_steps = 874, loss = 0.18773910403251648
Beginning epoch 8
In grad_steps = 875, loss = 0.09660301357507706
In grad_steps = 876, loss = 0.23651066422462463
In grad_steps = 877, loss = 0.20823754370212555
In grad_steps = 878, loss = 0.19291305541992188
In grad_steps = 879, loss = 0.2702510952949524
In grad_steps = 880, loss = 0.45837029814720154
In grad_steps = 881, loss = 0.259589821100235
In grad_steps = 882, loss = 0.3772050142288208
In grad_steps = 883, loss = 0.26523271203041077
In grad_steps = 884, loss = 0.20238150656223297
In grad_steps = 885, loss = 0.22842958569526672
In grad_steps = 886, loss = 0.4378220736980438
In grad_steps = 887, loss = 0.22917914390563965
In grad_steps = 888, loss = 0.3092564642429352
In grad_steps = 889, loss = 0.2849883735179901
In grad_steps = 890, loss = 0.2618023157119751
In grad_steps = 891, loss = 0.16363106667995453
In grad_steps = 892, loss = 0.24000170826911926
In grad_steps = 893, loss = 0.3953714072704315
In grad_steps = 894, loss = 0.48047515749931335
In grad_steps = 895, loss = 0.2842344045639038
In grad_steps = 896, loss = 0.21804143488407135
In grad_steps = 897, loss = 0.15434560179710388
In grad_steps = 898, loss = 0.05449283868074417
In grad_steps = 899, loss = 0.09594075381755829
In grad_steps = 900, loss = 0.03785690665245056
In grad_steps = 901, loss = 0.11615405976772308
In grad_steps = 902, loss = 0.08641126751899719
In grad_steps = 903, loss = 0.10144492238759995
In grad_steps = 904, loss = 0.22260414063930511
In grad_steps = 905, loss = 0.1698206514120102
In grad_steps = 906, loss = 0.01806875318288803
In grad_steps = 907, loss = 0.27842530608177185
In grad_steps = 908, loss = 0.15253645181655884
In grad_steps = 909, loss = 0.1467355638742447
In grad_steps = 910, loss = 0.018224721774458885
In grad_steps = 911, loss = 0.05126255005598068
In grad_steps = 912, loss = 0.07406299561262131
In grad_steps = 913, loss = 0.20410676300525665
In grad_steps = 914, loss = 0.2971626818180084
In grad_steps = 915, loss = 0.08644725382328033
In grad_steps = 916, loss = 0.05174553394317627
In grad_steps = 917, loss = 0.35167360305786133
In grad_steps = 918, loss = 0.06754504889249802
In grad_steps = 919, loss = 0.4294125735759735
In grad_steps = 920, loss = 0.08439785987138748
In grad_steps = 921, loss = 0.036458153277635574
In grad_steps = 922, loss = 0.1559421420097351
In grad_steps = 923, loss = 0.21045313775539398
In grad_steps = 924, loss = 0.1971627175807953
In grad_steps = 925, loss = 0.1933862417936325
In grad_steps = 926, loss = 0.049236077815294266
In grad_steps = 927, loss = 0.044930219650268555
In grad_steps = 928, loss = 0.5539746284484863
In grad_steps = 929, loss = 0.22450041770935059
In grad_steps = 930, loss = 0.4883424639701843
In grad_steps = 931, loss = 0.5762944221496582
In grad_steps = 932, loss = 0.4458315968513489
In grad_steps = 933, loss = 0.6810854077339172
In grad_steps = 934, loss = 0.2241416871547699
In grad_steps = 935, loss = 0.1744503229856491
In grad_steps = 936, loss = 0.4798719584941864
In grad_steps = 937, loss = 0.3261856138706207
In grad_steps = 938, loss = 0.25715214014053345
In grad_steps = 939, loss = 0.25132256746292114
In grad_steps = 940, loss = 0.5642516016960144
In grad_steps = 941, loss = 0.20490184426307678
In grad_steps = 942, loss = 0.32451164722442627
In grad_steps = 943, loss = 0.21554778516292572
In grad_steps = 944, loss = 0.2907264530658722
In grad_steps = 945, loss = 0.30974578857421875
In grad_steps = 946, loss = 0.30123454332351685
In grad_steps = 947, loss = 0.0851970911026001
In grad_steps = 948, loss = 0.11838731914758682
In grad_steps = 949, loss = 0.11503906548023224
In grad_steps = 950, loss = 0.05905374512076378
In grad_steps = 951, loss = 0.07676137983798981
In grad_steps = 952, loss = 0.056444521993398666
In grad_steps = 953, loss = 0.141764298081398
In grad_steps = 954, loss = 0.08323977887630463
In grad_steps = 955, loss = 0.0920574739575386
In grad_steps = 956, loss = 0.02235378324985504
In grad_steps = 957, loss = 0.1831541657447815
In grad_steps = 958, loss = 0.06568127125501633
In grad_steps = 959, loss = 0.06749247759580612
In grad_steps = 960, loss = 0.053611550480127335
In grad_steps = 961, loss = 0.17914408445358276
In grad_steps = 962, loss = 0.13088996708393097
In grad_steps = 963, loss = 0.35731109976768494
In grad_steps = 964, loss = 0.05726734176278114
In grad_steps = 965, loss = 0.15783749520778656
In grad_steps = 966, loss = 0.4616985619068146
In grad_steps = 967, loss = 0.47386497259140015
In grad_steps = 968, loss = 0.6025846004486084
In grad_steps = 969, loss = 0.04711532220244408
In grad_steps = 970, loss = 0.05258103460073471
In grad_steps = 971, loss = 0.34308937191963196
In grad_steps = 972, loss = 0.37243473529815674
In grad_steps = 973, loss = 0.06593631953001022
In grad_steps = 974, loss = 0.3468761146068573
In grad_steps = 975, loss = 0.7344438433647156
In grad_steps = 976, loss = 0.5868282914161682
In grad_steps = 977, loss = 0.1442369669675827
In grad_steps = 978, loss = 0.18795794248580933
In grad_steps = 979, loss = 0.30560165643692017
In grad_steps = 980, loss = 0.5891952514648438
In grad_steps = 981, loss = 0.5435818433761597
In grad_steps = 982, loss = 0.4692569077014923
In grad_steps = 983, loss = 0.5459743142127991
In grad_steps = 984, loss = 0.3695424497127533
In grad_steps = 985, loss = 0.29050391912460327
In grad_steps = 986, loss = 0.4778619408607483
In grad_steps = 987, loss = 0.25207608938217163
In grad_steps = 988, loss = 0.21445852518081665
In grad_steps = 989, loss = 0.28086400032043457
In grad_steps = 990, loss = 0.42180895805358887
In grad_steps = 991, loss = 0.3559733033180237
In grad_steps = 992, loss = 0.3104972541332245
In grad_steps = 993, loss = 0.33863645792007446
In grad_steps = 994, loss = 0.3124009370803833
In grad_steps = 995, loss = 0.2351764440536499
In grad_steps = 996, loss = 0.3819772005081177
In grad_steps = 997, loss = 0.185861274600029
In grad_steps = 998, loss = 0.18409350514411926
In grad_steps = 999, loss = 0.35578712821006775
Elapsed time: 3355.86749458313 seconds for ensemble 3 with 8 epochs
LoRA instance 3 evaluation complete. Data saved to /pscratch/sd/t/tianle/lucid/other_source/SURP_2024/results/tmp_lora_ensemble/Llama3-8B-set-3c/test_data_instance_3_seed_30280.npz.
lora instance i = 3 Successfully finished.
Training lora instance 4
DEBUG: Total parameters: 8037076992
DEBUG: Trainable parameters: 6815744
Beginning epoch 1
In grad_steps = 0, loss = 0.7359974980354309
In grad_steps = 1, loss = 0.8052975535392761
In grad_steps = 2, loss = 0.7686110138893127
In grad_steps = 3, loss = 0.6513367295265198
In grad_steps = 4, loss = 0.9087405204772949
In grad_steps = 5, loss = 0.7110732197761536
In grad_steps = 6, loss = 0.6406668424606323
In grad_steps = 7, loss = 0.7719110250473022
In grad_steps = 8, loss = 0.5908856391906738
In grad_steps = 9, loss = 0.8596948385238647
In grad_steps = 10, loss = 0.8544585108757019
In grad_steps = 11, loss = 0.7609535455703735
In grad_steps = 12, loss = 0.7100650072097778
In grad_steps = 13, loss = 0.6431960463523865
In grad_steps = 14, loss = 0.7998364567756653
In grad_steps = 15, loss = 0.8189253211021423
In grad_steps = 16, loss = 0.7167626619338989
In grad_steps = 17, loss = 0.809322714805603
In grad_steps = 18, loss = 0.7002752423286438
In grad_steps = 19, loss = 0.7235768437385559
In grad_steps = 20, loss = 0.7299394607543945
In grad_steps = 21, loss = 0.7668481469154358
In grad_steps = 22, loss = 0.7265846729278564
In grad_steps = 23, loss = 0.7675836086273193
In grad_steps = 24, loss = 0.7579175233840942
In grad_steps = 25, loss = 0.7113703489303589
In grad_steps = 26, loss = 0.6972371339797974
In grad_steps = 27, loss = 0.6331672072410583
In grad_steps = 28, loss = 0.7300286293029785
In grad_steps = 29, loss = 0.7461602687835693
In grad_steps = 30, loss = 0.7820837497711182
In grad_steps = 31, loss = 0.5714337229728699
In grad_steps = 32, loss = 0.7644461393356323
In grad_steps = 33, loss = 0.7489945888519287
In grad_steps = 34, loss = 0.6448806524276733
In grad_steps = 35, loss = 0.6686041951179504
In grad_steps = 36, loss = 0.6550868153572083
In grad_steps = 37, loss = 0.6888182163238525
In grad_steps = 38, loss = 0.7727397084236145
In grad_steps = 39, loss = 0.7267574667930603
In grad_steps = 40, loss = 0.7003397941589355
In grad_steps = 41, loss = 0.714138925075531
In grad_steps = 42, loss = 0.662658154964447
In grad_steps = 43, loss = 0.7064169049263
In grad_steps = 44, loss = 0.7311786413192749
In grad_steps = 45, loss = 0.7571418285369873
In grad_steps = 46, loss = 0.7825719118118286
In grad_steps = 47, loss = 0.7473932504653931
In grad_steps = 48, loss = 0.702369213104248
In grad_steps = 49, loss = 0.7027957439422607
In grad_steps = 50, loss = 0.6882452368736267
In grad_steps = 51, loss = 0.6860639452934265
In grad_steps = 52, loss = 0.7378917336463928
In grad_steps = 53, loss = 0.6641187071800232
In grad_steps = 54, loss = 0.7717204093933105
In grad_steps = 55, loss = 0.7407930493354797
In grad_steps = 56, loss = 0.7091090679168701
In grad_steps = 57, loss = 0.718991219997406
In grad_steps = 58, loss = 0.753865659236908
In grad_steps = 59, loss = 0.6828755736351013
In grad_steps = 60, loss = 0.6886762976646423
In grad_steps = 61, loss = 0.7045714855194092
In grad_steps = 62, loss = 0.7153722643852234
In grad_steps = 63, loss = 0.7116780877113342
In grad_steps = 64, loss = 0.668514609336853
In grad_steps = 65, loss = 0.6833473443984985
In grad_steps = 66, loss = 0.7407001852989197
In grad_steps = 67, loss = 0.6884961128234863
In grad_steps = 68, loss = 0.7124454379081726
In grad_steps = 69, loss = 0.6919659376144409
In grad_steps = 70, loss = 0.6956693530082703
In grad_steps = 71, loss = 0.682279646396637
In grad_steps = 72, loss = 0.7029730081558228
In grad_steps = 73, loss = 0.7008275389671326
In grad_steps = 74, loss = 0.6843835115432739
In grad_steps = 75, loss = 0.6838945150375366
In grad_steps = 76, loss = 0.7105600833892822
In grad_steps = 77, loss = 0.7183416485786438
In grad_steps = 78, loss = 0.692671000957489
In grad_steps = 79, loss = 0.6823502779006958
In grad_steps = 80, loss = 0.7219638228416443
In grad_steps = 81, loss = 0.7088945508003235
In grad_steps = 82, loss = 0.7037314772605896
In grad_steps = 83, loss = 0.6978597640991211
In grad_steps = 84, loss = 0.668400764465332
In grad_steps = 85, loss = 0.6660255193710327
In grad_steps = 86, loss = 0.7675604820251465
In grad_steps = 87, loss = 0.686495304107666
In grad_steps = 88, loss = 0.6907474398612976
In grad_steps = 89, loss = 0.7231852412223816
In grad_steps = 90, loss = 0.7179532647132874
In grad_steps = 91, loss = 0.7221038341522217
In grad_steps = 92, loss = 0.6854125261306763
In grad_steps = 93, loss = 0.7245176434516907
In grad_steps = 94, loss = 0.7001265287399292
In grad_steps = 95, loss = 0.6624143123626709
In grad_steps = 96, loss = 0.6662488579750061
In grad_steps = 97, loss = 0.6800284385681152
In grad_steps = 98, loss = 0.6994229555130005
In grad_steps = 99, loss = 0.7030975818634033
In grad_steps = 100, loss = 0.7114118337631226
In grad_steps = 101, loss = 0.6790880560874939
In grad_steps = 102, loss = 0.7037989497184753
In grad_steps = 103, loss = 0.7168951630592346
In grad_steps = 104, loss = 0.7248365879058838
In grad_steps = 105, loss = 0.6905950307846069
In grad_steps = 106, loss = 0.6970954537391663
In grad_steps = 107, loss = 0.6937664747238159
In grad_steps = 108, loss = 0.699918270111084
In grad_steps = 109, loss = 0.7114477157592773
In grad_steps = 110, loss = 0.6927582621574402
In grad_steps = 111, loss = 0.6759909391403198
In grad_steps = 112, loss = 0.6909130215644836
In grad_steps = 113, loss = 0.6434808373451233
In grad_steps = 114, loss = 0.7292685508728027
In grad_steps = 115, loss = 0.7436215281486511
In grad_steps = 116, loss = 0.647102952003479
In grad_steps = 117, loss = 0.7075233459472656
In grad_steps = 118, loss = 0.7298146486282349
In grad_steps = 119, loss = 0.6726737022399902
In grad_steps = 120, loss = 0.6995752453804016
In grad_steps = 121, loss = 0.7059880495071411
In grad_steps = 122, loss = 0.6795102953910828
In grad_steps = 123, loss = 0.7402992248535156
In grad_steps = 124, loss = 0.7144787311553955
Beginning epoch 2
In grad_steps = 125, loss = 0.692629337310791
In grad_steps = 126, loss = 0.6898892521858215
In grad_steps = 127, loss = 0.686418890953064
In grad_steps = 128, loss = 0.7043416500091553
In grad_steps = 129, loss = 0.6748248338699341
In grad_steps = 130, loss = 0.6654870510101318
In grad_steps = 131, loss = 0.6862972378730774
In grad_steps = 132, loss = 0.6771998405456543
In grad_steps = 133, loss = 0.6348714828491211
In grad_steps = 134, loss = 0.7289808988571167
In grad_steps = 135, loss = 0.7525489926338196
In grad_steps = 136, loss = 0.7318068146705627
In grad_steps = 137, loss = 0.7215655446052551
In grad_steps = 138, loss = 0.7321872115135193
In grad_steps = 139, loss = 0.6758731603622437
In grad_steps = 140, loss = 0.682976484298706
In grad_steps = 141, loss = 0.7119008302688599
In grad_steps = 142, loss = 0.6872940063476562
In grad_steps = 143, loss = 0.6847670078277588
In grad_steps = 144, loss = 0.6809511184692383
In grad_steps = 145, loss = 0.7046744227409363
In grad_steps = 146, loss = 0.6918888092041016
In grad_steps = 147, loss = 0.6929392218589783
In grad_steps = 148, loss = 0.6784622073173523
In grad_steps = 149, loss = 0.6459043622016907
In grad_steps = 150, loss = 0.6257277131080627
In grad_steps = 151, loss = 0.710576057434082
In grad_steps = 152, loss = 0.6223307251930237
In grad_steps = 153, loss = 0.7318027019500732
In grad_steps = 154, loss = 0.742008626461029
In grad_steps = 155, loss = 0.7478024959564209
In grad_steps = 156, loss = 0.5753052234649658
In grad_steps = 157, loss = 0.7204803228378296
In grad_steps = 158, loss = 0.7159320712089539
In grad_steps = 159, loss = 0.6395139694213867
In grad_steps = 160, loss = 0.6628527641296387
In grad_steps = 161, loss = 0.6617471575737
In grad_steps = 162, loss = 0.6830899715423584
In grad_steps = 163, loss = 0.7400429844856262
In grad_steps = 164, loss = 0.7175410389900208
In grad_steps = 165, loss = 0.700765073299408
In grad_steps = 166, loss = 0.6878032684326172
In grad_steps = 167, loss = 0.6999817490577698
In grad_steps = 168, loss = 0.6587507128715515
In grad_steps = 169, loss = 0.7035701870918274
In grad_steps = 170, loss = 0.7208673357963562
In grad_steps = 171, loss = 0.7168201208114624
In grad_steps = 172, loss = 0.7256004810333252
In grad_steps = 173, loss = 0.696507453918457
In grad_steps = 174, loss = 0.6922258138656616
In grad_steps = 175, loss = 0.6942230463027954
In grad_steps = 176, loss = 0.6897128820419312
In grad_steps = 177, loss = 0.692081868648529
In grad_steps = 178, loss = 0.6913132667541504
In grad_steps = 179, loss = 0.6964922547340393
In grad_steps = 180, loss = 0.6916953921318054
In grad_steps = 181, loss = 0.6914187669754028
In grad_steps = 182, loss = 0.6933465600013733
In grad_steps = 183, loss = 0.7004998922348022
In grad_steps = 184, loss = 0.6746475100517273
In grad_steps = 185, loss = 0.6854082942008972
In grad_steps = 186, loss = 0.6895089149475098
In grad_steps = 187, loss = 0.693716824054718
In grad_steps = 188, loss = 0.689537763595581
In grad_steps = 189, loss = 0.6784878969192505
In grad_steps = 190, loss = 0.6855338215827942
In grad_steps = 191, loss = 0.7021685838699341
In grad_steps = 192, loss = 0.6852961778640747
In grad_steps = 193, loss = 0.6979620456695557
In grad_steps = 194, loss = 0.684493899345398
In grad_steps = 195, loss = 0.6991356015205383
In grad_steps = 196, loss = 0.6900590658187866
In grad_steps = 197, loss = 0.6903241872787476
In grad_steps = 198, loss = 0.6688095331192017
In grad_steps = 199, loss = 0.68406742811203
In grad_steps = 200, loss = 0.6948148012161255
In grad_steps = 201, loss = 0.6945129632949829
In grad_steps = 202, loss = 0.6944190859794617
In grad_steps = 203, loss = 0.6767678260803223
In grad_steps = 204, loss = 0.6876779198646545
In grad_steps = 205, loss = 0.7153769135475159
In grad_steps = 206, loss = 0.6864194869995117
In grad_steps = 207, loss = 0.6989868879318237
In grad_steps = 208, loss = 0.7060155272483826
In grad_steps = 209, loss = 0.6617116332054138
In grad_steps = 210, loss = 0.6409503817558289
In grad_steps = 211, loss = 0.786029577255249
In grad_steps = 212, loss = 0.6828904151916504
In grad_steps = 213, loss = 0.6848990321159363
In grad_steps = 214, loss = 0.7262770533561707
In grad_steps = 215, loss = 0.7077620625495911
In grad_steps = 216, loss = 0.7278551459312439
In grad_steps = 217, loss = 0.6826596260070801
In grad_steps = 218, loss = 0.701947808265686
In grad_steps = 219, loss = 0.6838545799255371
In grad_steps = 220, loss = 0.6778616309165955
In grad_steps = 221, loss = 0.6718825697898865
In grad_steps = 222, loss = 0.693841814994812
In grad_steps = 223, loss = 0.7056037187576294
In grad_steps = 224, loss = 0.6947270631790161
In grad_steps = 225, loss = 0.7052868008613586
In grad_steps = 226, loss = 0.6838988661766052
In grad_steps = 227, loss = 0.6999280452728271
In grad_steps = 228, loss = 0.7105421423912048
In grad_steps = 229, loss = 0.7224693298339844
In grad_steps = 230, loss = 0.691712498664856
In grad_steps = 231, loss = 0.7048423290252686
In grad_steps = 232, loss = 0.6991767883300781
In grad_steps = 233, loss = 0.689918041229248
In grad_steps = 234, loss = 0.699516236782074
In grad_steps = 235, loss = 0.6887990236282349
In grad_steps = 236, loss = 0.6769004464149475
In grad_steps = 237, loss = 0.6874208450317383
In grad_steps = 238, loss = 0.6539087891578674
In grad_steps = 239, loss = 0.7201861143112183
In grad_steps = 240, loss = 0.7312287092208862
In grad_steps = 241, loss = 0.6499537825584412
In grad_steps = 242, loss = 0.7041090130805969
In grad_steps = 243, loss = 0.720301628112793
In grad_steps = 244, loss = 0.6708011627197266
In grad_steps = 245, loss = 0.6979019641876221
In grad_steps = 246, loss = 0.7045060396194458
In grad_steps = 247, loss = 0.6762799620628357
In grad_steps = 248, loss = 0.7455600500106812
In grad_steps = 249, loss = 0.7176181077957153
Beginning epoch 3
In grad_steps = 250, loss = 0.6984831094741821
In grad_steps = 251, loss = 0.674359917640686
In grad_steps = 252, loss = 0.6843119859695435
In grad_steps = 253, loss = 0.6953536868095398
In grad_steps = 254, loss = 0.677473247051239
In grad_steps = 255, loss = 0.6650528907775879
In grad_steps = 256, loss = 0.6799325942993164
In grad_steps = 257, loss = 0.6744372248649597
In grad_steps = 258, loss = 0.625647246837616
In grad_steps = 259, loss = 0.732743501663208
In grad_steps = 260, loss = 0.7610881924629211
In grad_steps = 261, loss = 0.7300494313240051
In grad_steps = 262, loss = 0.7066004872322083
In grad_steps = 263, loss = 0.730920135974884
In grad_steps = 264, loss = 0.6670807003974915
In grad_steps = 265, loss = 0.6766690611839294
In grad_steps = 266, loss = 0.6881999373435974
In grad_steps = 267, loss = 0.6901175379753113
In grad_steps = 268, loss = 0.6712205410003662
In grad_steps = 269, loss = 0.6718562841415405
In grad_steps = 270, loss = 0.6766923666000366
In grad_steps = 271, loss = 0.678575873374939
In grad_steps = 272, loss = 0.6885623335838318
In grad_steps = 273, loss = 0.6682921051979065
In grad_steps = 274, loss = 0.6230045557022095
In grad_steps = 275, loss = 0.614780604839325
In grad_steps = 276, loss = 0.6996403932571411
In grad_steps = 277, loss = 0.5826065540313721
In grad_steps = 278, loss = 0.7237527370452881
In grad_steps = 279, loss = 0.7703622579574585
In grad_steps = 280, loss = 0.6700049042701721
In grad_steps = 281, loss = 0.5391556024551392
In grad_steps = 282, loss = 0.6416109800338745
In grad_steps = 283, loss = 0.7058637142181396
In grad_steps = 284, loss = 0.6476265788078308
In grad_steps = 285, loss = 0.6566684246063232
In grad_steps = 286, loss = 0.6624406576156616
In grad_steps = 287, loss = 0.6423671841621399
In grad_steps = 288, loss = 0.7112458348274231
In grad_steps = 289, loss = 0.7118218541145325
In grad_steps = 290, loss = 0.684090793132782
In grad_steps = 291, loss = 0.6535459756851196
In grad_steps = 292, loss = 0.655823290348053
In grad_steps = 293, loss = 0.5433054566383362
In grad_steps = 294, loss = 0.7683319449424744
In grad_steps = 295, loss = 0.6881439685821533
In grad_steps = 296, loss = 0.7614039778709412
In grad_steps = 297, loss = 0.7756023406982422
In grad_steps = 298, loss = 0.7518715858459473
In grad_steps = 299, loss = 0.6247546672821045
In grad_steps = 300, loss = 0.6603183746337891
In grad_steps = 301, loss = 0.633444607257843
In grad_steps = 302, loss = 0.7255536913871765
In grad_steps = 303, loss = 0.6660417318344116
In grad_steps = 304, loss = 0.7085689902305603
In grad_steps = 305, loss = 0.7123273611068726
In grad_steps = 306, loss = 0.7195292711257935
In grad_steps = 307, loss = 0.6843445301055908
In grad_steps = 308, loss = 0.6875108480453491
In grad_steps = 309, loss = 0.6757757663726807
In grad_steps = 310, loss = 0.6501438617706299
In grad_steps = 311, loss = 0.7080552577972412
In grad_steps = 312, loss = 0.6921154260635376
In grad_steps = 313, loss = 0.6949818134307861
In grad_steps = 314, loss = 0.6525163650512695
In grad_steps = 315, loss = 0.6695101261138916
In grad_steps = 316, loss = 0.7081143260002136
In grad_steps = 317, loss = 0.6750016808509827
In grad_steps = 318, loss = 0.6688594818115234
In grad_steps = 319, loss = 0.642451286315918
In grad_steps = 320, loss = 0.7243523001670837
In grad_steps = 321, loss = 0.6226595640182495
In grad_steps = 322, loss = 0.701568067073822
In grad_steps = 323, loss = 0.604949414730072
In grad_steps = 324, loss = 0.6528860926628113
In grad_steps = 325, loss = 0.6588007807731628
In grad_steps = 326, loss = 0.6859956979751587
In grad_steps = 327, loss = 0.706301212310791
In grad_steps = 328, loss = 0.6597283482551575
In grad_steps = 329, loss = 0.6882133483886719
In grad_steps = 330, loss = 0.7173618078231812
In grad_steps = 331, loss = 0.6102660298347473
In grad_steps = 332, loss = 0.6819261312484741
In grad_steps = 333, loss = 0.7686012983322144
In grad_steps = 334, loss = 0.6260241270065308
In grad_steps = 335, loss = 0.5813260674476624
In grad_steps = 336, loss = 0.8609808087348938
In grad_steps = 337, loss = 0.6946138143539429
In grad_steps = 338, loss = 0.6672316193580627
In grad_steps = 339, loss = 0.7552317976951599
In grad_steps = 340, loss = 0.615056037902832
In grad_steps = 341, loss = 0.6867464184761047
In grad_steps = 342, loss = 0.6742812991142273
In grad_steps = 343, loss = 0.6339715123176575
In grad_steps = 344, loss = 0.6301009058952332
In grad_steps = 345, loss = 0.7569330930709839
In grad_steps = 346, loss = 0.6786104440689087
In grad_steps = 347, loss = 0.7528030276298523
In grad_steps = 348, loss = 0.7126328945159912
In grad_steps = 349, loss = 0.6657066941261292
In grad_steps = 350, loss = 0.7008194327354431
In grad_steps = 351, loss = 0.6812853217124939
In grad_steps = 352, loss = 0.692692756652832
In grad_steps = 353, loss = 0.7324924468994141
In grad_steps = 354, loss = 0.7475717067718506
In grad_steps = 355, loss = 0.6744601130485535
In grad_steps = 356, loss = 0.7442830204963684
In grad_steps = 357, loss = 0.7015441656112671
In grad_steps = 358, loss = 0.6825963854789734
In grad_steps = 359, loss = 0.6819398403167725
In grad_steps = 360, loss = 0.6596693396568298
In grad_steps = 361, loss = 0.6546708345413208
In grad_steps = 362, loss = 0.6895679235458374
In grad_steps = 363, loss = 0.6303396821022034
In grad_steps = 364, loss = 0.7452460527420044
In grad_steps = 365, loss = 0.7473822236061096
In grad_steps = 366, loss = 0.6278256177902222
In grad_steps = 367, loss = 0.7140675783157349
In grad_steps = 368, loss = 0.6837793588638306
In grad_steps = 369, loss = 0.6704748272895813
In grad_steps = 370, loss = 0.6887938976287842
In grad_steps = 371, loss = 0.6699995994567871
In grad_steps = 372, loss = 0.6599453687667847
In grad_steps = 373, loss = 0.6932383179664612
In grad_steps = 374, loss = 0.6808527112007141
Beginning epoch 4
In grad_steps = 375, loss = 0.6775316596031189
In grad_steps = 376, loss = 0.6901030540466309
In grad_steps = 377, loss = 0.6607859134674072
In grad_steps = 378, loss = 0.6893429756164551
In grad_steps = 379, loss = 0.6268861889839172
In grad_steps = 380, loss = 0.6128170490264893
In grad_steps = 381, loss = 0.6465696096420288
In grad_steps = 382, loss = 0.6485244631767273
In grad_steps = 383, loss = 0.5538378953933716
In grad_steps = 384, loss = 0.7800867557525635
In grad_steps = 385, loss = 0.7843825817108154
In grad_steps = 386, loss = 0.6364278197288513
In grad_steps = 387, loss = 0.5314344763755798
In grad_steps = 388, loss = 0.6665897369384766
In grad_steps = 389, loss = 0.668151319026947
In grad_steps = 390, loss = 0.6960008144378662
In grad_steps = 391, loss = 0.6055906414985657
In grad_steps = 392, loss = 0.8129270672798157
In grad_steps = 393, loss = 0.6924521923065186
In grad_steps = 394, loss = 0.6269286274909973
In grad_steps = 395, loss = 0.5972688794136047
In grad_steps = 396, loss = 0.6975898742675781
In grad_steps = 397, loss = 0.6841944456100464
In grad_steps = 398, loss = 0.6799588799476624
In grad_steps = 399, loss = 0.6513168215751648
In grad_steps = 400, loss = 0.6570513248443604
In grad_steps = 401, loss = 0.6066965460777283
In grad_steps = 402, loss = 0.555911123752594
In grad_steps = 403, loss = 0.6169575452804565
In grad_steps = 404, loss = 0.6908161044120789
In grad_steps = 405, loss = 0.5717560648918152
In grad_steps = 406, loss = 0.39593806862831116
In grad_steps = 407, loss = 0.5140818357467651
In grad_steps = 408, loss = 0.506049394607544
In grad_steps = 409, loss = 0.4507335424423218
In grad_steps = 410, loss = 0.4421793222427368
In grad_steps = 411, loss = 0.43348032236099243
In grad_steps = 412, loss = 0.3723749816417694
In grad_steps = 413, loss = 1.0711686611175537
In grad_steps = 414, loss = 0.499765008687973
In grad_steps = 415, loss = 0.539221465587616
In grad_steps = 416, loss = 0.5199400782585144
In grad_steps = 417, loss = 0.48330655694007874
In grad_steps = 418, loss = 0.3998509347438812
In grad_steps = 419, loss = 0.642598569393158
In grad_steps = 420, loss = 0.5944787263870239
In grad_steps = 421, loss = 0.5082249641418457
In grad_steps = 422, loss = 0.6234586238861084
In grad_steps = 423, loss = 0.5570821166038513
In grad_steps = 424, loss = 0.4598832428455353
In grad_steps = 425, loss = 0.559380054473877
In grad_steps = 426, loss = 0.45776838064193726
In grad_steps = 427, loss = 0.643639862537384
In grad_steps = 428, loss = 0.7399666905403137
In grad_steps = 429, loss = 0.5225315093994141
In grad_steps = 430, loss = 0.6761950254440308
In grad_steps = 431, loss = 0.6660753488540649
In grad_steps = 432, loss = 0.7888574004173279
In grad_steps = 433, loss = 0.5272527933120728
In grad_steps = 434, loss = 0.6643661856651306
In grad_steps = 435, loss = 0.5723227858543396
In grad_steps = 436, loss = 0.6973758935928345
In grad_steps = 437, loss = 0.608585000038147
In grad_steps = 438, loss = 0.6003891229629517
In grad_steps = 439, loss = 0.6781948804855347
In grad_steps = 440, loss = 0.7118293046951294
In grad_steps = 441, loss = 0.5817602276802063
In grad_steps = 442, loss = 0.7490910291671753
In grad_steps = 443, loss = 0.5986259579658508
In grad_steps = 444, loss = 0.6463102698326111
In grad_steps = 445, loss = 0.7071470022201538
In grad_steps = 446, loss = 0.632892370223999
In grad_steps = 447, loss = 0.5403261184692383
In grad_steps = 448, loss = 0.5141004323959351
In grad_steps = 449, loss = 0.5389507412910461
In grad_steps = 450, loss = 0.5919589400291443
In grad_steps = 451, loss = 0.5640063285827637
In grad_steps = 452, loss = 0.5483205914497375
In grad_steps = 453, loss = 0.6324934363365173
In grad_steps = 454, loss = 0.5447136759757996
In grad_steps = 455, loss = 0.788996696472168
In grad_steps = 456, loss = 0.6396347284317017
In grad_steps = 457, loss = 0.5573047995567322
In grad_steps = 458, loss = 0.6159119606018066
In grad_steps = 459, loss = 0.5078246593475342
In grad_steps = 460, loss = 0.4702402353286743
In grad_steps = 461, loss = 0.8651280403137207
In grad_steps = 462, loss = 0.649154782295227
In grad_steps = 463, loss = 0.5694501996040344
In grad_steps = 464, loss = 0.6880811452865601
In grad_steps = 465, loss = 0.4341928958892822
In grad_steps = 466, loss = 0.5880934596061707
In grad_steps = 467, loss = 0.6609776020050049
In grad_steps = 468, loss = 0.5963135361671448
In grad_steps = 469, loss = 0.5027182102203369
In grad_steps = 470, loss = 0.6874487996101379
In grad_steps = 471, loss = 0.5320602059364319
In grad_steps = 472, loss = 0.7531020641326904
In grad_steps = 473, loss = 0.5626786351203918
In grad_steps = 474, loss = 0.6894599795341492
In grad_steps = 475, loss = 0.8495078682899475
In grad_steps = 476, loss = 0.5266545414924622
In grad_steps = 477, loss = 0.6535317897796631
In grad_steps = 478, loss = 0.5980525016784668
In grad_steps = 479, loss = 0.5613606572151184
In grad_steps = 480, loss = 0.7397749423980713
In grad_steps = 481, loss = 0.6703463196754456
In grad_steps = 482, loss = 0.756006121635437
In grad_steps = 483, loss = 0.6713544726371765
In grad_steps = 484, loss = 0.6660341024398804
In grad_steps = 485, loss = 0.590069591999054
In grad_steps = 486, loss = 0.602524995803833
In grad_steps = 487, loss = 0.6705546379089355
In grad_steps = 488, loss = 0.6048099398612976
In grad_steps = 489, loss = 0.7617577910423279
In grad_steps = 490, loss = 0.695200502872467
In grad_steps = 491, loss = 0.630901038646698
In grad_steps = 492, loss = 0.688662052154541
In grad_steps = 493, loss = 0.6272340416908264
In grad_steps = 494, loss = 0.6833280920982361
In grad_steps = 495, loss = 0.6139345765113831
In grad_steps = 496, loss = 0.601362943649292
In grad_steps = 497, loss = 0.5620454549789429
In grad_steps = 498, loss = 0.6436479687690735
In grad_steps = 499, loss = 0.6123214364051819
Beginning epoch 5
In grad_steps = 500, loss = 0.7036523222923279
In grad_steps = 501, loss = 0.6377148032188416
In grad_steps = 502, loss = 0.5419412851333618
In grad_steps = 503, loss = 0.6104198098182678
In grad_steps = 504, loss = 0.5672826766967773
In grad_steps = 505, loss = 0.4978010058403015
In grad_steps = 506, loss = 0.5475334525108337
In grad_steps = 507, loss = 0.6039658784866333
In grad_steps = 508, loss = 0.39366593956947327
In grad_steps = 509, loss = 0.6456127166748047
In grad_steps = 510, loss = 0.621564507484436
In grad_steps = 511, loss = 0.4574443995952606
In grad_steps = 512, loss = 0.21770413219928741
In grad_steps = 513, loss = 0.5011235475540161
In grad_steps = 514, loss = 0.5590782165527344
In grad_steps = 515, loss = 0.6098014712333679
In grad_steps = 516, loss = 0.4843192398548126
In grad_steps = 517, loss = 0.49792835116386414
In grad_steps = 518, loss = 0.478018581867218
In grad_steps = 519, loss = 0.7565195560455322
In grad_steps = 520, loss = 0.5016694664955139
In grad_steps = 521, loss = 0.6536015272140503
In grad_steps = 522, loss = 0.8215571641921997
In grad_steps = 523, loss = 0.39743563532829285
In grad_steps = 524, loss = 0.5978573560714722
In grad_steps = 525, loss = 0.49810028076171875
In grad_steps = 526, loss = 0.5730608105659485
In grad_steps = 527, loss = 0.4444800615310669
In grad_steps = 528, loss = 0.5440739393234253
In grad_steps = 529, loss = 0.6700455546379089
In grad_steps = 530, loss = 0.45379453897476196
In grad_steps = 531, loss = 0.3147619366645813
In grad_steps = 532, loss = 0.44240495562553406
In grad_steps = 533, loss = 0.3961702287197113
In grad_steps = 534, loss = 0.3174583315849304
In grad_steps = 535, loss = 0.3040980100631714
In grad_steps = 536, loss = 0.3574063777923584
In grad_steps = 537, loss = 0.3097563683986664
In grad_steps = 538, loss = 0.21453382074832916
In grad_steps = 539, loss = 0.5736733078956604
In grad_steps = 540, loss = 0.4163699150085449
In grad_steps = 541, loss = 0.12448621541261673
In grad_steps = 542, loss = 0.34057554602622986
In grad_steps = 543, loss = 0.2530800402164459
In grad_steps = 544, loss = 0.9441190958023071
In grad_steps = 545, loss = 0.2876736521720886
In grad_steps = 546, loss = 0.1689746081829071
In grad_steps = 547, loss = 0.5971595048904419
In grad_steps = 548, loss = 0.77553391456604
In grad_steps = 549, loss = 0.17937567830085754
In grad_steps = 550, loss = 0.2926214039325714
In grad_steps = 551, loss = 0.22767463326454163
In grad_steps = 552, loss = 0.26325348019599915
In grad_steps = 553, loss = 0.4223020672798157
In grad_steps = 554, loss = 0.3988436758518219
In grad_steps = 555, loss = 0.4302429258823395
In grad_steps = 556, loss = 0.6406692862510681
In grad_steps = 557, loss = 0.481961190700531
In grad_steps = 558, loss = 0.4379768371582031
In grad_steps = 559, loss = 0.589432954788208
In grad_steps = 560, loss = 0.4186919927597046
In grad_steps = 561, loss = 0.5650662183761597
In grad_steps = 562, loss = 0.5105922818183899
In grad_steps = 563, loss = 0.4965178370475769
In grad_steps = 564, loss = 0.6012047529220581
In grad_steps = 565, loss = 0.7199297547340393
In grad_steps = 566, loss = 0.3889337182044983
In grad_steps = 567, loss = 0.6706591248512268
In grad_steps = 568, loss = 0.5306137800216675
In grad_steps = 569, loss = 0.534161388874054
In grad_steps = 570, loss = 0.6139382123947144
In grad_steps = 571, loss = 0.5755210518836975
In grad_steps = 572, loss = 0.4335712790489197
In grad_steps = 573, loss = 0.3769989013671875
In grad_steps = 574, loss = 0.45042505860328674
In grad_steps = 575, loss = 0.5063012838363647
In grad_steps = 576, loss = 0.49977317452430725
In grad_steps = 577, loss = 0.3800823986530304
In grad_steps = 578, loss = 0.48916685581207275
In grad_steps = 579, loss = 0.3948049545288086
In grad_steps = 580, loss = 0.33678939938545227
In grad_steps = 581, loss = 0.32517674565315247
In grad_steps = 582, loss = 0.3546929657459259
In grad_steps = 583, loss = 0.2935563325881958
In grad_steps = 584, loss = 0.3414762616157532
In grad_steps = 585, loss = 0.22387157380580902
In grad_steps = 586, loss = 0.45216941833496094
In grad_steps = 587, loss = 0.33000996708869934
In grad_steps = 588, loss = 0.2653999328613281
In grad_steps = 589, loss = 0.4289659559726715
In grad_steps = 590, loss = 0.10825395584106445
In grad_steps = 591, loss = 0.6097275614738464
In grad_steps = 592, loss = 0.30285823345184326
In grad_steps = 593, loss = 0.35398799180984497
In grad_steps = 594, loss = 0.09770214557647705
In grad_steps = 595, loss = 0.6083401441574097
In grad_steps = 596, loss = 0.32106131315231323
In grad_steps = 597, loss = 0.7127609848976135
In grad_steps = 598, loss = 0.37472081184387207
In grad_steps = 599, loss = 0.4341278076171875
In grad_steps = 600, loss = 0.5896580219268799
In grad_steps = 601, loss = 0.3372376561164856
In grad_steps = 602, loss = 0.4894407391548157
In grad_steps = 603, loss = 0.48703712224960327
In grad_steps = 604, loss = 0.5204832553863525
In grad_steps = 605, loss = 0.5919564366340637
In grad_steps = 606, loss = 0.5668609738349915
In grad_steps = 607, loss = 0.5269076228141785
In grad_steps = 608, loss = 0.6151002645492554
In grad_steps = 609, loss = 0.6517479419708252
In grad_steps = 610, loss = 0.4839569330215454
In grad_steps = 611, loss = 0.4811820387840271
In grad_steps = 612, loss = 0.7811281085014343
In grad_steps = 613, loss = 0.5167375206947327
In grad_steps = 614, loss = 0.6798778772354126
In grad_steps = 615, loss = 0.5188325047492981
In grad_steps = 616, loss = 0.6081809997558594
In grad_steps = 617, loss = 0.6573416590690613
In grad_steps = 618, loss = 0.6058353185653687
In grad_steps = 619, loss = 0.6311042308807373
In grad_steps = 620, loss = 0.4358068108558655
In grad_steps = 621, loss = 0.4656583070755005
In grad_steps = 622, loss = 0.5096400380134583
In grad_steps = 623, loss = 0.5901995301246643
In grad_steps = 624, loss = 0.4081670641899109
Beginning epoch 6
In grad_steps = 625, loss = 0.5930066108703613
In grad_steps = 626, loss = 0.4711941182613373
In grad_steps = 627, loss = 0.4450675845146179
In grad_steps = 628, loss = 0.4427775740623474
In grad_steps = 629, loss = 0.3778553009033203
In grad_steps = 630, loss = 0.4817284643650055
In grad_steps = 631, loss = 0.27399271726608276
In grad_steps = 632, loss = 0.6441435813903809
In grad_steps = 633, loss = 0.4640994966030121
In grad_steps = 634, loss = 0.3678273856639862
In grad_steps = 635, loss = 0.49397963285446167
In grad_steps = 636, loss = 0.4070698618888855
In grad_steps = 637, loss = 0.12475302815437317
In grad_steps = 638, loss = 0.2735891044139862
In grad_steps = 639, loss = 0.2394106537103653
In grad_steps = 640, loss = 0.3690916895866394
In grad_steps = 641, loss = 0.16100049018859863
In grad_steps = 642, loss = 0.35495495796203613
In grad_steps = 643, loss = 0.5686362385749817
In grad_steps = 644, loss = 0.16061119735240936
In grad_steps = 645, loss = 0.1296684443950653
In grad_steps = 646, loss = 0.1890668272972107
In grad_steps = 647, loss = 0.5113303065299988
In grad_steps = 648, loss = 0.32318001985549927
In grad_steps = 649, loss = 0.15494263172149658
In grad_steps = 650, loss = 0.21988189220428467
In grad_steps = 651, loss = 0.38891616463661194
In grad_steps = 652, loss = 0.26178666949272156
In grad_steps = 653, loss = 0.8287174105644226
In grad_steps = 654, loss = 0.4902186691761017
In grad_steps = 655, loss = 0.23724059760570526
In grad_steps = 656, loss = 0.08699386566877365
In grad_steps = 657, loss = 0.22435301542282104
In grad_steps = 658, loss = 0.26121240854263306
In grad_steps = 659, loss = 0.2955648601055145
In grad_steps = 660, loss = 0.2695673406124115
In grad_steps = 661, loss = 0.48434334993362427
In grad_steps = 662, loss = 0.3441556692123413
In grad_steps = 663, loss = 0.16141021251678467
In grad_steps = 664, loss = 0.21651500463485718
In grad_steps = 665, loss = 0.4725116789340973
In grad_steps = 666, loss = 0.5251178741455078
In grad_steps = 667, loss = 0.31717196106910706
In grad_steps = 668, loss = 0.27088984847068787
In grad_steps = 669, loss = 0.5192493796348572
In grad_steps = 670, loss = 0.43512940406799316
In grad_steps = 671, loss = 0.8828673958778381
In grad_steps = 672, loss = 0.46077051758766174
In grad_steps = 673, loss = 0.3032633662223816
In grad_steps = 674, loss = 0.13495704531669617
In grad_steps = 675, loss = 0.23524440824985504
In grad_steps = 676, loss = 0.2527655065059662
In grad_steps = 677, loss = 0.4314577579498291
In grad_steps = 678, loss = 0.5146116018295288
In grad_steps = 679, loss = 0.40029484033584595
In grad_steps = 680, loss = 0.5628620386123657
In grad_steps = 681, loss = 0.38034504652023315
In grad_steps = 682, loss = 0.3324310779571533
In grad_steps = 683, loss = 0.23944330215454102
In grad_steps = 684, loss = 0.6114388108253479
In grad_steps = 685, loss = 0.27490001916885376
In grad_steps = 686, loss = 0.7192688584327698
In grad_steps = 687, loss = 0.40360909700393677
In grad_steps = 688, loss = 0.5003345012664795
In grad_steps = 689, loss = 0.4181947112083435
In grad_steps = 690, loss = 0.42725852131843567
In grad_steps = 691, loss = 0.3169235289096832
In grad_steps = 692, loss = 0.630885899066925
In grad_steps = 693, loss = 0.2758385241031647
In grad_steps = 694, loss = 0.7403728365898132
In grad_steps = 695, loss = 0.6273739337921143
In grad_steps = 696, loss = 0.1870538890361786
In grad_steps = 697, loss = 0.30658385157585144
In grad_steps = 698, loss = 0.21545778214931488
In grad_steps = 699, loss = 0.4381568133831024
In grad_steps = 700, loss = 0.40964987874031067
In grad_steps = 701, loss = 0.4833737313747406
In grad_steps = 702, loss = 0.28031688928604126
In grad_steps = 703, loss = 0.27585411071777344
In grad_steps = 704, loss = 0.22968891263008118
In grad_steps = 705, loss = 0.25814199447631836
In grad_steps = 706, loss = 0.31313568353652954
In grad_steps = 707, loss = 0.30885761976242065
In grad_steps = 708, loss = 0.19311495125293732
In grad_steps = 709, loss = 0.26868098974227905
In grad_steps = 710, loss = 0.20950919389724731
In grad_steps = 711, loss = 0.26508820056915283
In grad_steps = 712, loss = 0.3319026827812195
In grad_steps = 713, loss = 0.08819971978664398
In grad_steps = 714, loss = 0.20386461913585663
In grad_steps = 715, loss = 0.4661182165145874
In grad_steps = 716, loss = 0.3970407247543335
In grad_steps = 717, loss = 0.19716127216815948
In grad_steps = 718, loss = 0.16163773834705353
In grad_steps = 719, loss = 0.12603212893009186
In grad_steps = 720, loss = 0.6569983959197998
In grad_steps = 721, loss = 0.4629479646682739
In grad_steps = 722, loss = 1.1074726581573486
In grad_steps = 723, loss = 0.6600437760353088
In grad_steps = 724, loss = 0.21830636262893677
In grad_steps = 725, loss = 0.41231217980384827
In grad_steps = 726, loss = 0.2844867706298828
In grad_steps = 727, loss = 0.5154623985290527
In grad_steps = 728, loss = 0.6044563055038452
In grad_steps = 729, loss = 0.5671346187591553
In grad_steps = 730, loss = 0.5033981204032898
In grad_steps = 731, loss = 0.568970263004303
In grad_steps = 732, loss = 0.5434703230857849
In grad_steps = 733, loss = 0.5779073238372803
In grad_steps = 734, loss = 0.7156211137771606
In grad_steps = 735, loss = 0.6397722959518433
In grad_steps = 736, loss = 0.48448988795280457
In grad_steps = 737, loss = 0.5695355534553528
In grad_steps = 738, loss = 0.46491318941116333
In grad_steps = 739, loss = 0.637507975101471
In grad_steps = 740, loss = 0.5879137516021729
In grad_steps = 741, loss = 0.5506143569946289
In grad_steps = 742, loss = 0.6455661058425903
In grad_steps = 743, loss = 0.4525049030780792
In grad_steps = 744, loss = 0.6965457797050476
In grad_steps = 745, loss = 0.3881949186325073
In grad_steps = 746, loss = 0.5663890242576599
In grad_steps = 747, loss = 0.5877084136009216
In grad_steps = 748, loss = 0.5690771341323853
In grad_steps = 749, loss = 0.6375142335891724
Beginning epoch 7
In grad_steps = 750, loss = 0.6021186709403992
In grad_steps = 751, loss = 0.30846938490867615
In grad_steps = 752, loss = 0.38502809405326843
In grad_steps = 753, loss = 0.46035024523735046
In grad_steps = 754, loss = 0.39813852310180664
In grad_steps = 755, loss = 0.42614251375198364
In grad_steps = 756, loss = 0.3495790958404541
In grad_steps = 757, loss = 0.2909991145133972
In grad_steps = 758, loss = 0.3044573664665222
In grad_steps = 759, loss = 0.4607182443141937
In grad_steps = 760, loss = 0.3120025396347046
In grad_steps = 761, loss = 0.2945078909397125
In grad_steps = 762, loss = 0.2786070704460144
In grad_steps = 763, loss = 0.3793242275714874
In grad_steps = 764, loss = 0.11282570660114288
In grad_steps = 765, loss = 0.1366957724094391
In grad_steps = 766, loss = 0.16775962710380554
In grad_steps = 767, loss = 0.1274200677871704
In grad_steps = 768, loss = 0.17441608011722565
In grad_steps = 769, loss = 0.026943642646074295
In grad_steps = 770, loss = 0.024769410490989685
In grad_steps = 771, loss = 0.33579885959625244
In grad_steps = 772, loss = 0.6927395462989807
In grad_steps = 773, loss = 0.21696129441261292
In grad_steps = 774, loss = 0.6518319845199585
In grad_steps = 775, loss = 0.38653022050857544
In grad_steps = 776, loss = 0.1858426034450531
In grad_steps = 777, loss = 0.3848099410533905
In grad_steps = 778, loss = 0.3325774669647217
In grad_steps = 779, loss = 0.37229013442993164
In grad_steps = 780, loss = 0.28096166253089905
In grad_steps = 781, loss = 0.18708975613117218
In grad_steps = 782, loss = 0.22976373136043549
In grad_steps = 783, loss = 0.2243478149175644
In grad_steps = 784, loss = 0.13413888216018677
In grad_steps = 785, loss = 0.35643115639686584
In grad_steps = 786, loss = 0.2072848677635193
In grad_steps = 787, loss = 0.29665330052375793
In grad_steps = 788, loss = 0.22772124409675598
In grad_steps = 789, loss = 0.1320287138223648
In grad_steps = 790, loss = 0.09278704226016998
In grad_steps = 791, loss = 0.10951307415962219
In grad_steps = 792, loss = 0.17778310179710388
In grad_steps = 793, loss = 0.12316856533288956
In grad_steps = 794, loss = 0.27367374300956726
In grad_steps = 795, loss = 0.43487510085105896
In grad_steps = 796, loss = 0.08923622965812683
In grad_steps = 797, loss = 0.08889620751142502
In grad_steps = 798, loss = 0.4537454843521118
In grad_steps = 799, loss = 0.02538149431347847
In grad_steps = 800, loss = 0.07282385975122452
In grad_steps = 801, loss = 0.12073222547769547
In grad_steps = 802, loss = 0.022005856037139893
In grad_steps = 803, loss = 0.4459965229034424
In grad_steps = 804, loss = 0.08685208112001419
In grad_steps = 805, loss = 0.4663406312465668
In grad_steps = 806, loss = 0.3409855365753174
In grad_steps = 807, loss = 0.44011348485946655
In grad_steps = 808, loss = 0.4555354714393616
In grad_steps = 809, loss = 0.5049148797988892
In grad_steps = 810, loss = 0.08353164792060852
In grad_steps = 811, loss = 0.5459285378456116
In grad_steps = 812, loss = 0.41314637660980225
In grad_steps = 813, loss = 0.31612613797187805
In grad_steps = 814, loss = 0.28562799096107483
In grad_steps = 815, loss = 0.5592143535614014
In grad_steps = 816, loss = 0.2771943211555481
In grad_steps = 817, loss = 0.46075940132141113
In grad_steps = 818, loss = 0.28704777359962463
In grad_steps = 819, loss = 0.3605298697948456
In grad_steps = 820, loss = 0.6777792572975159
In grad_steps = 821, loss = 0.16651912033557892
In grad_steps = 822, loss = 0.41472354531288147
In grad_steps = 823, loss = 0.21153408288955688
In grad_steps = 824, loss = 0.3502460718154907
In grad_steps = 825, loss = 0.1554621309041977
In grad_steps = 826, loss = 0.5445581674575806
In grad_steps = 827, loss = 0.5791938900947571
In grad_steps = 828, loss = 0.3401803970336914
In grad_steps = 829, loss = 0.2490527629852295
In grad_steps = 830, loss = 0.37074539065361023
In grad_steps = 831, loss = 0.2883215546607971
In grad_steps = 832, loss = 0.5094504356384277
In grad_steps = 833, loss = 0.37293991446495056
In grad_steps = 834, loss = 0.2327585220336914
In grad_steps = 835, loss = 0.29218971729278564
In grad_steps = 836, loss = 0.2781270444393158
In grad_steps = 837, loss = 0.26839762926101685
In grad_steps = 838, loss = 0.4919975697994232
In grad_steps = 839, loss = 0.256687730550766
In grad_steps = 840, loss = 0.14678743481636047
In grad_steps = 841, loss = 0.22269445657730103
In grad_steps = 842, loss = 0.1340208202600479
In grad_steps = 843, loss = 0.3441661298274994
In grad_steps = 844, loss = 0.6092903017997742
In grad_steps = 845, loss = 0.4977959096431732
In grad_steps = 846, loss = 0.06148627772927284
In grad_steps = 847, loss = 0.349767804145813
In grad_steps = 848, loss = 0.21470338106155396
In grad_steps = 849, loss = 0.6424642205238342
In grad_steps = 850, loss = 0.4950905442237854
In grad_steps = 851, loss = 0.3719204068183899
In grad_steps = 852, loss = 0.25321081280708313
In grad_steps = 853, loss = 0.2156650871038437
In grad_steps = 854, loss = 0.3390452563762665
In grad_steps = 855, loss = 0.3402617573738098
In grad_steps = 856, loss = 0.4512022137641907
In grad_steps = 857, loss = 0.6675876975059509
In grad_steps = 858, loss = 0.3794235289096832
In grad_steps = 859, loss = 0.3308335542678833
In grad_steps = 860, loss = 0.37522539496421814
In grad_steps = 861, loss = 0.7446070909500122
In grad_steps = 862, loss = 0.2939774990081787
In grad_steps = 863, loss = 0.39678269624710083
In grad_steps = 864, loss = 0.6087629795074463
In grad_steps = 865, loss = 0.5419827103614807
In grad_steps = 866, loss = 0.18986289203166962
In grad_steps = 867, loss = 0.6899552345275879
In grad_steps = 868, loss = 0.5760664343833923
In grad_steps = 869, loss = 0.513083279132843
In grad_steps = 870, loss = 0.3386141359806061
In grad_steps = 871, loss = 0.47345855832099915
In grad_steps = 872, loss = 0.32909536361694336
In grad_steps = 873, loss = 0.2496950924396515
In grad_steps = 874, loss = 0.24633334577083588
Beginning epoch 8
In grad_steps = 875, loss = 0.2701517939567566
In grad_steps = 876, loss = 0.3734946846961975
In grad_steps = 877, loss = 0.2665429711341858
In grad_steps = 878, loss = 0.3223549425601959
In grad_steps = 879, loss = 0.26653334498405457
In grad_steps = 880, loss = 0.2638990581035614
In grad_steps = 881, loss = 0.3180600702762604
In grad_steps = 882, loss = 0.42446592450141907
In grad_steps = 883, loss = 0.15075622498989105
In grad_steps = 884, loss = 0.15087838470935822
In grad_steps = 885, loss = 0.1683928668498993
In grad_steps = 886, loss = 0.09873759001493454
In grad_steps = 887, loss = 0.300896018743515
In grad_steps = 888, loss = 0.06284913420677185
In grad_steps = 889, loss = 0.0747142806649208
In grad_steps = 890, loss = 0.0571313351392746
In grad_steps = 891, loss = 0.09584380686283112
In grad_steps = 892, loss = 0.02652529813349247
In grad_steps = 893, loss = 0.04769929498434067
In grad_steps = 894, loss = 0.03333844244480133
In grad_steps = 895, loss = 0.024390043690800667
In grad_steps = 896, loss = 0.18744871020317078
In grad_steps = 897, loss = 0.08940362185239792
In grad_steps = 898, loss = 0.19994917511940002
In grad_steps = 899, loss = 0.04656235873699188
In grad_steps = 900, loss = 0.04085243120789528
In grad_steps = 901, loss = 0.023681608960032463
In grad_steps = 902, loss = 0.013246877118945122
In grad_steps = 903, loss = 0.07830675691366196
In grad_steps = 904, loss = 0.04838187247514725
In grad_steps = 905, loss = 0.5476399660110474
In grad_steps = 906, loss = 0.11365729570388794
In grad_steps = 907, loss = 0.41033127903938293
In grad_steps = 908, loss = 0.039534732699394226
In grad_steps = 909, loss = 0.00802039448171854
In grad_steps = 910, loss = 0.3752053380012512
In grad_steps = 911, loss = 0.033887505531311035
In grad_steps = 912, loss = 0.143045112490654
In grad_steps = 913, loss = 0.0835414007306099
In grad_steps = 914, loss = 0.23935279250144958
In grad_steps = 915, loss = 0.08838370442390442
In grad_steps = 916, loss = 0.06358731538057327
In grad_steps = 917, loss = 0.09952984750270844
In grad_steps = 918, loss = 0.08407551050186157
In grad_steps = 919, loss = 0.2055840939283371
In grad_steps = 920, loss = 0.15303343534469604
In grad_steps = 921, loss = 0.15220919251441956
In grad_steps = 922, loss = 0.48057425022125244
In grad_steps = 923, loss = 0.6819353699684143
In grad_steps = 924, loss = 0.08813659101724625
In grad_steps = 925, loss = 0.0344531424343586
In grad_steps = 926, loss = 0.06359750032424927
In grad_steps = 927, loss = 0.15734665095806122
In grad_steps = 928, loss = 0.2526918649673462
In grad_steps = 929, loss = 0.5819799304008484
In grad_steps = 930, loss = 0.3385724127292633
In grad_steps = 931, loss = 0.14439085125923157
In grad_steps = 932, loss = 0.11587134748697281
In grad_steps = 933, loss = 0.1043759137392044
In grad_steps = 934, loss = 0.09586863964796066
In grad_steps = 935, loss = 0.0686316266655922
In grad_steps = 936, loss = 0.2658074200153351
In grad_steps = 937, loss = 0.1761987954378128
In grad_steps = 938, loss = 0.16489101946353912
In grad_steps = 939, loss = 0.2948019206523895
In grad_steps = 940, loss = 0.7358477711677551
In grad_steps = 941, loss = 0.35699060559272766
In grad_steps = 942, loss = 0.11543296277523041
In grad_steps = 943, loss = 0.6001981496810913
In grad_steps = 944, loss = 0.35582447052001953
In grad_steps = 945, loss = 0.19761043787002563
In grad_steps = 946, loss = 0.09990061819553375
In grad_steps = 947, loss = 0.38622885942459106
In grad_steps = 948, loss = 0.07886134833097458
In grad_steps = 949, loss = 0.2960350215435028
In grad_steps = 950, loss = 0.1993361860513687
In grad_steps = 951, loss = 0.22401292622089386
In grad_steps = 952, loss = 0.2968708574771881
In grad_steps = 953, loss = 0.35332080721855164
In grad_steps = 954, loss = 0.2863001525402069
In grad_steps = 955, loss = 0.3204766511917114
In grad_steps = 956, loss = 0.14210836589336395
In grad_steps = 957, loss = 0.19058364629745483
In grad_steps = 958, loss = 0.19677306711673737
In grad_steps = 959, loss = 0.09715042263269424
In grad_steps = 960, loss = 0.08176372945308685
In grad_steps = 961, loss = 0.16168390214443207
In grad_steps = 962, loss = 0.16197358071804047
In grad_steps = 963, loss = 0.048508401960134506
In grad_steps = 964, loss = 0.08937127143144608
In grad_steps = 965, loss = 0.01695404201745987
In grad_steps = 966, loss = 0.09585366398096085
In grad_steps = 967, loss = 0.009280231781303883
In grad_steps = 968, loss = 0.03558143973350525
In grad_steps = 969, loss = 0.01570012979209423
In grad_steps = 970, loss = 0.09525986760854721
In grad_steps = 971, loss = 0.33285653591156006
In grad_steps = 972, loss = 0.230413556098938
In grad_steps = 973, loss = 0.07884576171636581
In grad_steps = 974, loss = 0.15241526067256927
In grad_steps = 975, loss = 0.03166661411523819
In grad_steps = 976, loss = 0.03616560623049736
In grad_steps = 977, loss = 0.04740129038691521
In grad_steps = 978, loss = 0.28070464730262756
In grad_steps = 979, loss = 0.32524290680885315
In grad_steps = 980, loss = 0.4691338539123535
In grad_steps = 981, loss = 0.164169043302536
In grad_steps = 982, loss = 0.6318840980529785
In grad_steps = 983, loss = 0.3784816563129425
In grad_steps = 984, loss = 0.17562703788280487
In grad_steps = 985, loss = 0.22689716517925262
In grad_steps = 986, loss = 0.8661382794380188
In grad_steps = 987, loss = 0.585125744342804
In grad_steps = 988, loss = 0.4373640716075897
In grad_steps = 989, loss = 0.27484703063964844
In grad_steps = 990, loss = 0.09948748350143433
In grad_steps = 991, loss = 0.1761593222618103
In grad_steps = 992, loss = 0.28189289569854736
In grad_steps = 993, loss = 0.42416906356811523
In grad_steps = 994, loss = 0.39477571845054626
In grad_steps = 995, loss = 0.32045769691467285
In grad_steps = 996, loss = 0.2935537099838257
In grad_steps = 997, loss = 0.19972151517868042
In grad_steps = 998, loss = 0.33501628041267395
In grad_steps = 999, loss = 0.22410018742084503
Elapsed time: 3355.2432820796967 seconds for ensemble 4 with 8 epochs
LoRA instance 4 evaluation complete. Data saved to /pscratch/sd/t/tianle/lucid/other_source/SURP_2024/results/tmp_lora_ensemble/Llama3-8B-set-3c/test_data_instance_4_seed_40373.npz.
lora instance i = 4 Successfully finished.
Final, Test average ensemble probabilities = 
[[0.26539192 0.73460805]
 [0.6165637  0.38343635]
 [0.4927256  0.50727445]
 [0.5717772  0.42822272]
 [0.5208575  0.4791425 ]
 [0.13173378 0.8682662 ]
 [0.19637914 0.80362093]
 [0.2679549  0.7320451 ]
 [0.5554059  0.44459414]
 [0.17111614 0.8288838 ]
 [0.7309699  0.2690301 ]
 [0.2902831  0.7097169 ]
 [0.49787146 0.50212854]
 [0.43515056 0.5648495 ]
 [0.30322105 0.6967789 ]
 [0.06281066 0.9371893 ]
 [0.43161926 0.5683807 ]
 [0.39063555 0.6093645 ]
 [0.49051923 0.50948083]
 [0.35554495 0.644455  ]
 [0.1420615  0.85793847]
 [0.7119547  0.28804532]
 [0.20939925 0.7906008 ]
 [0.16513757 0.8348624 ]
 [0.27514896 0.724851  ]
 [0.65729934 0.34270066]
 [0.6386771  0.36132285]
 [0.50826347 0.49173656]
 [0.46885815 0.5311418 ]
 [0.40328622 0.5967138 ]
 [0.7214361  0.2785639 ]
 [0.04648779 0.9535122 ]
 [0.29585433 0.7041456 ]
 [0.6206824  0.37931758]
 [0.45208198 0.547918  ]
 [0.0899178  0.91008216]
 [0.60568464 0.39431536]
 [0.86977655 0.13022351]
 [0.43028936 0.5697106 ]
 [0.5526422  0.44735774]
 [0.38383135 0.6161686 ]
 [0.504746   0.49525398]
 [0.45763725 0.5423628 ]
 [0.5818907  0.41810927]
 [0.26340097 0.736599  ]
 [0.503225   0.49677497]
 [0.6140483  0.3859517 ]
 [0.600344   0.399656  ]
 [0.6728016  0.32719836]
 [0.5303012  0.46969876]
 [0.39365697 0.60634303]
 [0.4535796  0.5464204 ]
 [0.4271492  0.5728508 ]
 [0.67185986 0.32814008]
 [0.4542946  0.54570544]
 [0.66809857 0.33190137]
 [0.48808146 0.51191854]
 [0.67994714 0.32005292]
 [0.36384    0.63616   ]
 [0.7194646  0.28053543]
 [0.12125888 0.8787411 ]
 [0.42351693 0.576483  ]
 [0.3622764  0.6377236 ]
 [0.5581394  0.44186062]
 [0.39594406 0.60405594]
 [0.5231403  0.47685975]
 [0.23782304 0.762177  ]
 [0.75567645 0.24432354]
 [0.44422883 0.5557712 ]
 [0.43019906 0.569801  ]
 [0.21831349 0.78168654]
 [0.41239685 0.5876031 ]
 [0.5192291  0.4807709 ]
 [0.3276673  0.67233264]
 [0.42378846 0.57621163]
 [0.37777624 0.6222238 ]
 [0.8553727  0.14462724]
 [0.46828133 0.5317186 ]
 [0.17948322 0.82051677]
 [0.7391179  0.26088214]
 [0.20875943 0.7912406 ]
 [0.15725973 0.84274024]
 [0.45022744 0.54977256]
 [0.34576383 0.6542362 ]
 [0.47973937 0.52026063]
 [0.01504689 0.9849531 ]
 [0.26195043 0.7380496 ]
 [0.06516334 0.9348367 ]
 [0.71657693 0.28342313]
 [0.1512409  0.84875906]
 [0.2698642  0.7301358 ]
 [0.45600754 0.54399246]
 [0.38067365 0.61932635]
 [0.5626461  0.43735385]
 [0.59043676 0.40956324]
 [0.36896643 0.6310336 ]
 [0.5596038  0.44039622]
 [0.21683998 0.78316003]
 [0.18491954 0.81508046]
 [0.3032618  0.69673824]
 [0.37962642 0.6203736 ]
 [0.20105429 0.7989457 ]
 [0.4846633  0.5153367 ]
 [0.19093369 0.8090663 ]
 [0.59127843 0.4087216 ]
 [0.33665717 0.66334283]
 [0.09747328 0.90252674]
 [0.3143046  0.68569547]
 [0.3728585  0.6271415 ]
 [0.6943173  0.30568266]
 [0.48825392 0.5117461 ]
 [0.7684947  0.23150527]
 [0.81441194 0.18558809]
 [0.22445384 0.7755462 ]
 [0.38099694 0.6190031 ]
 [0.10492142 0.89507854]
 [0.53701466 0.46298534]
 [0.23479636 0.76520365]
 [0.64712656 0.3528734 ]
 [0.7148212  0.28517884]
 [0.60917526 0.39082474]
 [0.44577885 0.55422115]
 [0.36937985 0.6306201 ]
 [0.5922667  0.4077333 ]
 [0.34000537 0.65999466]
 [0.42650604 0.57349396]
 [0.6055268  0.39447314]
 [0.50486505 0.49513498]
 [0.57896405 0.42103595]
 [0.7071186  0.29288143]
 [0.8392007  0.1607993 ]
 [0.44152594 0.55847406]
 [0.4294692  0.5705308 ]
 [0.5533346  0.44666544]
 [0.5042013  0.49579874]
 [0.42957297 0.570427  ]
 [0.51502454 0.48497543]
 [0.42919797 0.57080203]
 [0.667376   0.33262402]
 [0.47955522 0.52044475]
 [0.19779623 0.8022038 ]
 [0.54736805 0.45263195]
 [0.30215773 0.69784224]
 [0.49136782 0.5086322 ]
 [0.4653105  0.5346895 ]
 [0.48689157 0.51310843]
 [0.5288897  0.47111025]
 [0.6437451  0.35625485]
 [0.78659475 0.2134052 ]
 [0.71781737 0.28218263]
 [0.36720294 0.63279706]
 [0.6833722  0.31662777]
 [0.7670932  0.23290682]
 [0.08965328 0.9103467 ]
 [0.2054456  0.79455435]
 [0.27914548 0.7208545 ]
 [0.03180427 0.96819574]
 [0.5884258  0.41157418]
 [0.5594355  0.44056454]
 [0.3052569  0.69474316]
 [0.19335699 0.8066429 ]
 [0.48314238 0.5168576 ]
 [0.53326786 0.46673217]
 [0.61071944 0.38928056]
 [0.38728467 0.6127153 ]
 [0.5030354  0.4969645 ]
 [0.7101802  0.28981978]
 [0.7373208  0.2626792 ]
 [0.4081216  0.5918784 ]
 [0.31408122 0.6859188 ]
 [0.49689692 0.503103  ]
 [0.17793918 0.82206076]
 [0.43697715 0.56302285]
 [0.5497769  0.45022312]
 [0.6253116  0.3746884 ]
 [0.670395   0.329605  ]
 [0.48783153 0.5121684 ]
 [0.20305839 0.79694164]
 [0.73996663 0.26003337]
 [0.5360791  0.4639209 ]
 [0.6300365  0.3699636 ]
 [0.35012543 0.6498745 ]
 [0.31817967 0.68182033]
 [0.55356055 0.44643945]
 [0.5501555  0.44984454]
 [0.37207288 0.62792706]
 [0.6519302  0.34806982]
 [0.62632734 0.37367266]
 [0.23407726 0.76592267]
 [0.49126515 0.5087349 ]
 [0.19433519 0.8056647 ]
 [0.3925045  0.6074954 ]
 [0.38331228 0.61668766]
 [0.3516052  0.6483948 ]
 [0.43134332 0.5686567 ]
 [0.222948   0.77705204]
 [0.48336706 0.5166329 ]
 [0.30415076 0.69584924]
 [0.3661533  0.6338467 ]
 [0.75140345 0.24859655]
 [0.37632352 0.62367654]
 [0.6751728  0.3248272 ]
 [0.35946575 0.6405342 ]
 [0.5820165  0.41798344]
 [0.5861217  0.41387835]
 [0.36801058 0.6319894 ]
 [0.83596116 0.16403885]
 [0.38542354 0.6145764 ]
 [0.56847656 0.43152347]
 [0.4075079  0.59249204]
 [0.3751479  0.62485206]
 [0.59348094 0.4065191 ]
 [0.5432399  0.4567601 ]
 [0.50186956 0.49813047]
 [0.35491824 0.64508176]
 [0.256315   0.74368495]
 [0.10805148 0.8919485 ]
 [0.6861439  0.3138561 ]
 [0.7833314  0.21666856]
 [0.33182523 0.66817474]
 [0.4348465  0.5651535 ]
 [0.2854727  0.7145273 ]
 [0.31432074 0.6856793 ]]
Accuracy: 0.5202
MCC: 0.0134
AUC: 0.5338
Confusion Matrix:
tensor([[39, 58],
        [49, 77]])
Specificity: 0.4021
Precision (Macro): 0.5068
F1 Score (Macro): 0.5058
Expected Calibration Error (ECE): 0.1225
NLL loss: 0.7371
Ensemble evaluation complete.
