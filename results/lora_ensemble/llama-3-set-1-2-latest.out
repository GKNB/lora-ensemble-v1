Job is running on partition: gpu_ss11
Job is running under account: m2616_g
/pscratch/sd/t/tianle/conda/envs/lucid_surp_2024/lib/python3.11/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:10<00:32, 10.91s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:21<00:21, 10.99s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:32<00:10, 10.95s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:35<00:00,  7.57s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:35<00:00,  8.81s/it]
Llama3 has been loaded successfully.
Namespace(model_name='Llama3', config='/pscratch/sd/t/tianle/lucid/other_source/SURP_2024/src/config.json', repo_dir='/pscratch/sd/t/tianle/lucid/other_source/SURP_2024/', dataset='1.2', n_ensemble=5, seed=1, use_model_snapshot=False)
self.num_epochs = 12, self.batch_size = 16, self.max_length = 120
Model:  LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(128256, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)
          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)
          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)
          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)
)
Train dataset looks like:  Dataset({
    features: ['question', 'answer'],
    num_rows: 1198
})
Test dataset looks like:  Dataset({
    features: ['question', 'answer'],
    num_rows: 134
})
Training lora instance 0
DEBUG: Total parameters: 8037076992
DEBUG: Trainable parameters: 6815744
Beginning epoch 1
In grad_steps = 0, loss = 1.014610767364502
In grad_steps = 1, loss = 0.734398603439331
In grad_steps = 2, loss = 0.6848835945129395
In grad_steps = 3, loss = 0.6816635131835938
In grad_steps = 4, loss = 0.6307802200317383
In grad_steps = 5, loss = 0.6322624087333679
In grad_steps = 6, loss = 0.6491209864616394
In grad_steps = 7, loss = 0.9128122925758362
In grad_steps = 8, loss = 0.6799452304840088
In grad_steps = 9, loss = 0.6890852451324463
In grad_steps = 10, loss = 0.6909241676330566
In grad_steps = 11, loss = 0.6903861165046692
In grad_steps = 12, loss = 0.7654278874397278
In grad_steps = 13, loss = 0.6991981267929077
In grad_steps = 14, loss = 0.7458071112632751
In grad_steps = 15, loss = 0.6960771679878235
In grad_steps = 16, loss = 0.7200658321380615
In grad_steps = 17, loss = 0.7105062007904053
In grad_steps = 18, loss = 0.7083891034126282
In grad_steps = 19, loss = 0.701144278049469
In grad_steps = 20, loss = 0.6949218511581421
In grad_steps = 21, loss = 0.693547785282135
In grad_steps = 22, loss = 0.6969649195671082
In grad_steps = 23, loss = 0.6891176104545593
In grad_steps = 24, loss = 0.7085711359977722
In grad_steps = 25, loss = 0.698949933052063
In grad_steps = 26, loss = 0.7155579924583435
In grad_steps = 27, loss = 0.6884154081344604
In grad_steps = 28, loss = 0.6680761575698853
In grad_steps = 29, loss = 0.6443042159080505
In grad_steps = 30, loss = 0.7080003619194031
In grad_steps = 31, loss = 0.6716917157173157
In grad_steps = 32, loss = 0.7759568691253662
In grad_steps = 33, loss = 0.8612284660339355
In grad_steps = 34, loss = 0.6587984561920166
In grad_steps = 35, loss = 0.7447995543479919
In grad_steps = 36, loss = 0.7001475095748901
In grad_steps = 37, loss = 0.692927360534668
In grad_steps = 38, loss = 0.7506422996520996
In grad_steps = 39, loss = 0.671719491481781
In grad_steps = 40, loss = 0.6647195816040039
In grad_steps = 41, loss = 0.6994370222091675
In grad_steps = 42, loss = 0.6650964617729187
In grad_steps = 43, loss = 0.7329171299934387
In grad_steps = 44, loss = 0.6575350761413574
In grad_steps = 45, loss = 0.7286926507949829
In grad_steps = 46, loss = 0.7675109505653381
In grad_steps = 47, loss = 0.6510730385780334
In grad_steps = 48, loss = 0.702541172504425
In grad_steps = 49, loss = 0.6988569498062134
In grad_steps = 50, loss = 0.6886795163154602
In grad_steps = 51, loss = 0.6823388934135437
In grad_steps = 52, loss = 0.7155919075012207
In grad_steps = 53, loss = 0.7306556105613708
In grad_steps = 54, loss = 0.7329921722412109
In grad_steps = 55, loss = 0.652642548084259
In grad_steps = 56, loss = 0.6949368119239807
In grad_steps = 57, loss = 0.7019839882850647
In grad_steps = 58, loss = 0.6786060333251953
In grad_steps = 59, loss = 0.7041946649551392
In grad_steps = 60, loss = 0.7101056575775146
In grad_steps = 61, loss = 0.6822983622550964
In grad_steps = 62, loss = 0.7009398937225342
In grad_steps = 63, loss = 0.6991633176803589
In grad_steps = 64, loss = 0.6932709217071533
In grad_steps = 65, loss = 0.6853134632110596
In grad_steps = 66, loss = 0.665285050868988
In grad_steps = 67, loss = 0.7304143905639648
In grad_steps = 68, loss = 0.6994476318359375
In grad_steps = 69, loss = 0.701315701007843
In grad_steps = 70, loss = 0.7221723198890686
In grad_steps = 71, loss = 0.6786344051361084
In grad_steps = 72, loss = 0.7221435904502869
In grad_steps = 73, loss = 0.6736321449279785
In grad_steps = 74, loss = 0.714078962802887
Beginning epoch 2
In grad_steps = 75, loss = 0.6593031287193298
In grad_steps = 76, loss = 0.6730417013168335
In grad_steps = 77, loss = 0.6629882454872131
In grad_steps = 78, loss = 0.7018880248069763
In grad_steps = 79, loss = 0.7226247787475586
In grad_steps = 80, loss = 0.7140368223190308
In grad_steps = 81, loss = 0.6783962249755859
In grad_steps = 82, loss = 0.7067307233810425
In grad_steps = 83, loss = 0.6821379065513611
In grad_steps = 84, loss = 0.6812178492546082
In grad_steps = 85, loss = 0.7293672561645508
In grad_steps = 86, loss = 0.7325038909912109
In grad_steps = 87, loss = 0.6635579466819763
In grad_steps = 88, loss = 0.7217921614646912
In grad_steps = 89, loss = 0.7113121747970581
In grad_steps = 90, loss = 0.6897599697113037
In grad_steps = 91, loss = 0.6991457939147949
In grad_steps = 92, loss = 0.6832590699195862
In grad_steps = 93, loss = 0.6954870223999023
In grad_steps = 94, loss = 0.6937306523323059
In grad_steps = 95, loss = 0.686836838722229
In grad_steps = 96, loss = 0.6851919889450073
In grad_steps = 97, loss = 0.6870341300964355
In grad_steps = 98, loss = 0.6988081336021423
In grad_steps = 99, loss = 0.6870356798171997
In grad_steps = 100, loss = 0.686221718788147
In grad_steps = 101, loss = 0.6970520615577698
In grad_steps = 102, loss = 0.6934807300567627
In grad_steps = 103, loss = 0.6694918274879456
In grad_steps = 104, loss = 0.6364123821258545
In grad_steps = 105, loss = 0.7003015875816345
In grad_steps = 106, loss = 0.677683413028717
In grad_steps = 107, loss = 0.7609599828720093
In grad_steps = 108, loss = 0.8610595464706421
In grad_steps = 109, loss = 0.6533284187316895
In grad_steps = 110, loss = 0.7841787338256836
In grad_steps = 111, loss = 0.7510454654693604
In grad_steps = 112, loss = 0.6654484868049622
In grad_steps = 113, loss = 0.6764662861824036
In grad_steps = 114, loss = 0.6913190484046936
In grad_steps = 115, loss = 0.6748442649841309
In grad_steps = 116, loss = 0.6961723566055298
In grad_steps = 117, loss = 0.6701743006706238
In grad_steps = 118, loss = 0.7152299284934998
In grad_steps = 119, loss = 0.6526094079017639
In grad_steps = 120, loss = 0.7318134307861328
In grad_steps = 121, loss = 0.7970931529998779
In grad_steps = 122, loss = 0.6255452036857605
In grad_steps = 123, loss = 0.7173585295677185
In grad_steps = 124, loss = 0.6589869856834412
In grad_steps = 125, loss = 0.7028462290763855
In grad_steps = 126, loss = 0.7350289225578308
In grad_steps = 127, loss = 0.6877047419548035
In grad_steps = 128, loss = 0.7099652290344238
In grad_steps = 129, loss = 0.7184993028640747
In grad_steps = 130, loss = 0.6571619510650635
In grad_steps = 131, loss = 0.6832601428031921
In grad_steps = 132, loss = 0.698712170124054
In grad_steps = 133, loss = 0.6802768707275391
In grad_steps = 134, loss = 0.7109472751617432
In grad_steps = 135, loss = 0.7211695313453674
In grad_steps = 136, loss = 0.6755807995796204
In grad_steps = 137, loss = 0.6967576742172241
In grad_steps = 138, loss = 0.6925199627876282
In grad_steps = 139, loss = 0.6932082772254944
In grad_steps = 140, loss = 0.6803857088088989
In grad_steps = 141, loss = 0.6640415787696838
In grad_steps = 142, loss = 0.7327561378479004
In grad_steps = 143, loss = 0.699292004108429
In grad_steps = 144, loss = 0.7019850015640259
In grad_steps = 145, loss = 0.716030478477478
In grad_steps = 146, loss = 0.6681658029556274
In grad_steps = 147, loss = 0.7208620309829712
In grad_steps = 148, loss = 0.6714096665382385
In grad_steps = 149, loss = 0.7161611318588257
Beginning epoch 3
In grad_steps = 150, loss = 0.6463077664375305
In grad_steps = 151, loss = 0.6613920331001282
In grad_steps = 152, loss = 0.6555429697036743
In grad_steps = 153, loss = 0.7034562230110168
In grad_steps = 154, loss = 0.7278400659561157
In grad_steps = 155, loss = 0.7221448421478271
In grad_steps = 156, loss = 0.6811535954475403
In grad_steps = 157, loss = 0.6955003142356873
In grad_steps = 158, loss = 0.6792871356010437
In grad_steps = 159, loss = 0.6725881695747375
In grad_steps = 160, loss = 0.7226747274398804
In grad_steps = 161, loss = 0.7336342930793762
In grad_steps = 162, loss = 0.6597137451171875
In grad_steps = 163, loss = 0.7197299599647522
In grad_steps = 164, loss = 0.7082038521766663
In grad_steps = 165, loss = 0.6882184147834778
In grad_steps = 166, loss = 0.6962076425552368
In grad_steps = 167, loss = 0.6614775061607361
In grad_steps = 168, loss = 0.6888706088066101
In grad_steps = 169, loss = 0.6899718642234802
In grad_steps = 170, loss = 0.680221676826477
In grad_steps = 171, loss = 0.6758175492286682
In grad_steps = 172, loss = 0.6837558150291443
In grad_steps = 173, loss = 0.6906342506408691
In grad_steps = 174, loss = 0.6744400262832642
In grad_steps = 175, loss = 0.667846143245697
In grad_steps = 176, loss = 0.6806896328926086
In grad_steps = 177, loss = 0.6782933473587036
In grad_steps = 178, loss = 0.6460469365119934
In grad_steps = 179, loss = 0.5988909602165222
In grad_steps = 180, loss = 0.7176068425178528
In grad_steps = 181, loss = 0.6666214466094971
In grad_steps = 182, loss = 0.8178468346595764
In grad_steps = 183, loss = 0.8916386961936951
In grad_steps = 184, loss = 0.6380411386489868
In grad_steps = 185, loss = 0.747520923614502
In grad_steps = 186, loss = 0.715404748916626
In grad_steps = 187, loss = 0.6597920656204224
In grad_steps = 188, loss = 0.6962900757789612
In grad_steps = 189, loss = 0.6748417615890503
In grad_steps = 190, loss = 0.6627306342124939
In grad_steps = 191, loss = 0.6843653321266174
In grad_steps = 192, loss = 0.6607434153556824
In grad_steps = 193, loss = 0.7077670097351074
In grad_steps = 194, loss = 0.6574931740760803
In grad_steps = 195, loss = 0.7286508083343506
In grad_steps = 196, loss = 0.7775441408157349
In grad_steps = 197, loss = 0.6360732913017273
In grad_steps = 198, loss = 0.6994975209236145
In grad_steps = 199, loss = 0.669384777545929
In grad_steps = 200, loss = 0.6859069466590881
In grad_steps = 201, loss = 0.6905215978622437
In grad_steps = 202, loss = 0.6953673958778381
In grad_steps = 203, loss = 0.7170385718345642
In grad_steps = 204, loss = 0.7320165634155273
In grad_steps = 205, loss = 0.651337742805481
In grad_steps = 206, loss = 0.6736236810684204
In grad_steps = 207, loss = 0.6867518424987793
In grad_steps = 208, loss = 0.6790663599967957
In grad_steps = 209, loss = 0.6910465955734253
In grad_steps = 210, loss = 0.7026412487030029
In grad_steps = 211, loss = 0.681491494178772
In grad_steps = 212, loss = 0.6896219253540039
In grad_steps = 213, loss = 0.7019968032836914
In grad_steps = 214, loss = 0.6768184304237366
In grad_steps = 215, loss = 0.6704079508781433
In grad_steps = 216, loss = 0.6583983302116394
In grad_steps = 217, loss = 0.7335106134414673
In grad_steps = 218, loss = 0.691360592842102
In grad_steps = 219, loss = 0.7023831605911255
In grad_steps = 220, loss = 0.7176113128662109
In grad_steps = 221, loss = 0.6387715339660645
In grad_steps = 222, loss = 0.7000690698623657
In grad_steps = 223, loss = 0.672254204750061
In grad_steps = 224, loss = 0.6881405711174011
Beginning epoch 4
In grad_steps = 225, loss = 0.6462050676345825
In grad_steps = 226, loss = 0.6727486252784729
In grad_steps = 227, loss = 0.6654114127159119
In grad_steps = 228, loss = 0.6752223372459412
In grad_steps = 229, loss = 0.6997857093811035
In grad_steps = 230, loss = 0.6889592409133911
In grad_steps = 231, loss = 0.6640766859054565
In grad_steps = 232, loss = 0.7261958718299866
In grad_steps = 233, loss = 0.6568874716758728
In grad_steps = 234, loss = 0.649411678314209
In grad_steps = 235, loss = 0.7271502017974854
In grad_steps = 236, loss = 0.7451346516609192
In grad_steps = 237, loss = 0.652921199798584
In grad_steps = 238, loss = 0.6491619348526001
In grad_steps = 239, loss = 0.6777650117874146
In grad_steps = 240, loss = 0.6682761907577515
In grad_steps = 241, loss = 0.6800927519798279
In grad_steps = 242, loss = 0.5934448838233948
In grad_steps = 243, loss = 0.710586667060852
In grad_steps = 244, loss = 0.6769376993179321
In grad_steps = 245, loss = 0.6356703639030457
In grad_steps = 246, loss = 0.6523646712303162
In grad_steps = 247, loss = 0.6749405264854431
In grad_steps = 248, loss = 0.6505729556083679
In grad_steps = 249, loss = 0.6523078680038452
In grad_steps = 250, loss = 0.586846113204956
In grad_steps = 251, loss = 0.5959766507148743
In grad_steps = 252, loss = 0.6243461966514587
In grad_steps = 253, loss = 0.5547083020210266
In grad_steps = 254, loss = 0.5562047958374023
In grad_steps = 255, loss = 0.6573901176452637
In grad_steps = 256, loss = 0.5270693302154541
In grad_steps = 257, loss = 0.7597472071647644
In grad_steps = 258, loss = 0.4676048159599304
In grad_steps = 259, loss = 0.8718575239181519
In grad_steps = 260, loss = 0.5746053457260132
In grad_steps = 261, loss = 0.6708908677101135
In grad_steps = 262, loss = 0.617599606513977
In grad_steps = 263, loss = 0.5529724359512329
In grad_steps = 264, loss = 0.7150945067405701
In grad_steps = 265, loss = 0.7056006789207458
In grad_steps = 266, loss = 0.6662458181381226
In grad_steps = 267, loss = 0.6670100688934326
In grad_steps = 268, loss = 0.6682623624801636
In grad_steps = 269, loss = 0.6724622249603271
In grad_steps = 270, loss = 0.7505959868431091
In grad_steps = 271, loss = 0.8202254772186279
In grad_steps = 272, loss = 0.5767266750335693
In grad_steps = 273, loss = 0.681496798992157
In grad_steps = 274, loss = 0.5926344394683838
In grad_steps = 275, loss = 0.7027130126953125
In grad_steps = 276, loss = 0.7708242535591125
In grad_steps = 277, loss = 0.6033438444137573
In grad_steps = 278, loss = 0.6419743299484253
In grad_steps = 279, loss = 0.704909086227417
In grad_steps = 280, loss = 0.5882197022438049
In grad_steps = 281, loss = 0.6312893629074097
In grad_steps = 282, loss = 0.633640468120575
In grad_steps = 283, loss = 0.7044171690940857
In grad_steps = 284, loss = 0.6336196660995483
In grad_steps = 285, loss = 0.6843257546424866
In grad_steps = 286, loss = 0.6258100271224976
In grad_steps = 287, loss = 0.7208811640739441
In grad_steps = 288, loss = 0.7192884683609009
In grad_steps = 289, loss = 0.6437280774116516
In grad_steps = 290, loss = 0.6413862109184265
In grad_steps = 291, loss = 0.7028183341026306
In grad_steps = 292, loss = 0.6755306124687195
In grad_steps = 293, loss = 0.6317845582962036
In grad_steps = 294, loss = 0.63300621509552
In grad_steps = 295, loss = 0.7258571982383728
In grad_steps = 296, loss = 0.5638286471366882
In grad_steps = 297, loss = 0.6345629692077637
In grad_steps = 298, loss = 0.7051181197166443
In grad_steps = 299, loss = 0.6752936244010925
Beginning epoch 5
In grad_steps = 300, loss = 0.597695529460907
In grad_steps = 301, loss = 0.619586169719696
In grad_steps = 302, loss = 0.6825564503669739
In grad_steps = 303, loss = 0.5909547209739685
In grad_steps = 304, loss = 0.701421320438385
In grad_steps = 305, loss = 0.6533767580986023
In grad_steps = 306, loss = 0.6426433324813843
In grad_steps = 307, loss = 0.6714733839035034
In grad_steps = 308, loss = 0.5931433439254761
In grad_steps = 309, loss = 0.5469491481781006
In grad_steps = 310, loss = 0.6330427527427673
In grad_steps = 311, loss = 0.6214661598205566
In grad_steps = 312, loss = 0.5154277682304382
In grad_steps = 313, loss = 0.5920268893241882
In grad_steps = 314, loss = 0.5517813563346863
In grad_steps = 315, loss = 0.6038950681686401
In grad_steps = 316, loss = 0.6434708833694458
In grad_steps = 317, loss = 0.3368000388145447
In grad_steps = 318, loss = 0.5397216081619263
In grad_steps = 319, loss = 0.48472002148628235
In grad_steps = 320, loss = 0.38676717877388
In grad_steps = 321, loss = 0.5947508215904236
In grad_steps = 322, loss = 0.4826520085334778
In grad_steps = 323, loss = 0.6013641357421875
In grad_steps = 324, loss = 0.5759524703025818
In grad_steps = 325, loss = 0.36900660395622253
In grad_steps = 326, loss = 0.24749189615249634
In grad_steps = 327, loss = 0.24530956149101257
In grad_steps = 328, loss = 0.20777516067028046
In grad_steps = 329, loss = 0.18309123814105988
In grad_steps = 330, loss = 0.24541625380516052
In grad_steps = 331, loss = 0.16652974486351013
In grad_steps = 332, loss = 0.13999922573566437
In grad_steps = 333, loss = 0.5344605445861816
In grad_steps = 334, loss = 0.8035329580307007
In grad_steps = 335, loss = 0.28870999813079834
In grad_steps = 336, loss = 0.3225439488887787
In grad_steps = 337, loss = 0.3340296447277069
In grad_steps = 338, loss = 0.5659247636795044
In grad_steps = 339, loss = 0.7915706634521484
In grad_steps = 340, loss = 0.8305377960205078
In grad_steps = 341, loss = 0.500433623790741
In grad_steps = 342, loss = 0.4712768495082855
In grad_steps = 343, loss = 0.642198920249939
In grad_steps = 344, loss = 0.6847478151321411
In grad_steps = 345, loss = 0.7481669187545776
In grad_steps = 346, loss = 0.8105579614639282
In grad_steps = 347, loss = 0.5811828374862671
In grad_steps = 348, loss = 0.6608198881149292
In grad_steps = 349, loss = 0.6109812259674072
In grad_steps = 350, loss = 0.6609739661216736
In grad_steps = 351, loss = 0.6227803230285645
In grad_steps = 352, loss = 0.6536003947257996
In grad_steps = 353, loss = 0.7180259227752686
In grad_steps = 354, loss = 0.7677043080329895
In grad_steps = 355, loss = 0.5441789627075195
In grad_steps = 356, loss = 0.6352091431617737
In grad_steps = 357, loss = 0.6026953458786011
In grad_steps = 358, loss = 0.6433859467506409
In grad_steps = 359, loss = 0.5624992847442627
In grad_steps = 360, loss = 0.6092186570167542
In grad_steps = 361, loss = 0.529194176197052
In grad_steps = 362, loss = 0.5694426894187927
In grad_steps = 363, loss = 0.5418398380279541
In grad_steps = 364, loss = 0.5712850093841553
In grad_steps = 365, loss = 0.42890721559524536
In grad_steps = 366, loss = 0.6387550830841064
In grad_steps = 367, loss = 0.8002603650093079
In grad_steps = 368, loss = 0.45650580525398254
In grad_steps = 369, loss = 0.4142034351825714
In grad_steps = 370, loss = 0.34979763627052307
In grad_steps = 371, loss = 0.5647672414779663
In grad_steps = 372, loss = 0.3802914619445801
In grad_steps = 373, loss = 0.6122390627861023
In grad_steps = 374, loss = 0.6054353713989258
Beginning epoch 6
In grad_steps = 375, loss = 0.2866535782814026
In grad_steps = 376, loss = 0.4871007204055786
In grad_steps = 377, loss = 0.5514412522315979
In grad_steps = 378, loss = 0.607761561870575
In grad_steps = 379, loss = 0.7106248140335083
In grad_steps = 380, loss = 0.34040001034736633
In grad_steps = 381, loss = 0.4655861258506775
In grad_steps = 382, loss = 0.9409136772155762
In grad_steps = 383, loss = 0.536777675151825
In grad_steps = 384, loss = 0.45296767354011536
In grad_steps = 385, loss = 0.5500890612602234
In grad_steps = 386, loss = 0.4931073486804962
In grad_steps = 387, loss = 0.4786434471607208
In grad_steps = 388, loss = 0.5078701376914978
In grad_steps = 389, loss = 0.4544801414012909
In grad_steps = 390, loss = 0.563038170337677
In grad_steps = 391, loss = 0.42315202951431274
In grad_steps = 392, loss = 0.35386690497398376
In grad_steps = 393, loss = 0.3272523283958435
In grad_steps = 394, loss = 0.29835745692253113
In grad_steps = 395, loss = 0.2329142540693283
In grad_steps = 396, loss = 0.28394824266433716
In grad_steps = 397, loss = 0.19627663493156433
In grad_steps = 398, loss = 0.37686389684677124
In grad_steps = 399, loss = 0.17166495323181152
In grad_steps = 400, loss = 0.23278743028640747
In grad_steps = 401, loss = 0.0843544602394104
In grad_steps = 402, loss = 0.956723153591156
In grad_steps = 403, loss = 0.08311542868614197
In grad_steps = 404, loss = 0.08314783871173859
In grad_steps = 405, loss = 0.2248416543006897
In grad_steps = 406, loss = 0.41170457005500793
In grad_steps = 407, loss = 0.6982640027999878
In grad_steps = 408, loss = 0.3475643992424011
In grad_steps = 409, loss = 0.1966487467288971
In grad_steps = 410, loss = 0.08318797498941422
In grad_steps = 411, loss = 0.3047763407230377
In grad_steps = 412, loss = 0.8381984233856201
In grad_steps = 413, loss = 1.0737897157669067
In grad_steps = 414, loss = 0.3705965578556061
In grad_steps = 415, loss = 0.2717914879322052
In grad_steps = 416, loss = 0.25218138098716736
In grad_steps = 417, loss = 0.4072161018848419
In grad_steps = 418, loss = 0.6497412323951721
In grad_steps = 419, loss = 0.9338194727897644
In grad_steps = 420, loss = 0.8611360788345337
In grad_steps = 421, loss = 0.4619843363761902
In grad_steps = 422, loss = 0.7590000629425049
In grad_steps = 423, loss = 0.4936271905899048
In grad_steps = 424, loss = 0.41063085198402405
In grad_steps = 425, loss = 0.6705382466316223
In grad_steps = 426, loss = 1.1244299411773682
In grad_steps = 427, loss = 0.47997963428497314
In grad_steps = 428, loss = 0.4635014832019806
In grad_steps = 429, loss = 0.5133931636810303
In grad_steps = 430, loss = 0.580710768699646
In grad_steps = 431, loss = 0.5343378186225891
In grad_steps = 432, loss = 0.45542287826538086
In grad_steps = 433, loss = 0.6065682768821716
In grad_steps = 434, loss = 0.4335443079471588
In grad_steps = 435, loss = 0.4974145293235779
In grad_steps = 436, loss = 0.3176822364330292
In grad_steps = 437, loss = 0.5769680142402649
In grad_steps = 438, loss = 0.32515817880630493
In grad_steps = 439, loss = 0.5829481482505798
In grad_steps = 440, loss = 0.39124131202697754
In grad_steps = 441, loss = 0.29240092635154724
In grad_steps = 442, loss = 0.295381098985672
In grad_steps = 443, loss = 0.16083332896232605
In grad_steps = 444, loss = 0.1521645188331604
In grad_steps = 445, loss = 0.7260201573371887
In grad_steps = 446, loss = 0.21457155048847198
In grad_steps = 447, loss = 0.6183477640151978
In grad_steps = 448, loss = 0.17777405679225922
In grad_steps = 449, loss = 0.1144900918006897
Beginning epoch 7
In grad_steps = 450, loss = 0.16938088834285736
In grad_steps = 451, loss = 0.2276020050048828
In grad_steps = 452, loss = 0.6273569464683533
In grad_steps = 453, loss = 0.43629780411720276
In grad_steps = 454, loss = 0.5150710940361023
In grad_steps = 455, loss = 0.14390166103839874
In grad_steps = 456, loss = 0.19600629806518555
In grad_steps = 457, loss = 0.5328957438468933
In grad_steps = 458, loss = 0.32148146629333496
In grad_steps = 459, loss = 0.37998029589653015
In grad_steps = 460, loss = 0.4154911935329437
In grad_steps = 461, loss = 0.3756769299507141
In grad_steps = 462, loss = 0.28063228726387024
In grad_steps = 463, loss = 0.36391377449035645
In grad_steps = 464, loss = 0.3074415326118469
In grad_steps = 465, loss = 0.43896615505218506
In grad_steps = 466, loss = 0.40688323974609375
In grad_steps = 467, loss = 0.20521093904972076
In grad_steps = 468, loss = 0.4303306043148041
In grad_steps = 469, loss = 0.2612968683242798
In grad_steps = 470, loss = 0.1754903346300125
In grad_steps = 471, loss = 0.25080081820487976
In grad_steps = 472, loss = 0.36259254813194275
In grad_steps = 473, loss = 0.2921333611011505
In grad_steps = 474, loss = 0.11981724947690964
In grad_steps = 475, loss = 0.1534440815448761
In grad_steps = 476, loss = 0.1869487762451172
In grad_steps = 477, loss = 0.12734387814998627
In grad_steps = 478, loss = 0.15878930687904358
In grad_steps = 479, loss = 0.11254360526800156
In grad_steps = 480, loss = 0.07769818603992462
In grad_steps = 481, loss = 0.04562719911336899
In grad_steps = 482, loss = 0.010724798776209354
In grad_steps = 483, loss = 0.04275854304432869
In grad_steps = 484, loss = 0.5769141316413879
In grad_steps = 485, loss = 0.2015565037727356
In grad_steps = 486, loss = 0.045265112072229385
In grad_steps = 487, loss = 0.019283682107925415
In grad_steps = 488, loss = 0.4689711332321167
In grad_steps = 489, loss = 0.9587069153785706
In grad_steps = 490, loss = 0.23882263898849487
In grad_steps = 491, loss = 0.25965583324432373
In grad_steps = 492, loss = 0.0982075035572052
In grad_steps = 493, loss = 0.20971457660198212
In grad_steps = 494, loss = 0.18215428292751312
In grad_steps = 495, loss = 0.612801730632782
In grad_steps = 496, loss = 0.3805876672267914
In grad_steps = 497, loss = 0.5740854144096375
In grad_steps = 498, loss = 0.367534875869751
In grad_steps = 499, loss = 0.4329359829425812
In grad_steps = 500, loss = 0.4331539571285248
In grad_steps = 501, loss = 0.568619966506958
In grad_steps = 502, loss = 0.21059897541999817
In grad_steps = 503, loss = 0.3314662575721741
In grad_steps = 504, loss = 0.4645409882068634
In grad_steps = 505, loss = 0.42884355783462524
In grad_steps = 506, loss = 0.47492489218711853
In grad_steps = 507, loss = 0.4136001169681549
In grad_steps = 508, loss = 0.705890417098999
In grad_steps = 509, loss = 0.40656331181526184
In grad_steps = 510, loss = 0.24917013943195343
In grad_steps = 511, loss = 0.398570716381073
In grad_steps = 512, loss = 0.40353238582611084
In grad_steps = 513, loss = 0.20813024044036865
In grad_steps = 514, loss = 0.22433353960514069
In grad_steps = 515, loss = 0.23676417768001556
In grad_steps = 516, loss = 0.49622398614883423
In grad_steps = 517, loss = 0.5666109323501587
In grad_steps = 518, loss = 0.14818282425403595
In grad_steps = 519, loss = 0.19304858148097992
In grad_steps = 520, loss = 0.20658688247203827
In grad_steps = 521, loss = 0.0845855325460434
In grad_steps = 522, loss = 0.2332879602909088
In grad_steps = 523, loss = 0.3156643211841583
In grad_steps = 524, loss = 0.6323277354240417
Beginning epoch 8
In grad_steps = 525, loss = 0.08314204216003418
In grad_steps = 526, loss = 0.36003878712654114
In grad_steps = 527, loss = 0.4693033993244171
In grad_steps = 528, loss = 0.3866022229194641
In grad_steps = 529, loss = 0.16219721734523773
In grad_steps = 530, loss = 0.2121867835521698
In grad_steps = 531, loss = 0.27439355850219727
In grad_steps = 532, loss = 0.7145749926567078
In grad_steps = 533, loss = 0.3385021686553955
In grad_steps = 534, loss = 0.057343367487192154
In grad_steps = 535, loss = 0.25927290320396423
In grad_steps = 536, loss = 0.44964599609375
In grad_steps = 537, loss = 0.1400011032819748
In grad_steps = 538, loss = 0.1942078322172165
In grad_steps = 539, loss = 0.27377453446388245
In grad_steps = 540, loss = 0.18857359886169434
In grad_steps = 541, loss = 0.174375981092453
In grad_steps = 542, loss = 0.10681157559156418
In grad_steps = 543, loss = 0.19886928796768188
In grad_steps = 544, loss = 0.12906388938426971
In grad_steps = 545, loss = 0.14024046063423157
In grad_steps = 546, loss = 0.17252898216247559
In grad_steps = 547, loss = 0.11553949862718582
In grad_steps = 548, loss = 0.1052083671092987
In grad_steps = 549, loss = 0.2146010398864746
In grad_steps = 550, loss = 0.04039348289370537
In grad_steps = 551, loss = 0.033749740570783615
In grad_steps = 552, loss = 0.23352593183517456
In grad_steps = 553, loss = 0.43313562870025635
In grad_steps = 554, loss = 0.3065015971660614
In grad_steps = 555, loss = 0.05292379856109619
In grad_steps = 556, loss = 0.0354858934879303
In grad_steps = 557, loss = 0.016913799569010735
In grad_steps = 558, loss = 0.450229287147522
In grad_steps = 559, loss = 0.021250735968351364
In grad_steps = 560, loss = 0.5269307494163513
In grad_steps = 561, loss = 0.5376485586166382
In grad_steps = 562, loss = 0.48835983872413635
In grad_steps = 563, loss = 0.03726179525256157
In grad_steps = 564, loss = 0.09966002404689789
In grad_steps = 565, loss = 0.06733395159244537
In grad_steps = 566, loss = 0.0534532405436039
In grad_steps = 567, loss = 0.4249601364135742
In grad_steps = 568, loss = 0.6995546221733093
In grad_steps = 569, loss = 0.9129579663276672
In grad_steps = 570, loss = 0.4806070625782013
In grad_steps = 571, loss = 0.37417522072792053
In grad_steps = 572, loss = 0.44584134221076965
In grad_steps = 573, loss = 0.45191702246665955
In grad_steps = 574, loss = 0.563528835773468
In grad_steps = 575, loss = 0.4454081356525421
In grad_steps = 576, loss = 0.39982932806015015
In grad_steps = 577, loss = 0.33515065908432007
In grad_steps = 578, loss = 0.5021435022354126
In grad_steps = 579, loss = 0.49728238582611084
In grad_steps = 580, loss = 0.2272198498249054
In grad_steps = 581, loss = 0.33146002888679504
In grad_steps = 582, loss = 0.169004887342453
In grad_steps = 583, loss = 0.35429301857948303
In grad_steps = 584, loss = 0.29018545150756836
In grad_steps = 585, loss = 0.1499207466840744
In grad_steps = 586, loss = 0.33309027552604675
In grad_steps = 587, loss = 0.17096681892871857
In grad_steps = 588, loss = 0.14449508488178253
In grad_steps = 589, loss = 0.1372784972190857
In grad_steps = 590, loss = 0.2306639701128006
In grad_steps = 591, loss = 0.15606650710105896
In grad_steps = 592, loss = 0.08419952541589737
In grad_steps = 593, loss = 0.11486166715621948
In grad_steps = 594, loss = 0.19361042976379395
In grad_steps = 595, loss = 0.06281263381242752
In grad_steps = 596, loss = 0.07093033194541931
In grad_steps = 597, loss = 0.058330342173576355
In grad_steps = 598, loss = 0.04189136251807213
In grad_steps = 599, loss = 0.009654860012233257
Beginning epoch 9
In grad_steps = 600, loss = 0.05904141440987587
In grad_steps = 601, loss = 0.08792717009782791
In grad_steps = 602, loss = 0.10226287692785263
In grad_steps = 603, loss = 0.4739120900630951
In grad_steps = 604, loss = 0.32383573055267334
In grad_steps = 605, loss = 0.019813699647784233
In grad_steps = 606, loss = 0.023584557697176933
In grad_steps = 607, loss = 0.46278610825538635
In grad_steps = 608, loss = 0.2577223479747772
In grad_steps = 609, loss = 0.1848255693912506
In grad_steps = 610, loss = 0.10281754285097122
In grad_steps = 611, loss = 0.3289405405521393
In grad_steps = 612, loss = 0.032060496509075165
In grad_steps = 613, loss = 0.14892783761024475
In grad_steps = 614, loss = 0.3062857389450073
In grad_steps = 615, loss = 0.20370808243751526
In grad_steps = 616, loss = 0.19110092520713806
In grad_steps = 617, loss = 0.1551947444677353
In grad_steps = 618, loss = 0.17234161496162415
In grad_steps = 619, loss = 0.3622536063194275
In grad_steps = 620, loss = 0.19361060857772827
In grad_steps = 621, loss = 0.08927464485168457
In grad_steps = 622, loss = 0.14678172767162323
In grad_steps = 623, loss = 0.10875864326953888
In grad_steps = 624, loss = 0.06111879274249077
In grad_steps = 625, loss = 0.2426900714635849
In grad_steps = 626, loss = 0.1003529280424118
In grad_steps = 627, loss = 0.05675196647644043
In grad_steps = 628, loss = 0.03818931430578232
In grad_steps = 629, loss = 0.5004522800445557
In grad_steps = 630, loss = 0.2456231415271759
In grad_steps = 631, loss = 0.2039976269006729
In grad_steps = 632, loss = 0.0868339017033577
In grad_steps = 633, loss = 0.3245742619037628
In grad_steps = 634, loss = 0.01463034376502037
In grad_steps = 635, loss = 0.11067420244216919
In grad_steps = 636, loss = 0.1317979097366333
In grad_steps = 637, loss = 0.16058166325092316
In grad_steps = 638, loss = 0.036954473704099655
In grad_steps = 639, loss = 0.10445595532655716
In grad_steps = 640, loss = 0.13267464935779572
In grad_steps = 641, loss = 0.01914951577782631
In grad_steps = 642, loss = 0.4592517912387848
In grad_steps = 643, loss = 0.17184600234031677
In grad_steps = 644, loss = 0.36126771569252014
In grad_steps = 645, loss = 0.29303282499313354
In grad_steps = 646, loss = 0.369126558303833
In grad_steps = 647, loss = 0.5095983743667603
In grad_steps = 648, loss = 0.3235873579978943
In grad_steps = 649, loss = 0.3668910264968872
In grad_steps = 650, loss = 0.12910467386245728
In grad_steps = 651, loss = 0.40365102887153625
In grad_steps = 652, loss = 0.06346368789672852
In grad_steps = 653, loss = 0.21628129482269287
In grad_steps = 654, loss = 0.22742387652397156
In grad_steps = 655, loss = 0.15955305099487305
In grad_steps = 656, loss = 0.21238473057746887
In grad_steps = 657, loss = 0.08225890249013901
In grad_steps = 658, loss = 0.1970982849597931
In grad_steps = 659, loss = 0.2148660123348236
In grad_steps = 660, loss = 0.08217143267393112
In grad_steps = 661, loss = 0.14307020604610443
In grad_steps = 662, loss = 0.16007834672927856
In grad_steps = 663, loss = 0.05524654686450958
In grad_steps = 664, loss = 0.054187092930078506
In grad_steps = 665, loss = 0.07522022724151611
In grad_steps = 666, loss = 0.05451288819313049
In grad_steps = 667, loss = 0.04369625821709633
In grad_steps = 668, loss = 0.1926812082529068
In grad_steps = 669, loss = 0.171405628323555
In grad_steps = 670, loss = 0.15153612196445465
In grad_steps = 671, loss = 0.01669197529554367
In grad_steps = 672, loss = 0.05447112023830414
In grad_steps = 673, loss = 0.030155818909406662
In grad_steps = 674, loss = 0.037767160683870316
Beginning epoch 10
In grad_steps = 675, loss = 0.035897355526685715
In grad_steps = 676, loss = 0.13151206076145172
In grad_steps = 677, loss = 0.024195104837417603
In grad_steps = 678, loss = 0.037703461945056915
In grad_steps = 679, loss = 0.16183817386627197
In grad_steps = 680, loss = 0.18179717659950256
In grad_steps = 681, loss = 0.031637221574783325
In grad_steps = 682, loss = 0.09028569608926773
In grad_steps = 683, loss = 0.07610823214054108
In grad_steps = 684, loss = 0.01776784285902977
In grad_steps = 685, loss = 0.010880276560783386
In grad_steps = 686, loss = 0.056638453155756
In grad_steps = 687, loss = 0.18453043699264526
In grad_steps = 688, loss = 0.02073431760072708
In grad_steps = 689, loss = 0.00770188681781292
In grad_steps = 690, loss = 0.05634644255042076
In grad_steps = 691, loss = 0.2061063051223755
In grad_steps = 692, loss = 0.0024132642429322004
In grad_steps = 693, loss = 0.03296244516968727
In grad_steps = 694, loss = 0.003538569202646613
In grad_steps = 695, loss = 0.03250684589147568
In grad_steps = 696, loss = 0.048082008957862854
In grad_steps = 697, loss = 0.007518079597502947
In grad_steps = 698, loss = 0.06166024133563042
In grad_steps = 699, loss = 0.020146315917372704
In grad_steps = 700, loss = 0.0013667830498889089
In grad_steps = 701, loss = 0.17731381952762604
In grad_steps = 702, loss = 0.0015561592299491167
In grad_steps = 703, loss = 0.049006905406713486
In grad_steps = 704, loss = 0.007376103196293116
In grad_steps = 705, loss = 0.003415302373468876
In grad_steps = 706, loss = 0.0023907229769974947
In grad_steps = 707, loss = 0.001864873105660081
In grad_steps = 708, loss = 0.01811929978430271
In grad_steps = 709, loss = 0.058446384966373444
In grad_steps = 710, loss = 0.03586229309439659
In grad_steps = 711, loss = 0.03819114342331886
In grad_steps = 712, loss = 0.03356136754155159
In grad_steps = 713, loss = 0.006272683851420879
In grad_steps = 714, loss = 0.003730346914380789
In grad_steps = 715, loss = 0.0588887557387352
In grad_steps = 716, loss = 0.0014333430444821715
In grad_steps = 717, loss = 0.23323173820972443
In grad_steps = 718, loss = 0.0012650705175474286
In grad_steps = 719, loss = 0.380290687084198
In grad_steps = 720, loss = 0.005622031632810831
In grad_steps = 721, loss = 0.0482751727104187
In grad_steps = 722, loss = 0.39452385902404785
In grad_steps = 723, loss = 0.31538495421409607
In grad_steps = 724, loss = 0.11775974184274673
In grad_steps = 725, loss = 0.11299007385969162
In grad_steps = 726, loss = 0.29416555166244507
In grad_steps = 727, loss = 0.06376064568758011
In grad_steps = 728, loss = 0.3895685076713562
In grad_steps = 729, loss = 0.12576061487197876
In grad_steps = 730, loss = 0.09389232099056244
In grad_steps = 731, loss = 0.12087584286928177
In grad_steps = 732, loss = 0.09172738343477249
In grad_steps = 733, loss = 0.12240111827850342
In grad_steps = 734, loss = 0.103432796895504
In grad_steps = 735, loss = 0.10393685847520828
In grad_steps = 736, loss = 0.08460517227649689
In grad_steps = 737, loss = 0.11672822386026382
In grad_steps = 738, loss = 0.21362319588661194
In grad_steps = 739, loss = 0.06929352134466171
In grad_steps = 740, loss = 0.05152645707130432
In grad_steps = 741, loss = 0.20835831761360168
In grad_steps = 742, loss = 0.16523513197898865
In grad_steps = 743, loss = 0.14112389087677002
In grad_steps = 744, loss = 0.12932340800762177
In grad_steps = 745, loss = 0.21650652587413788
In grad_steps = 746, loss = 0.3863932192325592
In grad_steps = 747, loss = 0.04672319442033768
In grad_steps = 748, loss = 0.09392832219600677
In grad_steps = 749, loss = 0.40327420830726624
Beginning epoch 11
In grad_steps = 750, loss = 0.18957942724227905
In grad_steps = 751, loss = 0.06797080487012863
In grad_steps = 752, loss = 0.333787202835083
In grad_steps = 753, loss = 0.11527033895254135
In grad_steps = 754, loss = 0.27800947427749634
In grad_steps = 755, loss = 0.04134681075811386
In grad_steps = 756, loss = 0.0814860463142395
In grad_steps = 757, loss = 0.10335434973239899
In grad_steps = 758, loss = 0.14792566001415253
In grad_steps = 759, loss = 0.14473950862884521
In grad_steps = 760, loss = 0.027901623398065567
In grad_steps = 761, loss = 0.19466176629066467
In grad_steps = 762, loss = 0.012045912444591522
In grad_steps = 763, loss = 0.017807884141802788
In grad_steps = 764, loss = 0.02655099704861641
In grad_steps = 765, loss = 0.24086211621761322
In grad_steps = 766, loss = 0.15100134909152985
In grad_steps = 767, loss = 0.398566871881485
In grad_steps = 768, loss = 0.17133137583732605
In grad_steps = 769, loss = 0.05723845586180687
In grad_steps = 770, loss = 0.13361500203609467
In grad_steps = 771, loss = 0.013458346016705036
In grad_steps = 772, loss = 0.012965654954314232
In grad_steps = 773, loss = 0.017077114433050156
In grad_steps = 774, loss = 0.07542970031499863
In grad_steps = 775, loss = 0.07076826691627502
In grad_steps = 776, loss = 0.018469087779521942
In grad_steps = 777, loss = 0.01327981986105442
In grad_steps = 778, loss = 0.18820956349372864
In grad_steps = 779, loss = 0.19940301775932312
In grad_steps = 780, loss = 0.36121636629104614
In grad_steps = 781, loss = 0.09758959710597992
In grad_steps = 782, loss = 0.01253674179315567
In grad_steps = 783, loss = 0.2222171425819397
In grad_steps = 784, loss = 0.010515558533370495
In grad_steps = 785, loss = 0.00980326347053051
In grad_steps = 786, loss = 0.008894155733287334
In grad_steps = 787, loss = 0.045971475541591644
In grad_steps = 788, loss = 0.012699076905846596
In grad_steps = 789, loss = 0.02148313634097576
In grad_steps = 790, loss = 0.05471434444189072
In grad_steps = 791, loss = 0.011872878298163414
In grad_steps = 792, loss = 0.188137024641037
In grad_steps = 793, loss = 0.12828712165355682
In grad_steps = 794, loss = 0.06738954037427902
In grad_steps = 795, loss = 0.11278960853815079
In grad_steps = 796, loss = 0.02302519604563713
In grad_steps = 797, loss = 0.10633427649736404
In grad_steps = 798, loss = 0.018239354714751244
In grad_steps = 799, loss = 0.15445451438426971
In grad_steps = 800, loss = 0.07562708109617233
In grad_steps = 801, loss = 0.20400041341781616
In grad_steps = 802, loss = 0.04879596829414368
In grad_steps = 803, loss = 0.08673042058944702
In grad_steps = 804, loss = 0.30700430274009705
In grad_steps = 805, loss = 0.022954795509576797
In grad_steps = 806, loss = 0.17180845141410828
In grad_steps = 807, loss = 0.027970649302005768
In grad_steps = 808, loss = 0.20748475193977356
In grad_steps = 809, loss = 0.12122844159603119
In grad_steps = 810, loss = 0.010250156745314598
In grad_steps = 811, loss = 0.04502930864691734
In grad_steps = 812, loss = 0.025591380894184113
In grad_steps = 813, loss = 0.009191552177071571
In grad_steps = 814, loss = 0.012999489903450012
In grad_steps = 815, loss = 0.03322720155119896
In grad_steps = 816, loss = 0.024108827114105225
In grad_steps = 817, loss = 0.007864957675337791
In grad_steps = 818, loss = 0.0050494009628891945
In grad_steps = 819, loss = 0.03585932031273842
In grad_steps = 820, loss = 0.23437833786010742
In grad_steps = 821, loss = 0.004560804460197687
In grad_steps = 822, loss = 0.12317058444023132
In grad_steps = 823, loss = 0.18954527378082275
In grad_steps = 824, loss = 0.006428242195397615
Beginning epoch 12
In grad_steps = 825, loss = 0.006773228757083416
In grad_steps = 826, loss = 0.036495596170425415
In grad_steps = 827, loss = 0.09935083985328674
In grad_steps = 828, loss = 0.01828751340508461
In grad_steps = 829, loss = 0.06848645210266113
In grad_steps = 830, loss = 0.02611977979540825
In grad_steps = 831, loss = 0.07979130744934082
In grad_steps = 832, loss = 0.1353534758090973
In grad_steps = 833, loss = 0.02386706881225109
In grad_steps = 834, loss = 0.037112366408109665
In grad_steps = 835, loss = 0.07260142266750336
In grad_steps = 836, loss = 0.16437269747257233
In grad_steps = 837, loss = 0.02744477614760399
In grad_steps = 838, loss = 0.148508220911026
In grad_steps = 839, loss = 0.012580580078065395
In grad_steps = 840, loss = 0.022819532081484795
In grad_steps = 841, loss = 0.4859919250011444
In grad_steps = 842, loss = 0.0034125875681638718
In grad_steps = 843, loss = 0.0814906507730484
In grad_steps = 844, loss = 0.022812314331531525
In grad_steps = 845, loss = 0.21555644273757935
In grad_steps = 846, loss = 0.051402099430561066
In grad_steps = 847, loss = 0.06999100744724274
In grad_steps = 848, loss = 0.06595379114151001
In grad_steps = 849, loss = 0.23152944445610046
In grad_steps = 850, loss = 0.006300151813775301
In grad_steps = 851, loss = 0.010190448723733425
In grad_steps = 852, loss = 0.01497062761336565
In grad_steps = 853, loss = 0.004480094648897648
In grad_steps = 854, loss = 0.038244448602199554
In grad_steps = 855, loss = 0.1640491932630539
In grad_steps = 856, loss = 0.201640322804451
In grad_steps = 857, loss = 0.012519946321845055
In grad_steps = 858, loss = 0.01707863248884678
In grad_steps = 859, loss = 0.01760920323431492
In grad_steps = 860, loss = 0.00863820593804121
In grad_steps = 861, loss = 0.17545808851718903
In grad_steps = 862, loss = 0.2080935388803482
In grad_steps = 863, loss = 0.04045616835355759
In grad_steps = 864, loss = 0.06624287366867065
In grad_steps = 865, loss = 0.01857740990817547
In grad_steps = 866, loss = 0.01031668670475483
In grad_steps = 867, loss = 0.017022043466567993
In grad_steps = 868, loss = 0.012343298643827438
In grad_steps = 869, loss = 0.07097869366407394
In grad_steps = 870, loss = 0.16991783678531647
In grad_steps = 871, loss = 0.03596490994095802
In grad_steps = 872, loss = 0.024594444781541824
In grad_steps = 873, loss = 0.028484102338552475
In grad_steps = 874, loss = 0.09665437042713165
In grad_steps = 875, loss = 0.020077480003237724
In grad_steps = 876, loss = 0.015825018286705017
In grad_steps = 877, loss = 0.04218529909849167
In grad_steps = 878, loss = 0.025455189868807793
In grad_steps = 879, loss = 0.10346904397010803
In grad_steps = 880, loss = 0.042216479778289795
In grad_steps = 881, loss = 0.006816599518060684
In grad_steps = 882, loss = 0.01315214391797781
In grad_steps = 883, loss = 0.0484599731862545
In grad_steps = 884, loss = 0.17130963504314423
In grad_steps = 885, loss = 0.03620440512895584
In grad_steps = 886, loss = 0.0901767909526825
In grad_steps = 887, loss = 0.04859255999326706
In grad_steps = 888, loss = 0.0036182026378810406
In grad_steps = 889, loss = 0.0024865546729415655
In grad_steps = 890, loss = 0.06119640916585922
In grad_steps = 891, loss = 0.10160614550113678
In grad_steps = 892, loss = 0.006510764826089144
In grad_steps = 893, loss = 0.0010028245160356164
In grad_steps = 894, loss = 0.007991509512066841
In grad_steps = 895, loss = 0.011307276785373688
In grad_steps = 896, loss = 0.01073765754699707
In grad_steps = 897, loss = 0.05288606137037277
In grad_steps = 898, loss = 0.014463688246905804
In grad_steps = 899, loss = 0.004357785452157259
Elapsed time: 3018.4422187805176 seconds for ensemble 0 with 12 epochs
LoRA instance 0 evaluation complete. Data saved to /pscratch/sd/t/tianle/lucid/other_source/SURP_2024/results/tmp_lora_ensemble/Llama3-8B-set-1.2/test_data_instance_0_seed_1.npz.
lora instance i = 0 Successfully finished.
Training lora instance 1
DEBUG: Total parameters: 8037076992
DEBUG: Trainable parameters: 6815744
Beginning epoch 1
In grad_steps = 0, loss = 1.014610767364502
In grad_steps = 1, loss = 0.7350615859031677
In grad_steps = 2, loss = 0.6801801919937134
In grad_steps = 3, loss = 0.6904050707817078
In grad_steps = 4, loss = 0.6323255896568298
In grad_steps = 5, loss = 0.6282901167869568
In grad_steps = 6, loss = 0.6362040042877197
In grad_steps = 7, loss = 0.9174721240997314
In grad_steps = 8, loss = 0.6786285042762756
In grad_steps = 9, loss = 0.6877272129058838
In grad_steps = 10, loss = 0.6927505731582642
In grad_steps = 11, loss = 0.6940481066703796
In grad_steps = 12, loss = 0.7647899389266968
In grad_steps = 13, loss = 0.7020501494407654
In grad_steps = 14, loss = 0.7450201511383057
In grad_steps = 15, loss = 0.695273220539093
In grad_steps = 16, loss = 0.7196067571640015
In grad_steps = 17, loss = 0.710445761680603
In grad_steps = 18, loss = 0.7051480412483215
In grad_steps = 19, loss = 0.7022131085395813
In grad_steps = 20, loss = 0.6969404816627502
In grad_steps = 21, loss = 0.6944468021392822
In grad_steps = 22, loss = 0.700522243976593
In grad_steps = 23, loss = 0.6861701607704163
In grad_steps = 24, loss = 0.7071975469589233
In grad_steps = 25, loss = 0.6963930726051331
In grad_steps = 26, loss = 0.7134407758712769
In grad_steps = 27, loss = 0.6882404685020447
In grad_steps = 28, loss = 0.6642897725105286
In grad_steps = 29, loss = 0.6451554894447327
In grad_steps = 30, loss = 0.7106250524520874
In grad_steps = 31, loss = 0.6764165759086609
In grad_steps = 32, loss = 0.7787562608718872
In grad_steps = 33, loss = 0.8645325303077698
In grad_steps = 34, loss = 0.6485249400138855
In grad_steps = 35, loss = 0.7454269528388977
In grad_steps = 36, loss = 0.7008620500564575
In grad_steps = 37, loss = 0.6940644383430481
In grad_steps = 38, loss = 0.7595036625862122
In grad_steps = 39, loss = 0.6689829230308533
In grad_steps = 40, loss = 0.6657779216766357
In grad_steps = 41, loss = 0.7005677819252014
In grad_steps = 42, loss = 0.6651575565338135
In grad_steps = 43, loss = 0.7316676378250122
In grad_steps = 44, loss = 0.6566736102104187
In grad_steps = 45, loss = 0.7287980914115906
In grad_steps = 46, loss = 0.7639926075935364
In grad_steps = 47, loss = 0.6531945466995239
In grad_steps = 48, loss = 0.7016704678535461
In grad_steps = 49, loss = 0.6990139484405518
In grad_steps = 50, loss = 0.688142716884613
In grad_steps = 51, loss = 0.6759676337242126
In grad_steps = 52, loss = 0.7189178466796875
In grad_steps = 53, loss = 0.7325263023376465
In grad_steps = 54, loss = 0.7343145608901978
In grad_steps = 55, loss = 0.6559826731681824
In grad_steps = 56, loss = 0.6876242160797119
In grad_steps = 57, loss = 0.7001553773880005
In grad_steps = 58, loss = 0.6826270818710327
In grad_steps = 59, loss = 0.7008020877838135
In grad_steps = 60, loss = 0.7098680734634399
In grad_steps = 61, loss = 0.6828129887580872
In grad_steps = 62, loss = 0.7004464268684387
In grad_steps = 63, loss = 0.7026081681251526
In grad_steps = 64, loss = 0.6955912113189697
In grad_steps = 65, loss = 0.6848546266555786
In grad_steps = 66, loss = 0.6657475233078003
In grad_steps = 67, loss = 0.7300047874450684
In grad_steps = 68, loss = 0.6977895498275757
In grad_steps = 69, loss = 0.7040510177612305
In grad_steps = 70, loss = 0.7179882526397705
In grad_steps = 71, loss = 0.6772338151931763
In grad_steps = 72, loss = 0.7232396602630615
In grad_steps = 73, loss = 0.6757822036743164
In grad_steps = 74, loss = 0.7165651917457581
Beginning epoch 2
In grad_steps = 75, loss = 0.6587791442871094
In grad_steps = 76, loss = 0.6729966402053833
In grad_steps = 77, loss = 0.6626459360122681
In grad_steps = 78, loss = 0.6997156143188477
In grad_steps = 79, loss = 0.720674455165863
In grad_steps = 80, loss = 0.7172494530677795
In grad_steps = 81, loss = 0.6797342300415039
In grad_steps = 82, loss = 0.7086130976676941
In grad_steps = 83, loss = 0.6808950304985046
In grad_steps = 84, loss = 0.6807712912559509
In grad_steps = 85, loss = 0.7304129600524902
In grad_steps = 86, loss = 0.7318394780158997
In grad_steps = 87, loss = 0.6657518744468689
In grad_steps = 88, loss = 0.7230220437049866
In grad_steps = 89, loss = 0.709904670715332
In grad_steps = 90, loss = 0.6917064189910889
In grad_steps = 91, loss = 0.6974781155586243
In grad_steps = 92, loss = 0.6816827654838562
In grad_steps = 93, loss = 0.693239688873291
In grad_steps = 94, loss = 0.6935917139053345
In grad_steps = 95, loss = 0.6880465149879456
In grad_steps = 96, loss = 0.6850905418395996
In grad_steps = 97, loss = 0.6906183362007141
In grad_steps = 98, loss = 0.7016348242759705
In grad_steps = 99, loss = 0.6861695647239685
In grad_steps = 100, loss = 0.6876588463783264
In grad_steps = 101, loss = 0.6988416910171509
In grad_steps = 102, loss = 0.6948586702346802
In grad_steps = 103, loss = 0.6717870831489563
In grad_steps = 104, loss = 0.6286710500717163
In grad_steps = 105, loss = 0.7047797441482544
In grad_steps = 106, loss = 0.6763615012168884
In grad_steps = 107, loss = 0.7628864049911499
In grad_steps = 108, loss = 0.8571184873580933
In grad_steps = 109, loss = 0.6529797315597534
In grad_steps = 110, loss = 0.7879233360290527
In grad_steps = 111, loss = 0.755310595035553
In grad_steps = 112, loss = 0.6672772169113159
In grad_steps = 113, loss = 0.6777992844581604
In grad_steps = 114, loss = 0.6867339611053467
In grad_steps = 115, loss = 0.6745122075080872
In grad_steps = 116, loss = 0.6917500495910645
In grad_steps = 117, loss = 0.6675004363059998
In grad_steps = 118, loss = 0.7160117030143738
In grad_steps = 119, loss = 0.6523837447166443
In grad_steps = 120, loss = 0.7355623245239258
In grad_steps = 121, loss = 0.803564727306366
In grad_steps = 122, loss = 0.6267868876457214
In grad_steps = 123, loss = 0.7245877385139465
In grad_steps = 124, loss = 0.6551366448402405
In grad_steps = 125, loss = 0.7060770392417908
In grad_steps = 126, loss = 0.7487517595291138
In grad_steps = 127, loss = 0.6810613870620728
In grad_steps = 128, loss = 0.7039366960525513
In grad_steps = 129, loss = 0.7144831418991089
In grad_steps = 130, loss = 0.6636272668838501
In grad_steps = 131, loss = 0.6846134662628174
In grad_steps = 132, loss = 0.6974346041679382
In grad_steps = 133, loss = 0.6784676909446716
In grad_steps = 134, loss = 0.7145445346832275
In grad_steps = 135, loss = 0.7257496118545532
In grad_steps = 136, loss = 0.6776797771453857
In grad_steps = 137, loss = 0.7010273933410645
In grad_steps = 138, loss = 0.692740797996521
In grad_steps = 139, loss = 0.6920332908630371
In grad_steps = 140, loss = 0.6816290020942688
In grad_steps = 141, loss = 0.6621913313865662
In grad_steps = 142, loss = 0.7315330505371094
In grad_steps = 143, loss = 0.6958914399147034
In grad_steps = 144, loss = 0.7037336826324463
In grad_steps = 145, loss = 0.7208529114723206
In grad_steps = 146, loss = 0.6636959314346313
In grad_steps = 147, loss = 0.7199624180793762
In grad_steps = 148, loss = 0.6732410192489624
In grad_steps = 149, loss = 0.7136287689208984
Beginning epoch 3
In grad_steps = 150, loss = 0.6507067084312439
In grad_steps = 151, loss = 0.666317880153656
In grad_steps = 152, loss = 0.657413125038147
In grad_steps = 153, loss = 0.6977880001068115
In grad_steps = 154, loss = 0.7167114019393921
In grad_steps = 155, loss = 0.7158985137939453
In grad_steps = 156, loss = 0.6858938932418823
In grad_steps = 157, loss = 0.6958639621734619
In grad_steps = 158, loss = 0.676119327545166
In grad_steps = 159, loss = 0.6693266034126282
In grad_steps = 160, loss = 0.7161909937858582
In grad_steps = 161, loss = 0.7270583510398865
In grad_steps = 162, loss = 0.6638695001602173
In grad_steps = 163, loss = 0.7160213589668274
In grad_steps = 164, loss = 0.7074173092842102
In grad_steps = 165, loss = 0.6906133890151978
In grad_steps = 166, loss = 0.6935396790504456
In grad_steps = 167, loss = 0.6671224236488342
In grad_steps = 168, loss = 0.6856573820114136
In grad_steps = 169, loss = 0.6811845898628235
In grad_steps = 170, loss = 0.6806626319885254
In grad_steps = 171, loss = 0.678097128868103
In grad_steps = 172, loss = 0.6816860437393188
In grad_steps = 173, loss = 0.6851301193237305
In grad_steps = 174, loss = 0.6780498623847961
In grad_steps = 175, loss = 0.6679580807685852
In grad_steps = 176, loss = 0.6885563135147095
In grad_steps = 177, loss = 0.6910760998725891
In grad_steps = 178, loss = 0.6518956422805786
In grad_steps = 179, loss = 0.6098611354827881
In grad_steps = 180, loss = 0.7076693773269653
In grad_steps = 181, loss = 0.6613785028457642
In grad_steps = 182, loss = 0.8027076125144958
In grad_steps = 183, loss = 0.8801007866859436
In grad_steps = 184, loss = 0.6413312554359436
In grad_steps = 185, loss = 0.754470705986023
In grad_steps = 186, loss = 0.7215299010276794
In grad_steps = 187, loss = 0.6509168744087219
In grad_steps = 188, loss = 0.6910223960876465
In grad_steps = 189, loss = 0.6758078932762146
In grad_steps = 190, loss = 0.6639813184738159
In grad_steps = 191, loss = 0.6867347955703735
In grad_steps = 192, loss = 0.6595479249954224
In grad_steps = 193, loss = 0.7140237092971802
In grad_steps = 194, loss = 0.6596442461013794
In grad_steps = 195, loss = 0.7317683100700378
In grad_steps = 196, loss = 0.7719829082489014
In grad_steps = 197, loss = 0.6390974521636963
In grad_steps = 198, loss = 0.6956301331520081
In grad_steps = 199, loss = 0.6848262548446655
In grad_steps = 200, loss = 0.6819100379943848
In grad_steps = 201, loss = 0.6785793900489807
In grad_steps = 202, loss = 0.6995235681533813
In grad_steps = 203, loss = 0.7189508676528931
In grad_steps = 204, loss = 0.72756427526474
In grad_steps = 205, loss = 0.6530381441116333
In grad_steps = 206, loss = 0.6793801188468933
In grad_steps = 207, loss = 0.6871532797813416
In grad_steps = 208, loss = 0.6779943108558655
In grad_steps = 209, loss = 0.691408097743988
In grad_steps = 210, loss = 0.7017016410827637
In grad_steps = 211, loss = 0.6799676418304443
In grad_steps = 212, loss = 0.6892516613006592
In grad_steps = 213, loss = 0.695869505405426
In grad_steps = 214, loss = 0.6804215312004089
In grad_steps = 215, loss = 0.6794426441192627
In grad_steps = 216, loss = 0.6621834635734558
In grad_steps = 217, loss = 0.722241997718811
In grad_steps = 218, loss = 0.6926935315132141
In grad_steps = 219, loss = 0.6935586333274841
In grad_steps = 220, loss = 0.7148841023445129
In grad_steps = 221, loss = 0.6475276947021484
In grad_steps = 222, loss = 0.7131154537200928
In grad_steps = 223, loss = 0.6664453148841858
In grad_steps = 224, loss = 0.7072146534919739
Beginning epoch 4
In grad_steps = 225, loss = 0.6311332583427429
In grad_steps = 226, loss = 0.6603994965553284
In grad_steps = 227, loss = 0.6496619582176208
In grad_steps = 228, loss = 0.6764026284217834
In grad_steps = 229, loss = 0.7118332386016846
In grad_steps = 230, loss = 0.7007660269737244
In grad_steps = 231, loss = 0.6601986885070801
In grad_steps = 232, loss = 0.745155930519104
In grad_steps = 233, loss = 0.6550440788269043
In grad_steps = 234, loss = 0.6587965488433838
In grad_steps = 235, loss = 0.7485839128494263
In grad_steps = 236, loss = 0.7478762865066528
In grad_steps = 237, loss = 0.655769944190979
In grad_steps = 238, loss = 0.6589561104774475
In grad_steps = 239, loss = 0.6787276268005371
In grad_steps = 240, loss = 0.6685306429862976
In grad_steps = 241, loss = 0.6735674142837524
In grad_steps = 242, loss = 0.6166250109672546
In grad_steps = 243, loss = 0.7252154350280762
In grad_steps = 244, loss = 0.6967073082923889
In grad_steps = 245, loss = 0.6605694890022278
In grad_steps = 246, loss = 0.6647906303405762
In grad_steps = 247, loss = 0.6573211550712585
In grad_steps = 248, loss = 0.6666422486305237
In grad_steps = 249, loss = 0.6611137986183167
In grad_steps = 250, loss = 0.6339209079742432
In grad_steps = 251, loss = 0.6431096792221069
In grad_steps = 252, loss = 0.6133858561515808
In grad_steps = 253, loss = 0.5923570394515991
In grad_steps = 254, loss = 0.5631568431854248
In grad_steps = 255, loss = 0.6404085159301758
In grad_steps = 256, loss = 0.5789463520050049
In grad_steps = 257, loss = 0.7252153158187866
In grad_steps = 258, loss = 0.5660359859466553
In grad_steps = 259, loss = 0.7948187589645386
In grad_steps = 260, loss = 0.6096287369728088
In grad_steps = 261, loss = 0.6270256638526917
In grad_steps = 262, loss = 0.6006358861923218
In grad_steps = 263, loss = 0.5706820487976074
In grad_steps = 264, loss = 0.6771470904350281
In grad_steps = 265, loss = 0.680835485458374
In grad_steps = 266, loss = 0.6426834464073181
In grad_steps = 267, loss = 0.6759863495826721
In grad_steps = 268, loss = 0.6531299948692322
In grad_steps = 269, loss = 0.660816490650177
In grad_steps = 270, loss = 0.7867283821105957
In grad_steps = 271, loss = 0.8098734617233276
In grad_steps = 272, loss = 0.5934247374534607
In grad_steps = 273, loss = 0.6698364019393921
In grad_steps = 274, loss = 0.6017984747886658
In grad_steps = 275, loss = 0.7019844651222229
In grad_steps = 276, loss = 0.7655259370803833
In grad_steps = 277, loss = 0.6014952659606934
In grad_steps = 278, loss = 0.6382325887680054
In grad_steps = 279, loss = 0.7037738561630249
In grad_steps = 280, loss = 0.5980871915817261
In grad_steps = 281, loss = 0.6358183026313782
In grad_steps = 282, loss = 0.6822322607040405
In grad_steps = 283, loss = 0.6548473834991455
In grad_steps = 284, loss = 0.6640077829360962
In grad_steps = 285, loss = 0.689143717288971
In grad_steps = 286, loss = 0.6262824535369873
In grad_steps = 287, loss = 0.7014135718345642
In grad_steps = 288, loss = 0.6466331481933594
In grad_steps = 289, loss = 0.6546059846878052
In grad_steps = 290, loss = 0.6465656161308289
In grad_steps = 291, loss = 0.6797220706939697
In grad_steps = 292, loss = 0.6947260499000549
In grad_steps = 293, loss = 0.6567660570144653
In grad_steps = 294, loss = 0.6252568960189819
In grad_steps = 295, loss = 0.7310158610343933
In grad_steps = 296, loss = 0.5529556274414062
In grad_steps = 297, loss = 0.6335565447807312
In grad_steps = 298, loss = 0.7012118101119995
In grad_steps = 299, loss = 0.6993147134780884
Beginning epoch 5
In grad_steps = 300, loss = 0.5757328271865845
In grad_steps = 301, loss = 0.6131781935691833
In grad_steps = 302, loss = 0.6538426876068115
In grad_steps = 303, loss = 0.6185457110404968
In grad_steps = 304, loss = 0.6972851157188416
In grad_steps = 305, loss = 0.6996718049049377
In grad_steps = 306, loss = 0.6279953122138977
In grad_steps = 307, loss = 0.6633114218711853
In grad_steps = 308, loss = 0.5841283798217773
In grad_steps = 309, loss = 0.5411253571510315
In grad_steps = 310, loss = 0.6878339052200317
In grad_steps = 311, loss = 0.6640462875366211
In grad_steps = 312, loss = 0.5582566857337952
In grad_steps = 313, loss = 0.6112534403800964
In grad_steps = 314, loss = 0.5561102628707886
In grad_steps = 315, loss = 0.6265439391136169
In grad_steps = 316, loss = 0.7180051207542419
In grad_steps = 317, loss = 0.37724536657333374
In grad_steps = 318, loss = 0.574004590511322
In grad_steps = 319, loss = 0.5483318567276001
In grad_steps = 320, loss = 0.44230058789253235
In grad_steps = 321, loss = 0.6321686506271362
In grad_steps = 322, loss = 0.5622100234031677
In grad_steps = 323, loss = 0.5923757553100586
In grad_steps = 324, loss = 0.60627681016922
In grad_steps = 325, loss = 0.44949665665626526
In grad_steps = 326, loss = 0.4582780599594116
In grad_steps = 327, loss = 0.4399683177471161
In grad_steps = 328, loss = 0.4053373634815216
In grad_steps = 329, loss = 0.37587159872055054
In grad_steps = 330, loss = 0.4491102993488312
In grad_steps = 331, loss = 0.30731678009033203
In grad_steps = 332, loss = 0.2297183722257614
In grad_steps = 333, loss = 0.24667765200138092
In grad_steps = 334, loss = 0.14522895216941833
In grad_steps = 335, loss = 0.11890800297260284
In grad_steps = 336, loss = 0.25425851345062256
In grad_steps = 337, loss = 0.3555851876735687
In grad_steps = 338, loss = 0.7932119965553284
In grad_steps = 339, loss = 1.3125793933868408
In grad_steps = 340, loss = 1.0460959672927856
In grad_steps = 341, loss = 0.512570321559906
In grad_steps = 342, loss = 0.37732264399528503
In grad_steps = 343, loss = 0.48115235567092896
In grad_steps = 344, loss = 0.6089625954627991
In grad_steps = 345, loss = 0.7160863876342773
In grad_steps = 346, loss = 0.7134498357772827
In grad_steps = 347, loss = 0.553883969783783
In grad_steps = 348, loss = 0.6012064814567566
In grad_steps = 349, loss = 0.6181798577308655
In grad_steps = 350, loss = 0.595586895942688
In grad_steps = 351, loss = 0.5931874513626099
In grad_steps = 352, loss = 0.563183605670929
In grad_steps = 353, loss = 0.6244809627532959
In grad_steps = 354, loss = 0.6044797897338867
In grad_steps = 355, loss = 1.0529059171676636
In grad_steps = 356, loss = 0.42200934886932373
In grad_steps = 357, loss = 0.5752323865890503
In grad_steps = 358, loss = 0.6079664826393127
In grad_steps = 359, loss = 0.4693607687950134
In grad_steps = 360, loss = 0.5349578857421875
In grad_steps = 361, loss = 0.4709831476211548
In grad_steps = 362, loss = 0.44716423749923706
In grad_steps = 363, loss = 0.3483860492706299
In grad_steps = 364, loss = 0.6589409708976746
In grad_steps = 365, loss = 0.37552887201309204
In grad_steps = 366, loss = 0.5338091850280762
In grad_steps = 367, loss = 0.5317796468734741
In grad_steps = 368, loss = 0.24522969126701355
In grad_steps = 369, loss = 0.43902918696403503
In grad_steps = 370, loss = 0.82878178358078
In grad_steps = 371, loss = 0.5754208564758301
In grad_steps = 372, loss = 0.365587443113327
In grad_steps = 373, loss = 0.5579726099967957
In grad_steps = 374, loss = 0.5087823271751404
Beginning epoch 6
In grad_steps = 375, loss = 0.4896910786628723
In grad_steps = 376, loss = 0.6341047883033752
In grad_steps = 377, loss = 0.5469524264335632
In grad_steps = 378, loss = 0.592717707157135
In grad_steps = 379, loss = 0.6969932913780212
In grad_steps = 380, loss = 0.6885074973106384
In grad_steps = 381, loss = 0.5376609563827515
In grad_steps = 382, loss = 0.6144838333129883
In grad_steps = 383, loss = 0.4467480182647705
In grad_steps = 384, loss = 0.45855912566185
In grad_steps = 385, loss = 0.590808629989624
In grad_steps = 386, loss = 0.5473525524139404
In grad_steps = 387, loss = 0.41219931840896606
In grad_steps = 388, loss = 0.5214946866035461
In grad_steps = 389, loss = 0.34561723470687866
In grad_steps = 390, loss = 0.42832958698272705
In grad_steps = 391, loss = 0.4265047311782837
In grad_steps = 392, loss = 0.2421196699142456
In grad_steps = 393, loss = 0.28860464692115784
In grad_steps = 394, loss = 0.2841521203517914
In grad_steps = 395, loss = 0.17772583663463593
In grad_steps = 396, loss = 0.477539598941803
In grad_steps = 397, loss = 0.30947592854499817
In grad_steps = 398, loss = 0.5262110829353333
In grad_steps = 399, loss = 0.20825378596782684
In grad_steps = 400, loss = 0.2381945103406906
In grad_steps = 401, loss = 0.09255959093570709
In grad_steps = 402, loss = 0.38695064187049866
In grad_steps = 403, loss = 0.16107827425003052
In grad_steps = 404, loss = 0.2405650019645691
In grad_steps = 405, loss = 0.2979707419872284
In grad_steps = 406, loss = 0.22789977490901947
In grad_steps = 407, loss = 0.35550233721733093
In grad_steps = 408, loss = 0.20115943253040314
In grad_steps = 409, loss = 0.12329575419425964
In grad_steps = 410, loss = 0.12061305344104767
In grad_steps = 411, loss = 0.26834383606910706
In grad_steps = 412, loss = 0.24218380451202393
In grad_steps = 413, loss = 0.44761157035827637
In grad_steps = 414, loss = 0.3864881098270416
In grad_steps = 415, loss = 0.08619419485330582
In grad_steps = 416, loss = 0.09835986793041229
In grad_steps = 417, loss = 0.9573412537574768
In grad_steps = 418, loss = 0.7727183699607849
In grad_steps = 419, loss = 0.35690051317214966
In grad_steps = 420, loss = 0.6885262131690979
In grad_steps = 421, loss = 0.4890798032283783
In grad_steps = 422, loss = 0.3465728461742401
In grad_steps = 423, loss = 0.5134520530700684
In grad_steps = 424, loss = 0.5392599701881409
In grad_steps = 425, loss = 0.612288236618042
In grad_steps = 426, loss = 1.0429110527038574
In grad_steps = 427, loss = 0.406830370426178
In grad_steps = 428, loss = 0.38862624764442444
In grad_steps = 429, loss = 0.3818220794200897
In grad_steps = 430, loss = 0.43587419390678406
In grad_steps = 431, loss = 0.40275460481643677
In grad_steps = 432, loss = 0.43363189697265625
In grad_steps = 433, loss = 0.5250333547592163
In grad_steps = 434, loss = 0.4420038163661957
In grad_steps = 435, loss = 0.4577421545982361
In grad_steps = 436, loss = 0.3628571927547455
In grad_steps = 437, loss = 0.38100430369377136
In grad_steps = 438, loss = 0.23836125433444977
In grad_steps = 439, loss = 0.2838001251220703
In grad_steps = 440, loss = 0.20397569239139557
In grad_steps = 441, loss = 0.3290061056613922
In grad_steps = 442, loss = 0.2992633879184723
In grad_steps = 443, loss = 0.13369125127792358
In grad_steps = 444, loss = 0.25522276759147644
In grad_steps = 445, loss = 0.23279407620429993
In grad_steps = 446, loss = 0.23680266737937927
In grad_steps = 447, loss = 0.5077134370803833
In grad_steps = 448, loss = 0.3463720679283142
In grad_steps = 449, loss = 0.2535877525806427
Beginning epoch 7
In grad_steps = 450, loss = 0.19442912936210632
In grad_steps = 451, loss = 0.7464706301689148
In grad_steps = 452, loss = 0.696369469165802
In grad_steps = 453, loss = 0.24771270155906677
In grad_steps = 454, loss = 0.40054306387901306
In grad_steps = 455, loss = 0.15947465598583221
In grad_steps = 456, loss = 0.26231810450553894
In grad_steps = 457, loss = 0.31103163957595825
In grad_steps = 458, loss = 0.4762688875198364
In grad_steps = 459, loss = 0.4437181353569031
In grad_steps = 460, loss = 0.4121996760368347
In grad_steps = 461, loss = 0.504657506942749
In grad_steps = 462, loss = 0.5259655117988586
In grad_steps = 463, loss = 0.30047473311424255
In grad_steps = 464, loss = 0.31626051664352417
In grad_steps = 465, loss = 0.34942203760147095
In grad_steps = 466, loss = 0.5856980085372925
In grad_steps = 467, loss = 0.41737818717956543
In grad_steps = 468, loss = 0.37474602460861206
In grad_steps = 469, loss = 0.2945116460323334
In grad_steps = 470, loss = 0.20183074474334717
In grad_steps = 471, loss = 0.24967853724956512
In grad_steps = 472, loss = 0.3709654211997986
In grad_steps = 473, loss = 0.3335670232772827
In grad_steps = 474, loss = 0.31801337003707886
In grad_steps = 475, loss = 0.5598817467689514
In grad_steps = 476, loss = 0.17322087287902832
In grad_steps = 477, loss = 0.43621334433555603
In grad_steps = 478, loss = 0.17166155576705933
In grad_steps = 479, loss = 0.20702029764652252
In grad_steps = 480, loss = 0.11758729070425034
In grad_steps = 481, loss = 0.18124082684516907
In grad_steps = 482, loss = 0.2654435932636261
In grad_steps = 483, loss = 0.9322918653488159
In grad_steps = 484, loss = 0.510266125202179
In grad_steps = 485, loss = 0.43338149785995483
In grad_steps = 486, loss = 0.20506250858306885
In grad_steps = 487, loss = 0.06927798688411713
In grad_steps = 488, loss = 0.2229018360376358
In grad_steps = 489, loss = 0.32110485434532166
In grad_steps = 490, loss = 0.25251471996307373
In grad_steps = 491, loss = 0.5568946599960327
In grad_steps = 492, loss = 0.5873015522956848
In grad_steps = 493, loss = 0.5697597861289978
In grad_steps = 494, loss = 0.48146891593933105
In grad_steps = 495, loss = 0.5384105443954468
In grad_steps = 496, loss = 0.298121839761734
In grad_steps = 497, loss = 0.5906319618225098
In grad_steps = 498, loss = 0.5447234511375427
In grad_steps = 499, loss = 0.5401577949523926
In grad_steps = 500, loss = 0.4913818836212158
In grad_steps = 501, loss = 0.43635332584381104
In grad_steps = 502, loss = 0.2754039466381073
In grad_steps = 503, loss = 0.4085021913051605
In grad_steps = 504, loss = 0.3851020932197571
In grad_steps = 505, loss = 0.3446398079395294
In grad_steps = 506, loss = 0.28860655426979065
In grad_steps = 507, loss = 0.32057490944862366
In grad_steps = 508, loss = 0.7039048671722412
In grad_steps = 509, loss = 0.31223535537719727
In grad_steps = 510, loss = 0.29760318994522095
In grad_steps = 511, loss = 0.26884371042251587
In grad_steps = 512, loss = 0.30351683497428894
In grad_steps = 513, loss = 0.2127053588628769
In grad_steps = 514, loss = 0.18306253850460052
In grad_steps = 515, loss = 0.22658225893974304
In grad_steps = 516, loss = 0.3608758747577667
In grad_steps = 517, loss = 0.17514872550964355
In grad_steps = 518, loss = 0.24947760999202728
In grad_steps = 519, loss = 0.10645171999931335
In grad_steps = 520, loss = 0.041447963565588
In grad_steps = 521, loss = 0.16086533665657043
In grad_steps = 522, loss = 0.2602330446243286
In grad_steps = 523, loss = 0.2228396087884903
In grad_steps = 524, loss = 0.12917295098304749
Beginning epoch 8
In grad_steps = 525, loss = 0.3486397862434387
In grad_steps = 526, loss = 0.31884443759918213
In grad_steps = 527, loss = 0.030024884268641472
In grad_steps = 528, loss = 0.3719944357872009
In grad_steps = 529, loss = 0.19857878983020782
In grad_steps = 530, loss = 0.06369354575872421
In grad_steps = 531, loss = 0.37402403354644775
In grad_steps = 532, loss = 0.7845187783241272
In grad_steps = 533, loss = 0.2558519244194031
In grad_steps = 534, loss = 0.23079615831375122
In grad_steps = 535, loss = 0.11715027689933777
In grad_steps = 536, loss = 0.31124210357666016
In grad_steps = 537, loss = 0.08209192007780075
In grad_steps = 538, loss = 0.09894297271966934
In grad_steps = 539, loss = 0.24317139387130737
In grad_steps = 540, loss = 0.22156980633735657
In grad_steps = 541, loss = 0.15073949098587036
In grad_steps = 542, loss = 0.18695425987243652
In grad_steps = 543, loss = 0.28145137429237366
In grad_steps = 544, loss = 0.15275344252586365
In grad_steps = 545, loss = 0.15273210406303406
In grad_steps = 546, loss = 0.15413665771484375
In grad_steps = 547, loss = 0.1286775767803192
In grad_steps = 548, loss = 0.30557918548583984
In grad_steps = 549, loss = 0.20794206857681274
In grad_steps = 550, loss = 0.10090187937021255
In grad_steps = 551, loss = 0.12310819327831268
In grad_steps = 552, loss = 0.21389618515968323
In grad_steps = 553, loss = 0.2778162956237793
In grad_steps = 554, loss = 0.6684767603874207
In grad_steps = 555, loss = 0.2314094603061676
In grad_steps = 556, loss = 0.3300814926624298
In grad_steps = 557, loss = 0.05433874577283859
In grad_steps = 558, loss = 0.09483891725540161
In grad_steps = 559, loss = 0.08791083097457886
In grad_steps = 560, loss = 0.1956380307674408
In grad_steps = 561, loss = 0.431462824344635
In grad_steps = 562, loss = 0.7887123823165894
In grad_steps = 563, loss = 0.08445058017969131
In grad_steps = 564, loss = 0.1458028107881546
In grad_steps = 565, loss = 0.04129946231842041
In grad_steps = 566, loss = 0.06771165877580643
In grad_steps = 567, loss = 0.311904639005661
In grad_steps = 568, loss = 0.17729216814041138
In grad_steps = 569, loss = 0.31699472665786743
In grad_steps = 570, loss = 0.5527100563049316
In grad_steps = 571, loss = 0.5166670680046082
In grad_steps = 572, loss = 0.24509502947330475
In grad_steps = 573, loss = 0.14317403733730316
In grad_steps = 574, loss = 0.12424468249082565
In grad_steps = 575, loss = 0.14884965121746063
In grad_steps = 576, loss = 0.5174069404602051
In grad_steps = 577, loss = 0.07575581967830658
In grad_steps = 578, loss = 0.39524781703948975
In grad_steps = 579, loss = 0.4727814197540283
In grad_steps = 580, loss = 0.12424223124980927
In grad_steps = 581, loss = 0.2052294760942459
In grad_steps = 582, loss = 0.11791717261075974
In grad_steps = 583, loss = 0.3833763897418976
In grad_steps = 584, loss = 0.2997034192085266
In grad_steps = 585, loss = 0.0721418485045433
In grad_steps = 586, loss = 0.1728636920452118
In grad_steps = 587, loss = 0.1827545464038849
In grad_steps = 588, loss = 0.065248504281044
In grad_steps = 589, loss = 0.07930964976549149
In grad_steps = 590, loss = 0.08393833041191101
In grad_steps = 591, loss = 0.11471454054117203
In grad_steps = 592, loss = 0.06837081909179688
In grad_steps = 593, loss = 0.05589058995246887
In grad_steps = 594, loss = 0.0950559452176094
In grad_steps = 595, loss = 0.4074929356575012
In grad_steps = 596, loss = 0.021180253475904465
In grad_steps = 597, loss = 0.1605096161365509
In grad_steps = 598, loss = 0.09782116860151291
In grad_steps = 599, loss = 0.01627010479569435
Beginning epoch 9
In grad_steps = 600, loss = 0.07376539707183838
In grad_steps = 601, loss = 0.03699244558811188
In grad_steps = 602, loss = 0.15405799448490143
In grad_steps = 603, loss = 0.08281660079956055
In grad_steps = 604, loss = 0.22088295221328735
In grad_steps = 605, loss = 0.07787568122148514
In grad_steps = 606, loss = 0.045013222843408585
In grad_steps = 607, loss = 0.5272642970085144
In grad_steps = 608, loss = 0.1184140220284462
In grad_steps = 609, loss = 0.05146259814500809
In grad_steps = 610, loss = 0.024696970358490944
In grad_steps = 611, loss = 0.20249870419502258
In grad_steps = 612, loss = 0.12569375336170197
In grad_steps = 613, loss = 0.014619985595345497
In grad_steps = 614, loss = 0.03559475764632225
In grad_steps = 615, loss = 0.172549307346344
In grad_steps = 616, loss = 0.38320833444595337
In grad_steps = 617, loss = 0.018078166991472244
In grad_steps = 618, loss = 0.33465832471847534
In grad_steps = 619, loss = 0.023520126938819885
In grad_steps = 620, loss = 0.1979375183582306
In grad_steps = 621, loss = 0.09877762943506241
In grad_steps = 622, loss = 0.16741932928562164
In grad_steps = 623, loss = 0.16000749170780182
In grad_steps = 624, loss = 0.18506748974323273
In grad_steps = 625, loss = 0.13528543710708618
In grad_steps = 626, loss = 0.11079897731542587
In grad_steps = 627, loss = 0.060835517942905426
In grad_steps = 628, loss = 0.13996973633766174
In grad_steps = 629, loss = 0.23980015516281128
In grad_steps = 630, loss = 0.24903346598148346
In grad_steps = 631, loss = 0.036121685057878494
In grad_steps = 632, loss = 0.019033780321478844
In grad_steps = 633, loss = 0.252640962600708
In grad_steps = 634, loss = 0.10260898619890213
In grad_steps = 635, loss = 0.03803442418575287
In grad_steps = 636, loss = 0.05590401217341423
In grad_steps = 637, loss = 0.08752769231796265
In grad_steps = 638, loss = 0.06455721706151962
In grad_steps = 639, loss = 0.1318117380142212
In grad_steps = 640, loss = 0.39646756649017334
In grad_steps = 641, loss = 0.0946372002363205
In grad_steps = 642, loss = 0.07491236925125122
In grad_steps = 643, loss = 0.16584526002407074
In grad_steps = 644, loss = 0.3590029180049896
In grad_steps = 645, loss = 0.27233144640922546
In grad_steps = 646, loss = 0.2603302299976349
In grad_steps = 647, loss = 0.22752127051353455
In grad_steps = 648, loss = 0.06738100945949554
In grad_steps = 649, loss = 0.028928471729159355
In grad_steps = 650, loss = 0.13312453031539917
In grad_steps = 651, loss = 0.38781067728996277
In grad_steps = 652, loss = 0.17078936100006104
In grad_steps = 653, loss = 0.14819972217082977
In grad_steps = 654, loss = 0.20871827006340027
In grad_steps = 655, loss = 0.03309576213359833
In grad_steps = 656, loss = 0.056397948414087296
In grad_steps = 657, loss = 0.030816540122032166
In grad_steps = 658, loss = 0.11087080091238022
In grad_steps = 659, loss = 0.2170432209968567
In grad_steps = 660, loss = 0.029577067121863365
In grad_steps = 661, loss = 0.08348308503627777
In grad_steps = 662, loss = 0.05144794285297394
In grad_steps = 663, loss = 0.024139905348420143
In grad_steps = 664, loss = 0.09050852805376053
In grad_steps = 665, loss = 0.24335749447345734
In grad_steps = 666, loss = 0.07443969696760178
In grad_steps = 667, loss = 0.03640849143266678
In grad_steps = 668, loss = 0.0158406812697649
In grad_steps = 669, loss = 0.08074948191642761
In grad_steps = 670, loss = 0.021703151986002922
In grad_steps = 671, loss = 0.009352616034448147
In grad_steps = 672, loss = 0.12096331268548965
In grad_steps = 673, loss = 0.02868640050292015
In grad_steps = 674, loss = 0.0031108080875128508
Beginning epoch 10
In grad_steps = 675, loss = 0.021755166351795197
In grad_steps = 676, loss = 0.08844619989395142
In grad_steps = 677, loss = 0.015025601722300053
In grad_steps = 678, loss = 0.01953081227838993
In grad_steps = 679, loss = 0.13835979998111725
In grad_steps = 680, loss = 0.005249762441962957
In grad_steps = 681, loss = 0.008484208956360817
In grad_steps = 682, loss = 0.003643564647063613
In grad_steps = 683, loss = 0.020503439009189606
In grad_steps = 684, loss = 0.021808268502354622
In grad_steps = 685, loss = 0.011451640166342258
In grad_steps = 686, loss = 0.083582803606987
In grad_steps = 687, loss = 0.013048574328422546
In grad_steps = 688, loss = 0.0019410402746871114
In grad_steps = 689, loss = 0.0021187863312661648
In grad_steps = 690, loss = 0.0038516835775226355
In grad_steps = 691, loss = 0.009561832062900066
In grad_steps = 692, loss = 0.000565650756470859
In grad_steps = 693, loss = 0.24232131242752075
In grad_steps = 694, loss = 0.0036317799240350723
In grad_steps = 695, loss = 0.0039888303726911545
In grad_steps = 696, loss = 0.008680560626089573
In grad_steps = 697, loss = 0.13620445132255554
In grad_steps = 698, loss = 0.006072575692087412
In grad_steps = 699, loss = 0.23464220762252808
In grad_steps = 700, loss = 0.0023939060047268867
In grad_steps = 701, loss = 0.0036331277806311846
In grad_steps = 702, loss = 0.0020026296842843294
In grad_steps = 703, loss = 0.0048361970111727715
In grad_steps = 704, loss = 0.008730385452508926
In grad_steps = 705, loss = 0.032650239765644073
In grad_steps = 706, loss = 0.03870553895831108
In grad_steps = 707, loss = 0.041802000254392624
In grad_steps = 708, loss = 0.0009031167137436569
In grad_steps = 709, loss = 0.0026568055618554354
In grad_steps = 710, loss = 0.0006230745348148048
In grad_steps = 711, loss = 0.011234275065362453
In grad_steps = 712, loss = 0.0011537522077560425
In grad_steps = 713, loss = 0.002524050185456872
In grad_steps = 714, loss = 0.002496152650564909
In grad_steps = 715, loss = 0.0009810883784666657
In grad_steps = 716, loss = 0.0007576006464660168
In grad_steps = 717, loss = 0.05047810450196266
In grad_steps = 718, loss = 0.015302184037864208
In grad_steps = 719, loss = 0.23686480522155762
In grad_steps = 720, loss = 0.26039624214172363
In grad_steps = 721, loss = 0.009191662073135376
In grad_steps = 722, loss = 0.031316857784986496
In grad_steps = 723, loss = 0.19847708940505981
In grad_steps = 724, loss = 0.009309881366789341
In grad_steps = 725, loss = 0.016639549285173416
In grad_steps = 726, loss = 0.025508830323815346
In grad_steps = 727, loss = 0.0017950369510799646
In grad_steps = 728, loss = 0.015478450804948807
In grad_steps = 729, loss = 0.06937044858932495
In grad_steps = 730, loss = 0.028865166008472443
In grad_steps = 731, loss = 0.19233916699886322
In grad_steps = 732, loss = 0.03767845407128334
In grad_steps = 733, loss = 0.08738217502832413
In grad_steps = 734, loss = 0.10715006291866302
In grad_steps = 735, loss = 0.010163307189941406
In grad_steps = 736, loss = 0.09458386152982712
In grad_steps = 737, loss = 0.11283282190561295
In grad_steps = 738, loss = 0.015604153275489807
In grad_steps = 739, loss = 0.012847130186855793
In grad_steps = 740, loss = 0.012178223580121994
In grad_steps = 741, loss = 0.07427560538053513
In grad_steps = 742, loss = 0.016034262254834175
In grad_steps = 743, loss = 0.027253201231360435
In grad_steps = 744, loss = 0.017790785059332848
In grad_steps = 745, loss = 0.012329962104558945
In grad_steps = 746, loss = 0.011458434164524078
In grad_steps = 747, loss = 0.030214324593544006
In grad_steps = 748, loss = 0.054521434009075165
In grad_steps = 749, loss = 0.11087233573198318
Beginning epoch 11
In grad_steps = 750, loss = 0.006905062589794397
In grad_steps = 751, loss = 0.010981062427163124
In grad_steps = 752, loss = 0.04093782603740692
In grad_steps = 753, loss = 0.02290596440434456
In grad_steps = 754, loss = 0.20767781138420105
In grad_steps = 755, loss = 0.004159879870712757
In grad_steps = 756, loss = 0.00984491128474474
In grad_steps = 757, loss = 0.005460921209305525
In grad_steps = 758, loss = 0.008762813173234463
In grad_steps = 759, loss = 0.005223900079727173
In grad_steps = 760, loss = 0.006942433305084705
In grad_steps = 761, loss = 0.1528380960226059
In grad_steps = 762, loss = 0.00321267731487751
In grad_steps = 763, loss = 0.0027285064570605755
In grad_steps = 764, loss = 0.031142951920628548
In grad_steps = 765, loss = 0.023820525035262108
In grad_steps = 766, loss = 0.19230122864246368
In grad_steps = 767, loss = 0.0013171258615329862
In grad_steps = 768, loss = 0.08999791741371155
In grad_steps = 769, loss = 0.27770599722862244
In grad_steps = 770, loss = 0.4308083653450012
In grad_steps = 771, loss = 0.11519744992256165
In grad_steps = 772, loss = 0.08024606108665466
In grad_steps = 773, loss = 0.04886496812105179
In grad_steps = 774, loss = 0.021062631160020828
In grad_steps = 775, loss = 0.022118333727121353
In grad_steps = 776, loss = 0.04156382754445076
In grad_steps = 777, loss = 0.08636121451854706
In grad_steps = 778, loss = 0.04768881946802139
In grad_steps = 779, loss = 0.6536434888839722
In grad_steps = 780, loss = 0.1383739709854126
In grad_steps = 781, loss = 0.022719310596585274
In grad_steps = 782, loss = 0.034371837973594666
In grad_steps = 783, loss = 0.04408397525548935
In grad_steps = 784, loss = 0.08543054014444351
In grad_steps = 785, loss = 0.06280884891748428
In grad_steps = 786, loss = 0.16919738054275513
In grad_steps = 787, loss = 0.04842220991849899
In grad_steps = 788, loss = 0.16874206066131592
In grad_steps = 789, loss = 0.11515799164772034
In grad_steps = 790, loss = 0.03464455530047417
In grad_steps = 791, loss = 0.026644587516784668
In grad_steps = 792, loss = 0.021814288571476936
In grad_steps = 793, loss = 0.06705205142498016
In grad_steps = 794, loss = 0.26228365302085876
In grad_steps = 795, loss = 0.01846182905137539
In grad_steps = 796, loss = 0.07755497097969055
In grad_steps = 797, loss = 0.0344204343855381
In grad_steps = 798, loss = 0.2434428483247757
In grad_steps = 799, loss = 0.017414024099707603
In grad_steps = 800, loss = 0.014205931685864925
In grad_steps = 801, loss = 0.03246050700545311
In grad_steps = 802, loss = 0.019347209483385086
In grad_steps = 803, loss = 0.1528037190437317
In grad_steps = 804, loss = 0.16503340005874634
In grad_steps = 805, loss = 0.018463868647813797
In grad_steps = 806, loss = 0.1490338295698166
In grad_steps = 807, loss = 0.09894265234470367
In grad_steps = 808, loss = 0.06750842183828354
In grad_steps = 809, loss = 0.028697267174720764
In grad_steps = 810, loss = 0.0340411551296711
In grad_steps = 811, loss = 0.03336796909570694
In grad_steps = 812, loss = 0.3372313976287842
In grad_steps = 813, loss = 0.04490703344345093
In grad_steps = 814, loss = 0.0010592094622552395
In grad_steps = 815, loss = 0.016515981405973434
In grad_steps = 816, loss = 0.006841192487627268
In grad_steps = 817, loss = 0.041171617805957794
In grad_steps = 818, loss = 0.19695231318473816
In grad_steps = 819, loss = 0.004658217076212168
In grad_steps = 820, loss = 0.010601894930005074
In grad_steps = 821, loss = 0.04857594519853592
In grad_steps = 822, loss = 0.003744670655578375
In grad_steps = 823, loss = 0.1229531541466713
In grad_steps = 824, loss = 0.020242074504494667
Beginning epoch 12
In grad_steps = 825, loss = 0.004603452980518341
In grad_steps = 826, loss = 0.0031024578493088484
In grad_steps = 827, loss = 0.10876166820526123
In grad_steps = 828, loss = 0.14244720339775085
In grad_steps = 829, loss = 0.08872184157371521
In grad_steps = 830, loss = 0.013450918719172478
In grad_steps = 831, loss = 0.025173593312501907
In grad_steps = 832, loss = 0.2710117697715759
In grad_steps = 833, loss = 0.003290643449872732
In grad_steps = 834, loss = 0.014266854152083397
In grad_steps = 835, loss = 0.01360462699085474
In grad_steps = 836, loss = 0.06571220606565475
In grad_steps = 837, loss = 0.2626086175441742
In grad_steps = 838, loss = 0.11796170473098755
In grad_steps = 839, loss = 0.004280158318579197
In grad_steps = 840, loss = 0.21480363607406616
In grad_steps = 841, loss = 0.09680688381195068
In grad_steps = 842, loss = 0.04427270218729973
In grad_steps = 843, loss = 0.17201264202594757
In grad_steps = 844, loss = 0.07219795882701874
In grad_steps = 845, loss = 0.03769164904952049
In grad_steps = 846, loss = 0.023776795715093613
In grad_steps = 847, loss = 0.11495634913444519
In grad_steps = 848, loss = 0.07120606303215027
In grad_steps = 849, loss = 0.013196156360208988
In grad_steps = 850, loss = 0.01951301097869873
In grad_steps = 851, loss = 0.006731216795742512
In grad_steps = 852, loss = 0.10693340003490448
In grad_steps = 853, loss = 0.15857645869255066
In grad_steps = 854, loss = 0.14058679342269897
In grad_steps = 855, loss = 0.09841571003198624
In grad_steps = 856, loss = 0.13818131387233734
In grad_steps = 857, loss = 0.00936883594840765
In grad_steps = 858, loss = 0.011998865753412247
In grad_steps = 859, loss = 0.013921315781772137
In grad_steps = 860, loss = 0.010345003567636013
In grad_steps = 861, loss = 0.00591613445430994
In grad_steps = 862, loss = 0.003786872373893857
In grad_steps = 863, loss = 0.38029465079307556
In grad_steps = 864, loss = 0.020756665617227554
In grad_steps = 865, loss = 0.008018389344215393
In grad_steps = 866, loss = 0.032911647111177444
In grad_steps = 867, loss = 0.04728766158223152
In grad_steps = 868, loss = 0.19578561186790466
In grad_steps = 869, loss = 0.024980973452329636
In grad_steps = 870, loss = 0.9010639786720276
In grad_steps = 871, loss = 0.025639494881033897
In grad_steps = 872, loss = 0.076021708548069
In grad_steps = 873, loss = 0.035883236676454544
In grad_steps = 874, loss = 0.017734603956341743
In grad_steps = 875, loss = 0.09138419479131699
In grad_steps = 876, loss = 0.10579600930213928
In grad_steps = 877, loss = 0.031175075098872185
In grad_steps = 878, loss = 0.06643194705247879
In grad_steps = 879, loss = 0.04263410344719887
In grad_steps = 880, loss = 0.0500541627407074
In grad_steps = 881, loss = 0.1300554871559143
In grad_steps = 882, loss = 0.02194196544587612
In grad_steps = 883, loss = 0.044044964015483856
In grad_steps = 884, loss = 0.04778802767395973
In grad_steps = 885, loss = 0.020572016015648842
In grad_steps = 886, loss = 0.030802607536315918
In grad_steps = 887, loss = 0.10862462222576141
In grad_steps = 888, loss = 0.01937098056077957
In grad_steps = 889, loss = 0.006332011427730322
In grad_steps = 890, loss = 0.019355418160557747
In grad_steps = 891, loss = 0.0055920095182955265
In grad_steps = 892, loss = 0.009790848940610886
In grad_steps = 893, loss = 0.018852170556783676
In grad_steps = 894, loss = 0.016969095915555954
In grad_steps = 895, loss = 0.15738630294799805
In grad_steps = 896, loss = 0.0035450197756290436
In grad_steps = 897, loss = 0.0054403916001319885
In grad_steps = 898, loss = 0.006821040529757738
In grad_steps = 899, loss = 0.0010815698187798262
Elapsed time: 3013.411098718643 seconds for ensemble 1 with 12 epochs
LoRA instance 1 evaluation complete. Data saved to /pscratch/sd/t/tianle/lucid/other_source/SURP_2024/results/tmp_lora_ensemble/Llama3-8B-set-1.2/test_data_instance_1_seed_10094.npz.
lora instance i = 1 Successfully finished.
Training lora instance 2
DEBUG: Total parameters: 8037076992
DEBUG: Trainable parameters: 6815744
Beginning epoch 1
In grad_steps = 0, loss = 1.014610767364502
In grad_steps = 1, loss = 0.7371419072151184
In grad_steps = 2, loss = 0.6808889508247375
In grad_steps = 3, loss = 0.6873716115951538
In grad_steps = 4, loss = 0.6396929025650024
In grad_steps = 5, loss = 0.6331567764282227
In grad_steps = 6, loss = 0.6471773982048035
In grad_steps = 7, loss = 0.910367488861084
In grad_steps = 8, loss = 0.6774739623069763
In grad_steps = 9, loss = 0.6859179139137268
In grad_steps = 10, loss = 0.6932764053344727
In grad_steps = 11, loss = 0.697294294834137
In grad_steps = 12, loss = 0.7632735967636108
In grad_steps = 13, loss = 0.6979089975357056
In grad_steps = 14, loss = 0.7402448058128357
In grad_steps = 15, loss = 0.6960684061050415
In grad_steps = 16, loss = 0.7213032245635986
In grad_steps = 17, loss = 0.7092405557632446
In grad_steps = 18, loss = 0.708283007144928
In grad_steps = 19, loss = 0.6970064640045166
In grad_steps = 20, loss = 0.69828861951828
In grad_steps = 21, loss = 0.6934503316879272
In grad_steps = 22, loss = 0.7015756368637085
In grad_steps = 23, loss = 0.6864338517189026
In grad_steps = 24, loss = 0.7088764905929565
In grad_steps = 25, loss = 0.6956171989440918
In grad_steps = 26, loss = 0.7110680937767029
In grad_steps = 27, loss = 0.689302921295166
In grad_steps = 28, loss = 0.6677695512771606
In grad_steps = 29, loss = 0.643190860748291
In grad_steps = 30, loss = 0.7066164016723633
In grad_steps = 31, loss = 0.6743597984313965
In grad_steps = 32, loss = 0.7745895385742188
In grad_steps = 33, loss = 0.8596124053001404
In grad_steps = 34, loss = 0.6539964079856873
In grad_steps = 35, loss = 0.7435572743415833
In grad_steps = 36, loss = 0.6991441249847412
In grad_steps = 37, loss = 0.6934494972229004
In grad_steps = 38, loss = 0.7440215945243835
In grad_steps = 39, loss = 0.6729073524475098
In grad_steps = 40, loss = 0.6671189665794373
In grad_steps = 41, loss = 0.7003042697906494
In grad_steps = 42, loss = 0.6700785160064697
In grad_steps = 43, loss = 0.7326580286026001
In grad_steps = 44, loss = 0.6602231860160828
In grad_steps = 45, loss = 0.731186032295227
In grad_steps = 46, loss = 0.7646898031234741
In grad_steps = 47, loss = 0.6527107954025269
In grad_steps = 48, loss = 0.7055613398551941
In grad_steps = 49, loss = 0.7013078927993774
In grad_steps = 50, loss = 0.689652681350708
In grad_steps = 51, loss = 0.6790623664855957
In grad_steps = 52, loss = 0.7159443497657776
In grad_steps = 53, loss = 0.7322453856468201
In grad_steps = 54, loss = 0.7346301674842834
In grad_steps = 55, loss = 0.6551513671875
In grad_steps = 56, loss = 0.6959664821624756
In grad_steps = 57, loss = 0.6980211138725281
In grad_steps = 58, loss = 0.6802484393119812
In grad_steps = 59, loss = 0.7006658315658569
In grad_steps = 60, loss = 0.706365704536438
In grad_steps = 61, loss = 0.6827048659324646
In grad_steps = 62, loss = 0.7000396251678467
In grad_steps = 63, loss = 0.6994585990905762
In grad_steps = 64, loss = 0.6912565231323242
In grad_steps = 65, loss = 0.6872967481613159
In grad_steps = 66, loss = 0.6650821566581726
In grad_steps = 67, loss = 0.7301396727561951
In grad_steps = 68, loss = 0.7014299631118774
In grad_steps = 69, loss = 0.7047892212867737
In grad_steps = 70, loss = 0.7220451235771179
In grad_steps = 71, loss = 0.6833614706993103
In grad_steps = 72, loss = 0.7202672958374023
In grad_steps = 73, loss = 0.678287148475647
In grad_steps = 74, loss = 0.7202860713005066
Beginning epoch 2
In grad_steps = 75, loss = 0.6626220345497131
In grad_steps = 76, loss = 0.6745257377624512
In grad_steps = 77, loss = 0.6668511629104614
In grad_steps = 78, loss = 0.6980553865432739
In grad_steps = 79, loss = 0.7189796566963196
In grad_steps = 80, loss = 0.7115645408630371
In grad_steps = 81, loss = 0.6763966083526611
In grad_steps = 82, loss = 0.7067446708679199
In grad_steps = 83, loss = 0.6814112067222595
In grad_steps = 84, loss = 0.6807496547698975
In grad_steps = 85, loss = 0.7243326902389526
In grad_steps = 86, loss = 0.7265690565109253
In grad_steps = 87, loss = 0.6673224568367004
In grad_steps = 88, loss = 0.718294620513916
In grad_steps = 89, loss = 0.7058035135269165
In grad_steps = 90, loss = 0.685969889163971
In grad_steps = 91, loss = 0.6967050433158875
In grad_steps = 92, loss = 0.682288646697998
In grad_steps = 93, loss = 0.6997073292732239
In grad_steps = 94, loss = 0.6950823068618774
In grad_steps = 95, loss = 0.6870181560516357
In grad_steps = 96, loss = 0.6834173798561096
In grad_steps = 97, loss = 0.6891754269599915
In grad_steps = 98, loss = 0.6994587182998657
In grad_steps = 99, loss = 0.6813461780548096
In grad_steps = 100, loss = 0.6860899925231934
In grad_steps = 101, loss = 0.6925650835037231
In grad_steps = 102, loss = 0.6865440011024475
In grad_steps = 103, loss = 0.664061427116394
In grad_steps = 104, loss = 0.6085138320922852
In grad_steps = 105, loss = 0.7121394276618958
In grad_steps = 106, loss = 0.6786819696426392
In grad_steps = 107, loss = 0.7740083336830139
In grad_steps = 108, loss = 0.8751618266105652
In grad_steps = 109, loss = 0.6514739990234375
In grad_steps = 110, loss = 0.7835819125175476
In grad_steps = 111, loss = 0.7478404641151428
In grad_steps = 112, loss = 0.6727233529090881
In grad_steps = 113, loss = 0.6890429854393005
In grad_steps = 114, loss = 0.6812958717346191
In grad_steps = 115, loss = 0.6676138043403625
In grad_steps = 116, loss = 0.6897854804992676
In grad_steps = 117, loss = 0.664332926273346
In grad_steps = 118, loss = 0.721433699131012
In grad_steps = 119, loss = 0.6547403931617737
In grad_steps = 120, loss = 0.7397313117980957
In grad_steps = 121, loss = 0.8068445920944214
In grad_steps = 122, loss = 0.6311513781547546
In grad_steps = 123, loss = 0.722267210483551
In grad_steps = 124, loss = 0.6619001626968384
In grad_steps = 125, loss = 0.6999865770339966
In grad_steps = 126, loss = 0.7244684100151062
In grad_steps = 127, loss = 0.693076491355896
In grad_steps = 128, loss = 0.7138413190841675
In grad_steps = 129, loss = 0.726653516292572
In grad_steps = 130, loss = 0.6492180228233337
In grad_steps = 131, loss = 0.6882116198539734
In grad_steps = 132, loss = 0.700505793094635
In grad_steps = 133, loss = 0.6763566136360168
In grad_steps = 134, loss = 0.7105798721313477
In grad_steps = 135, loss = 0.7174206972122192
In grad_steps = 136, loss = 0.6794352531433105
In grad_steps = 137, loss = 0.7005715370178223
In grad_steps = 138, loss = 0.6973255276679993
In grad_steps = 139, loss = 0.6903265714645386
In grad_steps = 140, loss = 0.6708870530128479
In grad_steps = 141, loss = 0.654830813407898
In grad_steps = 142, loss = 0.7426320910453796
In grad_steps = 143, loss = 0.7052129507064819
In grad_steps = 144, loss = 0.7087769508361816
In grad_steps = 145, loss = 0.7172648906707764
In grad_steps = 146, loss = 0.6622378826141357
In grad_steps = 147, loss = 0.7163122892379761
In grad_steps = 148, loss = 0.6769880652427673
In grad_steps = 149, loss = 0.7034281492233276
Beginning epoch 3
In grad_steps = 150, loss = 0.6557809114456177
In grad_steps = 151, loss = 0.6736665964126587
In grad_steps = 152, loss = 0.6665348410606384
In grad_steps = 153, loss = 0.691334068775177
In grad_steps = 154, loss = 0.7067593932151794
In grad_steps = 155, loss = 0.7055350542068481
In grad_steps = 156, loss = 0.676826000213623
In grad_steps = 157, loss = 0.7022259831428528
In grad_steps = 158, loss = 0.6794891953468323
In grad_steps = 159, loss = 0.6736177802085876
In grad_steps = 160, loss = 0.717444658279419
In grad_steps = 161, loss = 0.7255492806434631
In grad_steps = 162, loss = 0.6631126403808594
In grad_steps = 163, loss = 0.7114905714988708
In grad_steps = 164, loss = 0.7005447745323181
In grad_steps = 165, loss = 0.6873849034309387
In grad_steps = 166, loss = 0.6927310824394226
In grad_steps = 167, loss = 0.6672866344451904
In grad_steps = 168, loss = 0.6891905069351196
In grad_steps = 169, loss = 0.6861484050750732
In grad_steps = 170, loss = 0.6804192066192627
In grad_steps = 171, loss = 0.6762771010398865
In grad_steps = 172, loss = 0.6794446706771851
In grad_steps = 173, loss = 0.6889923810958862
In grad_steps = 174, loss = 0.6812925934791565
In grad_steps = 175, loss = 0.6694461107254028
In grad_steps = 176, loss = 0.6840784549713135
In grad_steps = 177, loss = 0.6887562274932861
In grad_steps = 178, loss = 0.645933210849762
In grad_steps = 179, loss = 0.6059266328811646
In grad_steps = 180, loss = 0.7102156281471252
In grad_steps = 181, loss = 0.6648023724555969
In grad_steps = 182, loss = 0.8143181204795837
In grad_steps = 183, loss = 0.8896158933639526
In grad_steps = 184, loss = 0.6383747458457947
In grad_steps = 185, loss = 0.7277055382728577
In grad_steps = 186, loss = 0.6986681818962097
In grad_steps = 187, loss = 0.654104471206665
In grad_steps = 188, loss = 0.7207334041595459
In grad_steps = 189, loss = 0.6671293377876282
In grad_steps = 190, loss = 0.6587294936180115
In grad_steps = 191, loss = 0.6868388652801514
In grad_steps = 192, loss = 0.6630554795265198
In grad_steps = 193, loss = 0.710317850112915
In grad_steps = 194, loss = 0.6596300601959229
In grad_steps = 195, loss = 0.723943293094635
In grad_steps = 196, loss = 0.7421311736106873
In grad_steps = 197, loss = 0.6587287783622742
In grad_steps = 198, loss = 0.6917389631271362
In grad_steps = 199, loss = 0.7025631666183472
In grad_steps = 200, loss = 0.6740157008171082
In grad_steps = 201, loss = 0.6623503565788269
In grad_steps = 202, loss = 0.6989298462867737
In grad_steps = 203, loss = 0.7123863101005554
In grad_steps = 204, loss = 0.7200415134429932
In grad_steps = 205, loss = 0.6677418947219849
In grad_steps = 206, loss = 0.674350917339325
In grad_steps = 207, loss = 0.6836506724357605
In grad_steps = 208, loss = 0.6782689690589905
In grad_steps = 209, loss = 0.6825426816940308
In grad_steps = 210, loss = 0.6974040269851685
In grad_steps = 211, loss = 0.6816825866699219
In grad_steps = 212, loss = 0.6880596280097961
In grad_steps = 213, loss = 0.6993378400802612
In grad_steps = 214, loss = 0.681255042552948
In grad_steps = 215, loss = 0.6876422762870789
In grad_steps = 216, loss = 0.6750510334968567
In grad_steps = 217, loss = 0.7131814360618591
In grad_steps = 218, loss = 0.688517689704895
In grad_steps = 219, loss = 0.6948724389076233
In grad_steps = 220, loss = 0.7122694253921509
In grad_steps = 221, loss = 0.6381932497024536
In grad_steps = 222, loss = 0.703101396560669
In grad_steps = 223, loss = 0.6690800786018372
In grad_steps = 224, loss = 0.6980894207954407
Beginning epoch 4
In grad_steps = 225, loss = 0.6311433911323547
In grad_steps = 226, loss = 0.6695191264152527
In grad_steps = 227, loss = 0.6645223498344421
In grad_steps = 228, loss = 0.6752956509590149
In grad_steps = 229, loss = 0.7033971548080444
In grad_steps = 230, loss = 0.6915571093559265
In grad_steps = 231, loss = 0.6690860986709595
In grad_steps = 232, loss = 0.7448718547821045
In grad_steps = 233, loss = 0.6556630730628967
In grad_steps = 234, loss = 0.6517642736434937
In grad_steps = 235, loss = 0.7370268106460571
In grad_steps = 236, loss = 0.7375243306159973
In grad_steps = 237, loss = 0.6599275469779968
In grad_steps = 238, loss = 0.6488058567047119
In grad_steps = 239, loss = 0.6757627129554749
In grad_steps = 240, loss = 0.6677727699279785
In grad_steps = 241, loss = 0.680516242980957
In grad_steps = 242, loss = 0.6184059381484985
In grad_steps = 243, loss = 0.7289647459983826
In grad_steps = 244, loss = 0.6901534795761108
In grad_steps = 245, loss = 0.6520671844482422
In grad_steps = 246, loss = 0.6664079427719116
In grad_steps = 247, loss = 0.6507646441459656
In grad_steps = 248, loss = 0.6489749550819397
In grad_steps = 249, loss = 0.6474036574363708
In grad_steps = 250, loss = 0.6157750487327576
In grad_steps = 251, loss = 0.6484395265579224
In grad_steps = 252, loss = 0.6000902652740479
In grad_steps = 253, loss = 0.5738588571548462
In grad_steps = 254, loss = 0.5612391829490662
In grad_steps = 255, loss = 0.5916463732719421
In grad_steps = 256, loss = 0.5418500304222107
In grad_steps = 257, loss = 0.7276771068572998
In grad_steps = 258, loss = 0.513676643371582
In grad_steps = 259, loss = 0.8113773465156555
In grad_steps = 260, loss = 0.5799162983894348
In grad_steps = 261, loss = 0.6436960101127625
In grad_steps = 262, loss = 0.5881036520004272
In grad_steps = 263, loss = 0.5407689213752747
In grad_steps = 264, loss = 0.6851799488067627
In grad_steps = 265, loss = 0.6735445261001587
In grad_steps = 266, loss = 0.6268308162689209
In grad_steps = 267, loss = 0.6598630547523499
In grad_steps = 268, loss = 0.6524455547332764
In grad_steps = 269, loss = 0.6618855595588684
In grad_steps = 270, loss = 0.8098317384719849
In grad_steps = 271, loss = 0.8420337438583374
In grad_steps = 272, loss = 0.5836084485054016
In grad_steps = 273, loss = 0.6735786199569702
In grad_steps = 274, loss = 0.611485481262207
In grad_steps = 275, loss = 0.6517131328582764
In grad_steps = 276, loss = 0.7012845873832703
In grad_steps = 277, loss = 0.6211621165275574
In grad_steps = 278, loss = 0.653269350528717
In grad_steps = 279, loss = 0.7461541295051575
In grad_steps = 280, loss = 0.5507783889770508
In grad_steps = 281, loss = 0.6289189457893372
In grad_steps = 282, loss = 0.643696665763855
In grad_steps = 283, loss = 0.6481884717941284
In grad_steps = 284, loss = 0.6317395567893982
In grad_steps = 285, loss = 0.6817806959152222
In grad_steps = 286, loss = 0.6048664450645447
In grad_steps = 287, loss = 0.7199872732162476
In grad_steps = 288, loss = 0.7116159796714783
In grad_steps = 289, loss = 0.6276857852935791
In grad_steps = 290, loss = 0.6049243211746216
In grad_steps = 291, loss = 0.6662310361862183
In grad_steps = 292, loss = 0.712836742401123
In grad_steps = 293, loss = 0.6699197292327881
In grad_steps = 294, loss = 0.6236011981964111
In grad_steps = 295, loss = 0.7191156148910522
In grad_steps = 296, loss = 0.5531956553459167
In grad_steps = 297, loss = 0.5990336537361145
In grad_steps = 298, loss = 0.7123541235923767
In grad_steps = 299, loss = 0.6617454886436462
Beginning epoch 5
In grad_steps = 300, loss = 0.5301458239555359
In grad_steps = 301, loss = 0.5595864057540894
In grad_steps = 302, loss = 0.6165682077407837
In grad_steps = 303, loss = 0.6206567883491516
In grad_steps = 304, loss = 0.7759020924568176
In grad_steps = 305, loss = 0.7161882519721985
In grad_steps = 306, loss = 0.5948110818862915
In grad_steps = 307, loss = 0.7363524436950684
In grad_steps = 308, loss = 0.5814616084098816
In grad_steps = 309, loss = 0.4349101185798645
In grad_steps = 310, loss = 0.6721207499504089
In grad_steps = 311, loss = 0.6688216328620911
In grad_steps = 312, loss = 0.5323185920715332
In grad_steps = 313, loss = 0.5414630770683289
In grad_steps = 314, loss = 0.539112389087677
In grad_steps = 315, loss = 0.6665387749671936
In grad_steps = 316, loss = 0.6821166276931763
In grad_steps = 317, loss = 0.4467626214027405
In grad_steps = 318, loss = 0.5609396696090698
In grad_steps = 319, loss = 0.5180633068084717
In grad_steps = 320, loss = 0.4819314181804657
In grad_steps = 321, loss = 0.637530505657196
In grad_steps = 322, loss = 0.5550206899642944
In grad_steps = 323, loss = 0.591057538986206
In grad_steps = 324, loss = 0.605360209941864
In grad_steps = 325, loss = 0.41970616579055786
In grad_steps = 326, loss = 0.48398417234420776
In grad_steps = 327, loss = 0.4946802258491516
In grad_steps = 328, loss = 0.3788002133369446
In grad_steps = 329, loss = 0.40912073850631714
In grad_steps = 330, loss = 0.3849647343158722
In grad_steps = 331, loss = 0.2792981266975403
In grad_steps = 332, loss = 0.20185422897338867
In grad_steps = 333, loss = 0.39156991243362427
In grad_steps = 334, loss = 0.14199690520763397
In grad_steps = 335, loss = 0.16660213470458984
In grad_steps = 336, loss = 0.562691330909729
In grad_steps = 337, loss = 0.7387160062789917
In grad_steps = 338, loss = 0.8131294250488281
In grad_steps = 339, loss = 0.4005641043186188
In grad_steps = 340, loss = 0.6626347899436951
In grad_steps = 341, loss = 0.6261389255523682
In grad_steps = 342, loss = 0.46385374665260315
In grad_steps = 343, loss = 0.5200454592704773
In grad_steps = 344, loss = 0.672694206237793
In grad_steps = 345, loss = 0.7708137631416321
In grad_steps = 346, loss = 0.7881316542625427
In grad_steps = 347, loss = 0.5482479929924011
In grad_steps = 348, loss = 0.7008038759231567
In grad_steps = 349, loss = 0.5733764171600342
In grad_steps = 350, loss = 0.6502829790115356
In grad_steps = 351, loss = 0.6607062816619873
In grad_steps = 352, loss = 0.6806591749191284
In grad_steps = 353, loss = 0.7405911684036255
In grad_steps = 354, loss = 0.7330642342567444
In grad_steps = 355, loss = 0.5214229822158813
In grad_steps = 356, loss = 0.5823726654052734
In grad_steps = 357, loss = 0.5873705148696899
In grad_steps = 358, loss = 0.5976252555847168
In grad_steps = 359, loss = 0.5219120979309082
In grad_steps = 360, loss = 0.5533758401870728
In grad_steps = 361, loss = 0.5659163594245911
In grad_steps = 362, loss = 0.44124636054039
In grad_steps = 363, loss = 0.4857073724269867
In grad_steps = 364, loss = 0.4877842366695404
In grad_steps = 365, loss = 0.4247463047504425
In grad_steps = 366, loss = 0.45268896222114563
In grad_steps = 367, loss = 1.025125503540039
In grad_steps = 368, loss = 0.39283668994903564
In grad_steps = 369, loss = 0.39841651916503906
In grad_steps = 370, loss = 0.6423937082290649
In grad_steps = 371, loss = 0.49907535314559937
In grad_steps = 372, loss = 0.3723132312297821
In grad_steps = 373, loss = 0.5138377547264099
In grad_steps = 374, loss = 0.49040624499320984
Beginning epoch 6
In grad_steps = 375, loss = 0.34520524740219116
In grad_steps = 376, loss = 0.4889132082462311
In grad_steps = 377, loss = 0.5457423329353333
In grad_steps = 378, loss = 0.4812292456626892
In grad_steps = 379, loss = 0.5404195189476013
In grad_steps = 380, loss = 0.35766178369522095
In grad_steps = 381, loss = 0.3429926037788391
In grad_steps = 382, loss = 0.43560805916786194
In grad_steps = 383, loss = 0.23840776085853577
In grad_steps = 384, loss = 0.38237109780311584
In grad_steps = 385, loss = 0.4074137806892395
In grad_steps = 386, loss = 0.40542298555374146
In grad_steps = 387, loss = 0.19884593784809113
In grad_steps = 388, loss = 0.2740691006183624
In grad_steps = 389, loss = 0.22876642644405365
In grad_steps = 390, loss = 0.40703508257865906
In grad_steps = 391, loss = 0.41528525948524475
In grad_steps = 392, loss = 0.06851160526275635
In grad_steps = 393, loss = 0.25587230920791626
In grad_steps = 394, loss = 0.27529454231262207
In grad_steps = 395, loss = 0.10621900111436844
In grad_steps = 396, loss = 0.09167198091745377
In grad_steps = 397, loss = 0.2533993124961853
In grad_steps = 398, loss = 0.05551629886031151
In grad_steps = 399, loss = 0.03984428942203522
In grad_steps = 400, loss = 0.24789169430732727
In grad_steps = 401, loss = 0.42195653915405273
In grad_steps = 402, loss = 0.623511791229248
In grad_steps = 403, loss = 0.3007076680660248
In grad_steps = 404, loss = 0.34725335240364075
In grad_steps = 405, loss = 0.26437199115753174
In grad_steps = 406, loss = 0.1640196144580841
In grad_steps = 407, loss = 0.6642358899116516
In grad_steps = 408, loss = 0.7325745224952698
In grad_steps = 409, loss = 0.26818209886550903
In grad_steps = 410, loss = 0.3646481931209564
In grad_steps = 411, loss = 0.3111315965652466
In grad_steps = 412, loss = 0.3727588653564453
In grad_steps = 413, loss = 0.6271190047264099
In grad_steps = 414, loss = 0.48937487602233887
In grad_steps = 415, loss = 0.39685574173927307
In grad_steps = 416, loss = 0.3944815397262573
In grad_steps = 417, loss = 0.3131944239139557
In grad_steps = 418, loss = 0.47086092829704285
In grad_steps = 419, loss = 0.5044752955436707
In grad_steps = 420, loss = 0.9798456430435181
In grad_steps = 421, loss = 0.7006673812866211
In grad_steps = 422, loss = 0.7452592849731445
In grad_steps = 423, loss = 0.5326030850410461
In grad_steps = 424, loss = 0.4256686568260193
In grad_steps = 425, loss = 0.550405740737915
In grad_steps = 426, loss = 0.9523717761039734
In grad_steps = 427, loss = 0.4368664622306824
In grad_steps = 428, loss = 0.532760739326477
In grad_steps = 429, loss = 0.6112368106842041
In grad_steps = 430, loss = 0.5643545985221863
In grad_steps = 431, loss = 0.44384169578552246
In grad_steps = 432, loss = 0.45625340938568115
In grad_steps = 433, loss = 0.5909513831138611
In grad_steps = 434, loss = 0.4262830913066864
In grad_steps = 435, loss = 0.431766539812088
In grad_steps = 436, loss = 0.3674135208129883
In grad_steps = 437, loss = 0.46039122343063354
In grad_steps = 438, loss = 0.3121587336063385
In grad_steps = 439, loss = 0.3682801127433777
In grad_steps = 440, loss = 0.3464096784591675
In grad_steps = 441, loss = 0.39272570610046387
In grad_steps = 442, loss = 0.20695306360721588
In grad_steps = 443, loss = 0.1882501244544983
In grad_steps = 444, loss = 0.22390766441822052
In grad_steps = 445, loss = 0.5906009078025818
In grad_steps = 446, loss = 0.15436755120754242
In grad_steps = 447, loss = 0.46277159452438354
In grad_steps = 448, loss = 0.15889808535575867
In grad_steps = 449, loss = 0.18329577147960663
Beginning epoch 7
In grad_steps = 450, loss = 0.07662587612867355
In grad_steps = 451, loss = 0.3727141320705414
In grad_steps = 452, loss = 0.27031856775283813
In grad_steps = 453, loss = 0.44985198974609375
In grad_steps = 454, loss = 0.6607387661933899
In grad_steps = 455, loss = 0.09940700978040695
In grad_steps = 456, loss = 0.1481025218963623
In grad_steps = 457, loss = 0.21218609809875488
In grad_steps = 458, loss = 0.28706249594688416
In grad_steps = 459, loss = 0.4046430289745331
In grad_steps = 460, loss = 0.13397379219532013
In grad_steps = 461, loss = 0.2416234016418457
In grad_steps = 462, loss = 0.06997368484735489
In grad_steps = 463, loss = 0.08507303148508072
In grad_steps = 464, loss = 0.36548304557800293
In grad_steps = 465, loss = 0.6036310195922852
In grad_steps = 466, loss = 0.22295524179935455
In grad_steps = 467, loss = 0.2443665862083435
In grad_steps = 468, loss = 0.35277462005615234
In grad_steps = 469, loss = 0.3796031177043915
In grad_steps = 470, loss = 0.16977016627788544
In grad_steps = 471, loss = 0.26596227288246155
In grad_steps = 472, loss = 0.4930945932865143
In grad_steps = 473, loss = 0.21142885088920593
In grad_steps = 474, loss = 0.15040090680122375
In grad_steps = 475, loss = 0.07537473738193512
In grad_steps = 476, loss = 0.08013331890106201
In grad_steps = 477, loss = 0.07893984019756317
In grad_steps = 478, loss = 0.19754192233085632
In grad_steps = 479, loss = 0.2808544337749481
In grad_steps = 480, loss = 0.10323517769575119
In grad_steps = 481, loss = 0.05614974722266197
In grad_steps = 482, loss = 0.08334282785654068
In grad_steps = 483, loss = 0.37387436628341675
In grad_steps = 484, loss = 0.031840380281209946
In grad_steps = 485, loss = 0.29008960723876953
In grad_steps = 486, loss = 0.2484837770462036
In grad_steps = 487, loss = 0.093504399061203
In grad_steps = 488, loss = 0.35194849967956543
In grad_steps = 489, loss = 0.6752822399139404
In grad_steps = 490, loss = 0.21343345940113068
In grad_steps = 491, loss = 0.11502629518508911
In grad_steps = 492, loss = 0.19074679911136627
In grad_steps = 493, loss = 0.3018606901168823
In grad_steps = 494, loss = 0.815390408039093
In grad_steps = 495, loss = 0.7634098529815674
In grad_steps = 496, loss = 0.6401436924934387
In grad_steps = 497, loss = 0.7894030809402466
In grad_steps = 498, loss = 0.6587963700294495
In grad_steps = 499, loss = 0.20569053292274475
In grad_steps = 500, loss = 0.3588619530200958
In grad_steps = 501, loss = 0.43068641424179077
In grad_steps = 502, loss = 0.25230780243873596
In grad_steps = 503, loss = 0.2920353412628174
In grad_steps = 504, loss = 0.3193983733654022
In grad_steps = 505, loss = 0.4569053649902344
In grad_steps = 506, loss = 0.3520732522010803
In grad_steps = 507, loss = 0.41785892844200134
In grad_steps = 508, loss = 0.5322024822235107
In grad_steps = 509, loss = 0.23243795335292816
In grad_steps = 510, loss = 0.33255672454833984
In grad_steps = 511, loss = 0.2929312288761139
In grad_steps = 512, loss = 0.22328244149684906
In grad_steps = 513, loss = 0.18227636814117432
In grad_steps = 514, loss = 0.17995481193065643
In grad_steps = 515, loss = 0.292069673538208
In grad_steps = 516, loss = 0.2526882290840149
In grad_steps = 517, loss = 0.08706054836511612
In grad_steps = 518, loss = 0.016207193955779076
In grad_steps = 519, loss = 0.2061776965856552
In grad_steps = 520, loss = 0.38468772172927856
In grad_steps = 521, loss = 0.047237999737262726
In grad_steps = 522, loss = 0.11327507346868515
In grad_steps = 523, loss = 0.20439498126506805
In grad_steps = 524, loss = 0.03695289418101311
Beginning epoch 8
In grad_steps = 525, loss = 0.010854135267436504
In grad_steps = 526, loss = 0.042001873254776
In grad_steps = 527, loss = 0.5861121416091919
In grad_steps = 528, loss = 0.11920911073684692
In grad_steps = 529, loss = 0.32116270065307617
In grad_steps = 530, loss = 0.201238214969635
In grad_steps = 531, loss = 0.3202366530895233
In grad_steps = 532, loss = 0.49024316668510437
In grad_steps = 533, loss = 0.264606237411499
In grad_steps = 534, loss = 0.098751001060009
In grad_steps = 535, loss = 0.18446604907512665
In grad_steps = 536, loss = 0.3792334794998169
In grad_steps = 537, loss = 0.20940829813480377
In grad_steps = 538, loss = 0.08797386288642883
In grad_steps = 539, loss = 0.13261140882968903
In grad_steps = 540, loss = 0.15945854783058167
In grad_steps = 541, loss = 0.1138877123594284
In grad_steps = 542, loss = 0.13121914863586426
In grad_steps = 543, loss = 0.16587769985198975
In grad_steps = 544, loss = 0.09175248444080353
In grad_steps = 545, loss = 0.36016467213630676
In grad_steps = 546, loss = 0.15063589811325073
In grad_steps = 547, loss = 0.19082708656787872
In grad_steps = 548, loss = 0.2099798172712326
In grad_steps = 549, loss = 0.13224828243255615
In grad_steps = 550, loss = 0.03388012945652008
In grad_steps = 551, loss = 0.021126221865415573
In grad_steps = 552, loss = 0.1264723390340805
In grad_steps = 553, loss = 0.04595475643873215
In grad_steps = 554, loss = 0.16911140084266663
In grad_steps = 555, loss = 0.02009938284754753
In grad_steps = 556, loss = 0.059399623423814774
In grad_steps = 557, loss = 0.06780814379453659
In grad_steps = 558, loss = 0.27393409609794617
In grad_steps = 559, loss = 0.01970227248966694
In grad_steps = 560, loss = 0.17106924951076508
In grad_steps = 561, loss = 0.24133940041065216
In grad_steps = 562, loss = 0.16919007897377014
In grad_steps = 563, loss = 0.16252478957176208
In grad_steps = 564, loss = 0.0180205125361681
In grad_steps = 565, loss = 0.005314268637448549
In grad_steps = 566, loss = 0.20487192273139954
In grad_steps = 567, loss = 0.4970915913581848
In grad_steps = 568, loss = 0.3852273225784302
In grad_steps = 569, loss = 0.2388848066329956
In grad_steps = 570, loss = 0.391043096780777
In grad_steps = 571, loss = 0.06712494790554047
In grad_steps = 572, loss = 0.5001708269119263
In grad_steps = 573, loss = 0.2729742228984833
In grad_steps = 574, loss = 0.39413562417030334
In grad_steps = 575, loss = 0.4555644392967224
In grad_steps = 576, loss = 0.1427818387746811
In grad_steps = 577, loss = 0.19427022337913513
In grad_steps = 578, loss = 0.24993182718753815
In grad_steps = 579, loss = 0.36742615699768066
In grad_steps = 580, loss = 0.33666154742240906
In grad_steps = 581, loss = 0.19750496745109558
In grad_steps = 582, loss = 0.1674659550189972
In grad_steps = 583, loss = 0.8103399276733398
In grad_steps = 584, loss = 0.12118328362703323
In grad_steps = 585, loss = 0.3504163920879364
In grad_steps = 586, loss = 0.37953871488571167
In grad_steps = 587, loss = 0.10228142887353897
In grad_steps = 588, loss = 0.10937175899744034
In grad_steps = 589, loss = 0.09346207976341248
In grad_steps = 590, loss = 0.14302071928977966
In grad_steps = 591, loss = 0.20042437314987183
In grad_steps = 592, loss = 0.18970851600170135
In grad_steps = 593, loss = 0.15535539388656616
In grad_steps = 594, loss = 0.42032861709594727
In grad_steps = 595, loss = 0.17363691329956055
In grad_steps = 596, loss = 0.0703531801700592
In grad_steps = 597, loss = 0.06796535849571228
In grad_steps = 598, loss = 0.04689034819602966
In grad_steps = 599, loss = 0.324556827545166
Beginning epoch 9
In grad_steps = 600, loss = 0.41388949751853943
In grad_steps = 601, loss = 0.16371256113052368
In grad_steps = 602, loss = 0.28266575932502747
In grad_steps = 603, loss = 0.12033389508724213
In grad_steps = 604, loss = 0.22514469921588898
In grad_steps = 605, loss = 0.08961094170808792
In grad_steps = 606, loss = 0.06316075474023819
In grad_steps = 607, loss = 0.13156116008758545
In grad_steps = 608, loss = 0.14994214475154877
In grad_steps = 609, loss = 0.2588726878166199
In grad_steps = 610, loss = 0.08031763881444931
In grad_steps = 611, loss = 0.2038034200668335
In grad_steps = 612, loss = 0.055771131068468094
In grad_steps = 613, loss = 0.04200619086623192
In grad_steps = 614, loss = 0.04967562481760979
In grad_steps = 615, loss = 0.07169274240732193
In grad_steps = 616, loss = 0.3658188581466675
In grad_steps = 617, loss = 0.025037625804543495
In grad_steps = 618, loss = 0.34093043208122253
In grad_steps = 619, loss = 0.018496986478567123
In grad_steps = 620, loss = 0.2558039128780365
In grad_steps = 621, loss = 0.04117433354258537
In grad_steps = 622, loss = 0.03914337605237961
In grad_steps = 623, loss = 0.08397437632083893
In grad_steps = 624, loss = 0.022257475182414055
In grad_steps = 625, loss = 0.02767554670572281
In grad_steps = 626, loss = 0.010902057401835918
In grad_steps = 627, loss = 0.055200789123773575
In grad_steps = 628, loss = 0.03501106798648834
In grad_steps = 629, loss = 0.04058116301894188
In grad_steps = 630, loss = 0.019882243126630783
In grad_steps = 631, loss = 0.00720205856487155
In grad_steps = 632, loss = 0.023031050339341164
In grad_steps = 633, loss = 0.1610872894525528
In grad_steps = 634, loss = 0.006806110963225365
In grad_steps = 635, loss = 0.3263683319091797
In grad_steps = 636, loss = 0.003863019635900855
In grad_steps = 637, loss = 0.018831612542271614
In grad_steps = 638, loss = 0.04402855783700943
In grad_steps = 639, loss = 0.01696157455444336
In grad_steps = 640, loss = 0.023678725585341454
In grad_steps = 641, loss = 0.0296481903642416
In grad_steps = 642, loss = 0.00708512170240283
In grad_steps = 643, loss = 0.005074923392385244
In grad_steps = 644, loss = 0.20225363969802856
In grad_steps = 645, loss = 0.013733342289924622
In grad_steps = 646, loss = 0.05104369297623634
In grad_steps = 647, loss = 0.015020973049104214
In grad_steps = 648, loss = 0.09721902757883072
In grad_steps = 649, loss = 0.17304402589797974
In grad_steps = 650, loss = 0.05826200544834137
In grad_steps = 651, loss = 0.01758800446987152
In grad_steps = 652, loss = 0.0019515001913532615
In grad_steps = 653, loss = 0.31083837151527405
In grad_steps = 654, loss = 0.06800590455532074
In grad_steps = 655, loss = 0.005795298144221306
In grad_steps = 656, loss = 0.09462694078683853
In grad_steps = 657, loss = 0.011536216363310814
In grad_steps = 658, loss = 0.3828289210796356
In grad_steps = 659, loss = 0.12150329351425171
In grad_steps = 660, loss = 0.0128535907715559
In grad_steps = 661, loss = 0.15484021604061127
In grad_steps = 662, loss = 0.07112818211317062
In grad_steps = 663, loss = 0.012699796818196774
In grad_steps = 664, loss = 0.029671115800738335
In grad_steps = 665, loss = 0.02085319720208645
In grad_steps = 666, loss = 0.02958587184548378
In grad_steps = 667, loss = 0.013869200833141804
In grad_steps = 668, loss = 0.047159142792224884
In grad_steps = 669, loss = 0.013357948511838913
In grad_steps = 670, loss = 0.021229229867458344
In grad_steps = 671, loss = 0.02151312492787838
In grad_steps = 672, loss = 0.02476835809648037
In grad_steps = 673, loss = 0.07255733013153076
In grad_steps = 674, loss = 0.003343511139973998
Beginning epoch 10
In grad_steps = 675, loss = 0.01730138622224331
In grad_steps = 676, loss = 0.0074799251742661
In grad_steps = 677, loss = 0.004740259610116482
In grad_steps = 678, loss = 0.05236508324742317
In grad_steps = 679, loss = 0.0923968032002449
In grad_steps = 680, loss = 0.005129592958837748
In grad_steps = 681, loss = 0.004696625284850597
In grad_steps = 682, loss = 0.0033005811274051666
In grad_steps = 683, loss = 0.005981309339404106
In grad_steps = 684, loss = 0.0017833622405305505
In grad_steps = 685, loss = 0.0012761065736413002
In grad_steps = 686, loss = 0.07082173973321915
In grad_steps = 687, loss = 0.03514141961932182
In grad_steps = 688, loss = 0.0005361742805689573
In grad_steps = 689, loss = 0.24118450284004211
In grad_steps = 690, loss = 0.0029096603393554688
In grad_steps = 691, loss = 0.02343965321779251
In grad_steps = 692, loss = 0.002339610829949379
In grad_steps = 693, loss = 0.05166309326887131
In grad_steps = 694, loss = 0.0029693073593080044
In grad_steps = 695, loss = 0.32870927453041077
In grad_steps = 696, loss = 0.014415877871215343
In grad_steps = 697, loss = 0.004902621265500784
In grad_steps = 698, loss = 0.14371664822101593
In grad_steps = 699, loss = 0.07417936623096466
In grad_steps = 700, loss = 0.09677856415510178
In grad_steps = 701, loss = 0.02226998470723629
In grad_steps = 702, loss = 0.005380338057875633
In grad_steps = 703, loss = 0.0027312366291880608
In grad_steps = 704, loss = 0.4697036147117615
In grad_steps = 705, loss = 0.009845543652772903
In grad_steps = 706, loss = 0.010899929329752922
In grad_steps = 707, loss = 0.011945226229727268
In grad_steps = 708, loss = 0.07068000733852386
In grad_steps = 709, loss = 0.010354685597121716
In grad_steps = 710, loss = 0.03513583168387413
In grad_steps = 711, loss = 0.014457086101174355
In grad_steps = 712, loss = 0.04680324345827103
In grad_steps = 713, loss = 0.03148360550403595
In grad_steps = 714, loss = 0.24700433015823364
In grad_steps = 715, loss = 0.1152237057685852
In grad_steps = 716, loss = 0.03539295494556427
In grad_steps = 717, loss = 0.06649936735630035
In grad_steps = 718, loss = 0.02185904048383236
In grad_steps = 719, loss = 0.2678880989551544
In grad_steps = 720, loss = 0.10384374856948853
In grad_steps = 721, loss = 0.35661330819129944
In grad_steps = 722, loss = 0.028494078665971756
In grad_steps = 723, loss = 0.05985559523105621
In grad_steps = 724, loss = 0.01909228414297104
In grad_steps = 725, loss = 0.3126232326030731
In grad_steps = 726, loss = 0.09243666380643845
In grad_steps = 727, loss = 0.027289651334285736
In grad_steps = 728, loss = 0.08253399282693863
In grad_steps = 729, loss = 0.10531236976385117
In grad_steps = 730, loss = 0.12675832211971283
In grad_steps = 731, loss = 0.031478650867938995
In grad_steps = 732, loss = 0.044327422976493835
In grad_steps = 733, loss = 0.3909819722175598
In grad_steps = 734, loss = 0.01841679960489273
In grad_steps = 735, loss = 0.013451553881168365
In grad_steps = 736, loss = 0.016458045691251755
In grad_steps = 737, loss = 0.18591587245464325
In grad_steps = 738, loss = 0.03388819843530655
In grad_steps = 739, loss = 0.04182146117091179
In grad_steps = 740, loss = 0.0072845895774662495
In grad_steps = 741, loss = 0.006398907862603664
In grad_steps = 742, loss = 0.014036055654287338
In grad_steps = 743, loss = 0.015231028199195862
In grad_steps = 744, loss = 0.014906140975654125
In grad_steps = 745, loss = 0.04409347474575043
In grad_steps = 746, loss = 0.11479510366916656
In grad_steps = 747, loss = 0.1331588178873062
In grad_steps = 748, loss = 0.06355158984661102
In grad_steps = 749, loss = 0.009083918295800686
Beginning epoch 11
In grad_steps = 750, loss = 0.008102363906800747
In grad_steps = 751, loss = 0.02974036894738674
In grad_steps = 752, loss = 0.002613538410514593
In grad_steps = 753, loss = 0.01933017000555992
In grad_steps = 754, loss = 0.24189914762973785
In grad_steps = 755, loss = 0.043181102722883224
In grad_steps = 756, loss = 0.0052231475710868835
In grad_steps = 757, loss = 0.003795269411057234
In grad_steps = 758, loss = 0.20379823446273804
In grad_steps = 759, loss = 0.012379027903079987
In grad_steps = 760, loss = 0.002886970294639468
In grad_steps = 761, loss = 0.158241406083107
In grad_steps = 762, loss = 0.003023653058335185
In grad_steps = 763, loss = 0.0021283344831317663
In grad_steps = 764, loss = 0.27161940932273865
In grad_steps = 765, loss = 0.02992011420428753
In grad_steps = 766, loss = 0.03442526236176491
In grad_steps = 767, loss = 0.376944899559021
In grad_steps = 768, loss = 0.022674240171909332
In grad_steps = 769, loss = 0.007275611627846956
In grad_steps = 770, loss = 0.05249596759676933
In grad_steps = 771, loss = 0.162314772605896
In grad_steps = 772, loss = 0.012414230965077877
In grad_steps = 773, loss = 0.039809152483940125
In grad_steps = 774, loss = 0.04611358046531677
In grad_steps = 775, loss = 0.014026465825736523
In grad_steps = 776, loss = 0.013184416107833385
In grad_steps = 777, loss = 0.005209098570048809
In grad_steps = 778, loss = 0.009737744927406311
In grad_steps = 779, loss = 0.13405337929725647
In grad_steps = 780, loss = 0.020730040967464447
In grad_steps = 781, loss = 0.044234760105609894
In grad_steps = 782, loss = 0.035669803619384766
In grad_steps = 783, loss = 0.008176259696483612
In grad_steps = 784, loss = 0.018184076994657516
In grad_steps = 785, loss = 0.0031190114095807076
In grad_steps = 786, loss = 0.01861451379954815
In grad_steps = 787, loss = 0.0028999631758779287
In grad_steps = 788, loss = 0.3194134533405304
In grad_steps = 789, loss = 0.0023905220441520214
In grad_steps = 790, loss = 0.020403580740094185
In grad_steps = 791, loss = 0.008043676614761353
In grad_steps = 792, loss = 0.0123903201892972
In grad_steps = 793, loss = 0.02571113035082817
In grad_steps = 794, loss = 0.041187841445207596
In grad_steps = 795, loss = 0.0037603245582431555
In grad_steps = 796, loss = 0.3679087162017822
In grad_steps = 797, loss = 0.024733593687415123
In grad_steps = 798, loss = 0.007225569803267717
In grad_steps = 799, loss = 0.06630510836839676
In grad_steps = 800, loss = 0.045146141201257706
In grad_steps = 801, loss = 0.00524115702137351
In grad_steps = 802, loss = 0.0023579918779432774
In grad_steps = 803, loss = 0.01828835904598236
In grad_steps = 804, loss = 0.018775463104248047
In grad_steps = 805, loss = 0.028478994965553284
In grad_steps = 806, loss = 0.0027168570086359978
In grad_steps = 807, loss = 0.026783116161823273
In grad_steps = 808, loss = 0.11297609657049179
In grad_steps = 809, loss = 0.007919604890048504
In grad_steps = 810, loss = 0.00486734276637435
In grad_steps = 811, loss = 0.03661694750189781
In grad_steps = 812, loss = 0.0039475285448133945
In grad_steps = 813, loss = 0.0030242500361055136
In grad_steps = 814, loss = 0.0021796640940010548
In grad_steps = 815, loss = 0.005394601728767157
In grad_steps = 816, loss = 0.00904901698231697
In grad_steps = 817, loss = 0.00606132484972477
In grad_steps = 818, loss = 0.17306219041347504
In grad_steps = 819, loss = 0.002797926776111126
In grad_steps = 820, loss = 0.10575500875711441
In grad_steps = 821, loss = 0.002237810753285885
In grad_steps = 822, loss = 0.004728433210402727
In grad_steps = 823, loss = 0.003439821768552065
In grad_steps = 824, loss = 0.001888655824586749
Beginning epoch 12
In grad_steps = 825, loss = 0.004808743949979544
In grad_steps = 826, loss = 0.006579205393791199
In grad_steps = 827, loss = 0.0015770556638017297
In grad_steps = 828, loss = 0.0238270815461874
In grad_steps = 829, loss = 0.06177742779254913
In grad_steps = 830, loss = 0.03992755711078644
In grad_steps = 831, loss = 0.008177547715604305
In grad_steps = 832, loss = 0.002543318783864379
In grad_steps = 833, loss = 0.004883124027401209
In grad_steps = 834, loss = 0.005964759737253189
In grad_steps = 835, loss = 0.04892908036708832
In grad_steps = 836, loss = 0.004841442219913006
In grad_steps = 837, loss = 0.001563689555041492
In grad_steps = 838, loss = 0.0005717439926229417
In grad_steps = 839, loss = 0.0012231344589963555
In grad_steps = 840, loss = 0.0015085921622812748
In grad_steps = 841, loss = 0.009937657974660397
In grad_steps = 842, loss = 0.0010935934260487556
In grad_steps = 843, loss = 0.017355816438794136
In grad_steps = 844, loss = 0.005955878645181656
In grad_steps = 845, loss = 0.04606713354587555
In grad_steps = 846, loss = 0.0018968230579048395
In grad_steps = 847, loss = 0.009241816587746143
In grad_steps = 848, loss = 0.0011143942829221487
In grad_steps = 849, loss = 0.0013497209874913096
In grad_steps = 850, loss = 0.002496255561709404
In grad_steps = 851, loss = 0.0034332226496189833
In grad_steps = 852, loss = 0.0005384655669331551
In grad_steps = 853, loss = 0.0949895903468132
In grad_steps = 854, loss = 0.002227783203125
In grad_steps = 855, loss = 0.0008761448552832007
In grad_steps = 856, loss = 0.00032939304946921766
In grad_steps = 857, loss = 0.0005973697989247739
In grad_steps = 858, loss = 0.0007648438913747668
In grad_steps = 859, loss = 0.0004965068656019866
In grad_steps = 860, loss = 0.0003733344201464206
In grad_steps = 861, loss = 0.236262246966362
In grad_steps = 862, loss = 0.0005997641710564494
In grad_steps = 863, loss = 0.15007218718528748
In grad_steps = 864, loss = 0.11175493896007538
In grad_steps = 865, loss = 0.00044753195834346116
In grad_steps = 866, loss = 0.0022482748609036207
In grad_steps = 867, loss = 0.0029456771444529295
In grad_steps = 868, loss = 0.0908447876572609
In grad_steps = 869, loss = 0.0065581356175243855
In grad_steps = 870, loss = 0.013647930696606636
In grad_steps = 871, loss = 0.004896456375718117
In grad_steps = 872, loss = 0.020937850698828697
In grad_steps = 873, loss = 0.006860534194856882
In grad_steps = 874, loss = 0.2308063805103302
In grad_steps = 875, loss = 0.1310490071773529
In grad_steps = 876, loss = 0.013711974024772644
In grad_steps = 877, loss = 0.05342292785644531
In grad_steps = 878, loss = 0.02779475972056389
In grad_steps = 879, loss = 0.003998948726803064
In grad_steps = 880, loss = 0.010396676138043404
In grad_steps = 881, loss = 0.007690015248954296
In grad_steps = 882, loss = 0.01288384385406971
In grad_steps = 883, loss = 0.024607684463262558
In grad_steps = 884, loss = 0.0366956889629364
In grad_steps = 885, loss = 0.009687729179859161
In grad_steps = 886, loss = 0.03485618904232979
In grad_steps = 887, loss = 0.0042686243541538715
In grad_steps = 888, loss = 0.005063941236585379
In grad_steps = 889, loss = 0.010063361376523972
In grad_steps = 890, loss = 0.012264002114534378
In grad_steps = 891, loss = 0.06061436980962753
In grad_steps = 892, loss = 0.0037634726613759995
In grad_steps = 893, loss = 0.012259220704436302
In grad_steps = 894, loss = 0.0027664785739034414
In grad_steps = 895, loss = 0.006861949805170298
In grad_steps = 896, loss = 0.0011624462204053998
In grad_steps = 897, loss = 0.006307126022875309
In grad_steps = 898, loss = 0.001686928910203278
In grad_steps = 899, loss = 0.001218959572724998
Elapsed time: 3015.4820008277893 seconds for ensemble 2 with 12 epochs
LoRA instance 2 evaluation complete. Data saved to /pscratch/sd/t/tianle/lucid/other_source/SURP_2024/results/tmp_lora_ensemble/Llama3-8B-set-1.2/test_data_instance_2_seed_20187.npz.
lora instance i = 2 Successfully finished.
Training lora instance 3
DEBUG: Total parameters: 8037076992
DEBUG: Trainable parameters: 6815744
Beginning epoch 1
In grad_steps = 0, loss = 1.014610767364502
In grad_steps = 1, loss = 0.7356508374214172
In grad_steps = 2, loss = 0.6776108741760254
In grad_steps = 3, loss = 0.6862396001815796
In grad_steps = 4, loss = 0.6345187425613403
In grad_steps = 5, loss = 0.6320177912712097
In grad_steps = 6, loss = 0.6491125226020813
In grad_steps = 7, loss = 0.9205631017684937
In grad_steps = 8, loss = 0.6777678728103638
In grad_steps = 9, loss = 0.6890043020248413
In grad_steps = 10, loss = 0.6923909783363342
In grad_steps = 11, loss = 0.696552574634552
In grad_steps = 12, loss = 0.7644407749176025
In grad_steps = 13, loss = 0.6988121867179871
In grad_steps = 14, loss = 0.7436891198158264
In grad_steps = 15, loss = 0.6963676810264587
In grad_steps = 16, loss = 0.7207061052322388
In grad_steps = 17, loss = 0.7147620320320129
In grad_steps = 18, loss = 0.7114667296409607
In grad_steps = 19, loss = 0.6993134021759033
In grad_steps = 20, loss = 0.7014747262001038
In grad_steps = 21, loss = 0.696092426776886
In grad_steps = 22, loss = 0.6992839574813843
In grad_steps = 23, loss = 0.6870960593223572
In grad_steps = 24, loss = 0.7075337171554565
In grad_steps = 25, loss = 0.6964250802993774
In grad_steps = 26, loss = 0.7138402462005615
In grad_steps = 27, loss = 0.688217282295227
In grad_steps = 28, loss = 0.6721044182777405
In grad_steps = 29, loss = 0.6455961465835571
In grad_steps = 30, loss = 0.709883987903595
In grad_steps = 31, loss = 0.6752854585647583
In grad_steps = 32, loss = 0.78170245885849
In grad_steps = 33, loss = 0.867594838142395
In grad_steps = 34, loss = 0.6564184427261353
In grad_steps = 35, loss = 0.7509528994560242
In grad_steps = 36, loss = 0.7023780941963196
In grad_steps = 37, loss = 0.6955188512802124
In grad_steps = 38, loss = 0.7467396259307861
In grad_steps = 39, loss = 0.6714988946914673
In grad_steps = 40, loss = 0.6670693755149841
In grad_steps = 41, loss = 0.6991351842880249
In grad_steps = 42, loss = 0.66594398021698
In grad_steps = 43, loss = 0.7305773496627808
In grad_steps = 44, loss = 0.6557486653327942
In grad_steps = 45, loss = 0.7303956747055054
In grad_steps = 46, loss = 0.7677475810050964
In grad_steps = 47, loss = 0.6554248332977295
In grad_steps = 48, loss = 0.7024449706077576
In grad_steps = 49, loss = 0.6989253163337708
In grad_steps = 50, loss = 0.6878665089607239
In grad_steps = 51, loss = 0.6750247478485107
In grad_steps = 52, loss = 0.7178698778152466
In grad_steps = 53, loss = 0.735705554485321
In grad_steps = 54, loss = 0.7357542514801025
In grad_steps = 55, loss = 0.6542772054672241
In grad_steps = 56, loss = 0.6915110349655151
In grad_steps = 57, loss = 0.7013156414031982
In grad_steps = 58, loss = 0.6808684468269348
In grad_steps = 59, loss = 0.7009048461914062
In grad_steps = 60, loss = 0.7062718272209167
In grad_steps = 61, loss = 0.6830586791038513
In grad_steps = 62, loss = 0.6994337439537048
In grad_steps = 63, loss = 0.698562502861023
In grad_steps = 64, loss = 0.6946013569831848
In grad_steps = 65, loss = 0.6823841333389282
In grad_steps = 66, loss = 0.6640684604644775
In grad_steps = 67, loss = 0.7288749814033508
In grad_steps = 68, loss = 0.6980314254760742
In grad_steps = 69, loss = 0.7062854766845703
In grad_steps = 70, loss = 0.7218751311302185
In grad_steps = 71, loss = 0.6787762641906738
In grad_steps = 72, loss = 0.722110390663147
In grad_steps = 73, loss = 0.6746762990951538
In grad_steps = 74, loss = 0.7166360020637512
Beginning epoch 2
In grad_steps = 75, loss = 0.6573861241340637
In grad_steps = 76, loss = 0.6741384267807007
In grad_steps = 77, loss = 0.6630754470825195
In grad_steps = 78, loss = 0.6992663145065308
In grad_steps = 79, loss = 0.7204093337059021
In grad_steps = 80, loss = 0.711747407913208
In grad_steps = 81, loss = 0.6764659881591797
In grad_steps = 82, loss = 0.7110756039619446
In grad_steps = 83, loss = 0.6823941469192505
In grad_steps = 84, loss = 0.6812227368354797
In grad_steps = 85, loss = 0.730344831943512
In grad_steps = 86, loss = 0.7352757453918457
In grad_steps = 87, loss = 0.6624435186386108
In grad_steps = 88, loss = 0.7199839949607849
In grad_steps = 89, loss = 0.7085931301116943
In grad_steps = 90, loss = 0.6900808811187744
In grad_steps = 91, loss = 0.6988877654075623
In grad_steps = 92, loss = 0.6820082664489746
In grad_steps = 93, loss = 0.692895233631134
In grad_steps = 94, loss = 0.6951894760131836
In grad_steps = 95, loss = 0.6867642402648926
In grad_steps = 96, loss = 0.6828926205635071
In grad_steps = 97, loss = 0.6897982954978943
In grad_steps = 98, loss = 0.6987712979316711
In grad_steps = 99, loss = 0.6893233060836792
In grad_steps = 100, loss = 0.6900243163108826
In grad_steps = 101, loss = 0.6991457343101501
In grad_steps = 102, loss = 0.6979162693023682
In grad_steps = 103, loss = 0.6691120266914368
In grad_steps = 104, loss = 0.6329664587974548
In grad_steps = 105, loss = 0.7057405710220337
In grad_steps = 106, loss = 0.6748521327972412
In grad_steps = 107, loss = 0.7637506723403931
In grad_steps = 108, loss = 0.861897349357605
In grad_steps = 109, loss = 0.6544939875602722
In grad_steps = 110, loss = 0.7855857014656067
In grad_steps = 111, loss = 0.7528116106987
In grad_steps = 112, loss = 0.6706944108009338
In grad_steps = 113, loss = 0.6755605936050415
In grad_steps = 114, loss = 0.6881656646728516
In grad_steps = 115, loss = 0.6740580797195435
In grad_steps = 116, loss = 0.6936419010162354
In grad_steps = 117, loss = 0.6697414517402649
In grad_steps = 118, loss = 0.7171774506568909
In grad_steps = 119, loss = 0.6536067724227905
In grad_steps = 120, loss = 0.7333794236183167
In grad_steps = 121, loss = 0.8020445108413696
In grad_steps = 122, loss = 0.6243262887001038
In grad_steps = 123, loss = 0.7246813178062439
In grad_steps = 124, loss = 0.652840256690979
In grad_steps = 125, loss = 0.7096720933914185
In grad_steps = 126, loss = 0.7503674626350403
In grad_steps = 127, loss = 0.678364634513855
In grad_steps = 128, loss = 0.7027987241744995
In grad_steps = 129, loss = 0.7128003239631653
In grad_steps = 130, loss = 0.66147780418396
In grad_steps = 131, loss = 0.6835108995437622
In grad_steps = 132, loss = 0.6995502710342407
In grad_steps = 133, loss = 0.6843816637992859
In grad_steps = 134, loss = 0.7153179049491882
In grad_steps = 135, loss = 0.7275450229644775
In grad_steps = 136, loss = 0.6782543063163757
In grad_steps = 137, loss = 0.7013370990753174
In grad_steps = 138, loss = 0.6876031756401062
In grad_steps = 139, loss = 0.6926456093788147
In grad_steps = 140, loss = 0.6836515665054321
In grad_steps = 141, loss = 0.6682005524635315
In grad_steps = 142, loss = 0.726701021194458
In grad_steps = 143, loss = 0.6990264654159546
In grad_steps = 144, loss = 0.7005566358566284
In grad_steps = 145, loss = 0.717187225818634
In grad_steps = 146, loss = 0.6684027314186096
In grad_steps = 147, loss = 0.7241207361221313
In grad_steps = 148, loss = 0.6701295375823975
In grad_steps = 149, loss = 0.7198256254196167
Beginning epoch 3
In grad_steps = 150, loss = 0.6456913352012634
In grad_steps = 151, loss = 0.6612394452095032
In grad_steps = 152, loss = 0.6539604663848877
In grad_steps = 153, loss = 0.705367922782898
In grad_steps = 154, loss = 0.7269328832626343
In grad_steps = 155, loss = 0.7234119176864624
In grad_steps = 156, loss = 0.690121054649353
In grad_steps = 157, loss = 0.69106525182724
In grad_steps = 158, loss = 0.6809426546096802
In grad_steps = 159, loss = 0.6716779470443726
In grad_steps = 160, loss = 0.7165561318397522
In grad_steps = 161, loss = 0.7260770201683044
In grad_steps = 162, loss = 0.6611259579658508
In grad_steps = 163, loss = 0.71611487865448
In grad_steps = 164, loss = 0.7063207030296326
In grad_steps = 165, loss = 0.6936854124069214
In grad_steps = 166, loss = 0.697351336479187
In grad_steps = 167, loss = 0.6701627969741821
In grad_steps = 168, loss = 0.6872107982635498
In grad_steps = 169, loss = 0.6838607788085938
In grad_steps = 170, loss = 0.6798242926597595
In grad_steps = 171, loss = 0.678482174873352
In grad_steps = 172, loss = 0.6816256046295166
In grad_steps = 173, loss = 0.6886518597602844
In grad_steps = 174, loss = 0.6777749061584473
In grad_steps = 175, loss = 0.671472430229187
In grad_steps = 176, loss = 0.6887432336807251
In grad_steps = 177, loss = 0.6891857981681824
In grad_steps = 178, loss = 0.6543030738830566
In grad_steps = 179, loss = 0.619800865650177
In grad_steps = 180, loss = 0.7034281492233276
In grad_steps = 181, loss = 0.659625768661499
In grad_steps = 182, loss = 0.7906935214996338
In grad_steps = 183, loss = 0.8748493194580078
In grad_steps = 184, loss = 0.6380640268325806
In grad_steps = 185, loss = 0.7638009786605835
In grad_steps = 186, loss = 0.7331725358963013
In grad_steps = 187, loss = 0.6554656624794006
In grad_steps = 188, loss = 0.6805540919303894
In grad_steps = 189, loss = 0.6797995567321777
In grad_steps = 190, loss = 0.6644361019134521
In grad_steps = 191, loss = 0.6861139535903931
In grad_steps = 192, loss = 0.6669139862060547
In grad_steps = 193, loss = 0.708676815032959
In grad_steps = 194, loss = 0.6559039950370789
In grad_steps = 195, loss = 0.734381377696991
In grad_steps = 196, loss = 0.7891861200332642
In grad_steps = 197, loss = 0.625922679901123
In grad_steps = 198, loss = 0.7070932984352112
In grad_steps = 199, loss = 0.6594704389572144
In grad_steps = 200, loss = 0.6941197514533997
In grad_steps = 201, loss = 0.7102482914924622
In grad_steps = 202, loss = 0.6815477013587952
In grad_steps = 203, loss = 0.707683801651001
In grad_steps = 204, loss = 0.7207069993019104
In grad_steps = 205, loss = 0.6513317823410034
In grad_steps = 206, loss = 0.6767391562461853
In grad_steps = 207, loss = 0.6887081861495972
In grad_steps = 208, loss = 0.6783585548400879
In grad_steps = 209, loss = 0.7012377977371216
In grad_steps = 210, loss = 0.712059736251831
In grad_steps = 211, loss = 0.6813514232635498
In grad_steps = 212, loss = 0.6899802684783936
In grad_steps = 213, loss = 0.6873759031295776
In grad_steps = 214, loss = 0.6871113777160645
In grad_steps = 215, loss = 0.6765482425689697
In grad_steps = 216, loss = 0.6656813621520996
In grad_steps = 217, loss = 0.7192538976669312
In grad_steps = 218, loss = 0.6941655874252319
In grad_steps = 219, loss = 0.6979576349258423
In grad_steps = 220, loss = 0.7160944938659668
In grad_steps = 221, loss = 0.6521379351615906
In grad_steps = 222, loss = 0.7216039896011353
In grad_steps = 223, loss = 0.6610418558120728
In grad_steps = 224, loss = 0.7143670916557312
Beginning epoch 4
In grad_steps = 225, loss = 0.6250052452087402
In grad_steps = 226, loss = 0.6517066359519958
In grad_steps = 227, loss = 0.6446701884269714
In grad_steps = 228, loss = 0.6923752427101135
In grad_steps = 229, loss = 0.7271801829338074
In grad_steps = 230, loss = 0.7062203884124756
In grad_steps = 231, loss = 0.6693016886711121
In grad_steps = 232, loss = 0.7199897170066833
In grad_steps = 233, loss = 0.666354238986969
In grad_steps = 234, loss = 0.6585842370986938
In grad_steps = 235, loss = 0.7401269674301147
In grad_steps = 236, loss = 0.7586458921432495
In grad_steps = 237, loss = 0.6469259262084961
In grad_steps = 238, loss = 0.6908649802207947
In grad_steps = 239, loss = 0.6838452816009521
In grad_steps = 240, loss = 0.6682246327400208
In grad_steps = 241, loss = 0.6801103353500366
In grad_steps = 242, loss = 0.6325646042823792
In grad_steps = 243, loss = 0.7112010717391968
In grad_steps = 244, loss = 0.6952109932899475
In grad_steps = 245, loss = 0.6649481058120728
In grad_steps = 246, loss = 0.6643239259719849
In grad_steps = 247, loss = 0.6640699505805969
In grad_steps = 248, loss = 0.6778042316436768
In grad_steps = 249, loss = 0.6585943698883057
In grad_steps = 250, loss = 0.6351432800292969
In grad_steps = 251, loss = 0.6417871713638306
In grad_steps = 252, loss = 0.6349098682403564
In grad_steps = 253, loss = 0.5973369479179382
In grad_steps = 254, loss = 0.5589112043380737
In grad_steps = 255, loss = 0.6648666858673096
In grad_steps = 256, loss = 0.5915842652320862
In grad_steps = 257, loss = 0.8280086517333984
In grad_steps = 258, loss = 0.6542578339576721
In grad_steps = 259, loss = 0.6483902931213379
In grad_steps = 260, loss = 0.5858340263366699
In grad_steps = 261, loss = 0.6533880829811096
In grad_steps = 262, loss = 0.6359504461288452
In grad_steps = 263, loss = 0.6639358997344971
In grad_steps = 264, loss = 0.6571635007858276
In grad_steps = 265, loss = 0.6740376949310303
In grad_steps = 266, loss = 0.694288969039917
In grad_steps = 267, loss = 0.7339808940887451
In grad_steps = 268, loss = 0.6529771685600281
In grad_steps = 269, loss = 0.6669795513153076
In grad_steps = 270, loss = 0.7435680627822876
In grad_steps = 271, loss = 0.7248740792274475
In grad_steps = 272, loss = 0.6281731128692627
In grad_steps = 273, loss = 0.6658983826637268
In grad_steps = 274, loss = 0.6061731576919556
In grad_steps = 275, loss = 0.7036025524139404
In grad_steps = 276, loss = 0.8464717864990234
In grad_steps = 277, loss = 0.5876044034957886
In grad_steps = 278, loss = 0.6304324865341187
In grad_steps = 279, loss = 0.6593883633613586
In grad_steps = 280, loss = 0.6442804336547852
In grad_steps = 281, loss = 0.6379715800285339
In grad_steps = 282, loss = 0.6819480061531067
In grad_steps = 283, loss = 0.6511954665184021
In grad_steps = 284, loss = 0.6713466644287109
In grad_steps = 285, loss = 0.7180524468421936
In grad_steps = 286, loss = 0.6130973100662231
In grad_steps = 287, loss = 0.721331775188446
In grad_steps = 288, loss = 0.6724479794502258
In grad_steps = 289, loss = 0.6787166595458984
In grad_steps = 290, loss = 0.6730289459228516
In grad_steps = 291, loss = 0.682805597782135
In grad_steps = 292, loss = 0.7030072808265686
In grad_steps = 293, loss = 0.705954372882843
In grad_steps = 294, loss = 0.7000473737716675
In grad_steps = 295, loss = 0.7339738011360168
In grad_steps = 296, loss = 0.5783903002738953
In grad_steps = 297, loss = 0.6750778555870056
In grad_steps = 298, loss = 0.6934360861778259
In grad_steps = 299, loss = 0.6686908006668091
Beginning epoch 5
In grad_steps = 300, loss = 0.6111242175102234
In grad_steps = 301, loss = 0.6417925357818604
In grad_steps = 302, loss = 0.6831926107406616
In grad_steps = 303, loss = 0.6322336196899414
In grad_steps = 304, loss = 0.6642016768455505
In grad_steps = 305, loss = 0.6896073818206787
In grad_steps = 306, loss = 0.6432469487190247
In grad_steps = 307, loss = 0.6628632545471191
In grad_steps = 308, loss = 0.6308871507644653
In grad_steps = 309, loss = 0.5864132642745972
In grad_steps = 310, loss = 0.6712764501571655
In grad_steps = 311, loss = 0.673006534576416
In grad_steps = 312, loss = 0.5861337780952454
In grad_steps = 313, loss = 0.6149904131889343
In grad_steps = 314, loss = 0.6057117581367493
In grad_steps = 315, loss = 0.6135977506637573
In grad_steps = 316, loss = 0.6758750081062317
In grad_steps = 317, loss = 0.45711079239845276
In grad_steps = 318, loss = 0.5547568798065186
In grad_steps = 319, loss = 0.5226113796234131
In grad_steps = 320, loss = 0.43606266379356384
In grad_steps = 321, loss = 0.5781097412109375
In grad_steps = 322, loss = 0.5844756364822388
In grad_steps = 323, loss = 0.6004391312599182
In grad_steps = 324, loss = 0.6474652886390686
In grad_steps = 325, loss = 0.4416104257106781
In grad_steps = 326, loss = 0.4582058787345886
In grad_steps = 327, loss = 0.37715646624565125
In grad_steps = 328, loss = 0.4662854075431824
In grad_steps = 329, loss = 0.42740458250045776
In grad_steps = 330, loss = 0.4075292944908142
In grad_steps = 331, loss = 0.20564767718315125
In grad_steps = 332, loss = 0.30034932494163513
In grad_steps = 333, loss = 0.3086363673210144
In grad_steps = 334, loss = 0.23201963305473328
In grad_steps = 335, loss = 0.05218815058469772
In grad_steps = 336, loss = 0.1534709632396698
In grad_steps = 337, loss = 0.21602673828601837
In grad_steps = 338, loss = 0.3500719666481018
In grad_steps = 339, loss = 2.3355579376220703
In grad_steps = 340, loss = 1.3328193426132202
In grad_steps = 341, loss = 0.38608214259147644
In grad_steps = 342, loss = 0.3687170445919037
In grad_steps = 343, loss = 0.6860648989677429
In grad_steps = 344, loss = 0.8967596888542175
In grad_steps = 345, loss = 0.6916500926017761
In grad_steps = 346, loss = 0.6336085200309753
In grad_steps = 347, loss = 0.7695243954658508
In grad_steps = 348, loss = 0.6517881751060486
In grad_steps = 349, loss = 0.8476449847221375
In grad_steps = 350, loss = 0.6600326895713806
In grad_steps = 351, loss = 0.5402191281318665
In grad_steps = 352, loss = 0.639959454536438
In grad_steps = 353, loss = 0.627586305141449
In grad_steps = 354, loss = 0.6206591725349426
In grad_steps = 355, loss = 0.6866875886917114
In grad_steps = 356, loss = 0.640007734298706
In grad_steps = 357, loss = 0.542007327079773
In grad_steps = 358, loss = 0.6736295223236084
In grad_steps = 359, loss = 0.49970686435699463
In grad_steps = 360, loss = 0.5875341892242432
In grad_steps = 361, loss = 0.4778665602207184
In grad_steps = 362, loss = 0.6773185729980469
In grad_steps = 363, loss = 0.6541519165039062
In grad_steps = 364, loss = 0.5345019698143005
In grad_steps = 365, loss = 0.4318046569824219
In grad_steps = 366, loss = 0.5739452242851257
In grad_steps = 367, loss = 0.6490882039070129
In grad_steps = 368, loss = 0.5659788250923157
In grad_steps = 369, loss = 0.5620720982551575
In grad_steps = 370, loss = 0.6434311866760254
In grad_steps = 371, loss = 0.511525571346283
In grad_steps = 372, loss = 0.49641671776771545
In grad_steps = 373, loss = 0.737798273563385
In grad_steps = 374, loss = 0.6607409715652466
Beginning epoch 6
In grad_steps = 375, loss = 0.4343426823616028
In grad_steps = 376, loss = 0.49854421615600586
In grad_steps = 377, loss = 0.5065480470657349
In grad_steps = 378, loss = 0.850350022315979
In grad_steps = 379, loss = 0.9232084155082703
In grad_steps = 380, loss = 0.5798218250274658
In grad_steps = 381, loss = 0.5093705058097839
In grad_steps = 382, loss = 0.8458082675933838
In grad_steps = 383, loss = 0.5642005205154419
In grad_steps = 384, loss = 0.49864283204078674
In grad_steps = 385, loss = 0.6855428814888
In grad_steps = 386, loss = 0.6725780367851257
In grad_steps = 387, loss = 0.54808509349823
In grad_steps = 388, loss = 0.5250107049942017
In grad_steps = 389, loss = 0.5537513494491577
In grad_steps = 390, loss = 0.504575252532959
In grad_steps = 391, loss = 0.5142077803611755
In grad_steps = 392, loss = 0.4168723225593567
In grad_steps = 393, loss = 0.3731483817100525
In grad_steps = 394, loss = 0.35099461674690247
In grad_steps = 395, loss = 0.26867109537124634
In grad_steps = 396, loss = 0.20162039995193481
In grad_steps = 397, loss = 0.21892622113227844
In grad_steps = 398, loss = 0.41540470719337463
In grad_steps = 399, loss = 0.1438915729522705
In grad_steps = 400, loss = 0.23795261979103088
In grad_steps = 401, loss = 0.2854822874069214
In grad_steps = 402, loss = 0.12276556342840195
In grad_steps = 403, loss = 0.5633425712585449
In grad_steps = 404, loss = 0.7695422172546387
In grad_steps = 405, loss = 0.1239498034119606
In grad_steps = 406, loss = 0.3080185353755951
In grad_steps = 407, loss = 0.6075213551521301
In grad_steps = 408, loss = 0.7941162586212158
In grad_steps = 409, loss = 0.28226009011268616
In grad_steps = 410, loss = 0.3754643201828003
In grad_steps = 411, loss = 0.4026870131492615
In grad_steps = 412, loss = 0.6124749779701233
In grad_steps = 413, loss = 0.7732978463172913
In grad_steps = 414, loss = 0.5867694616317749
In grad_steps = 415, loss = 0.5615144968032837
In grad_steps = 416, loss = 0.4096110761165619
In grad_steps = 417, loss = 0.5595114231109619
In grad_steps = 418, loss = 0.4576672613620758
In grad_steps = 419, loss = 0.494436115026474
In grad_steps = 420, loss = 0.6784390211105347
In grad_steps = 421, loss = 0.6657491326332092
In grad_steps = 422, loss = 0.5727912187576294
In grad_steps = 423, loss = 0.560504138469696
In grad_steps = 424, loss = 0.45089292526245117
In grad_steps = 425, loss = 0.6294577121734619
In grad_steps = 426, loss = 0.9145656228065491
In grad_steps = 427, loss = 0.36386609077453613
In grad_steps = 428, loss = 0.5483030676841736
In grad_steps = 429, loss = 0.6048169136047363
In grad_steps = 430, loss = 0.4343191385269165
In grad_steps = 431, loss = 0.47626250982284546
In grad_steps = 432, loss = 0.38881444931030273
In grad_steps = 433, loss = 0.5240021347999573
In grad_steps = 434, loss = 0.4243031144142151
In grad_steps = 435, loss = 0.3652278184890747
In grad_steps = 436, loss = 0.3199145495891571
In grad_steps = 437, loss = 0.4207037091255188
In grad_steps = 438, loss = 0.2896980047225952
In grad_steps = 439, loss = 0.29311418533325195
In grad_steps = 440, loss = 0.2906838655471802
In grad_steps = 441, loss = 0.36300966143608093
In grad_steps = 442, loss = 0.28558987379074097
In grad_steps = 443, loss = 0.0930858626961708
In grad_steps = 444, loss = 0.31376969814300537
In grad_steps = 445, loss = 0.9159714579582214
In grad_steps = 446, loss = 0.2488589882850647
In grad_steps = 447, loss = 0.3112962543964386
In grad_steps = 448, loss = 0.5151785016059875
In grad_steps = 449, loss = 0.3244040906429291
Beginning epoch 7
In grad_steps = 450, loss = 0.20007017254829407
In grad_steps = 451, loss = 0.5597633719444275
In grad_steps = 452, loss = 0.18718546628952026
In grad_steps = 453, loss = 0.5886409282684326
In grad_steps = 454, loss = 1.0235812664031982
In grad_steps = 455, loss = 0.35924139618873596
In grad_steps = 456, loss = 0.3960927724838257
In grad_steps = 457, loss = 0.5710253715515137
In grad_steps = 458, loss = 0.424193799495697
In grad_steps = 459, loss = 0.1777677834033966
In grad_steps = 460, loss = 0.5213736891746521
In grad_steps = 461, loss = 0.6948551535606384
In grad_steps = 462, loss = 0.4189172387123108
In grad_steps = 463, loss = 0.4320243000984192
In grad_steps = 464, loss = 0.31082624197006226
In grad_steps = 465, loss = 0.38668015599250793
In grad_steps = 466, loss = 0.48747676610946655
In grad_steps = 467, loss = 0.1873392015695572
In grad_steps = 468, loss = 0.24536079168319702
In grad_steps = 469, loss = 0.24307329952716827
In grad_steps = 470, loss = 0.24288171529769897
In grad_steps = 471, loss = 0.23862382769584656
In grad_steps = 472, loss = 0.32132768630981445
In grad_steps = 473, loss = 0.3864153027534485
In grad_steps = 474, loss = 0.2777215838432312
In grad_steps = 475, loss = 0.20648302137851715
In grad_steps = 476, loss = 0.15321414172649384
In grad_steps = 477, loss = 0.1163020208477974
In grad_steps = 478, loss = 0.08053581416606903
In grad_steps = 479, loss = 0.18505123257637024
In grad_steps = 480, loss = 0.12505000829696655
In grad_steps = 481, loss = 0.07960755378007889
In grad_steps = 482, loss = 0.034215085208415985
In grad_steps = 483, loss = 0.2942884564399719
In grad_steps = 484, loss = 0.05371991917490959
In grad_steps = 485, loss = 0.42833006381988525
In grad_steps = 486, loss = 0.16025066375732422
In grad_steps = 487, loss = 0.09142905473709106
In grad_steps = 488, loss = 0.4575406312942505
In grad_steps = 489, loss = 0.3625186085700989
In grad_steps = 490, loss = 0.3616706132888794
In grad_steps = 491, loss = 0.17282186448574066
In grad_steps = 492, loss = 0.47934556007385254
In grad_steps = 493, loss = 0.190304696559906
In grad_steps = 494, loss = 0.46502894163131714
In grad_steps = 495, loss = 0.5792247653007507
In grad_steps = 496, loss = 0.4790845513343811
In grad_steps = 497, loss = 0.4542442858219147
In grad_steps = 498, loss = 0.27546921372413635
In grad_steps = 499, loss = 0.29476359486579895
In grad_steps = 500, loss = 0.27019137144088745
In grad_steps = 501, loss = 0.3718065619468689
In grad_steps = 502, loss = 0.18049614131450653
In grad_steps = 503, loss = 0.2793065309524536
In grad_steps = 504, loss = 0.23970428109169006
In grad_steps = 505, loss = 0.29496899247169495
In grad_steps = 506, loss = 0.28818485140800476
In grad_steps = 507, loss = 0.23058469593524933
In grad_steps = 508, loss = 0.34568601846694946
In grad_steps = 509, loss = 0.30291908979415894
In grad_steps = 510, loss = 0.11930155009031296
In grad_steps = 511, loss = 0.15313684940338135
In grad_steps = 512, loss = 0.2038629949092865
In grad_steps = 513, loss = 0.10490255057811737
In grad_steps = 514, loss = 0.2521921694278717
In grad_steps = 515, loss = 0.4420798122882843
In grad_steps = 516, loss = 0.33228832483291626
In grad_steps = 517, loss = 0.06551588326692581
In grad_steps = 518, loss = 0.07657559961080551
In grad_steps = 519, loss = 0.10057184845209122
In grad_steps = 520, loss = 0.5199247598648071
In grad_steps = 521, loss = 0.21703383326530457
In grad_steps = 522, loss = 0.24770782887935638
In grad_steps = 523, loss = 0.29972925782203674
In grad_steps = 524, loss = 0.10145137459039688
Beginning epoch 8
In grad_steps = 525, loss = 0.04905233159661293
In grad_steps = 526, loss = 0.4691607654094696
In grad_steps = 527, loss = 0.3301384747028351
In grad_steps = 528, loss = 0.4884791970252991
In grad_steps = 529, loss = 0.38681748509407043
In grad_steps = 530, loss = 0.13459980487823486
In grad_steps = 531, loss = 0.13726674020290375
In grad_steps = 532, loss = 0.41613492369651794
In grad_steps = 533, loss = 0.19661904871463776
In grad_steps = 534, loss = 0.3533720374107361
In grad_steps = 535, loss = 0.41410547494888306
In grad_steps = 536, loss = 0.4887930154800415
In grad_steps = 537, loss = 0.25604987144470215
In grad_steps = 538, loss = 0.15574876964092255
In grad_steps = 539, loss = 0.1433364599943161
In grad_steps = 540, loss = 0.10551612079143524
In grad_steps = 541, loss = 0.43249237537384033
In grad_steps = 542, loss = 0.0807884931564331
In grad_steps = 543, loss = 0.3377110958099365
In grad_steps = 544, loss = 0.1508117914199829
In grad_steps = 545, loss = 0.2010631561279297
In grad_steps = 546, loss = 0.3217431604862213
In grad_steps = 547, loss = 0.09605291485786438
In grad_steps = 548, loss = 0.08476798236370087
In grad_steps = 549, loss = 0.07385461032390594
In grad_steps = 550, loss = 0.038510218262672424
In grad_steps = 551, loss = 0.032495636492967606
In grad_steps = 552, loss = 0.13912677764892578
In grad_steps = 553, loss = 0.04998452216386795
In grad_steps = 554, loss = 0.19104576110839844
In grad_steps = 555, loss = 0.07880116254091263
In grad_steps = 556, loss = 0.45948272943496704
In grad_steps = 557, loss = 0.03273160010576248
In grad_steps = 558, loss = 0.03219243139028549
In grad_steps = 559, loss = 0.1307080090045929
In grad_steps = 560, loss = 0.3167946934700012
In grad_steps = 561, loss = 0.03890494629740715
In grad_steps = 562, loss = 0.3696660101413727
In grad_steps = 563, loss = 0.10360048711299896
In grad_steps = 564, loss = 0.12025637179613113
In grad_steps = 565, loss = 0.12533976137638092
In grad_steps = 566, loss = 0.024339521303772926
In grad_steps = 567, loss = 0.10893625766038895
In grad_steps = 568, loss = 0.1770651638507843
In grad_steps = 569, loss = 0.3916606605052948
In grad_steps = 570, loss = 0.236343652009964
In grad_steps = 571, loss = 0.402197003364563
In grad_steps = 572, loss = 0.2242022305727005
In grad_steps = 573, loss = 0.0973360538482666
In grad_steps = 574, loss = 0.1464676409959793
In grad_steps = 575, loss = 0.20800961554050446
In grad_steps = 576, loss = 0.12563905119895935
In grad_steps = 577, loss = 0.0765906497836113
In grad_steps = 578, loss = 0.2567078471183777
In grad_steps = 579, loss = 0.3203284740447998
In grad_steps = 580, loss = 0.20262911915779114
In grad_steps = 581, loss = 0.19960419833660126
In grad_steps = 582, loss = 0.05728037655353546
In grad_steps = 583, loss = 0.09623133391141891
In grad_steps = 584, loss = 0.068980872631073
In grad_steps = 585, loss = 0.09644091874361038
In grad_steps = 586, loss = 0.07471656054258347
In grad_steps = 587, loss = 0.1566563993692398
In grad_steps = 588, loss = 0.02328459918498993
In grad_steps = 589, loss = 0.061200082302093506
In grad_steps = 590, loss = 0.16107068955898285
In grad_steps = 591, loss = 0.08993768692016602
In grad_steps = 592, loss = 0.02160554938018322
In grad_steps = 593, loss = 0.08696624636650085
In grad_steps = 594, loss = 0.04106380417943001
In grad_steps = 595, loss = 0.03525373339653015
In grad_steps = 596, loss = 0.015389395877718925
In grad_steps = 597, loss = 0.13512897491455078
In grad_steps = 598, loss = 0.07366858422756195
In grad_steps = 599, loss = 0.09343086183071136
Beginning epoch 9
In grad_steps = 600, loss = 0.029778916388750076
In grad_steps = 601, loss = 0.0057609230279922485
In grad_steps = 602, loss = 0.6485008597373962
In grad_steps = 603, loss = 0.0700758695602417
In grad_steps = 604, loss = 0.018025118857622147
In grad_steps = 605, loss = 0.1553407907485962
In grad_steps = 606, loss = 0.10271701961755753
In grad_steps = 607, loss = 0.3856780230998993
In grad_steps = 608, loss = 0.0532783642411232
In grad_steps = 609, loss = 0.08392581343650818
In grad_steps = 610, loss = 0.1349783092737198
In grad_steps = 611, loss = 0.08291979879140854
In grad_steps = 612, loss = 0.14951582252979279
In grad_steps = 613, loss = 0.011815180070698261
In grad_steps = 614, loss = 0.30610620975494385
In grad_steps = 615, loss = 0.049075961112976074
In grad_steps = 616, loss = 0.3581443130970001
In grad_steps = 617, loss = 0.07322443276643753
In grad_steps = 618, loss = 0.17225360870361328
In grad_steps = 619, loss = 0.09485254436731339
In grad_steps = 620, loss = 0.02731156349182129
In grad_steps = 621, loss = 0.06901399791240692
In grad_steps = 622, loss = 0.2485678344964981
In grad_steps = 623, loss = 0.14896629750728607
In grad_steps = 624, loss = 0.12977495789527893
In grad_steps = 625, loss = 0.09572317451238632
In grad_steps = 626, loss = 0.04300557076931
In grad_steps = 627, loss = 0.05455387011170387
In grad_steps = 628, loss = 0.04596109688282013
In grad_steps = 629, loss = 0.07646749913692474
In grad_steps = 630, loss = 0.018370775505900383
In grad_steps = 631, loss = 0.3478711247444153
In grad_steps = 632, loss = 0.07419663667678833
In grad_steps = 633, loss = 0.035780273377895355
In grad_steps = 634, loss = 0.025311434641480446
In grad_steps = 635, loss = 0.0359969362616539
In grad_steps = 636, loss = 0.03938155248761177
In grad_steps = 637, loss = 0.04570791870355606
In grad_steps = 638, loss = 0.14350686967372894
In grad_steps = 639, loss = 0.21612131595611572
In grad_steps = 640, loss = 0.023910081014037132
In grad_steps = 641, loss = 0.015266960486769676
In grad_steps = 642, loss = 0.1858101189136505
In grad_steps = 643, loss = 0.06775818765163422
In grad_steps = 644, loss = 0.5267965197563171
In grad_steps = 645, loss = 0.027250487357378006
In grad_steps = 646, loss = 0.22374586760997772
In grad_steps = 647, loss = 0.15976044535636902
In grad_steps = 648, loss = 0.19021719694137573
In grad_steps = 649, loss = 0.01623576320707798
In grad_steps = 650, loss = 0.10410118103027344
In grad_steps = 651, loss = 0.6491354703903198
In grad_steps = 652, loss = 0.16752977669239044
In grad_steps = 653, loss = 0.0705980733036995
In grad_steps = 654, loss = 0.23891595005989075
In grad_steps = 655, loss = 0.06714797765016556
In grad_steps = 656, loss = 0.08806145191192627
In grad_steps = 657, loss = 0.0718761533498764
In grad_steps = 658, loss = 0.14698582887649536
In grad_steps = 659, loss = 0.5638564229011536
In grad_steps = 660, loss = 0.37980136275291443
In grad_steps = 661, loss = 0.10656920075416565
In grad_steps = 662, loss = 0.3444066047668457
In grad_steps = 663, loss = 0.07093063741922379
In grad_steps = 664, loss = 0.0971258208155632
In grad_steps = 665, loss = 0.1127922311425209
In grad_steps = 666, loss = 0.294108510017395
In grad_steps = 667, loss = 0.19274012744426727
In grad_steps = 668, loss = 0.11981780081987381
In grad_steps = 669, loss = 0.1593705117702484
In grad_steps = 670, loss = 0.4711989462375641
In grad_steps = 671, loss = 0.061969492584466934
In grad_steps = 672, loss = 0.22850613296031952
In grad_steps = 673, loss = 0.15083520114421844
In grad_steps = 674, loss = 0.49953529238700867
Beginning epoch 10
In grad_steps = 675, loss = 0.21123366057872772
In grad_steps = 676, loss = 0.17087696492671967
In grad_steps = 677, loss = 0.053617075085639954
In grad_steps = 678, loss = 0.09924333542585373
In grad_steps = 679, loss = 0.327666312456131
In grad_steps = 680, loss = 0.09237628430128098
In grad_steps = 681, loss = 0.029457125812768936
In grad_steps = 682, loss = 0.3856443762779236
In grad_steps = 683, loss = 0.2876170873641968
In grad_steps = 684, loss = 0.07066365331411362
In grad_steps = 685, loss = 0.12135957181453705
In grad_steps = 686, loss = 0.10934048891067505
In grad_steps = 687, loss = 0.016101256012916565
In grad_steps = 688, loss = 0.010321208275854588
In grad_steps = 689, loss = 0.01608728989958763
In grad_steps = 690, loss = 0.053927093744277954
In grad_steps = 691, loss = 0.02742253988981247
In grad_steps = 692, loss = 0.00849695224314928
In grad_steps = 693, loss = 0.47109588980674744
In grad_steps = 694, loss = 0.02032095566391945
In grad_steps = 695, loss = 0.05834177881479263
In grad_steps = 696, loss = 0.012126628309488297
In grad_steps = 697, loss = 0.05647718533873558
In grad_steps = 698, loss = 0.013561631552875042
In grad_steps = 699, loss = 0.017929108813405037
In grad_steps = 700, loss = 0.1571149379014969
In grad_steps = 701, loss = 0.147355854511261
In grad_steps = 702, loss = 0.011436604894697666
In grad_steps = 703, loss = 0.005615590140223503
In grad_steps = 704, loss = 0.016582906246185303
In grad_steps = 705, loss = 0.026868823915719986
In grad_steps = 706, loss = 0.006483498960733414
In grad_steps = 707, loss = 0.006428108550608158
In grad_steps = 708, loss = 0.02090114913880825
In grad_steps = 709, loss = 0.004940090235322714
In grad_steps = 710, loss = 0.0062415688298642635
In grad_steps = 711, loss = 0.008163382299244404
In grad_steps = 712, loss = 0.0163237527012825
In grad_steps = 713, loss = 0.1069556251168251
In grad_steps = 714, loss = 0.14449554681777954
In grad_steps = 715, loss = 0.008343584835529327
In grad_steps = 716, loss = 0.008217042312026024
In grad_steps = 717, loss = 0.06823990494012833
In grad_steps = 718, loss = 0.06359183043241501
In grad_steps = 719, loss = 0.16870266199111938
In grad_steps = 720, loss = 0.3161960542201996
In grad_steps = 721, loss = 0.010962517000734806
In grad_steps = 722, loss = 0.16141296923160553
In grad_steps = 723, loss = 0.026373092085123062
In grad_steps = 724, loss = 0.2989097833633423
In grad_steps = 725, loss = 0.025682790204882622
In grad_steps = 726, loss = 0.16382020711898804
In grad_steps = 727, loss = 0.005439480300992727
In grad_steps = 728, loss = 0.06302428990602493
In grad_steps = 729, loss = 0.2850879728794098
In grad_steps = 730, loss = 0.12954212725162506
In grad_steps = 731, loss = 0.06952861696481705
In grad_steps = 732, loss = 0.04732222110033035
In grad_steps = 733, loss = 0.22569197416305542
In grad_steps = 734, loss = 0.12314319610595703
In grad_steps = 735, loss = 0.00977014284580946
In grad_steps = 736, loss = 0.02195863425731659
In grad_steps = 737, loss = 0.0578799806535244
In grad_steps = 738, loss = 0.016974065452814102
In grad_steps = 739, loss = 0.192671537399292
In grad_steps = 740, loss = 0.035385988652706146
In grad_steps = 741, loss = 0.11191488802433014
In grad_steps = 742, loss = 0.13787056505680084
In grad_steps = 743, loss = 0.28664201498031616
In grad_steps = 744, loss = 0.09322725236415863
In grad_steps = 745, loss = 0.050935130566358566
In grad_steps = 746, loss = 0.13559089601039886
In grad_steps = 747, loss = 0.02272140420973301
In grad_steps = 748, loss = 0.04838738590478897
In grad_steps = 749, loss = 0.06019638106226921
Beginning epoch 11
In grad_steps = 750, loss = 0.03496221825480461
In grad_steps = 751, loss = 0.34060367941856384
In grad_steps = 752, loss = 0.08460211008787155
In grad_steps = 753, loss = 0.31640031933784485
In grad_steps = 754, loss = 0.29222366213798523
In grad_steps = 755, loss = 0.11382752656936646
In grad_steps = 756, loss = 0.03514622896909714
In grad_steps = 757, loss = 0.0313231386244297
In grad_steps = 758, loss = 0.03495616093277931
In grad_steps = 759, loss = 0.017906520515680313
In grad_steps = 760, loss = 0.023351503536105156
In grad_steps = 761, loss = 0.10073183476924896
In grad_steps = 762, loss = 0.062264684587717056
In grad_steps = 763, loss = 0.05306437984108925
In grad_steps = 764, loss = 0.024613788351416588
In grad_steps = 765, loss = 0.054424554109573364
In grad_steps = 766, loss = 0.24839825928211212
In grad_steps = 767, loss = 0.006561524700373411
In grad_steps = 768, loss = 0.12497598677873611
In grad_steps = 769, loss = 0.010012106038630009
In grad_steps = 770, loss = 0.1126897782087326
In grad_steps = 771, loss = 0.0033186215441673994
In grad_steps = 772, loss = 0.0060693067498505116
In grad_steps = 773, loss = 0.2913314700126648
In grad_steps = 774, loss = 0.06138915196061134
In grad_steps = 775, loss = 0.105726458132267
In grad_steps = 776, loss = 0.007558232173323631
In grad_steps = 777, loss = 0.00644881185144186
In grad_steps = 778, loss = 0.16258175671100616
In grad_steps = 779, loss = 0.0942174643278122
In grad_steps = 780, loss = 0.10310635715723038
In grad_steps = 781, loss = 0.0032600504346191883
In grad_steps = 782, loss = 0.009702298790216446
In grad_steps = 783, loss = 0.2082216590642929
In grad_steps = 784, loss = 0.008268900215625763
In grad_steps = 785, loss = 0.0948290005326271
In grad_steps = 786, loss = 0.111513651907444
In grad_steps = 787, loss = 0.04541396349668503
In grad_steps = 788, loss = 0.0036502680741250515
In grad_steps = 789, loss = 0.057675328105688095
In grad_steps = 790, loss = 0.010734030045568943
In grad_steps = 791, loss = 0.06346024572849274
In grad_steps = 792, loss = 0.017680099233984947
In grad_steps = 793, loss = 0.32055890560150146
In grad_steps = 794, loss = 0.06864165514707565
In grad_steps = 795, loss = 0.28680843114852905
In grad_steps = 796, loss = 0.1890113353729248
In grad_steps = 797, loss = 0.0062799593433737755
In grad_steps = 798, loss = 0.06601452827453613
In grad_steps = 799, loss = 0.014517160132527351
In grad_steps = 800, loss = 0.06839295476675034
In grad_steps = 801, loss = 0.020406724885106087
In grad_steps = 802, loss = 0.06719828397035599
In grad_steps = 803, loss = 0.012211239896714687
In grad_steps = 804, loss = 0.0949985533952713
In grad_steps = 805, loss = 0.012336846441030502
In grad_steps = 806, loss = 0.003654907224699855
In grad_steps = 807, loss = 0.019623763859272003
In grad_steps = 808, loss = 0.02653244510293007
In grad_steps = 809, loss = 0.02309870347380638
In grad_steps = 810, loss = 0.06211131438612938
In grad_steps = 811, loss = 0.30636876821517944
In grad_steps = 812, loss = 0.2229025959968567
In grad_steps = 813, loss = 0.10507479310035706
In grad_steps = 814, loss = 0.016822079196572304
In grad_steps = 815, loss = 0.21100576221942902
In grad_steps = 816, loss = 0.04567130282521248
In grad_steps = 817, loss = 0.06537538766860962
In grad_steps = 818, loss = 0.2549203634262085
In grad_steps = 819, loss = 0.2293485403060913
In grad_steps = 820, loss = 0.04084237292408943
In grad_steps = 821, loss = 0.020392725244164467
In grad_steps = 822, loss = 0.02297866716980934
In grad_steps = 823, loss = 0.04686541110277176
In grad_steps = 824, loss = 0.04515048861503601
Beginning epoch 12
In grad_steps = 825, loss = 0.008599589578807354
In grad_steps = 826, loss = 0.016565166413784027
In grad_steps = 827, loss = 0.017286140471696854
In grad_steps = 828, loss = 0.22214670479297638
In grad_steps = 829, loss = 0.5470861196517944
In grad_steps = 830, loss = 0.03556418791413307
In grad_steps = 831, loss = 0.1922297179698944
In grad_steps = 832, loss = 0.4020023047924042
In grad_steps = 833, loss = 0.17491988837718964
In grad_steps = 834, loss = 0.20351317524909973
In grad_steps = 835, loss = 0.018519336357712746
In grad_steps = 836, loss = 0.04030422866344452
In grad_steps = 837, loss = 0.08571399748325348
In grad_steps = 838, loss = 0.026340046897530556
In grad_steps = 839, loss = 0.24167144298553467
In grad_steps = 840, loss = 0.07206083834171295
In grad_steps = 841, loss = 0.12376361340284348
In grad_steps = 842, loss = 0.25457000732421875
In grad_steps = 843, loss = 0.0495169535279274
In grad_steps = 844, loss = 0.05528917908668518
In grad_steps = 845, loss = 0.27671560645103455
In grad_steps = 846, loss = 0.057735856622457504
In grad_steps = 847, loss = 0.10794493556022644
In grad_steps = 848, loss = 0.02421768382191658
In grad_steps = 849, loss = 0.1980830579996109
In grad_steps = 850, loss = 0.01011769101023674
In grad_steps = 851, loss = 0.01956992596387863
In grad_steps = 852, loss = 0.014693527482450008
In grad_steps = 853, loss = 0.02809799648821354
In grad_steps = 854, loss = 0.04118701443076134
In grad_steps = 855, loss = 0.05978339537978172
In grad_steps = 856, loss = 0.01194947212934494
In grad_steps = 857, loss = 0.016239315271377563
In grad_steps = 858, loss = 0.02281506173312664
In grad_steps = 859, loss = 0.005752439610660076
In grad_steps = 860, loss = 0.00809372216463089
In grad_steps = 861, loss = 0.01294675562530756
In grad_steps = 862, loss = 0.007440480869263411
In grad_steps = 863, loss = 0.007516591809689999
In grad_steps = 864, loss = 0.009372727945446968
In grad_steps = 865, loss = 0.0030137216672301292
In grad_steps = 866, loss = 0.0021738149225711823
In grad_steps = 867, loss = 0.18655729293823242
In grad_steps = 868, loss = 0.004411384463310242
In grad_steps = 869, loss = 0.29246121644973755
In grad_steps = 870, loss = 0.006741207093000412
In grad_steps = 871, loss = 0.0009384617442265153
In grad_steps = 872, loss = 0.013811795972287655
In grad_steps = 873, loss = 0.00516096455976367
In grad_steps = 874, loss = 0.003460504347458482
In grad_steps = 875, loss = 0.004369182046502829
In grad_steps = 876, loss = 0.04121382534503937
In grad_steps = 877, loss = 0.0014448142610490322
In grad_steps = 878, loss = 0.0018238635966554284
In grad_steps = 879, loss = 0.001659678528085351
In grad_steps = 880, loss = 0.0025321797002106905
In grad_steps = 881, loss = 0.0011505751172080636
In grad_steps = 882, loss = 0.08753632754087448
In grad_steps = 883, loss = 0.3258538544178009
In grad_steps = 884, loss = 0.055366016924381256
In grad_steps = 885, loss = 0.008121751248836517
In grad_steps = 886, loss = 0.05061400309205055
In grad_steps = 887, loss = 0.2176274210214615
In grad_steps = 888, loss = 0.07611458748579025
In grad_steps = 889, loss = 0.060722459107637405
In grad_steps = 890, loss = 0.07544029504060745
In grad_steps = 891, loss = 0.004495123401284218
In grad_steps = 892, loss = 0.007719628978520632
In grad_steps = 893, loss = 0.0034604165703058243
In grad_steps = 894, loss = 0.10703698545694351
In grad_steps = 895, loss = 0.018939746543765068
In grad_steps = 896, loss = 0.16665776073932648
In grad_steps = 897, loss = 0.041710879653692245
In grad_steps = 898, loss = 0.09199347347021103
In grad_steps = 899, loss = 0.017499707639217377
Elapsed time: 3016.073209285736 seconds for ensemble 3 with 12 epochs
LoRA instance 3 evaluation complete. Data saved to /pscratch/sd/t/tianle/lucid/other_source/SURP_2024/results/tmp_lora_ensemble/Llama3-8B-set-1.2/test_data_instance_3_seed_30280.npz.
lora instance i = 3 Successfully finished.
Training lora instance 4
DEBUG: Total parameters: 8037076992
DEBUG: Trainable parameters: 6815744
Beginning epoch 1
In grad_steps = 0, loss = 1.014610767364502
In grad_steps = 1, loss = 0.7362850904464722
In grad_steps = 2, loss = 0.6769920587539673
In grad_steps = 3, loss = 0.6872846484184265
In grad_steps = 4, loss = 0.6312108039855957
In grad_steps = 5, loss = 0.6279463171958923
In grad_steps = 6, loss = 0.643194854259491
In grad_steps = 7, loss = 0.9244389533996582
In grad_steps = 8, loss = 0.679960310459137
In grad_steps = 9, loss = 0.6917962431907654
In grad_steps = 10, loss = 0.6971551775932312
In grad_steps = 11, loss = 0.6943697929382324
In grad_steps = 12, loss = 0.7724267244338989
In grad_steps = 13, loss = 0.6987305879592896
In grad_steps = 14, loss = 0.7409062385559082
In grad_steps = 15, loss = 0.6954173445701599
In grad_steps = 16, loss = 0.7221619486808777
In grad_steps = 17, loss = 0.7121067047119141
In grad_steps = 18, loss = 0.7048447132110596
In grad_steps = 19, loss = 0.7027402520179749
In grad_steps = 20, loss = 0.6994256973266602
In grad_steps = 21, loss = 0.6901320815086365
In grad_steps = 22, loss = 0.6979246139526367
In grad_steps = 23, loss = 0.6864003539085388
In grad_steps = 24, loss = 0.7093216776847839
In grad_steps = 25, loss = 0.6967310309410095
In grad_steps = 26, loss = 0.7111181020736694
In grad_steps = 27, loss = 0.689206600189209
In grad_steps = 28, loss = 0.6680273413658142
In grad_steps = 29, loss = 0.645543098449707
In grad_steps = 30, loss = 0.7049627900123596
In grad_steps = 31, loss = 0.6733150482177734
In grad_steps = 32, loss = 0.7781382203102112
In grad_steps = 33, loss = 0.8616318702697754
In grad_steps = 34, loss = 0.6553679704666138
In grad_steps = 35, loss = 0.748910129070282
In grad_steps = 36, loss = 0.7057531476020813
In grad_steps = 37, loss = 0.6902914643287659
In grad_steps = 38, loss = 0.7449788451194763
In grad_steps = 39, loss = 0.6723352074623108
In grad_steps = 40, loss = 0.664773166179657
In grad_steps = 41, loss = 0.6990041732788086
In grad_steps = 42, loss = 0.6653647422790527
In grad_steps = 43, loss = 0.7352926135063171
In grad_steps = 44, loss = 0.6584633588790894
In grad_steps = 45, loss = 0.7295390367507935
In grad_steps = 46, loss = 0.7697945237159729
In grad_steps = 47, loss = 0.6460549235343933
In grad_steps = 48, loss = 0.7018969058990479
In grad_steps = 49, loss = 0.6941737532615662
In grad_steps = 50, loss = 0.6902977824211121
In grad_steps = 51, loss = 0.6788048148155212
In grad_steps = 52, loss = 0.7151084542274475
In grad_steps = 53, loss = 0.7354573011398315
In grad_steps = 54, loss = 0.7360056638717651
In grad_steps = 55, loss = 0.6530079245567322
In grad_steps = 56, loss = 0.6885052919387817
In grad_steps = 57, loss = 0.7002251744270325
In grad_steps = 58, loss = 0.6782401204109192
In grad_steps = 59, loss = 0.7034287452697754
In grad_steps = 60, loss = 0.710350751876831
In grad_steps = 61, loss = 0.6840215921401978
In grad_steps = 62, loss = 0.7030277252197266
In grad_steps = 63, loss = 0.7022156715393066
In grad_steps = 64, loss = 0.6960687041282654
In grad_steps = 65, loss = 0.6833817362785339
In grad_steps = 66, loss = 0.66487717628479
In grad_steps = 67, loss = 0.7272589802742004
In grad_steps = 68, loss = 0.6972874402999878
In grad_steps = 69, loss = 0.7020420432090759
In grad_steps = 70, loss = 0.7211069464683533
In grad_steps = 71, loss = 0.6763287782669067
In grad_steps = 72, loss = 0.7208304405212402
In grad_steps = 73, loss = 0.6771256327629089
In grad_steps = 74, loss = 0.7185197472572327
Beginning epoch 2
In grad_steps = 75, loss = 0.655269980430603
In grad_steps = 76, loss = 0.6738230586051941
In grad_steps = 77, loss = 0.6606199145317078
In grad_steps = 78, loss = 0.7015479803085327
In grad_steps = 79, loss = 0.7203253507614136
In grad_steps = 80, loss = 0.71522057056427
In grad_steps = 81, loss = 0.6786400079727173
In grad_steps = 82, loss = 0.7059417366981506
In grad_steps = 83, loss = 0.6780840158462524
In grad_steps = 84, loss = 0.6798568367958069
In grad_steps = 85, loss = 0.7286484837532043
In grad_steps = 86, loss = 0.7323459982872009
In grad_steps = 87, loss = 0.6648933291435242
In grad_steps = 88, loss = 0.7223836779594421
In grad_steps = 89, loss = 0.712496817111969
In grad_steps = 90, loss = 0.6939206719398499
In grad_steps = 91, loss = 0.6976929306983948
In grad_steps = 92, loss = 0.6827100515365601
In grad_steps = 93, loss = 0.6941728591918945
In grad_steps = 94, loss = 0.6922175884246826
In grad_steps = 95, loss = 0.6839139461517334
In grad_steps = 96, loss = 0.6841809749603271
In grad_steps = 97, loss = 0.687073826789856
In grad_steps = 98, loss = 0.6965698599815369
In grad_steps = 99, loss = 0.6873680949211121
In grad_steps = 100, loss = 0.6866084337234497
In grad_steps = 101, loss = 0.6932323575019836
In grad_steps = 102, loss = 0.6975471377372742
In grad_steps = 103, loss = 0.6718990206718445
In grad_steps = 104, loss = 0.6330553293228149
In grad_steps = 105, loss = 0.7003113031387329
In grad_steps = 106, loss = 0.6757920980453491
In grad_steps = 107, loss = 0.7545343041419983
In grad_steps = 108, loss = 0.8492296934127808
In grad_steps = 109, loss = 0.6536980271339417
In grad_steps = 110, loss = 0.7899599075317383
In grad_steps = 111, loss = 0.755314826965332
In grad_steps = 112, loss = 0.6623385548591614
In grad_steps = 113, loss = 0.6708014011383057
In grad_steps = 114, loss = 0.6895798444747925
In grad_steps = 115, loss = 0.676671028137207
In grad_steps = 116, loss = 0.6928158402442932
In grad_steps = 117, loss = 0.666833221912384
In grad_steps = 118, loss = 0.718144953250885
In grad_steps = 119, loss = 0.6490397453308105
In grad_steps = 120, loss = 0.7429614067077637
In grad_steps = 121, loss = 0.8036895990371704
In grad_steps = 122, loss = 0.6219280958175659
In grad_steps = 123, loss = 0.7169092893600464
In grad_steps = 124, loss = 0.6638097763061523
In grad_steps = 125, loss = 0.6979034543037415
In grad_steps = 126, loss = 0.7326973676681519
In grad_steps = 127, loss = 0.691891074180603
In grad_steps = 128, loss = 0.7105642557144165
In grad_steps = 129, loss = 0.7180158495903015
In grad_steps = 130, loss = 0.6581237316131592
In grad_steps = 131, loss = 0.6891957521438599
In grad_steps = 132, loss = 0.6968203186988831
In grad_steps = 133, loss = 0.6763905882835388
In grad_steps = 134, loss = 0.7110742926597595
In grad_steps = 135, loss = 0.7159101963043213
In grad_steps = 136, loss = 0.6739596128463745
In grad_steps = 137, loss = 0.6993713974952698
In grad_steps = 138, loss = 0.6899340152740479
In grad_steps = 139, loss = 0.6913521885871887
In grad_steps = 140, loss = 0.6812514066696167
In grad_steps = 141, loss = 0.6599557995796204
In grad_steps = 142, loss = 0.7331130504608154
In grad_steps = 143, loss = 0.7061753869056702
In grad_steps = 144, loss = 0.7058167457580566
In grad_steps = 145, loss = 0.7151044011116028
In grad_steps = 146, loss = 0.6707459688186646
In grad_steps = 147, loss = 0.7201252579689026
In grad_steps = 148, loss = 0.6760584115982056
In grad_steps = 149, loss = 0.7150492668151855
Beginning epoch 3
In grad_steps = 150, loss = 0.6487600803375244
In grad_steps = 151, loss = 0.6682946085929871
In grad_steps = 152, loss = 0.6569749712944031
In grad_steps = 153, loss = 0.6972277760505676
In grad_steps = 154, loss = 0.7166969776153564
In grad_steps = 155, loss = 0.7176952362060547
In grad_steps = 156, loss = 0.6867412328720093
In grad_steps = 157, loss = 0.6980000734329224
In grad_steps = 158, loss = 0.6719253063201904
In grad_steps = 159, loss = 0.6723949909210205
In grad_steps = 160, loss = 0.7197096347808838
In grad_steps = 161, loss = 0.7254290580749512
In grad_steps = 162, loss = 0.6625308990478516
In grad_steps = 163, loss = 0.7174237966537476
In grad_steps = 164, loss = 0.6993118524551392
In grad_steps = 165, loss = 0.6650947332382202
In grad_steps = 166, loss = 0.6911486387252808
In grad_steps = 167, loss = 0.6624471545219421
In grad_steps = 168, loss = 0.7050541639328003
In grad_steps = 169, loss = 0.6952391862869263
In grad_steps = 170, loss = 0.6873074173927307
In grad_steps = 171, loss = 0.6783909201622009
In grad_steps = 172, loss = 0.6796748638153076
In grad_steps = 173, loss = 0.6905071139335632
In grad_steps = 174, loss = 0.6725965738296509
In grad_steps = 175, loss = 0.6605507135391235
In grad_steps = 176, loss = 0.6817575693130493
In grad_steps = 177, loss = 0.6607738137245178
In grad_steps = 178, loss = 0.6414119005203247
In grad_steps = 179, loss = 0.5566279292106628
In grad_steps = 180, loss = 0.7323704957962036
In grad_steps = 181, loss = 0.6753811240196228
In grad_steps = 182, loss = 0.8306379914283752
In grad_steps = 183, loss = 0.8565393686294556
In grad_steps = 184, loss = 0.6499980092048645
In grad_steps = 185, loss = 0.7147766351699829
In grad_steps = 186, loss = 0.6729539036750793
In grad_steps = 187, loss = 0.6760403513908386
In grad_steps = 188, loss = 0.798424243927002
In grad_steps = 189, loss = 0.6598793268203735
In grad_steps = 190, loss = 0.6464274525642395
In grad_steps = 191, loss = 0.6914991140365601
In grad_steps = 192, loss = 0.6602713465690613
In grad_steps = 193, loss = 0.7013134360313416
In grad_steps = 194, loss = 0.6573485136032104
In grad_steps = 195, loss = 0.7160358428955078
In grad_steps = 196, loss = 0.7213863134384155
In grad_steps = 197, loss = 0.671674370765686
In grad_steps = 198, loss = 0.682891845703125
In grad_steps = 199, loss = 0.6986908316612244
In grad_steps = 200, loss = 0.6756912469863892
In grad_steps = 201, loss = 0.6840561628341675
In grad_steps = 202, loss = 0.6942260265350342
In grad_steps = 203, loss = 0.7079982757568359
In grad_steps = 204, loss = 0.7126728296279907
In grad_steps = 205, loss = 0.672993540763855
In grad_steps = 206, loss = 0.673408031463623
In grad_steps = 207, loss = 0.6836763620376587
In grad_steps = 208, loss = 0.6729806661605835
In grad_steps = 209, loss = 0.684583306312561
In grad_steps = 210, loss = 0.7023440003395081
In grad_steps = 211, loss = 0.6770135760307312
In grad_steps = 212, loss = 0.6972514390945435
In grad_steps = 213, loss = 0.6937210559844971
In grad_steps = 214, loss = 0.678167462348938
In grad_steps = 215, loss = 0.6870871782302856
In grad_steps = 216, loss = 0.6692581176757812
In grad_steps = 217, loss = 0.7135782241821289
In grad_steps = 218, loss = 0.6903117895126343
In grad_steps = 219, loss = 0.6940642595291138
In grad_steps = 220, loss = 0.7184386849403381
In grad_steps = 221, loss = 0.6369071006774902
In grad_steps = 222, loss = 0.7122655510902405
In grad_steps = 223, loss = 0.6767651438713074
In grad_steps = 224, loss = 0.6969817280769348
Beginning epoch 4
In grad_steps = 225, loss = 0.6285040974617004
In grad_steps = 226, loss = 0.6701228618621826
In grad_steps = 227, loss = 0.6484857201576233
In grad_steps = 228, loss = 0.6808627843856812
In grad_steps = 229, loss = 0.6944392919540405
In grad_steps = 230, loss = 0.6945368051528931
In grad_steps = 231, loss = 0.6623114943504333
In grad_steps = 232, loss = 0.7458354830741882
In grad_steps = 233, loss = 0.6501772403717041
In grad_steps = 234, loss = 0.6470608115196228
In grad_steps = 235, loss = 0.7312870025634766
In grad_steps = 236, loss = 0.7561932802200317
In grad_steps = 237, loss = 0.6495371460914612
In grad_steps = 238, loss = 0.6416110992431641
In grad_steps = 239, loss = 0.6655488014221191
In grad_steps = 240, loss = 0.6499847769737244
In grad_steps = 241, loss = 0.6881986856460571
In grad_steps = 242, loss = 0.6229644417762756
In grad_steps = 243, loss = 0.7251911759376526
In grad_steps = 244, loss = 0.7058547735214233
In grad_steps = 245, loss = 0.6463121771812439
In grad_steps = 246, loss = 0.6593393087387085
In grad_steps = 247, loss = 0.6301319599151611
In grad_steps = 248, loss = 0.6157028079032898
In grad_steps = 249, loss = 0.7075978517532349
In grad_steps = 250, loss = 0.6611276865005493
In grad_steps = 251, loss = 0.6035104393959045
In grad_steps = 252, loss = 0.6503475308418274
In grad_steps = 253, loss = 0.6207203269004822
In grad_steps = 254, loss = 0.6183984875679016
In grad_steps = 255, loss = 0.6385440230369568
In grad_steps = 256, loss = 0.6140105128288269
In grad_steps = 257, loss = 0.7751071453094482
In grad_steps = 258, loss = 0.6884292960166931
In grad_steps = 259, loss = 0.6884948015213013
In grad_steps = 260, loss = 0.5849868059158325
In grad_steps = 261, loss = 0.6114968061447144
In grad_steps = 262, loss = 0.6898857355117798
In grad_steps = 263, loss = 0.7718911170959473
In grad_steps = 264, loss = 0.6352855563163757
In grad_steps = 265, loss = 0.6151578426361084
In grad_steps = 266, loss = 0.6672462821006775
In grad_steps = 267, loss = 0.690962553024292
In grad_steps = 268, loss = 0.6612480878829956
In grad_steps = 269, loss = 0.6891072392463684
In grad_steps = 270, loss = 0.7649261355400085
In grad_steps = 271, loss = 0.7117795348167419
In grad_steps = 272, loss = 0.6267142295837402
In grad_steps = 273, loss = 0.6676522493362427
In grad_steps = 274, loss = 0.6178779602050781
In grad_steps = 275, loss = 0.7074342966079712
In grad_steps = 276, loss = 0.8294888138771057
In grad_steps = 277, loss = 0.5882392525672913
In grad_steps = 278, loss = 0.645331621170044
In grad_steps = 279, loss = 0.6815193295478821
In grad_steps = 280, loss = 0.6155666708946228
In grad_steps = 281, loss = 0.6469193696975708
In grad_steps = 282, loss = 0.6693645119667053
In grad_steps = 283, loss = 0.6957378387451172
In grad_steps = 284, loss = 0.679874062538147
In grad_steps = 285, loss = 0.683426022529602
In grad_steps = 286, loss = 0.61775803565979
In grad_steps = 287, loss = 0.6941893696784973
In grad_steps = 288, loss = 0.6558663845062256
In grad_steps = 289, loss = 0.625687301158905
In grad_steps = 290, loss = 0.675017237663269
In grad_steps = 291, loss = 0.6486713886260986
In grad_steps = 292, loss = 0.7332119345664978
In grad_steps = 293, loss = 0.6790847778320312
In grad_steps = 294, loss = 0.559428870677948
In grad_steps = 295, loss = 0.735234797000885
In grad_steps = 296, loss = 0.5387682914733887
In grad_steps = 297, loss = 0.6437391638755798
In grad_steps = 298, loss = 0.6713832020759583
In grad_steps = 299, loss = 0.6806791424751282
Beginning epoch 5
In grad_steps = 300, loss = 0.5368931293487549
In grad_steps = 301, loss = 0.5820837020874023
In grad_steps = 302, loss = 0.6797590255737305
In grad_steps = 303, loss = 0.6185742020606995
In grad_steps = 304, loss = 0.7806715369224548
In grad_steps = 305, loss = 0.736657977104187
In grad_steps = 306, loss = 0.5439296960830688
In grad_steps = 307, loss = 0.6613771915435791
In grad_steps = 308, loss = 0.5750643014907837
In grad_steps = 309, loss = 0.5467162728309631
In grad_steps = 310, loss = 0.7032608985900879
In grad_steps = 311, loss = 0.660682201385498
In grad_steps = 312, loss = 0.5608002543449402
In grad_steps = 313, loss = 0.5399048924446106
In grad_steps = 314, loss = 0.5837752223014832
In grad_steps = 315, loss = 0.6111568808555603
In grad_steps = 316, loss = 0.7062616944313049
In grad_steps = 317, loss = 0.5370002388954163
In grad_steps = 318, loss = 0.6526669859886169
In grad_steps = 319, loss = 0.6001177430152893
In grad_steps = 320, loss = 0.5348745584487915
In grad_steps = 321, loss = 0.6864117980003357
In grad_steps = 322, loss = 0.6510584950447083
In grad_steps = 323, loss = 0.6254549622535706
In grad_steps = 324, loss = 0.5737922787666321
In grad_steps = 325, loss = 0.47179409861564636
In grad_steps = 326, loss = 0.6287326216697693
In grad_steps = 327, loss = 0.5988393425941467
In grad_steps = 328, loss = 0.4367103576660156
In grad_steps = 329, loss = 0.45237284898757935
In grad_steps = 330, loss = 0.5360954999923706
In grad_steps = 331, loss = 0.6885727643966675
In grad_steps = 332, loss = 0.6332765817642212
In grad_steps = 333, loss = 0.4275880455970764
In grad_steps = 334, loss = 0.2967340350151062
In grad_steps = 335, loss = 0.26041239500045776
In grad_steps = 336, loss = 0.600344181060791
In grad_steps = 337, loss = 0.40061628818511963
In grad_steps = 338, loss = 0.4698915183544159
In grad_steps = 339, loss = 0.4668775200843811
In grad_steps = 340, loss = 0.44219544529914856
In grad_steps = 341, loss = 0.4057471752166748
In grad_steps = 342, loss = 0.46932417154312134
In grad_steps = 343, loss = 0.6705954670906067
In grad_steps = 344, loss = 0.8074361085891724
In grad_steps = 345, loss = 0.7951506972312927
In grad_steps = 346, loss = 0.5513412952423096
In grad_steps = 347, loss = 0.5994235277175903
In grad_steps = 348, loss = 0.6111820936203003
In grad_steps = 349, loss = 0.553970992565155
In grad_steps = 350, loss = 0.6200426816940308
In grad_steps = 351, loss = 0.6661206483840942
In grad_steps = 352, loss = 0.5487978458404541
In grad_steps = 353, loss = 0.5855666399002075
In grad_steps = 354, loss = 0.6122362613677979
In grad_steps = 355, loss = 0.549965500831604
In grad_steps = 356, loss = 0.5572002530097961
In grad_steps = 357, loss = 0.44529178738594055
In grad_steps = 358, loss = 0.6203137040138245
In grad_steps = 359, loss = 0.4942656457424164
In grad_steps = 360, loss = 0.42356860637664795
In grad_steps = 361, loss = 0.4107683002948761
In grad_steps = 362, loss = 0.36128321290016174
In grad_steps = 363, loss = 0.18744029104709625
In grad_steps = 364, loss = 0.6418522596359253
In grad_steps = 365, loss = 0.6227288842201233
In grad_steps = 366, loss = 0.3551197648048401
In grad_steps = 367, loss = 0.5176946520805359
In grad_steps = 368, loss = 0.3616848289966583
In grad_steps = 369, loss = 0.46096470952033997
In grad_steps = 370, loss = 0.5721907019615173
In grad_steps = 371, loss = 0.2267293930053711
In grad_steps = 372, loss = 0.3179168403148651
In grad_steps = 373, loss = 0.4554596543312073
In grad_steps = 374, loss = 0.3968977630138397
Beginning epoch 6
In grad_steps = 375, loss = 0.33892348408699036
In grad_steps = 376, loss = 0.2967491149902344
In grad_steps = 377, loss = 0.226033017039299
In grad_steps = 378, loss = 0.5523178577423096
In grad_steps = 379, loss = 0.9595850110054016
In grad_steps = 380, loss = 0.5400784611701965
In grad_steps = 381, loss = 0.15502150356769562
In grad_steps = 382, loss = 0.4485781490802765
In grad_steps = 383, loss = 0.2538565695285797
In grad_steps = 384, loss = 0.30373498797416687
In grad_steps = 385, loss = 0.30475276708602905
In grad_steps = 386, loss = 0.32818612456321716
In grad_steps = 387, loss = 0.21306216716766357
In grad_steps = 388, loss = 0.16483183205127716
In grad_steps = 389, loss = 0.2363608330488205
In grad_steps = 390, loss = 0.5966339707374573
In grad_steps = 391, loss = 0.45578888058662415
In grad_steps = 392, loss = 0.06573622673749924
In grad_steps = 393, loss = 0.5785178542137146
In grad_steps = 394, loss = 0.6297894716262817
In grad_steps = 395, loss = 0.28638985753059387
In grad_steps = 396, loss = 0.19286680221557617
In grad_steps = 397, loss = 0.24249902367591858
In grad_steps = 398, loss = 0.37642282247543335
In grad_steps = 399, loss = 0.414951354265213
In grad_steps = 400, loss = 0.23401299118995667
In grad_steps = 401, loss = 0.31812262535095215
In grad_steps = 402, loss = 0.40739914774894714
In grad_steps = 403, loss = 0.18938669562339783
In grad_steps = 404, loss = 0.44723421335220337
In grad_steps = 405, loss = 0.42065536975860596
In grad_steps = 406, loss = 0.3338926434516907
In grad_steps = 407, loss = 0.33393481373786926
In grad_steps = 408, loss = 0.9734816551208496
In grad_steps = 409, loss = 0.09700455516576767
In grad_steps = 410, loss = 0.14705239236354828
In grad_steps = 411, loss = 0.24500612914562225
In grad_steps = 412, loss = 0.5050477385520935
In grad_steps = 413, loss = 0.7259858250617981
In grad_steps = 414, loss = 0.31254294514656067
In grad_steps = 415, loss = 0.27459657192230225
In grad_steps = 416, loss = 0.12362123280763626
In grad_steps = 417, loss = 0.44110772013664246
In grad_steps = 418, loss = 0.5515407919883728
In grad_steps = 419, loss = 0.5646674036979675
In grad_steps = 420, loss = 0.5066876411437988
In grad_steps = 421, loss = 0.5020192265510559
In grad_steps = 422, loss = 0.3858848214149475
In grad_steps = 423, loss = 0.38748955726623535
In grad_steps = 424, loss = 0.3586120009422302
In grad_steps = 425, loss = 0.6096099615097046
In grad_steps = 426, loss = 1.028016448020935
In grad_steps = 427, loss = 0.3353048264980316
In grad_steps = 428, loss = 0.40611985325813293
In grad_steps = 429, loss = 0.4477550685405731
In grad_steps = 430, loss = 0.4153422713279724
In grad_steps = 431, loss = 0.32658347487449646
In grad_steps = 432, loss = 0.3232394754886627
In grad_steps = 433, loss = 0.4314481317996979
In grad_steps = 434, loss = 0.3403441607952118
In grad_steps = 435, loss = 0.35675400495529175
In grad_steps = 436, loss = 0.27775469422340393
In grad_steps = 437, loss = 0.20970577001571655
In grad_steps = 438, loss = 0.19660820066928864
In grad_steps = 439, loss = 0.1825396865606308
In grad_steps = 440, loss = 0.13060064613819122
In grad_steps = 441, loss = 0.35769718885421753
In grad_steps = 442, loss = 0.04240129888057709
In grad_steps = 443, loss = 0.489188015460968
In grad_steps = 444, loss = 0.34198811650276184
In grad_steps = 445, loss = 0.38548997044563293
In grad_steps = 446, loss = 0.4417407810688019
In grad_steps = 447, loss = 0.8898364305496216
In grad_steps = 448, loss = 0.3646804690361023
In grad_steps = 449, loss = 0.3121650218963623
Beginning epoch 7
In grad_steps = 450, loss = 0.2179749757051468
In grad_steps = 451, loss = 0.559126615524292
In grad_steps = 452, loss = 0.661311149597168
In grad_steps = 453, loss = 0.5459211468696594
In grad_steps = 454, loss = 0.2727162837982178
In grad_steps = 455, loss = 0.230228453874588
In grad_steps = 456, loss = 0.41652610898017883
In grad_steps = 457, loss = 0.2735579013824463
In grad_steps = 458, loss = 0.36428871750831604
In grad_steps = 459, loss = 0.4408959746360779
In grad_steps = 460, loss = 0.3414422869682312
In grad_steps = 461, loss = 0.2843793034553528
In grad_steps = 462, loss = 0.19363227486610413
In grad_steps = 463, loss = 0.16257084906101227
In grad_steps = 464, loss = 0.27745336294174194
In grad_steps = 465, loss = 0.40676450729370117
In grad_steps = 466, loss = 0.7986546158790588
In grad_steps = 467, loss = 0.2673949897289276
In grad_steps = 468, loss = 0.26612645387649536
In grad_steps = 469, loss = 0.24402616918087006
In grad_steps = 470, loss = 0.1536145657300949
In grad_steps = 471, loss = 0.20688696205615997
In grad_steps = 472, loss = 0.6175557971000671
In grad_steps = 473, loss = 0.3789028525352478
In grad_steps = 474, loss = 0.13005022704601288
In grad_steps = 475, loss = 0.21282823383808136
In grad_steps = 476, loss = 0.16210031509399414
In grad_steps = 477, loss = 0.44649040699005127
In grad_steps = 478, loss = 0.29101765155792236
In grad_steps = 479, loss = 0.22485321760177612
In grad_steps = 480, loss = 0.31170743703842163
In grad_steps = 481, loss = 0.149297297000885
In grad_steps = 482, loss = 0.15216737985610962
In grad_steps = 483, loss = 0.43190109729766846
In grad_steps = 484, loss = 0.26854708790779114
In grad_steps = 485, loss = 0.39638856053352356
In grad_steps = 486, loss = 0.4641008675098419
In grad_steps = 487, loss = 0.40013834834098816
In grad_steps = 488, loss = 0.14587406814098358
In grad_steps = 489, loss = 0.30748450756073
In grad_steps = 490, loss = 0.19916462898254395
In grad_steps = 491, loss = 0.22804352641105652
In grad_steps = 492, loss = 0.37906673550605774
In grad_steps = 493, loss = 0.588805615901947
In grad_steps = 494, loss = 0.6764923930168152
In grad_steps = 495, loss = 0.5043429732322693
In grad_steps = 496, loss = 0.33453628420829773
In grad_steps = 497, loss = 0.4725955128669739
In grad_steps = 498, loss = 0.3574318587779999
In grad_steps = 499, loss = 0.29259270429611206
In grad_steps = 500, loss = 0.3002127707004547
In grad_steps = 501, loss = 0.3030634820461273
In grad_steps = 502, loss = 0.3339824974536896
In grad_steps = 503, loss = 0.3159707486629486
In grad_steps = 504, loss = 0.21314163506031036
In grad_steps = 505, loss = 0.2189757525920868
In grad_steps = 506, loss = 0.24946823716163635
In grad_steps = 507, loss = 0.1389322131872177
In grad_steps = 508, loss = 0.25620096921920776
In grad_steps = 509, loss = 0.3649155795574188
In grad_steps = 510, loss = 0.14625729620456696
In grad_steps = 511, loss = 0.21136027574539185
In grad_steps = 512, loss = 0.6168519854545593
In grad_steps = 513, loss = 0.09912383556365967
In grad_steps = 514, loss = 0.043350279331207275
In grad_steps = 515, loss = 0.07290998846292496
In grad_steps = 516, loss = 0.20285238325595856
In grad_steps = 517, loss = 0.03524427115917206
In grad_steps = 518, loss = 0.2219303846359253
In grad_steps = 519, loss = 0.23057463765144348
In grad_steps = 520, loss = 0.09144627302885056
In grad_steps = 521, loss = 0.06096404418349266
In grad_steps = 522, loss = 0.025304114446043968
In grad_steps = 523, loss = 0.04068456590175629
In grad_steps = 524, loss = 0.3193724751472473
Beginning epoch 8
In grad_steps = 525, loss = 0.04071342945098877
In grad_steps = 526, loss = 0.10840222984552383
In grad_steps = 527, loss = 0.08435240387916565
In grad_steps = 528, loss = 0.46661195158958435
In grad_steps = 529, loss = 0.28736409544944763
In grad_steps = 530, loss = 0.04472581669688225
In grad_steps = 531, loss = 0.34206852316856384
In grad_steps = 532, loss = 0.518215537071228
In grad_steps = 533, loss = 0.24005930125713348
In grad_steps = 534, loss = 0.03281103074550629
In grad_steps = 535, loss = 0.1674000769853592
In grad_steps = 536, loss = 0.24574366211891174
In grad_steps = 537, loss = 0.1893489509820938
In grad_steps = 538, loss = 0.09799686819314957
In grad_steps = 539, loss = 0.18462792038917542
In grad_steps = 540, loss = 0.17110416293144226
In grad_steps = 541, loss = 0.25127923488616943
In grad_steps = 542, loss = 0.06950505822896957
In grad_steps = 543, loss = 0.2923565208911896
In grad_steps = 544, loss = 0.2672182321548462
In grad_steps = 545, loss = 0.11151658743619919
In grad_steps = 546, loss = 0.14576907455921173
In grad_steps = 547, loss = 0.14651517570018768
In grad_steps = 548, loss = 0.12269790470600128
In grad_steps = 549, loss = 0.03886279836297035
In grad_steps = 550, loss = 0.2196032553911209
In grad_steps = 551, loss = 0.014695728197693825
In grad_steps = 552, loss = 0.3096370995044708
In grad_steps = 553, loss = 0.00683301966637373
In grad_steps = 554, loss = 0.33475765585899353
In grad_steps = 555, loss = 0.3045477569103241
In grad_steps = 556, loss = 0.026802940294146538
In grad_steps = 557, loss = 0.04991542175412178
In grad_steps = 558, loss = 0.2829247713088989
In grad_steps = 559, loss = 0.020174188539385796
In grad_steps = 560, loss = 0.11119293421506882
In grad_steps = 561, loss = 0.220257967710495
In grad_steps = 562, loss = 0.1859566867351532
In grad_steps = 563, loss = 0.05751636624336243
In grad_steps = 564, loss = 0.12418366223573685
In grad_steps = 565, loss = 0.027659546583890915
In grad_steps = 566, loss = 0.012433590367436409
In grad_steps = 567, loss = 0.23896944522857666
In grad_steps = 568, loss = 0.10113576054573059
In grad_steps = 569, loss = 0.07999324053525925
In grad_steps = 570, loss = 0.09769061952829361
In grad_steps = 571, loss = 0.20196326076984406
In grad_steps = 572, loss = 0.10114035755395889
In grad_steps = 573, loss = 0.17348797619342804
In grad_steps = 574, loss = 0.017674455419182777
In grad_steps = 575, loss = 0.12175453454256058
In grad_steps = 576, loss = 0.3846115469932556
In grad_steps = 577, loss = 0.04984449967741966
In grad_steps = 578, loss = 0.1434735804796219
In grad_steps = 579, loss = 0.26502808928489685
In grad_steps = 580, loss = 0.021482178941369057
In grad_steps = 581, loss = 0.017938245087862015
In grad_steps = 582, loss = 0.03956037759780884
In grad_steps = 583, loss = 0.3015903830528259
In grad_steps = 584, loss = 0.053618334233760834
In grad_steps = 585, loss = 0.05487213283777237
In grad_steps = 586, loss = 0.08932030200958252
In grad_steps = 587, loss = 0.09525258839130402
In grad_steps = 588, loss = 0.26920995116233826
In grad_steps = 589, loss = 0.03260944038629532
In grad_steps = 590, loss = 0.03182900696992874
In grad_steps = 591, loss = 0.24200288951396942
In grad_steps = 592, loss = 0.030028335750102997
In grad_steps = 593, loss = 0.02684534713625908
In grad_steps = 594, loss = 0.1659877598285675
In grad_steps = 595, loss = 0.027546316385269165
In grad_steps = 596, loss = 0.12808774411678314
In grad_steps = 597, loss = 0.26292097568511963
In grad_steps = 598, loss = 0.0947052612900734
In grad_steps = 599, loss = 0.020152786746621132
Beginning epoch 9
In grad_steps = 600, loss = 0.026582958176732063
In grad_steps = 601, loss = 0.023542137816548347
In grad_steps = 602, loss = 0.029382431879639626
In grad_steps = 603, loss = 0.14042173326015472
In grad_steps = 604, loss = 0.7007892727851868
In grad_steps = 605, loss = 0.2744024991989136
In grad_steps = 606, loss = 0.013791163451969624
In grad_steps = 607, loss = 0.061063602566719055
In grad_steps = 608, loss = 0.015875214710831642
In grad_steps = 609, loss = 0.07314561307430267
In grad_steps = 610, loss = 0.019340259954333305
In grad_steps = 611, loss = 0.20260374248027802
In grad_steps = 612, loss = 0.03337586298584938
In grad_steps = 613, loss = 0.07446698099374771
In grad_steps = 614, loss = 0.030851511284708977
In grad_steps = 615, loss = 0.028324585407972336
In grad_steps = 616, loss = 0.19882239401340485
In grad_steps = 617, loss = 0.037888865917921066
In grad_steps = 618, loss = 0.12431763857603073
In grad_steps = 619, loss = 0.04654797911643982
In grad_steps = 620, loss = 0.1658082902431488
In grad_steps = 621, loss = 0.3331040143966675
In grad_steps = 622, loss = 0.05589736998081207
In grad_steps = 623, loss = 0.14172539114952087
In grad_steps = 624, loss = 0.07804610580205917
In grad_steps = 625, loss = 0.41371530294418335
In grad_steps = 626, loss = 0.3356565535068512
In grad_steps = 627, loss = 0.2457084208726883
In grad_steps = 628, loss = 0.1156531572341919
In grad_steps = 629, loss = 0.25263145565986633
In grad_steps = 630, loss = 0.05146978795528412
In grad_steps = 631, loss = 0.028521792963147163
In grad_steps = 632, loss = 0.04377586394548416
In grad_steps = 633, loss = 0.044208426028490067
In grad_steps = 634, loss = 0.017555896192789078
In grad_steps = 635, loss = 0.1700553297996521
In grad_steps = 636, loss = 0.03239326924085617
In grad_steps = 637, loss = 0.16847319900989532
In grad_steps = 638, loss = 0.3476504683494568
In grad_steps = 639, loss = 0.032296113669872284
In grad_steps = 640, loss = 0.21003982424736023
In grad_steps = 641, loss = 0.0167026836425066
In grad_steps = 642, loss = 0.1908247321844101
In grad_steps = 643, loss = 0.11461042612791061
In grad_steps = 644, loss = 0.17544884979724884
In grad_steps = 645, loss = 0.058567117899656296
In grad_steps = 646, loss = 0.07467792928218842
In grad_steps = 647, loss = 0.028253309428691864
In grad_steps = 648, loss = 0.21491843461990356
In grad_steps = 649, loss = 0.04351157322525978
In grad_steps = 650, loss = 0.2025115042924881
In grad_steps = 651, loss = 0.14611777663230896
In grad_steps = 652, loss = 0.020100388675928116
In grad_steps = 653, loss = 0.014494732022285461
In grad_steps = 654, loss = 0.006021875888109207
In grad_steps = 655, loss = 0.12055034190416336
In grad_steps = 656, loss = 0.013876817189157009
In grad_steps = 657, loss = 0.22299885749816895
In grad_steps = 658, loss = 0.04838738217949867
In grad_steps = 659, loss = 0.38932785391807556
In grad_steps = 660, loss = 0.10023748129606247
In grad_steps = 661, loss = 0.01170111820101738
In grad_steps = 662, loss = 0.05163118243217468
In grad_steps = 663, loss = 0.023882701992988586
In grad_steps = 664, loss = 0.01503921952098608
In grad_steps = 665, loss = 0.0701235830783844
In grad_steps = 666, loss = 0.011845111846923828
In grad_steps = 667, loss = 0.038284823298454285
In grad_steps = 668, loss = 0.27229568362236023
In grad_steps = 669, loss = 0.09568136930465698
In grad_steps = 670, loss = 0.10482672601938248
In grad_steps = 671, loss = 0.007439605891704559
In grad_steps = 672, loss = 0.05906519293785095
In grad_steps = 673, loss = 0.03918757289648056
In grad_steps = 674, loss = 0.004698961041867733
Beginning epoch 10
In grad_steps = 675, loss = 0.10223991423845291
In grad_steps = 676, loss = 0.02017109841108322
In grad_steps = 677, loss = 0.009921588003635406
In grad_steps = 678, loss = 0.08869774639606476
In grad_steps = 679, loss = 0.23323261737823486
In grad_steps = 680, loss = 0.005620669107884169
In grad_steps = 681, loss = 0.010460751131176949
In grad_steps = 682, loss = 0.5986546874046326
In grad_steps = 683, loss = 0.16357511281967163
In grad_steps = 684, loss = 0.010293388739228249
In grad_steps = 685, loss = 0.25049975514411926
In grad_steps = 686, loss = 0.10206307470798492
In grad_steps = 687, loss = 0.026949144899845123
In grad_steps = 688, loss = 0.16706587374210358
In grad_steps = 689, loss = 0.0423114039003849
In grad_steps = 690, loss = 0.032290779054164886
In grad_steps = 691, loss = 0.08429472893476486
In grad_steps = 692, loss = 0.023198502138257027
In grad_steps = 693, loss = 0.12312053143978119
In grad_steps = 694, loss = 0.38973772525787354
In grad_steps = 695, loss = 0.21160465478897095
In grad_steps = 696, loss = 0.05906267091631889
In grad_steps = 697, loss = 0.11687131226062775
In grad_steps = 698, loss = 0.14669057726860046
In grad_steps = 699, loss = 0.04856865480542183
In grad_steps = 700, loss = 0.029427852481603622
In grad_steps = 701, loss = 0.2381952702999115
In grad_steps = 702, loss = 0.03117447905242443
In grad_steps = 703, loss = 0.0246221162378788
In grad_steps = 704, loss = 0.09138977527618408
In grad_steps = 705, loss = 0.04090601205825806
In grad_steps = 706, loss = 0.08106135576963425
In grad_steps = 707, loss = 0.16841432452201843
In grad_steps = 708, loss = 0.5045405626296997
In grad_steps = 709, loss = 0.025522984564304352
In grad_steps = 710, loss = 0.007113988045603037
In grad_steps = 711, loss = 0.01585279032588005
In grad_steps = 712, loss = 0.011684167198836803
In grad_steps = 713, loss = 0.00820811465382576
In grad_steps = 714, loss = 0.05675617232918739
In grad_steps = 715, loss = 0.022212352603673935
In grad_steps = 716, loss = 0.012424425221979618
In grad_steps = 717, loss = 0.018684858456254005
In grad_steps = 718, loss = 0.040445562452077866
In grad_steps = 719, loss = 0.032465316355228424
In grad_steps = 720, loss = 0.08264835923910141
In grad_steps = 721, loss = 0.023456905037164688
In grad_steps = 722, loss = 0.018879009410738945
In grad_steps = 723, loss = 0.09660390019416809
In grad_steps = 724, loss = 0.0067679546773433685
In grad_steps = 725, loss = 0.022041095420718193
In grad_steps = 726, loss = 0.04492223262786865
In grad_steps = 727, loss = 0.0038868749979883432
In grad_steps = 728, loss = 0.004329892806708813
In grad_steps = 729, loss = 0.0013495937455445528
In grad_steps = 730, loss = 0.22837673127651215
In grad_steps = 731, loss = 0.07834099233150482
In grad_steps = 732, loss = 0.0013325669569894671
In grad_steps = 733, loss = 0.06037535145878792
In grad_steps = 734, loss = 0.0019008599920198321
In grad_steps = 735, loss = 0.0073509677313268185
In grad_steps = 736, loss = 0.02099926583468914
In grad_steps = 737, loss = 0.004625333473086357
In grad_steps = 738, loss = 0.0064211091957986355
In grad_steps = 739, loss = 0.002182233612984419
In grad_steps = 740, loss = 0.0016996923368424177
In grad_steps = 741, loss = 0.002595691941678524
In grad_steps = 742, loss = 0.001251954585313797
In grad_steps = 743, loss = 0.18202702701091766
In grad_steps = 744, loss = 0.07541657984256744
In grad_steps = 745, loss = 0.0924004539847374
In grad_steps = 746, loss = 0.0652148425579071
In grad_steps = 747, loss = 0.1419510543346405
In grad_steps = 748, loss = 0.00990685261785984
In grad_steps = 749, loss = 0.15174373984336853
Beginning epoch 11
In grad_steps = 750, loss = 0.004882278386503458
In grad_steps = 751, loss = 0.06452595442533493
In grad_steps = 752, loss = 0.0101627167314291
In grad_steps = 753, loss = 0.1616339236497879
In grad_steps = 754, loss = 0.16166548430919647
In grad_steps = 755, loss = 0.12149835377931595
In grad_steps = 756, loss = 0.3011019229888916
In grad_steps = 757, loss = 0.05873050540685654
In grad_steps = 758, loss = 0.03613496199250221
In grad_steps = 759, loss = 0.007352494169026613
In grad_steps = 760, loss = 0.07505194842815399
In grad_steps = 761, loss = 0.04705745354294777
In grad_steps = 762, loss = 0.00965535081923008
In grad_steps = 763, loss = 0.009835666045546532
In grad_steps = 764, loss = 0.11512163281440735
In grad_steps = 765, loss = 0.029501425102353096
In grad_steps = 766, loss = 0.015351720154285431
In grad_steps = 767, loss = 0.007794783916324377
In grad_steps = 768, loss = 0.10943222790956497
In grad_steps = 769, loss = 0.056848056614398956
In grad_steps = 770, loss = 0.20762719213962555
In grad_steps = 771, loss = 0.14591121673583984
In grad_steps = 772, loss = 0.15943197906017303
In grad_steps = 773, loss = 0.029582932591438293
In grad_steps = 774, loss = 0.024123873561620712
In grad_steps = 775, loss = 0.015596036799252033
In grad_steps = 776, loss = 0.16632936894893646
In grad_steps = 777, loss = 0.056853312999010086
In grad_steps = 778, loss = 0.05738845467567444
In grad_steps = 779, loss = 0.018694262951612473
In grad_steps = 780, loss = 0.09105880558490753
In grad_steps = 781, loss = 0.015110384672880173
In grad_steps = 782, loss = 0.02357313223183155
In grad_steps = 783, loss = 0.012958688661456108
In grad_steps = 784, loss = 0.011353670619428158
In grad_steps = 785, loss = 0.0038483645766973495
In grad_steps = 786, loss = 0.01992730423808098
In grad_steps = 787, loss = 0.01193299237638712
In grad_steps = 788, loss = 0.01138146873563528
In grad_steps = 789, loss = 0.11429571360349655
In grad_steps = 790, loss = 0.4893657863140106
In grad_steps = 791, loss = 0.11069297790527344
In grad_steps = 792, loss = 0.04571066051721573
In grad_steps = 793, loss = 0.0033913685474544764
In grad_steps = 794, loss = 0.3935338854789734
In grad_steps = 795, loss = 0.0016672143246978521
In grad_steps = 796, loss = 0.022091591730713844
In grad_steps = 797, loss = 0.013363349251449108
In grad_steps = 798, loss = 0.01566014438867569
In grad_steps = 799, loss = 0.06463968753814697
In grad_steps = 800, loss = 0.046719640493392944
In grad_steps = 801, loss = 0.08977798372507095
In grad_steps = 802, loss = 0.008187290281057358
In grad_steps = 803, loss = 0.15290188789367676
In grad_steps = 804, loss = 0.01317032240331173
In grad_steps = 805, loss = 0.0188951026648283
In grad_steps = 806, loss = 0.14728857576847076
In grad_steps = 807, loss = 0.0997164249420166
In grad_steps = 808, loss = 0.02808055840432644
In grad_steps = 809, loss = 0.17817296087741852
In grad_steps = 810, loss = 0.0716429278254509
In grad_steps = 811, loss = 0.44856274127960205
In grad_steps = 812, loss = 0.0666007474064827
In grad_steps = 813, loss = 0.01895558461546898
In grad_steps = 814, loss = 0.050359901040792465
In grad_steps = 815, loss = 0.08343453705310822
In grad_steps = 816, loss = 0.0590013712644577
In grad_steps = 817, loss = 0.025182433426380157
In grad_steps = 818, loss = 0.045854587107896805
In grad_steps = 819, loss = 0.01913784071803093
In grad_steps = 820, loss = 0.2456771582365036
In grad_steps = 821, loss = 0.0037278467789292336
In grad_steps = 822, loss = 0.020374182611703873
In grad_steps = 823, loss = 0.005851278081536293
In grad_steps = 824, loss = 0.008386443369090557
Beginning epoch 12
In grad_steps = 825, loss = 0.008166737854480743
In grad_steps = 826, loss = 0.0034243836998939514
In grad_steps = 827, loss = 0.37066230177879333
In grad_steps = 828, loss = 0.07315421104431152
In grad_steps = 829, loss = 0.19422616064548492
In grad_steps = 830, loss = 0.16651450097560883
In grad_steps = 831, loss = 0.011489180848002434
In grad_steps = 832, loss = 0.2201511114835739
In grad_steps = 833, loss = 0.14000533521175385
In grad_steps = 834, loss = 0.17504499852657318
In grad_steps = 835, loss = 0.022497383877635002
In grad_steps = 836, loss = 0.2745465636253357
In grad_steps = 837, loss = 0.32280245423316956
In grad_steps = 838, loss = 0.005850117187947035
In grad_steps = 839, loss = 0.02361473999917507
In grad_steps = 840, loss = 0.0525662936270237
In grad_steps = 841, loss = 0.28502073884010315
In grad_steps = 842, loss = 0.12789645791053772
In grad_steps = 843, loss = 0.21165092289447784
In grad_steps = 844, loss = 0.03548762947320938
In grad_steps = 845, loss = 0.22565285861492157
In grad_steps = 846, loss = 0.03294738009572029
In grad_steps = 847, loss = 0.038698360323905945
In grad_steps = 848, loss = 0.07218314707279205
In grad_steps = 849, loss = 0.04122554138302803
In grad_steps = 850, loss = 0.09012245386838913
In grad_steps = 851, loss = 0.028011348098516464
In grad_steps = 852, loss = 0.018114324659109116
In grad_steps = 853, loss = 0.018230808898806572
In grad_steps = 854, loss = 0.08334291726350784
In grad_steps = 855, loss = 0.02466229349374771
In grad_steps = 856, loss = 0.014668671414256096
In grad_steps = 857, loss = 0.026563610881567
In grad_steps = 858, loss = 0.015915963798761368
In grad_steps = 859, loss = 0.004252356477081776
In grad_steps = 860, loss = 0.006559372879564762
In grad_steps = 861, loss = 0.06918534636497498
In grad_steps = 862, loss = 0.03015209548175335
In grad_steps = 863, loss = 0.06817039847373962
In grad_steps = 864, loss = 0.20437218248844147
In grad_steps = 865, loss = 0.007823022082448006
In grad_steps = 866, loss = 0.001487423200160265
In grad_steps = 867, loss = 0.009455845691263676
In grad_steps = 868, loss = 0.025300363078713417
In grad_steps = 869, loss = 0.00935217086225748
In grad_steps = 870, loss = 0.4466462731361389
In grad_steps = 871, loss = 0.14565783739089966
In grad_steps = 872, loss = 0.2613278031349182
In grad_steps = 873, loss = 0.019537797197699547
In grad_steps = 874, loss = 0.4154030680656433
In grad_steps = 875, loss = 0.04220719635486603
In grad_steps = 876, loss = 0.04521431401371956
In grad_steps = 877, loss = 0.07359617948532104
In grad_steps = 878, loss = 0.042819585651159286
In grad_steps = 879, loss = 0.23577167093753815
In grad_steps = 880, loss = 0.023874647915363312
In grad_steps = 881, loss = 0.010372355580329895
In grad_steps = 882, loss = 0.008256767876446247
In grad_steps = 883, loss = 0.11826571822166443
In grad_steps = 884, loss = 0.02311372198164463
In grad_steps = 885, loss = 0.020087769255042076
In grad_steps = 886, loss = 0.026398830115795135
In grad_steps = 887, loss = 0.021832240745425224
In grad_steps = 888, loss = 0.050097640603780746
In grad_steps = 889, loss = 0.07835029810667038
In grad_steps = 890, loss = 0.052258238196372986
In grad_steps = 891, loss = 0.09750866889953613
In grad_steps = 892, loss = 0.028170466423034668
In grad_steps = 893, loss = 0.023448338732123375
In grad_steps = 894, loss = 0.030983449891209602
In grad_steps = 895, loss = 0.38528186082839966
In grad_steps = 896, loss = 0.2829155921936035
In grad_steps = 897, loss = 0.12571629881858826
In grad_steps = 898, loss = 0.019099868834018707
In grad_steps = 899, loss = 0.01611345075070858
Elapsed time: 3014.9963178634644 seconds for ensemble 4 with 12 epochs
LoRA instance 4 evaluation complete. Data saved to /pscratch/sd/t/tianle/lucid/other_source/SURP_2024/results/tmp_lora_ensemble/Llama3-8B-set-1.2/test_data_instance_4_seed_40373.npz.
lora instance i = 4 Successfully finished.
Final, Test average ensemble probabilities = 
[[2.13606358e-01 7.86393642e-01]
 [9.31843579e-01 6.81564361e-02]
 [9.93083477e-01 6.91651693e-03]
 [3.85537520e-02 9.61446166e-01]
 [1.56857073e-01 8.43142867e-01]
 [1.64154708e-01 8.35845292e-01]
 [1.03576062e-03 9.98964310e-01]
 [3.63437623e-01 6.36562467e-01]
 [3.98478270e-01 6.01521730e-01]
 [5.95324039e-02 9.40467536e-01]
 [9.95212913e-01 4.78705019e-03]
 [9.72535729e-01 2.74642650e-02]
 [6.95860386e-03 9.93041337e-01]
 [3.39654535e-01 6.60345435e-01]
 [9.28008080e-01 7.19919354e-02]
 [7.68744767e-01 2.31255218e-01]
 [8.07410121e-01 1.92589909e-01]
 [2.52340138e-01 7.47659862e-01]
 [4.70450103e-01 5.29549897e-01]
 [7.16869593e-01 2.83130407e-01]
 [9.95153069e-01 4.84696636e-03]
 [8.32412720e-01 1.67587236e-01]
 [2.76604712e-01 7.23395228e-01]
 [8.29346299e-01 1.70653716e-01]
 [9.97209847e-01 2.79016886e-03]
 [5.17192413e-04 9.99482810e-01]
 [8.09763372e-03 9.91902351e-01]
 [1.48214012e-01 8.51785958e-01]
 [5.90098083e-01 4.09901917e-01]
 [5.84335208e-01 4.15664732e-01]
 [6.54689670e-01 3.45310271e-01]
 [1.43111944e-01 8.56888115e-01]
 [6.32538736e-01 3.67461234e-01]
 [3.98418456e-01 6.01581573e-01]
 [6.50388062e-01 3.49611938e-01]
 [9.70761478e-01 2.92385221e-02]
 [6.71159178e-02 9.32884097e-01]
 [3.47089857e-01 6.52910113e-01]
 [4.13366616e-01 5.86633384e-01]
 [7.75644958e-01 2.24355027e-01]
 [4.14209753e-01 5.85790277e-01]
 [9.82585609e-01 1.74143296e-02]
 [4.21450436e-01 5.78549564e-01]
 [6.96484625e-01 3.03515375e-01]
 [1.24653235e-01 8.75346780e-01]
 [9.88885581e-01 1.11144427e-02]
 [6.01793408e-01 3.98206621e-01]
 [2.26049230e-01 7.73950756e-01]
 [9.96473312e-01 3.52666900e-03]
 [9.98526573e-01 1.47340144e-03]
 [5.93735754e-01 4.06264216e-01]
 [1.29311038e-02 9.87068951e-01]
 [9.94789481e-01 5.21045690e-03]
 [7.58492053e-01 2.41507962e-01]
 [9.17698979e-01 8.23010355e-02]
 [9.98897851e-01 1.10216881e-03]
 [1.99243814e-01 8.00756156e-01]
 [1.39009301e-03 9.98609900e-01]
 [3.34362656e-01 6.65637374e-01]
 [5.31118274e-01 4.68881756e-01]
 [6.51300013e-01 3.48699957e-01]
 [7.28334606e-01 2.71665365e-01]
 [7.81511188e-01 2.18488842e-01]
 [3.69814098e-01 6.30185902e-01]
 [8.37937534e-01 1.62062407e-01]
 [2.46357679e-01 7.53642380e-01]
 [8.04835141e-01 1.95164829e-01]
 [2.34748840e-01 7.65251160e-01]
 [8.03204715e-01 1.96795300e-01]
 [9.96204376e-01 3.79558280e-03]
 [7.18760610e-01 2.81239450e-01]
 [4.18237686e-01 5.81762314e-01]
 [5.66575646e-01 4.33424383e-01]
 [9.93005276e-01 6.99474430e-03]
 [7.44588017e-01 2.55411923e-01]
 [1.27112120e-03 9.98728871e-01]
 [5.47711313e-01 4.52288628e-01]
 [4.46564585e-01 5.53435445e-01]
 [5.24890097e-03 9.94751155e-01]
 [9.86088097e-01 1.39119523e-02]
 [4.89491075e-01 5.10508895e-01]
 [2.20517427e-01 7.79482543e-01]
 [5.28794348e-01 4.71205711e-01]
 [9.91200149e-01 8.79980437e-03]
 [2.39611432e-01 7.60388494e-01]
 [6.45697787e-02 9.35430169e-01]
 [8.14335495e-02 9.18566406e-01]
 [9.29948911e-02 9.07005131e-01]
 [9.98002708e-01 1.99728692e-03]
 [2.07193464e-01 7.92806506e-01]
 [7.12195754e-01 2.87804186e-01]
 [5.39376318e-01 4.60623741e-01]
 [5.11478245e-01 4.88521755e-01]
 [1.48127675e-02 9.85187232e-01]
 [1.39401168e-01 8.60598743e-01]
 [9.99185681e-01 8.14386643e-04]
 [5.02518475e-01 4.97481525e-01]
 [3.19051832e-01 6.80948138e-01]
 [4.81896818e-01 5.18103182e-01]
 [8.91620293e-02 9.10837948e-01]
 [1.88886464e-01 8.11113536e-01]
 [1.52580887e-01 8.47419083e-01]
 [8.93384457e-01 1.06615625e-01]
 [2.60053962e-01 7.39946008e-01]
 [5.89607537e-01 4.10392433e-01]
 [1.54749244e-01 8.45250726e-01]
 [3.80465508e-01 6.19534492e-01]
 [9.61439833e-02 9.03855979e-01]
 [5.30038178e-01 4.69961882e-01]
 [4.05364662e-01 5.94635308e-01]
 [6.91246927e-01 3.08753103e-01]
 [9.30070400e-01 6.99296221e-02]
 [9.27847683e-01 7.21523017e-02]
 [8.92161965e-01 1.07838020e-01]
 [3.13670561e-03 9.96863246e-01]
 [5.81547141e-01 4.18452889e-01]
 [3.47542204e-02 9.65245843e-01]
 [5.61058298e-02 9.43894207e-01]
 [7.53148735e-01 2.46851236e-01]
 [5.46072125e-01 4.53927845e-01]
 [1.17778361e-01 8.82221580e-01]
 [6.55744731e-01 3.44255269e-01]
 [9.74115729e-01 2.58842651e-02]
 [7.91553199e-01 2.08446831e-01]
 [9.26294684e-01 7.37052038e-02]
 [3.34057957e-01 6.65942073e-01]
 [1.03887439e-01 8.96112561e-01]
 [8.74173820e-01 1.25826240e-01]
 [5.73744178e-01 4.26255792e-01]
 [7.68391609e-01 2.31608391e-01]
 [7.59212882e-04 9.99240756e-01]
 [1.27626747e-01 8.72373223e-01]
 [3.12359426e-02 9.68764126e-01]
 [1.63819700e-01 8.36180329e-01]]
Accuracy: 0.5224
MCC: 0.0448
AUC: 0.5723
Confusion Matrix:
tensor([[36, 31],
        [33, 34]])
Specificity: 0.5373
Precision (Macro): 0.5224
F1 Score (Macro): 0.5223
Expected Calibration Error (ECE): 0.2886
NLL loss: 1.0882
Ensemble evaluation complete.
