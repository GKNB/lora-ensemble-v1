Job is running on partition: gpu_ss11
Job is running under account: m2616_g
/pscratch/sd/t/tianle/conda/envs/lucid_surp_2024/lib/python3.11/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:10<00:30, 10.25s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:20<00:20, 10.32s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:30<00:10, 10.22s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:33<00:00,  7.12s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:33<00:00,  8.27s/it]
Llama3 has been loaded successfully.
Namespace(model_name='Llama3', config='/pscratch/sd/t/tianle/lucid/other_source/SURP_2024/src/config.json', repo_dir='/pscratch/sd/t/tianle/lucid/other_source/SURP_2024/', dataset='6', n_ensemble=5, seed=1, use_model_snapshot=False)
self.num_epochs = 10, self.batch_size = 16, self.max_length = 50
Model:  LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(128256, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)
          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)
          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)
          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)
)
Train dataset looks like:  Dataset({
    features: ['question', 'answer'],
    num_rows: 1679
})
Test dataset looks like:  Dataset({
    features: ['question', 'answer'],
    num_rows: 187
})
Training lora instance 0
DEBUG: Total parameters: 8037076992
DEBUG: Trainable parameters: 6815744
Beginning epoch 1
In grad_steps = 0, loss = 0.8738614320755005
In grad_steps = 1, loss = 0.7858085036277771
In grad_steps = 2, loss = 0.73335862159729
In grad_steps = 3, loss = 0.6991602182388306
In grad_steps = 4, loss = 0.7503541707992554
In grad_steps = 5, loss = 0.6921828389167786
In grad_steps = 6, loss = 0.712441086769104
In grad_steps = 7, loss = 0.6604147553443909
In grad_steps = 8, loss = 0.7004908919334412
In grad_steps = 9, loss = 0.6523058414459229
In grad_steps = 10, loss = 0.6771184206008911
In grad_steps = 11, loss = 0.5789357423782349
In grad_steps = 12, loss = 0.6808433532714844
In grad_steps = 13, loss = 0.7725309133529663
In grad_steps = 14, loss = 0.6151409149169922
In grad_steps = 15, loss = 0.6353400945663452
In grad_steps = 16, loss = 0.6810903549194336
In grad_steps = 17, loss = 0.5386177897453308
In grad_steps = 18, loss = 0.6489465832710266
In grad_steps = 19, loss = 0.4939191937446594
In grad_steps = 20, loss = 0.5883400440216064
In grad_steps = 21, loss = 0.5664366483688354
In grad_steps = 22, loss = 0.5536001324653625
In grad_steps = 23, loss = 0.3540407717227936
In grad_steps = 24, loss = 0.5687991380691528
In grad_steps = 25, loss = 0.7173113822937012
In grad_steps = 26, loss = 0.5848780870437622
In grad_steps = 27, loss = 0.3682883679866791
In grad_steps = 28, loss = 0.2682185471057892
In grad_steps = 29, loss = 0.40126755833625793
In grad_steps = 30, loss = 0.6423031687736511
In grad_steps = 31, loss = 0.3106860816478729
In grad_steps = 32, loss = 0.7112439870834351
In grad_steps = 33, loss = 0.46646538376808167
In grad_steps = 34, loss = 0.7013260722160339
In grad_steps = 35, loss = 0.2190711498260498
In grad_steps = 36, loss = 0.34621086716651917
In grad_steps = 37, loss = 0.549419641494751
In grad_steps = 38, loss = 0.6866447925567627
In grad_steps = 39, loss = 0.2919502258300781
In grad_steps = 40, loss = 0.514380931854248
In grad_steps = 41, loss = 0.6016927361488342
In grad_steps = 42, loss = 0.3682587146759033
In grad_steps = 43, loss = 0.3266047537326813
In grad_steps = 44, loss = 0.2924330234527588
In grad_steps = 45, loss = 0.5370951890945435
In grad_steps = 46, loss = 0.2817944884300232
In grad_steps = 47, loss = 0.3481733202934265
In grad_steps = 48, loss = 0.31758251786231995
In grad_steps = 49, loss = 0.1338071972131729
In grad_steps = 50, loss = 0.76229327917099
In grad_steps = 51, loss = 0.13218198716640472
In grad_steps = 52, loss = 0.29035964608192444
In grad_steps = 53, loss = 0.5652503967285156
In grad_steps = 54, loss = 0.3387647569179535
In grad_steps = 55, loss = 0.374440461397171
In grad_steps = 56, loss = 0.21676816046237946
In grad_steps = 57, loss = 0.23990267515182495
In grad_steps = 58, loss = 0.2530667781829834
In grad_steps = 59, loss = 0.2550203204154968
In grad_steps = 60, loss = 0.5190436840057373
In grad_steps = 61, loss = 0.3148772418498993
In grad_steps = 62, loss = 0.2378973811864853
In grad_steps = 63, loss = 0.27411729097366333
In grad_steps = 64, loss = 0.14423143863677979
In grad_steps = 65, loss = 0.2345542162656784
In grad_steps = 66, loss = 0.35429227352142334
In grad_steps = 67, loss = 0.38113850355148315
In grad_steps = 68, loss = 0.23981162905693054
In grad_steps = 69, loss = 0.34787920117378235
In grad_steps = 70, loss = 0.34968671202659607
In grad_steps = 71, loss = 0.1745358556509018
In grad_steps = 72, loss = 0.422773152589798
In grad_steps = 73, loss = 0.5218620896339417
In grad_steps = 74, loss = 0.25870054960250854
In grad_steps = 75, loss = 0.16344544291496277
In grad_steps = 76, loss = 0.39024844765663147
In grad_steps = 77, loss = 0.10395798087120056
In grad_steps = 78, loss = 0.1520891785621643
In grad_steps = 79, loss = 0.21672865748405457
In grad_steps = 80, loss = 0.30903929471969604
In grad_steps = 81, loss = 0.3364235460758209
In grad_steps = 82, loss = 0.12451057881116867
In grad_steps = 83, loss = 0.44352808594703674
In grad_steps = 84, loss = 0.2466466724872589
In grad_steps = 85, loss = 0.056922901421785355
In grad_steps = 86, loss = 0.3119717836380005
In grad_steps = 87, loss = 0.9329811334609985
In grad_steps = 88, loss = 0.7237628698348999
In grad_steps = 89, loss = 0.3910984694957733
In grad_steps = 90, loss = 0.3202689290046692
In grad_steps = 91, loss = 0.31383055448532104
In grad_steps = 92, loss = 0.2462792992591858
In grad_steps = 93, loss = 0.2996562421321869
In grad_steps = 94, loss = 0.23004722595214844
In grad_steps = 95, loss = 0.30990496277809143
In grad_steps = 96, loss = 0.4363183379173279
In grad_steps = 97, loss = 0.3053479790687561
In grad_steps = 98, loss = 0.5590818524360657
In grad_steps = 99, loss = 0.13520613312721252
In grad_steps = 100, loss = 0.19036751985549927
In grad_steps = 101, loss = 0.12082261592149734
In grad_steps = 102, loss = 0.1452345997095108
In grad_steps = 103, loss = 0.6587988138198853
In grad_steps = 104, loss = 0.2594473361968994
Beginning epoch 2
In grad_steps = 105, loss = 0.2580469846725464
In grad_steps = 106, loss = 0.33904433250427246
In grad_steps = 107, loss = 0.2663460671901703
In grad_steps = 108, loss = 0.21669445931911469
In grad_steps = 109, loss = 0.4932304620742798
In grad_steps = 110, loss = 0.3708934783935547
In grad_steps = 111, loss = 0.297526478767395
In grad_steps = 112, loss = 0.14569100737571716
In grad_steps = 113, loss = 0.18791811168193817
In grad_steps = 114, loss = 0.15149469673633575
In grad_steps = 115, loss = 0.217295840382576
In grad_steps = 116, loss = 0.15466025471687317
In grad_steps = 117, loss = 0.25301605463027954
In grad_steps = 118, loss = 0.8325228095054626
In grad_steps = 119, loss = 0.11304798722267151
In grad_steps = 120, loss = 0.22338832914829254
In grad_steps = 121, loss = 0.05652455613017082
In grad_steps = 122, loss = 0.20681039988994598
In grad_steps = 123, loss = 0.09999893605709076
In grad_steps = 124, loss = 0.14443837106227875
In grad_steps = 125, loss = 0.43210896849632263
In grad_steps = 126, loss = 0.11289365589618683
In grad_steps = 127, loss = 0.10824831575155258
In grad_steps = 128, loss = 0.12933297455310822
In grad_steps = 129, loss = 0.23548898100852966
In grad_steps = 130, loss = 0.21561098098754883
In grad_steps = 131, loss = 0.15823736786842346
In grad_steps = 132, loss = 0.19265705347061157
In grad_steps = 133, loss = 0.10185472667217255
In grad_steps = 134, loss = 0.10367796570062637
In grad_steps = 135, loss = 0.11260883510112762
In grad_steps = 136, loss = 0.030362525954842567
In grad_steps = 137, loss = 0.14402402937412262
In grad_steps = 138, loss = 0.08459371328353882
In grad_steps = 139, loss = 0.24799928069114685
In grad_steps = 140, loss = 0.012663043104112148
In grad_steps = 141, loss = 0.07286971807479858
In grad_steps = 142, loss = 0.26242297887802124
In grad_steps = 143, loss = 0.43661409616470337
In grad_steps = 144, loss = 0.2359549105167389
In grad_steps = 145, loss = 0.08196794241666794
In grad_steps = 146, loss = 0.29544559121131897
In grad_steps = 147, loss = 0.01971478760242462
In grad_steps = 148, loss = 0.19107504189014435
In grad_steps = 149, loss = 0.046550821512937546
In grad_steps = 150, loss = 0.19200506806373596
In grad_steps = 151, loss = 0.25159770250320435
In grad_steps = 152, loss = 0.34472355246543884
In grad_steps = 153, loss = 0.061086732894182205
In grad_steps = 154, loss = 0.03307613730430603
In grad_steps = 155, loss = 0.19671736657619476
In grad_steps = 156, loss = 0.17986008524894714
In grad_steps = 157, loss = 0.2957104742527008
In grad_steps = 158, loss = 0.25371262431144714
In grad_steps = 159, loss = 0.1379871666431427
In grad_steps = 160, loss = 0.09375815838575363
In grad_steps = 161, loss = 0.05143502354621887
In grad_steps = 162, loss = 0.09042300283908844
In grad_steps = 163, loss = 0.10661716014146805
In grad_steps = 164, loss = 0.07758715748786926
In grad_steps = 165, loss = 0.221276193857193
In grad_steps = 166, loss = 0.06760784983634949
In grad_steps = 167, loss = 0.08067168295383453
In grad_steps = 168, loss = 0.07784245163202286
In grad_steps = 169, loss = 0.026776446029543877
In grad_steps = 170, loss = 0.026820464059710503
In grad_steps = 171, loss = 0.09537900239229202
In grad_steps = 172, loss = 0.05364471301436424
In grad_steps = 173, loss = 0.023924585431814194
In grad_steps = 174, loss = 0.01464827824383974
In grad_steps = 175, loss = 0.08046697825193405
In grad_steps = 176, loss = 0.04021435230970383
In grad_steps = 177, loss = 0.1306886076927185
In grad_steps = 178, loss = 0.6869391798973083
In grad_steps = 179, loss = 0.16487592458724976
In grad_steps = 180, loss = 0.006186558865010738
In grad_steps = 181, loss = 0.12431038916110992
In grad_steps = 182, loss = 0.010424099862575531
In grad_steps = 183, loss = 0.007886822335422039
In grad_steps = 184, loss = 0.05983524024486542
In grad_steps = 185, loss = 0.04185214266180992
In grad_steps = 186, loss = 0.28476381301879883
In grad_steps = 187, loss = 0.0091398311778903
In grad_steps = 188, loss = 0.03509445860981941
In grad_steps = 189, loss = 0.1497461199760437
In grad_steps = 190, loss = 0.03367476537823677
In grad_steps = 191, loss = 0.0036263505462557077
In grad_steps = 192, loss = 0.17551222443580627
In grad_steps = 193, loss = 0.3175515830516815
In grad_steps = 194, loss = 0.0944724902510643
In grad_steps = 195, loss = 0.17972904443740845
In grad_steps = 196, loss = 0.2309867888689041
In grad_steps = 197, loss = 0.05836033821105957
In grad_steps = 198, loss = 0.12486565858125687
In grad_steps = 199, loss = 0.32983654737472534
In grad_steps = 200, loss = 0.20159302651882172
In grad_steps = 201, loss = 0.2507302761077881
In grad_steps = 202, loss = 0.15636226534843445
In grad_steps = 203, loss = 0.2678758203983307
In grad_steps = 204, loss = 0.06261322647333145
In grad_steps = 205, loss = 0.1564607173204422
In grad_steps = 206, loss = 0.027943382039666176
In grad_steps = 207, loss = 0.10840228945016861
In grad_steps = 208, loss = 0.15548518300056458
In grad_steps = 209, loss = 0.08719373494386673
Beginning epoch 3
In grad_steps = 210, loss = 0.1791197955608368
In grad_steps = 211, loss = 0.04264368489384651
In grad_steps = 212, loss = 0.14856038987636566
In grad_steps = 213, loss = 0.16774135828018188
In grad_steps = 214, loss = 0.23238036036491394
In grad_steps = 215, loss = 0.019893692806363106
In grad_steps = 216, loss = 0.11754310876131058
In grad_steps = 217, loss = 0.06118186190724373
In grad_steps = 218, loss = 0.11232564598321915
In grad_steps = 219, loss = 0.03340708836913109
In grad_steps = 220, loss = 0.23322100937366486
In grad_steps = 221, loss = 0.10180900990962982
In grad_steps = 222, loss = 0.14311158657073975
In grad_steps = 223, loss = 0.4503651261329651
In grad_steps = 224, loss = 0.06789693236351013
In grad_steps = 225, loss = 0.15289759635925293
In grad_steps = 226, loss = 0.007258645724505186
In grad_steps = 227, loss = 0.1605791598558426
In grad_steps = 228, loss = 0.14974956214427948
In grad_steps = 229, loss = 0.015186989679932594
In grad_steps = 230, loss = 0.029846318066120148
In grad_steps = 231, loss = 0.06270003318786621
In grad_steps = 232, loss = 0.03229604288935661
In grad_steps = 233, loss = 0.11431325227022171
In grad_steps = 234, loss = 0.42873552441596985
In grad_steps = 235, loss = 0.03185555711388588
In grad_steps = 236, loss = 0.23706869781017303
In grad_steps = 237, loss = 0.26024627685546875
In grad_steps = 238, loss = 0.2050004005432129
In grad_steps = 239, loss = 0.08988069742918015
In grad_steps = 240, loss = 0.010648633353412151
In grad_steps = 241, loss = 0.04705631732940674
In grad_steps = 242, loss = 0.0359555184841156
In grad_steps = 243, loss = 0.07930415123701096
In grad_steps = 244, loss = 0.08195973187685013
In grad_steps = 245, loss = 0.0069139511324465275
In grad_steps = 246, loss = 0.0570380836725235
In grad_steps = 247, loss = 0.10030177235603333
In grad_steps = 248, loss = 0.14783214032649994
In grad_steps = 249, loss = 0.1232345849275589
In grad_steps = 250, loss = 0.019293248653411865
In grad_steps = 251, loss = 0.2673935890197754
In grad_steps = 252, loss = 0.01426792610436678
In grad_steps = 253, loss = 0.013733438216149807
In grad_steps = 254, loss = 0.00803445465862751
In grad_steps = 255, loss = 0.15934593975543976
In grad_steps = 256, loss = 0.25400519371032715
In grad_steps = 257, loss = 0.22523251175880432
In grad_steps = 258, loss = 0.05507636070251465
In grad_steps = 259, loss = 0.050204500555992126
In grad_steps = 260, loss = 0.0053740558214485645
In grad_steps = 261, loss = 0.12450293451547623
In grad_steps = 262, loss = 0.139488086104393
In grad_steps = 263, loss = 0.06190172955393791
In grad_steps = 264, loss = 0.31523585319519043
In grad_steps = 265, loss = 0.052977610379457474
In grad_steps = 266, loss = 0.06905968487262726
In grad_steps = 267, loss = 0.008946527726948261
In grad_steps = 268, loss = 0.03257251903414726
In grad_steps = 269, loss = 0.025175083428621292
In grad_steps = 270, loss = 0.09765979647636414
In grad_steps = 271, loss = 0.03588680550456047
In grad_steps = 272, loss = 0.0356181338429451
In grad_steps = 273, loss = 0.041247278451919556
In grad_steps = 274, loss = 0.008634178899228573
In grad_steps = 275, loss = 0.08322399109601974
In grad_steps = 276, loss = 0.04728992283344269
In grad_steps = 277, loss = 0.017559276893734932
In grad_steps = 278, loss = 0.2815472185611725
In grad_steps = 279, loss = 0.009250873699784279
In grad_steps = 280, loss = 0.005482237320393324
In grad_steps = 281, loss = 0.006824237760156393
In grad_steps = 282, loss = 0.323520302772522
In grad_steps = 283, loss = 0.2990516126155853
In grad_steps = 284, loss = 0.007409066893160343
In grad_steps = 285, loss = 0.01836100034415722
In grad_steps = 286, loss = 0.04516158998012543
In grad_steps = 287, loss = 0.006500623654574156
In grad_steps = 288, loss = 0.08070853352546692
In grad_steps = 289, loss = 0.11378829926252365
In grad_steps = 290, loss = 0.01945473439991474
In grad_steps = 291, loss = 0.12772782146930695
In grad_steps = 292, loss = 0.010943221859633923
In grad_steps = 293, loss = 0.008544550277292728
In grad_steps = 294, loss = 0.07016192376613617
In grad_steps = 295, loss = 0.007610061205923557
In grad_steps = 296, loss = 0.007640938740223646
In grad_steps = 297, loss = 0.02774078957736492
In grad_steps = 298, loss = 0.21723493933677673
In grad_steps = 299, loss = 0.023352906107902527
In grad_steps = 300, loss = 0.08551885187625885
In grad_steps = 301, loss = 0.05462180823087692
In grad_steps = 302, loss = 0.004612892400473356
In grad_steps = 303, loss = 0.027596522122621536
In grad_steps = 304, loss = 0.6106798052787781
In grad_steps = 305, loss = 0.058447469025850296
In grad_steps = 306, loss = 0.12095380574464798
In grad_steps = 307, loss = 0.01558595709502697
In grad_steps = 308, loss = 0.6523383259773254
In grad_steps = 309, loss = 0.0035607577301561832
In grad_steps = 310, loss = 0.01880710758268833
In grad_steps = 311, loss = 0.04563131928443909
In grad_steps = 312, loss = 0.27045586705207825
In grad_steps = 313, loss = 0.13271194696426392
In grad_steps = 314, loss = 0.07593898475170135
Beginning epoch 4
In grad_steps = 315, loss = 0.0701785758137703
In grad_steps = 316, loss = 0.02641259878873825
In grad_steps = 317, loss = 0.107963927090168
In grad_steps = 318, loss = 0.0995878130197525
In grad_steps = 319, loss = 0.04905754327774048
In grad_steps = 320, loss = 0.056556735187768936
In grad_steps = 321, loss = 0.12483592331409454
In grad_steps = 322, loss = 0.04261648654937744
In grad_steps = 323, loss = 0.10951098799705505
In grad_steps = 324, loss = 0.03496300056576729
In grad_steps = 325, loss = 0.038175299763679504
In grad_steps = 326, loss = 0.035587847232818604
In grad_steps = 327, loss = 0.04103415459394455
In grad_steps = 328, loss = 0.32476988434791565
In grad_steps = 329, loss = 0.001451435498893261
In grad_steps = 330, loss = 0.009298307821154594
In grad_steps = 331, loss = 0.0034990354906767607
In grad_steps = 332, loss = 0.023611465469002724
In grad_steps = 333, loss = 0.006497558671981096
In grad_steps = 334, loss = 0.004705496598035097
In grad_steps = 335, loss = 0.01644575409591198
In grad_steps = 336, loss = 0.08003285527229309
In grad_steps = 337, loss = 0.00864709634333849
In grad_steps = 338, loss = 0.10136520117521286
In grad_steps = 339, loss = 0.33621081709861755
In grad_steps = 340, loss = 0.010303642600774765
In grad_steps = 341, loss = 0.039665013551712036
In grad_steps = 342, loss = 0.18727165460586548
In grad_steps = 343, loss = 0.03058265522122383
In grad_steps = 344, loss = 0.05140490084886551
In grad_steps = 345, loss = 0.041393913328647614
In grad_steps = 346, loss = 0.007380293682217598
In grad_steps = 347, loss = 0.023144103586673737
In grad_steps = 348, loss = 0.004552347119897604
In grad_steps = 349, loss = 0.27121666073799133
In grad_steps = 350, loss = 0.0033648882526904345
In grad_steps = 351, loss = 0.013147038407623768
In grad_steps = 352, loss = 0.17799879610538483
In grad_steps = 353, loss = 0.05590255558490753
In grad_steps = 354, loss = 0.058493152260780334
In grad_steps = 355, loss = 0.0266733281314373
In grad_steps = 356, loss = 0.10227834433317184
In grad_steps = 357, loss = 0.0024577376898378134
In grad_steps = 358, loss = 0.1435600221157074
In grad_steps = 359, loss = 0.0034199205692857504
In grad_steps = 360, loss = 0.007655954919755459
In grad_steps = 361, loss = 0.0434221588075161
In grad_steps = 362, loss = 0.024403853341937065
In grad_steps = 363, loss = 0.019115494564175606
In grad_steps = 364, loss = 0.03265403211116791
In grad_steps = 365, loss = 0.006285515613853931
In grad_steps = 366, loss = 0.503420889377594
In grad_steps = 367, loss = 0.014690278097987175
In grad_steps = 368, loss = 0.01828482560813427
In grad_steps = 369, loss = 0.05972382053732872
In grad_steps = 370, loss = 0.018897976726293564
In grad_steps = 371, loss = 0.009317279793322086
In grad_steps = 372, loss = 0.06959854066371918
In grad_steps = 373, loss = 0.04347677528858185
In grad_steps = 374, loss = 0.09369311481714249
In grad_steps = 375, loss = 0.4108763635158539
In grad_steps = 376, loss = 0.16973625123500824
In grad_steps = 377, loss = 0.2093922346830368
In grad_steps = 378, loss = 0.01135045662522316
In grad_steps = 379, loss = 0.0167187862098217
In grad_steps = 380, loss = 0.028931131586432457
In grad_steps = 381, loss = 0.02211569994688034
In grad_steps = 382, loss = 0.017692726105451584
In grad_steps = 383, loss = 0.10940641909837723
In grad_steps = 384, loss = 0.15195399522781372
In grad_steps = 385, loss = 0.22593213617801666
In grad_steps = 386, loss = 0.014375568367540836
In grad_steps = 387, loss = 0.059561602771282196
In grad_steps = 388, loss = 0.10636191070079803
In grad_steps = 389, loss = 0.026704680174589157
In grad_steps = 390, loss = 0.11586154997348785
In grad_steps = 391, loss = 0.005649458151310682
In grad_steps = 392, loss = 0.008240262977778912
In grad_steps = 393, loss = 0.0050079962238669395
In grad_steps = 394, loss = 0.04532341659069061
In grad_steps = 395, loss = 0.41893476247787476
In grad_steps = 396, loss = 0.25998446345329285
In grad_steps = 397, loss = 0.004702783655375242
In grad_steps = 398, loss = 0.002775458386167884
In grad_steps = 399, loss = 0.01738481968641281
In grad_steps = 400, loss = 0.002582645509392023
In grad_steps = 401, loss = 0.0010547626297920942
In grad_steps = 402, loss = 0.026609256863594055
In grad_steps = 403, loss = 0.12573029100894928
In grad_steps = 404, loss = 0.03560273349285126
In grad_steps = 405, loss = 0.12495219707489014
In grad_steps = 406, loss = 0.04048600047826767
In grad_steps = 407, loss = 0.08300845324993134
In grad_steps = 408, loss = 0.08884517848491669
In grad_steps = 409, loss = 0.01231464184820652
In grad_steps = 410, loss = 0.013690237887203693
In grad_steps = 411, loss = 0.018108773976564407
In grad_steps = 412, loss = 0.024277452379465103
In grad_steps = 413, loss = 0.00735457194969058
In grad_steps = 414, loss = 0.009019563905894756
In grad_steps = 415, loss = 0.0032376295421272516
In grad_steps = 416, loss = 0.0014254385605454445
In grad_steps = 417, loss = 0.017726294696331024
In grad_steps = 418, loss = 0.15496690571308136
In grad_steps = 419, loss = 0.0025746123865246773
Beginning epoch 5
In grad_steps = 420, loss = 0.290077805519104
In grad_steps = 421, loss = 0.0014918100787326694
In grad_steps = 422, loss = 0.0017170999199151993
In grad_steps = 423, loss = 0.13785536587238312
In grad_steps = 424, loss = 0.0032551323529332876
In grad_steps = 425, loss = 0.002032181015238166
In grad_steps = 426, loss = 0.025814004242420197
In grad_steps = 427, loss = 0.026572061702609062
In grad_steps = 428, loss = 0.6061593294143677
In grad_steps = 429, loss = 0.01228372287005186
In grad_steps = 430, loss = 0.05625312030315399
In grad_steps = 431, loss = 0.03142008185386658
In grad_steps = 432, loss = 0.006328281480818987
In grad_steps = 433, loss = 0.3432285785675049
In grad_steps = 434, loss = 0.02721737138926983
In grad_steps = 435, loss = 0.07376408576965332
In grad_steps = 436, loss = 0.003602699376642704
In grad_steps = 437, loss = 0.19802911579608917
In grad_steps = 438, loss = 0.36337777972221375
In grad_steps = 439, loss = 0.040289297699928284
In grad_steps = 440, loss = 0.08684314787387848
In grad_steps = 441, loss = 0.06464020907878876
In grad_steps = 442, loss = 0.09842029213905334
In grad_steps = 443, loss = 0.1454293131828308
In grad_steps = 444, loss = 0.18552857637405396
In grad_steps = 445, loss = 0.013666285201907158
In grad_steps = 446, loss = 0.08213698118925095
In grad_steps = 447, loss = 0.04614158347249031
In grad_steps = 448, loss = 0.16768011450767517
In grad_steps = 449, loss = 0.1527225375175476
In grad_steps = 450, loss = 0.2117612659931183
In grad_steps = 451, loss = 0.046441443264484406
In grad_steps = 452, loss = 0.07177489250898361
In grad_steps = 453, loss = 0.02244872972369194
In grad_steps = 454, loss = 0.03589293733239174
In grad_steps = 455, loss = 0.004566690418869257
In grad_steps = 456, loss = 0.015188781544566154
In grad_steps = 457, loss = 0.03636953607201576
In grad_steps = 458, loss = 0.0051947422325611115
In grad_steps = 459, loss = 0.0030491661746054888
In grad_steps = 460, loss = 0.004648525267839432
In grad_steps = 461, loss = 0.26935434341430664
In grad_steps = 462, loss = 0.0071455566212534904
In grad_steps = 463, loss = 0.5252959728240967
In grad_steps = 464, loss = 0.0004826798976864666
In grad_steps = 465, loss = 0.003661214606836438
In grad_steps = 466, loss = 0.0064260102808475494
In grad_steps = 467, loss = 0.032845206558704376
In grad_steps = 468, loss = 0.05609142407774925
In grad_steps = 469, loss = 0.0313895083963871
In grad_steps = 470, loss = 0.04098948463797569
In grad_steps = 471, loss = 0.5741323828697205
In grad_steps = 472, loss = 0.03568333387374878
In grad_steps = 473, loss = 0.21590565145015717
In grad_steps = 474, loss = 0.6561852097511292
In grad_steps = 475, loss = 0.22419552505016327
In grad_steps = 476, loss = 0.041058383882045746
In grad_steps = 477, loss = 0.01730295829474926
In grad_steps = 478, loss = 0.0332297720015049
In grad_steps = 479, loss = 0.024854103103280067
In grad_steps = 480, loss = 0.09368141740560532
In grad_steps = 481, loss = 0.1834140419960022
In grad_steps = 482, loss = 0.0780695304274559
In grad_steps = 483, loss = 0.06695382297039032
In grad_steps = 484, loss = 0.0137839550152421
In grad_steps = 485, loss = 0.003824925050139427
In grad_steps = 486, loss = 0.07395932078361511
In grad_steps = 487, loss = 0.1529482752084732
In grad_steps = 488, loss = 0.005453429650515318
In grad_steps = 489, loss = 0.0011349975829944015
In grad_steps = 490, loss = 0.008395301178097725
In grad_steps = 491, loss = 0.14708133041858673
In grad_steps = 492, loss = 0.1743205189704895
In grad_steps = 493, loss = 0.00420411117374897
In grad_steps = 494, loss = 0.033009544014930725
In grad_steps = 495, loss = 0.061771683394908905
In grad_steps = 496, loss = 0.00323046138510108
In grad_steps = 497, loss = 0.00035846419632434845
In grad_steps = 498, loss = 8.77093625604175e-05
In grad_steps = 499, loss = 0.00034560251515358686
In grad_steps = 500, loss = 0.009309429675340652
In grad_steps = 501, loss = 0.546863317489624
In grad_steps = 502, loss = 0.0028040921315550804
In grad_steps = 503, loss = 0.00030131591483950615
In grad_steps = 504, loss = 0.021472174674272537
In grad_steps = 505, loss = 0.00043652934255078435
In grad_steps = 506, loss = 0.00025551312137395144
In grad_steps = 507, loss = 0.25690656900405884
In grad_steps = 508, loss = 0.01956336386501789
In grad_steps = 509, loss = 0.007802536245435476
In grad_steps = 510, loss = 0.12465839833021164
In grad_steps = 511, loss = 0.016341526061296463
In grad_steps = 512, loss = 0.002185086254030466
In grad_steps = 513, loss = 0.004354170057922602
In grad_steps = 514, loss = 0.29687586426734924
In grad_steps = 515, loss = 0.1503184586763382
In grad_steps = 516, loss = 0.015797629952430725
In grad_steps = 517, loss = 0.04332105815410614
In grad_steps = 518, loss = 0.12767985463142395
In grad_steps = 519, loss = 0.006836461368948221
In grad_steps = 520, loss = 0.014456119388341904
In grad_steps = 521, loss = 0.0049690320156514645
In grad_steps = 522, loss = 0.08120124042034149
In grad_steps = 523, loss = 0.012824511155486107
In grad_steps = 524, loss = 0.013986487872898579
Beginning epoch 6
In grad_steps = 525, loss = 0.11460687965154648
In grad_steps = 526, loss = 0.027816548943519592
In grad_steps = 527, loss = 0.027297725901007652
In grad_steps = 528, loss = 0.009917858988046646
In grad_steps = 529, loss = 0.18010783195495605
In grad_steps = 530, loss = 0.007153499871492386
In grad_steps = 531, loss = 0.0202883742749691
In grad_steps = 532, loss = 0.006158329546451569
In grad_steps = 533, loss = 0.005033113062381744
In grad_steps = 534, loss = 0.013682514429092407
In grad_steps = 535, loss = 0.012526500970125198
In grad_steps = 536, loss = 0.012021495029330254
In grad_steps = 537, loss = 0.008397873491048813
In grad_steps = 538, loss = 0.13052360713481903
In grad_steps = 539, loss = 0.002400551224127412
In grad_steps = 540, loss = 0.01824568770825863
In grad_steps = 541, loss = 0.0015708430437371135
In grad_steps = 542, loss = 0.19968929886817932
In grad_steps = 543, loss = 0.0021999503951519728
In grad_steps = 544, loss = 0.0022014291025698185
In grad_steps = 545, loss = 0.05056098848581314
In grad_steps = 546, loss = 0.021043920889496803
In grad_steps = 547, loss = 0.02858849987387657
In grad_steps = 548, loss = 0.08831574022769928
In grad_steps = 549, loss = 0.11319342255592346
In grad_steps = 550, loss = 0.0024099487345665693
In grad_steps = 551, loss = 0.09487524628639221
In grad_steps = 552, loss = 0.0019119058270007372
In grad_steps = 553, loss = 0.006833886727690697
In grad_steps = 554, loss = 0.0031323516741394997
In grad_steps = 555, loss = 0.008177246898412704
In grad_steps = 556, loss = 0.020716428756713867
In grad_steps = 557, loss = 0.0064865173771977425
In grad_steps = 558, loss = 0.009395711123943329
In grad_steps = 559, loss = 0.010804010555148125
In grad_steps = 560, loss = 0.014928738586604595
In grad_steps = 561, loss = 0.006275245454162359
In grad_steps = 562, loss = 0.05699223279953003
In grad_steps = 563, loss = 0.015281924977898598
In grad_steps = 564, loss = 0.10349959880113602
In grad_steps = 565, loss = 0.003157882485538721
In grad_steps = 566, loss = 0.01206446997821331
In grad_steps = 567, loss = 0.0008375455508939922
In grad_steps = 568, loss = 0.2909492552280426
In grad_steps = 569, loss = 0.012922556139528751
In grad_steps = 570, loss = 0.00510301673784852
In grad_steps = 571, loss = 0.01252396497875452
In grad_steps = 572, loss = 0.14262989163398743
In grad_steps = 573, loss = 0.002635345095768571
In grad_steps = 574, loss = 0.04402037337422371
In grad_steps = 575, loss = 0.14337608218193054
In grad_steps = 576, loss = 0.0008657027501612902
In grad_steps = 577, loss = 0.0016253056237474084
In grad_steps = 578, loss = 0.002623168984428048
In grad_steps = 579, loss = 0.03231065347790718
In grad_steps = 580, loss = 0.01325114257633686
In grad_steps = 581, loss = 0.0021242895163595676
In grad_steps = 582, loss = 0.0012435850221663713
In grad_steps = 583, loss = 0.002658551326021552
In grad_steps = 584, loss = 0.10191384702920914
In grad_steps = 585, loss = 0.05717533454298973
In grad_steps = 586, loss = 0.011248046532273293
In grad_steps = 587, loss = 0.07266991585493088
In grad_steps = 588, loss = 0.35781505703926086
In grad_steps = 589, loss = 0.008264400996267796
In grad_steps = 590, loss = 0.006789595820009708
In grad_steps = 591, loss = 0.004583810921758413
In grad_steps = 592, loss = 0.0046657961793243885
In grad_steps = 593, loss = 0.007577795535326004
In grad_steps = 594, loss = 0.009563091211020947
In grad_steps = 595, loss = 0.010691949166357517
In grad_steps = 596, loss = 0.09981473535299301
In grad_steps = 597, loss = 0.08962687849998474
In grad_steps = 598, loss = 0.13746166229248047
In grad_steps = 599, loss = 0.0034466467332094908
In grad_steps = 600, loss = 0.0016473183641210198
In grad_steps = 601, loss = 0.021898560225963593
In grad_steps = 602, loss = 0.004376324359327555
In grad_steps = 603, loss = 0.012795737944543362
In grad_steps = 604, loss = 0.008030451834201813
In grad_steps = 605, loss = 0.01939309947192669
In grad_steps = 606, loss = 0.019354498013854027
In grad_steps = 607, loss = 0.0021554375998675823
In grad_steps = 608, loss = 0.014051152393221855
In grad_steps = 609, loss = 0.024481499567627907
In grad_steps = 610, loss = 0.0027094632387161255
In grad_steps = 611, loss = 0.0010310894576832652
In grad_steps = 612, loss = 0.0039946939796209335
In grad_steps = 613, loss = 0.003559671575203538
In grad_steps = 614, loss = 0.007677181623876095
In grad_steps = 615, loss = 0.006600989494472742
In grad_steps = 616, loss = 0.022958675399422646
In grad_steps = 617, loss = 0.0007020207121968269
In grad_steps = 618, loss = 0.003332003951072693
In grad_steps = 619, loss = 0.27435484528541565
In grad_steps = 620, loss = 0.0003864259924739599
In grad_steps = 621, loss = 0.0031016995199024677
In grad_steps = 622, loss = 0.0021263333037495613
In grad_steps = 623, loss = 0.046484462916851044
In grad_steps = 624, loss = 0.03625742346048355
In grad_steps = 625, loss = 0.002185588702559471
In grad_steps = 626, loss = 0.0008374342578463256
In grad_steps = 627, loss = 0.04568662494421005
In grad_steps = 628, loss = 0.065758615732193
In grad_steps = 629, loss = 0.00941013265401125
Beginning epoch 7
In grad_steps = 630, loss = 0.008380051702260971
In grad_steps = 631, loss = 0.00042723887600004673
In grad_steps = 632, loss = 0.10088010877370834
In grad_steps = 633, loss = 0.004841489251703024
In grad_steps = 634, loss = 0.0013512076111510396
In grad_steps = 635, loss = 0.00295896059833467
In grad_steps = 636, loss = 0.0021736882627010345
In grad_steps = 637, loss = 0.0022956724278628826
In grad_steps = 638, loss = 0.0009309942834079266
In grad_steps = 639, loss = 0.002409912645816803
In grad_steps = 640, loss = 0.021495049819350243
In grad_steps = 641, loss = 0.002398664364591241
In grad_steps = 642, loss = 0.004884215071797371
In grad_steps = 643, loss = 0.051090389490127563
In grad_steps = 644, loss = 0.00031404304900206625
In grad_steps = 645, loss = 0.017873503267765045
In grad_steps = 646, loss = 0.0008884482085704803
In grad_steps = 647, loss = 0.0011940476251766086
In grad_steps = 648, loss = 0.0004025334492325783
In grad_steps = 649, loss = 0.00031619667424820364
In grad_steps = 650, loss = 0.006480691954493523
In grad_steps = 651, loss = 0.0229593887925148
In grad_steps = 652, loss = 0.0015308231813833117
In grad_steps = 653, loss = 0.009199650958180428
In grad_steps = 654, loss = 0.004866861272603273
In grad_steps = 655, loss = 0.0007035120506770909
In grad_steps = 656, loss = 0.009933380410075188
In grad_steps = 657, loss = 0.00039012881461530924
In grad_steps = 658, loss = 0.0009760134271346033
In grad_steps = 659, loss = 0.00031025789212435484
In grad_steps = 660, loss = 0.0002693654678296298
In grad_steps = 661, loss = 0.00025656947400420904
In grad_steps = 662, loss = 0.00017803465016186237
In grad_steps = 663, loss = 0.0013091349974274635
In grad_steps = 664, loss = 0.0016293041408061981
In grad_steps = 665, loss = 0.00018106876814272255
In grad_steps = 666, loss = 0.0005564635503105819
In grad_steps = 667, loss = 0.0006199872586876154
In grad_steps = 668, loss = 0.0005952155333943665
In grad_steps = 669, loss = 0.00747302733361721
In grad_steps = 670, loss = 0.0013061906211078167
In grad_steps = 671, loss = 0.010347466915845871
In grad_steps = 672, loss = 0.00021156617731321603
In grad_steps = 673, loss = 0.00024688109988346696
In grad_steps = 674, loss = 0.00011168465425726026
In grad_steps = 675, loss = 0.00027049609343521297
In grad_steps = 676, loss = 0.000930304522626102
In grad_steps = 677, loss = 0.00010395549179520458
In grad_steps = 678, loss = 0.00010084104724228382
In grad_steps = 679, loss = 0.001276345457881689
In grad_steps = 680, loss = 0.0003565911902114749
In grad_steps = 681, loss = 0.00012584708747453988
In grad_steps = 682, loss = 0.0002807668934110552
In grad_steps = 683, loss = 0.00019282633729744703
In grad_steps = 684, loss = 0.0003498360747471452
In grad_steps = 685, loss = 0.00019836242427118123
In grad_steps = 686, loss = 0.00016479496844112873
In grad_steps = 687, loss = 5.956450695521198e-05
In grad_steps = 688, loss = 8.650252129882574e-05
In grad_steps = 689, loss = 7.568927685497329e-05
In grad_steps = 690, loss = 0.00010922650108113885
In grad_steps = 691, loss = 5.1898256060667336e-05
In grad_steps = 692, loss = 0.0001635942462598905
In grad_steps = 693, loss = 0.000258065527305007
In grad_steps = 694, loss = 0.00023298029555007815
In grad_steps = 695, loss = 7.873205322539434e-05
In grad_steps = 696, loss = 9.15863856789656e-05
In grad_steps = 697, loss = 0.00010079585626954213
In grad_steps = 698, loss = 8.068409078987315e-05
In grad_steps = 699, loss = 0.00011656207061605528
In grad_steps = 700, loss = 0.00010020419722422957
In grad_steps = 701, loss = 0.00014303851639851928
In grad_steps = 702, loss = 0.0001973300677491352
In grad_steps = 703, loss = 0.03293779119849205
In grad_steps = 704, loss = 0.0001269486383534968
In grad_steps = 705, loss = 0.0001247553009307012
In grad_steps = 706, loss = 0.000501594680827111
In grad_steps = 707, loss = 0.00019053093274123967
In grad_steps = 708, loss = 4.6698722144356e-05
In grad_steps = 709, loss = 0.00020739289175253361
In grad_steps = 710, loss = 0.00018802999693434685
In grad_steps = 711, loss = 0.0001542393147246912
In grad_steps = 712, loss = 0.00010439710604259744
In grad_steps = 713, loss = 7.698107947362587e-05
In grad_steps = 714, loss = 0.00048678769962862134
In grad_steps = 715, loss = 0.00032934488262981176
In grad_steps = 716, loss = 0.00016108607815112919
In grad_steps = 717, loss = 0.0003265321138314903
In grad_steps = 718, loss = 0.00034992879955098033
In grad_steps = 719, loss = 0.000981722492724657
In grad_steps = 720, loss = 0.00022673298371955752
In grad_steps = 721, loss = 0.00018127141811419278
In grad_steps = 722, loss = 9.162662172457203e-05
In grad_steps = 723, loss = 0.00011307026579743251
In grad_steps = 724, loss = 0.20781385898590088
In grad_steps = 725, loss = 9.6416704764124e-05
In grad_steps = 726, loss = 0.00015099871961865574
In grad_steps = 727, loss = 0.0016870716353878379
In grad_steps = 728, loss = 0.00015369665925391018
In grad_steps = 729, loss = 0.10843435674905777
In grad_steps = 730, loss = 0.00014890336024109274
In grad_steps = 731, loss = 0.00026893397443927824
In grad_steps = 732, loss = 0.0007669551996514201
In grad_steps = 733, loss = 0.00019269440963398665
In grad_steps = 734, loss = 0.0001469444832764566
Beginning epoch 8
In grad_steps = 735, loss = 0.002583493711426854
In grad_steps = 736, loss = 0.00017678146832622588
In grad_steps = 737, loss = 0.0027600927278399467
In grad_steps = 738, loss = 0.005146421492099762
In grad_steps = 739, loss = 0.0005603569443337619
In grad_steps = 740, loss = 0.0053796018473804
In grad_steps = 741, loss = 0.0007442400674335659
In grad_steps = 742, loss = 0.0003902427852153778
In grad_steps = 743, loss = 0.022465473040938377
In grad_steps = 744, loss = 0.0005436483770608902
In grad_steps = 745, loss = 0.001058741007000208
In grad_steps = 746, loss = 0.0015510502271354198
In grad_steps = 747, loss = 0.000698425923474133
In grad_steps = 748, loss = 0.000916113262064755
In grad_steps = 749, loss = 0.00021365395514294505
In grad_steps = 750, loss = 0.0002958885161206126
In grad_steps = 751, loss = 0.0003749945608433336
In grad_steps = 752, loss = 0.0006304259295575321
In grad_steps = 753, loss = 0.00026295005227439106
In grad_steps = 754, loss = 0.000205671283765696
In grad_steps = 755, loss = 0.001341533032245934
In grad_steps = 756, loss = 0.00041911547305062413
In grad_steps = 757, loss = 0.0006382066057994962
In grad_steps = 758, loss = 0.0014652960235252976
In grad_steps = 759, loss = 0.0009056187118403614
In grad_steps = 760, loss = 0.00041516387136653066
In grad_steps = 761, loss = 0.0006332778721116483
In grad_steps = 762, loss = 0.00022883636120241135
In grad_steps = 763, loss = 0.0009339922107756138
In grad_steps = 764, loss = 0.0005272344569675624
In grad_steps = 765, loss = 0.000397190626244992
In grad_steps = 766, loss = 0.00032577477395534515
In grad_steps = 767, loss = 0.00025356662808917463
In grad_steps = 768, loss = 0.0013307110639289021
In grad_steps = 769, loss = 0.0012649360578507185
In grad_steps = 770, loss = 0.00011298395838821307
In grad_steps = 771, loss = 0.000410338951041922
In grad_steps = 772, loss = 0.0005595686961896718
In grad_steps = 773, loss = 0.0004682582803070545
In grad_steps = 774, loss = 0.0006265880074352026
In grad_steps = 775, loss = 0.0005308700492605567
In grad_steps = 776, loss = 0.010821045376360416
In grad_steps = 777, loss = 0.00020869498257525265
In grad_steps = 778, loss = 0.00022830368834547698
In grad_steps = 779, loss = 6.97843061061576e-05
In grad_steps = 780, loss = 0.0002144516765838489
In grad_steps = 781, loss = 0.0015161759220063686
In grad_steps = 782, loss = 0.00012001150753349066
In grad_steps = 783, loss = 0.0001330971863353625
In grad_steps = 784, loss = 0.0002676202857401222
In grad_steps = 785, loss = 0.0007920836796984076
In grad_steps = 786, loss = 0.00011526474554557353
In grad_steps = 787, loss = 0.0025383438915014267
In grad_steps = 788, loss = 0.00020812050206586719
In grad_steps = 789, loss = 0.0003857848059851676
In grad_steps = 790, loss = 0.00010791786917252466
In grad_steps = 791, loss = 8.115100354189053e-05
In grad_steps = 792, loss = 8.358414197573438e-05
In grad_steps = 793, loss = 0.00010094482422573492
In grad_steps = 794, loss = 6.961251347092912e-05
In grad_steps = 795, loss = 0.00017224153270944953
In grad_steps = 796, loss = 0.00013077362382318825
In grad_steps = 797, loss = 0.00025754683883860707
In grad_steps = 798, loss = 0.00015227678522933275
In grad_steps = 799, loss = 0.00015273941971827298
In grad_steps = 800, loss = 0.00010232205386273563
In grad_steps = 801, loss = 0.00024053802189882845
In grad_steps = 802, loss = 7.763472967781126e-05
In grad_steps = 803, loss = 9.963282354874536e-05
In grad_steps = 804, loss = 8.827584679238498e-05
In grad_steps = 805, loss = 6.129732355475426e-05
In grad_steps = 806, loss = 0.00017651630332693458
In grad_steps = 807, loss = 0.00024757051141932607
In grad_steps = 808, loss = 0.00022413696569856256
In grad_steps = 809, loss = 0.00013575644697993994
In grad_steps = 810, loss = 0.0001378433807985857
In grad_steps = 811, loss = 0.0008090219926089048
In grad_steps = 812, loss = 0.0001378823653794825
In grad_steps = 813, loss = 6.0792099247919396e-05
In grad_steps = 814, loss = 0.00016204507846850902
In grad_steps = 815, loss = 0.0001349483645753935
In grad_steps = 816, loss = 0.00021725852275267243
In grad_steps = 817, loss = 7.978589565027505e-05
In grad_steps = 818, loss = 6.84055921738036e-05
In grad_steps = 819, loss = 0.00026462972164154053
In grad_steps = 820, loss = 0.00010771959205158055
In grad_steps = 821, loss = 3.4346052416367456e-05
In grad_steps = 822, loss = 0.0002034646604442969
In grad_steps = 823, loss = 0.00027160276658833027
In grad_steps = 824, loss = 0.0001694628008408472
In grad_steps = 825, loss = 0.00011556436220416799
In grad_steps = 826, loss = 7.035546150291339e-05
In grad_steps = 827, loss = 7.039942283881828e-05
In grad_steps = 828, loss = 0.00011454871128080413
In grad_steps = 829, loss = 0.0001549281005281955
In grad_steps = 830, loss = 3.045684570679441e-05
In grad_steps = 831, loss = 8.874363265931606e-05
In grad_steps = 832, loss = 0.00017434891196899116
In grad_steps = 833, loss = 0.0001383196795359254
In grad_steps = 834, loss = 7.709956844337285e-05
In grad_steps = 835, loss = 5.857216820004396e-05
In grad_steps = 836, loss = 6.030585791449994e-05
In grad_steps = 837, loss = 0.0005347824771888554
In grad_steps = 838, loss = 7.819761958671734e-05
In grad_steps = 839, loss = 2.7377778678783216e-05
Beginning epoch 9
In grad_steps = 840, loss = 6.156874587759376e-05
In grad_steps = 841, loss = 6.0540107369888574e-05
In grad_steps = 842, loss = 8.186433115042746e-05
In grad_steps = 843, loss = 6.326581933535635e-05
In grad_steps = 844, loss = 8.34652892081067e-05
In grad_steps = 845, loss = 6.612983997911215e-05
In grad_steps = 846, loss = 7.206950249383226e-05
In grad_steps = 847, loss = 5.153141319169663e-05
In grad_steps = 848, loss = 4.4946049456484616e-05
In grad_steps = 849, loss = 0.00015747056750115007
In grad_steps = 850, loss = 0.00012258913193363696
In grad_steps = 851, loss = 0.0005496583180502057
In grad_steps = 852, loss = 6.337692320812494e-05
In grad_steps = 853, loss = 0.00017071278125513345
In grad_steps = 854, loss = 2.9518316296162084e-05
In grad_steps = 855, loss = 4.2034203943330795e-05
In grad_steps = 856, loss = 3.8257072446867824e-05
In grad_steps = 857, loss = 0.0001250393979717046
In grad_steps = 858, loss = 4.046998583362438e-05
In grad_steps = 859, loss = 2.4459479391225614e-05
In grad_steps = 860, loss = 9.955236600944772e-05
In grad_steps = 861, loss = 5.8541791077004746e-05
In grad_steps = 862, loss = 9.337743540527299e-05
In grad_steps = 863, loss = 0.00019304135639686137
In grad_steps = 864, loss = 0.0001128971271100454
In grad_steps = 865, loss = 6.314394704531878e-05
In grad_steps = 866, loss = 0.0001295397523790598
In grad_steps = 867, loss = 4.4968805013922974e-05
In grad_steps = 868, loss = 0.0002011112228501588
In grad_steps = 869, loss = 9.180156484944746e-05
In grad_steps = 870, loss = 5.885154314455576e-05
In grad_steps = 871, loss = 0.00035711287637241185
In grad_steps = 872, loss = 4.531695958576165e-05
In grad_steps = 873, loss = 0.0002021571563091129
In grad_steps = 874, loss = 0.00010747103806352243
In grad_steps = 875, loss = 2.5882729460136034e-05
In grad_steps = 876, loss = 0.0001036003086483106
In grad_steps = 877, loss = 8.184804755728692e-05
In grad_steps = 878, loss = 0.00011091848136857152
In grad_steps = 879, loss = 0.00010937246406683698
In grad_steps = 880, loss = 9.872815280687064e-05
In grad_steps = 881, loss = 0.0001343488838756457
In grad_steps = 882, loss = 2.8453128834371455e-05
In grad_steps = 883, loss = 7.057879702188075e-05
In grad_steps = 884, loss = 1.8708038624026813e-05
In grad_steps = 885, loss = 5.771331052528694e-05
In grad_steps = 886, loss = 0.0003458668652456254
In grad_steps = 887, loss = 2.6821368010132574e-05
In grad_steps = 888, loss = 3.158873369102366e-05
In grad_steps = 889, loss = 7.719440327491611e-05
In grad_steps = 890, loss = 0.00015945988707244396
In grad_steps = 891, loss = 3.9269736589631066e-05
In grad_steps = 892, loss = 3.7355897802626714e-05
In grad_steps = 893, loss = 4.4049404095858335e-05
In grad_steps = 894, loss = 0.0001470570859964937
In grad_steps = 895, loss = 4.543662362266332e-05
In grad_steps = 896, loss = 2.404246697551571e-05
In grad_steps = 897, loss = 3.1797080737305805e-05
In grad_steps = 898, loss = 3.4725879231700674e-05
In grad_steps = 899, loss = 2.6575335141387768e-05
In grad_steps = 900, loss = 5.2966675866628066e-05
In grad_steps = 901, loss = 2.2507132598548196e-05
In grad_steps = 902, loss = 7.91532511357218e-05
In grad_steps = 903, loss = 5.715560837415978e-05
In grad_steps = 904, loss = 5.365375545807183e-05
In grad_steps = 905, loss = 4.158706360612996e-05
In grad_steps = 906, loss = 3.583560464903712e-05
In grad_steps = 907, loss = 3.1551717256661505e-05
In grad_steps = 908, loss = 4.413109490997158e-05
In grad_steps = 909, loss = 3.392902362975292e-05
In grad_steps = 910, loss = 2.522661270631943e-05
In grad_steps = 911, loss = 6.909195508342236e-05
In grad_steps = 912, loss = 7.223508146125823e-05
In grad_steps = 913, loss = 9.611359564587474e-05
In grad_steps = 914, loss = 8.982256258605048e-05
In grad_steps = 915, loss = 6.198995106387883e-05
In grad_steps = 916, loss = 0.00012062473251717165
In grad_steps = 917, loss = 7.01146200299263e-05
In grad_steps = 918, loss = 2.6247496862197295e-05
In grad_steps = 919, loss = 6.447682972066104e-05
In grad_steps = 920, loss = 3.967210795963183e-05
In grad_steps = 921, loss = 0.00012902454182039946
In grad_steps = 922, loss = 2.672287155291997e-05
In grad_steps = 923, loss = 2.421383214823436e-05
In grad_steps = 924, loss = 0.00011602751328609884
In grad_steps = 925, loss = 6.629627023357898e-05
In grad_steps = 926, loss = 1.5415027519338764e-05
In grad_steps = 927, loss = 8.699221507413313e-05
In grad_steps = 928, loss = 0.0001729413925204426
In grad_steps = 929, loss = 5.9310234064469114e-05
In grad_steps = 930, loss = 4.8446683649672195e-05
In grad_steps = 931, loss = 2.9785909646307118e-05
In grad_steps = 932, loss = 3.2937052310444415e-05
In grad_steps = 933, loss = 5.632400279864669e-05
In grad_steps = 934, loss = 7.706769974902272e-05
In grad_steps = 935, loss = 1.6681446140864864e-05
In grad_steps = 936, loss = 3.0486611649394035e-05
In grad_steps = 937, loss = 8.63681998453103e-05
In grad_steps = 938, loss = 7.268918852787465e-05
In grad_steps = 939, loss = 3.728112642420456e-05
In grad_steps = 940, loss = 3.1387662602355704e-05
In grad_steps = 941, loss = 3.166082387906499e-05
In grad_steps = 942, loss = 0.00010237997776130214
In grad_steps = 943, loss = 2.2812499082647264e-05
In grad_steps = 944, loss = 1.186518511531176e-05
Beginning epoch 10
In grad_steps = 945, loss = 0.00011681162141030654
In grad_steps = 946, loss = 2.6216826881864108e-05
In grad_steps = 947, loss = 3.086384094785899e-05
In grad_steps = 948, loss = 3.221510269213468e-05
In grad_steps = 949, loss = 3.794339863816276e-05
In grad_steps = 950, loss = 3.7151286960579455e-05
In grad_steps = 951, loss = 3.631213621702045e-05
In grad_steps = 952, loss = 2.2202031686902046e-05
In grad_steps = 953, loss = 2.214235792052932e-05
In grad_steps = 954, loss = 0.00010691284842323512
In grad_steps = 955, loss = 8.03414368419908e-05
In grad_steps = 956, loss = 0.00025649668532423675
In grad_steps = 957, loss = 3.0233308280003257e-05
In grad_steps = 958, loss = 6.28105117357336e-05
In grad_steps = 959, loss = 1.427511506335577e-05
In grad_steps = 960, loss = 2.1591265976894647e-05
In grad_steps = 961, loss = 1.9341285224072635e-05
In grad_steps = 962, loss = 6.810967897763476e-05
In grad_steps = 963, loss = 1.5772684491821565e-05
In grad_steps = 964, loss = 1.3209641110734083e-05
In grad_steps = 965, loss = 4.345337220001966e-05
In grad_steps = 966, loss = 2.7834365027956665e-05
In grad_steps = 967, loss = 5.537500328500755e-05
In grad_steps = 968, loss = 9.538285667076707e-05
In grad_steps = 969, loss = 5.497015081346035e-05
In grad_steps = 970, loss = 3.459805157035589e-05
In grad_steps = 971, loss = 5.932849308010191e-05
In grad_steps = 972, loss = 3.692973768920638e-05
In grad_steps = 973, loss = 0.00011957797687500715
In grad_steps = 974, loss = 3.628952617873438e-05
In grad_steps = 975, loss = 3.834431117866188e-05
In grad_steps = 976, loss = 2.144952850358095e-05
In grad_steps = 977, loss = 2.930028858827427e-05
In grad_steps = 978, loss = 7.675711822230369e-05
In grad_steps = 979, loss = 5.1275983423693106e-05
In grad_steps = 980, loss = 1.2911713383800816e-05
In grad_steps = 981, loss = 5.471832992043346e-05
In grad_steps = 982, loss = 6.935861165402457e-05
In grad_steps = 983, loss = 4.197934322291985e-05
In grad_steps = 984, loss = 6.16031902609393e-05
In grad_steps = 985, loss = 5.6239408877445385e-05
In grad_steps = 986, loss = 5.9712809161283076e-05
In grad_steps = 987, loss = 1.5318195437430404e-05
In grad_steps = 988, loss = 3.113431012025103e-05
In grad_steps = 989, loss = 1.0661661690392066e-05
In grad_steps = 990, loss = 2.7848634999827482e-05
In grad_steps = 991, loss = 0.00017642282182350755
In grad_steps = 992, loss = 1.9676508600241505e-05
In grad_steps = 993, loss = 1.7873226170195267e-05
In grad_steps = 994, loss = 4.035326128359884e-05
In grad_steps = 995, loss = 6.624405068578199e-05
In grad_steps = 996, loss = 2.1754995032097213e-05
In grad_steps = 997, loss = 1.87453770195134e-05
In grad_steps = 998, loss = 2.6581959900795482e-05
In grad_steps = 999, loss = 8.006697316886857e-05
In grad_steps = 1000, loss = 2.400443008809816e-05
In grad_steps = 1001, loss = 1.4148444279271644e-05
In grad_steps = 1002, loss = 1.5563866327283904e-05
In grad_steps = 1003, loss = 1.8640987036633305e-05
In grad_steps = 1004, loss = 1.4640127119491808e-05
In grad_steps = 1005, loss = 3.607902181101963e-05
In grad_steps = 1006, loss = 2.2670430553262122e-05
In grad_steps = 1007, loss = 5.565388346440159e-05
In grad_steps = 1008, loss = 3.0180905014276505e-05
In grad_steps = 1009, loss = 2.6499339583097026e-05
In grad_steps = 1010, loss = 2.3416479962179437e-05
In grad_steps = 1011, loss = 2.33643932006089e-05
In grad_steps = 1012, loss = 1.590666943229735e-05
In grad_steps = 1013, loss = 2.0309285901021212e-05
In grad_steps = 1014, loss = 1.856658491306007e-05
In grad_steps = 1015, loss = 1.304566740145674e-05
In grad_steps = 1016, loss = 3.7537665775744244e-05
In grad_steps = 1017, loss = 4.1240054997615516e-05
In grad_steps = 1018, loss = 5.2934228733647615e-05
In grad_steps = 1019, loss = 3.2772572012618184e-05
In grad_steps = 1020, loss = 3.821127029368654e-05
In grad_steps = 1021, loss = 9.430474165128544e-05
In grad_steps = 1022, loss = 4.2032188503071666e-05
In grad_steps = 1023, loss = 1.5571396943414584e-05
In grad_steps = 1024, loss = 3.618503615143709e-05
In grad_steps = 1025, loss = 1.9780865841312334e-05
In grad_steps = 1026, loss = 6.372483039740473e-05
In grad_steps = 1027, loss = 1.8312330212211236e-05
In grad_steps = 1028, loss = 1.458058522985084e-05
In grad_steps = 1029, loss = 5.5888605857035145e-05
In grad_steps = 1030, loss = 3.5470449802232906e-05
In grad_steps = 1031, loss = 9.462155503570102e-06
In grad_steps = 1032, loss = 4.846353840548545e-05
In grad_steps = 1033, loss = 7.020963676040992e-05
In grad_steps = 1034, loss = 3.2982548873405904e-05
In grad_steps = 1035, loss = 2.5651310352259316e-05
In grad_steps = 1036, loss = 1.7918120647664182e-05
In grad_steps = 1037, loss = 1.731470547383651e-05
In grad_steps = 1038, loss = 2.8905977160320617e-05
In grad_steps = 1039, loss = 4.54648288723547e-05
In grad_steps = 1040, loss = 9.290763046010397e-06
In grad_steps = 1041, loss = 1.776926546881441e-05
In grad_steps = 1042, loss = 5.064176730229519e-05
In grad_steps = 1043, loss = 4.8272369895130396e-05
In grad_steps = 1044, loss = 2.0019282601424493e-05
In grad_steps = 1045, loss = 1.4364480193762574e-05
In grad_steps = 1046, loss = 1.5079212971613742e-05
In grad_steps = 1047, loss = 5.712287384085357e-05
In grad_steps = 1048, loss = 1.2680596228165086e-05
In grad_steps = 1049, loss = 7.224039563880069e-06
Elapsed time: 1625.7047221660614 seconds for ensemble 0 with 10 epochs
LoRA instance 0 evaluation complete. Data saved to /pscratch/sd/t/tianle/lucid/other_source/SURP_2024/results/tmp_lora_ensemble/Llama3-8B-set-6/test_data_instance_0_seed_1.npz.
lora instance i = 0 Successfully finished.
Training lora instance 1
DEBUG: Total parameters: 8037076992
DEBUG: Trainable parameters: 6815744
Beginning epoch 1
In grad_steps = 0, loss = 0.8738614320755005
In grad_steps = 1, loss = 0.783888041973114
In grad_steps = 2, loss = 0.7368127703666687
In grad_steps = 3, loss = 0.698614239692688
In grad_steps = 4, loss = 0.7510697245597839
In grad_steps = 5, loss = 0.6977925300598145
In grad_steps = 6, loss = 0.7141181826591492
In grad_steps = 7, loss = 0.6596130132675171
In grad_steps = 8, loss = 0.7015935182571411
In grad_steps = 9, loss = 0.6522625684738159
In grad_steps = 10, loss = 0.6768433451652527
In grad_steps = 11, loss = 0.576714038848877
In grad_steps = 12, loss = 0.6847231388092041
In grad_steps = 13, loss = 0.7769191265106201
In grad_steps = 14, loss = 0.6147096157073975
In grad_steps = 15, loss = 0.6316645741462708
In grad_steps = 16, loss = 0.680248498916626
In grad_steps = 17, loss = 0.5385530591011047
In grad_steps = 18, loss = 0.6439089775085449
In grad_steps = 19, loss = 0.49655306339263916
In grad_steps = 20, loss = 0.5907250046730042
In grad_steps = 21, loss = 0.5690213441848755
In grad_steps = 22, loss = 0.5586690902709961
In grad_steps = 23, loss = 0.3607822358608246
In grad_steps = 24, loss = 0.5689297318458557
In grad_steps = 25, loss = 0.6958500146865845
In grad_steps = 26, loss = 0.5679522752761841
In grad_steps = 27, loss = 0.3537048101425171
In grad_steps = 28, loss = 0.27061727643013
In grad_steps = 29, loss = 0.4084424078464508
In grad_steps = 30, loss = 0.622769296169281
In grad_steps = 31, loss = 0.3389013111591339
In grad_steps = 32, loss = 0.7424420714378357
In grad_steps = 33, loss = 0.4526806175708771
In grad_steps = 34, loss = 0.7215214967727661
In grad_steps = 35, loss = 0.21633195877075195
In grad_steps = 36, loss = 0.35002368688583374
In grad_steps = 37, loss = 0.5645431876182556
In grad_steps = 38, loss = 0.6905906200408936
In grad_steps = 39, loss = 0.30385255813598633
In grad_steps = 40, loss = 0.47977375984191895
In grad_steps = 41, loss = 0.6019465923309326
In grad_steps = 42, loss = 0.38853582739830017
In grad_steps = 43, loss = 0.3390749990940094
In grad_steps = 44, loss = 0.29143157601356506
In grad_steps = 45, loss = 0.6420743465423584
In grad_steps = 46, loss = 0.3143114745616913
In grad_steps = 47, loss = 0.3881426453590393
In grad_steps = 48, loss = 0.36400309205055237
In grad_steps = 49, loss = 0.14320029318332672
In grad_steps = 50, loss = 0.7037849426269531
In grad_steps = 51, loss = 0.1649240106344223
In grad_steps = 52, loss = 0.2946111261844635
In grad_steps = 53, loss = 0.4859065115451813
In grad_steps = 54, loss = 0.35204020142555237
In grad_steps = 55, loss = 0.37361857295036316
In grad_steps = 56, loss = 0.1910056322813034
In grad_steps = 57, loss = 0.27414482831954956
In grad_steps = 58, loss = 0.23808859288692474
In grad_steps = 59, loss = 0.2572430670261383
In grad_steps = 60, loss = 0.5343093276023865
In grad_steps = 61, loss = 0.3806643486022949
In grad_steps = 62, loss = 0.2270529717206955
In grad_steps = 63, loss = 0.28583014011383057
In grad_steps = 64, loss = 0.15114840865135193
In grad_steps = 65, loss = 0.22082021832466125
In grad_steps = 66, loss = 0.3973320722579956
In grad_steps = 67, loss = 0.35919857025146484
In grad_steps = 68, loss = 0.24782885611057281
In grad_steps = 69, loss = 0.3069537878036499
In grad_steps = 70, loss = 0.3553719222545624
In grad_steps = 71, loss = 0.10050410777330399
In grad_steps = 72, loss = 0.386443555355072
In grad_steps = 73, loss = 0.438265323638916
In grad_steps = 74, loss = 0.215315580368042
In grad_steps = 75, loss = 0.13051250576972961
In grad_steps = 76, loss = 0.44853201508522034
In grad_steps = 77, loss = 0.09470261633396149
In grad_steps = 78, loss = 0.16193783283233643
In grad_steps = 79, loss = 0.2150696963071823
In grad_steps = 80, loss = 0.2642025053501129
In grad_steps = 81, loss = 0.2997320294380188
In grad_steps = 82, loss = 0.14477631449699402
In grad_steps = 83, loss = 0.5771932601928711
In grad_steps = 84, loss = 0.337409108877182
In grad_steps = 85, loss = 0.049168508499860764
In grad_steps = 86, loss = 0.43668103218078613
In grad_steps = 87, loss = 1.0694142580032349
In grad_steps = 88, loss = 0.6982236504554749
In grad_steps = 89, loss = 0.4049568176269531
In grad_steps = 90, loss = 0.3735387623310089
In grad_steps = 91, loss = 0.3277242183685303
In grad_steps = 92, loss = 0.3266567885875702
In grad_steps = 93, loss = 0.35509687662124634
In grad_steps = 94, loss = 0.2838486433029175
In grad_steps = 95, loss = 0.3374159634113312
In grad_steps = 96, loss = 0.43616896867752075
In grad_steps = 97, loss = 0.30885568261146545
In grad_steps = 98, loss = 0.5438412427902222
In grad_steps = 99, loss = 0.15358449518680573
In grad_steps = 100, loss = 0.19248932600021362
In grad_steps = 101, loss = 0.11268473416566849
In grad_steps = 102, loss = 0.146060973405838
In grad_steps = 103, loss = 0.6517866253852844
In grad_steps = 104, loss = 0.2061312198638916
Beginning epoch 2
In grad_steps = 105, loss = 0.29112935066223145
In grad_steps = 106, loss = 0.371212899684906
In grad_steps = 107, loss = 0.30521371960639954
In grad_steps = 108, loss = 0.26512035727500916
In grad_steps = 109, loss = 0.4907923936843872
In grad_steps = 110, loss = 0.38515782356262207
In grad_steps = 111, loss = 0.29502126574516296
In grad_steps = 112, loss = 0.13004980981349945
In grad_steps = 113, loss = 0.21777880191802979
In grad_steps = 114, loss = 0.13508816063404083
In grad_steps = 115, loss = 0.24194373190402985
In grad_steps = 116, loss = 0.14383675158023834
In grad_steps = 117, loss = 0.28064122796058655
In grad_steps = 118, loss = 0.8404629826545715
In grad_steps = 119, loss = 0.09036143869161606
In grad_steps = 120, loss = 0.23809018731117249
In grad_steps = 121, loss = 0.06073382869362831
In grad_steps = 122, loss = 0.18192151188850403
In grad_steps = 123, loss = 0.1666857898235321
In grad_steps = 124, loss = 0.2808751165866852
In grad_steps = 125, loss = 0.47123220562934875
In grad_steps = 126, loss = 0.13702866435050964
In grad_steps = 127, loss = 0.1191009134054184
In grad_steps = 128, loss = 0.1364039182662964
In grad_steps = 129, loss = 0.28115126490592957
In grad_steps = 130, loss = 0.22965067625045776
In grad_steps = 131, loss = 0.17888501286506653
In grad_steps = 132, loss = 0.147393137216568
In grad_steps = 133, loss = 0.09610310196876526
In grad_steps = 134, loss = 0.1119609996676445
In grad_steps = 135, loss = 0.14711840450763702
In grad_steps = 136, loss = 0.04820628836750984
In grad_steps = 137, loss = 0.11203907430171967
In grad_steps = 138, loss = 0.06170405447483063
In grad_steps = 139, loss = 0.2391628473997116
In grad_steps = 140, loss = 0.015445079654455185
In grad_steps = 141, loss = 0.137312114238739
In grad_steps = 142, loss = 0.23927365243434906
In grad_steps = 143, loss = 0.39751961827278137
In grad_steps = 144, loss = 0.18731385469436646
In grad_steps = 145, loss = 0.08565925806760788
In grad_steps = 146, loss = 0.4645107388496399
In grad_steps = 147, loss = 0.027109429240226746
In grad_steps = 148, loss = 0.17187535762786865
In grad_steps = 149, loss = 0.04482901468873024
In grad_steps = 150, loss = 0.15275095403194427
In grad_steps = 151, loss = 0.18804733455181122
In grad_steps = 152, loss = 0.3809249699115753
In grad_steps = 153, loss = 0.08500361442565918
In grad_steps = 154, loss = 0.06363876909017563
In grad_steps = 155, loss = 0.11071758717298508
In grad_steps = 156, loss = 0.27896198630332947
In grad_steps = 157, loss = 0.2796941101551056
In grad_steps = 158, loss = 0.32214218378067017
In grad_steps = 159, loss = 0.09989337623119354
In grad_steps = 160, loss = 0.15535274147987366
In grad_steps = 161, loss = 0.1033565029501915
In grad_steps = 162, loss = 0.16250458359718323
In grad_steps = 163, loss = 0.06545351445674896
In grad_steps = 164, loss = 0.09050966054201126
In grad_steps = 165, loss = 0.18036359548568726
In grad_steps = 166, loss = 0.02997242473065853
In grad_steps = 167, loss = 0.10214202105998993
In grad_steps = 168, loss = 0.1275404691696167
In grad_steps = 169, loss = 0.044299136847257614
In grad_steps = 170, loss = 0.04007678106427193
In grad_steps = 171, loss = 0.1562451273202896
In grad_steps = 172, loss = 0.03204957768321037
In grad_steps = 173, loss = 0.012713605538010597
In grad_steps = 174, loss = 0.008480217307806015
In grad_steps = 175, loss = 0.07296860218048096
In grad_steps = 176, loss = 0.016426434740424156
In grad_steps = 177, loss = 0.13676391541957855
In grad_steps = 178, loss = 0.5044977068901062
In grad_steps = 179, loss = 0.1282455325126648
In grad_steps = 180, loss = 0.00859837606549263
In grad_steps = 181, loss = 0.06465080380439758
In grad_steps = 182, loss = 0.008262340910732746
In grad_steps = 183, loss = 0.015865692868828773
In grad_steps = 184, loss = 0.06572379916906357
In grad_steps = 185, loss = 0.04735580086708069
In grad_steps = 186, loss = 0.25483834743499756
In grad_steps = 187, loss = 0.008251004852354527
In grad_steps = 188, loss = 0.02472703345119953
In grad_steps = 189, loss = 0.4314572811126709
In grad_steps = 190, loss = 0.05223487690091133
In grad_steps = 191, loss = 0.004544968716800213
In grad_steps = 192, loss = 0.32624509930610657
In grad_steps = 193, loss = 0.338188111782074
In grad_steps = 194, loss = 0.1928248405456543
In grad_steps = 195, loss = 0.1953773945569992
In grad_steps = 196, loss = 0.3732360005378723
In grad_steps = 197, loss = 0.14133021235466003
In grad_steps = 198, loss = 0.1589420586824417
In grad_steps = 199, loss = 0.3602243661880493
In grad_steps = 200, loss = 0.22449959814548492
In grad_steps = 201, loss = 0.37940898537635803
In grad_steps = 202, loss = 0.11744201183319092
In grad_steps = 203, loss = 0.3258814215660095
In grad_steps = 204, loss = 0.16665084660053253
In grad_steps = 205, loss = 0.2253192961215973
In grad_steps = 206, loss = 0.04932112619280815
In grad_steps = 207, loss = 0.16757957637310028
In grad_steps = 208, loss = 0.2764628231525421
In grad_steps = 209, loss = 0.08734419196844101
Beginning epoch 3
In grad_steps = 210, loss = 0.1275104582309723
In grad_steps = 211, loss = 0.06887798756361008
In grad_steps = 212, loss = 0.11297090351581573
In grad_steps = 213, loss = 0.1677381694316864
In grad_steps = 214, loss = 0.28318196535110474
In grad_steps = 215, loss = 0.04803959280252457
In grad_steps = 216, loss = 0.09408926218748093
In grad_steps = 217, loss = 0.057661909610033035
In grad_steps = 218, loss = 0.16407081484794617
In grad_steps = 219, loss = 0.023331375792622566
In grad_steps = 220, loss = 0.21788892149925232
In grad_steps = 221, loss = 0.033419251441955566
In grad_steps = 222, loss = 0.17927904427051544
In grad_steps = 223, loss = 0.3749240040779114
In grad_steps = 224, loss = 0.007183015812188387
In grad_steps = 225, loss = 0.12529157102108002
In grad_steps = 226, loss = 0.00399682717397809
In grad_steps = 227, loss = 0.1005813404917717
In grad_steps = 228, loss = 0.02043495699763298
In grad_steps = 229, loss = 0.005845414940267801
In grad_steps = 230, loss = 0.02703768201172352
In grad_steps = 231, loss = 0.054364971816539764
In grad_steps = 232, loss = 0.012557599693536758
In grad_steps = 233, loss = 0.10007016360759735
In grad_steps = 234, loss = 0.36671182513237
In grad_steps = 235, loss = 0.024240901693701744
In grad_steps = 236, loss = 0.4154137969017029
In grad_steps = 237, loss = 0.08227618038654327
In grad_steps = 238, loss = 0.046665895730257034
In grad_steps = 239, loss = 0.06925879418849945
In grad_steps = 240, loss = 0.02730581723153591
In grad_steps = 241, loss = 0.010726545006036758
In grad_steps = 242, loss = 0.037971362471580505
In grad_steps = 243, loss = 0.11645413935184479
In grad_steps = 244, loss = 0.21883456408977509
In grad_steps = 245, loss = 0.003947890363633633
In grad_steps = 246, loss = 0.03013141080737114
In grad_steps = 247, loss = 0.07536203414201736
In grad_steps = 248, loss = 0.11119870841503143
In grad_steps = 249, loss = 0.10852207243442535
In grad_steps = 250, loss = 0.02182186394929886
In grad_steps = 251, loss = 0.18059042096138
In grad_steps = 252, loss = 0.017514187842607498
In grad_steps = 253, loss = 0.030893992632627487
In grad_steps = 254, loss = 0.002802922623232007
In grad_steps = 255, loss = 0.27469295263290405
In grad_steps = 256, loss = 0.11488326638936996
In grad_steps = 257, loss = 0.09162893146276474
In grad_steps = 258, loss = 0.03332442417740822
In grad_steps = 259, loss = 0.016461564227938652
In grad_steps = 260, loss = 0.05721687525510788
In grad_steps = 261, loss = 0.08516817539930344
In grad_steps = 262, loss = 0.16077721118927002
In grad_steps = 263, loss = 0.09683149307966232
In grad_steps = 264, loss = 0.4699670970439911
In grad_steps = 265, loss = 0.05177987366914749
In grad_steps = 266, loss = 0.08161461353302002
In grad_steps = 267, loss = 0.02830805815756321
In grad_steps = 268, loss = 0.024219660088419914
In grad_steps = 269, loss = 0.07874177396297455
In grad_steps = 270, loss = 0.04048224911093712
In grad_steps = 271, loss = 0.040872592478990555
In grad_steps = 272, loss = 0.07282885164022446
In grad_steps = 273, loss = 0.02850337326526642
In grad_steps = 274, loss = 0.13196972012519836
In grad_steps = 275, loss = 0.06461545825004578
In grad_steps = 276, loss = 0.0421500988304615
In grad_steps = 277, loss = 0.012130285613238811
In grad_steps = 278, loss = 0.488525390625
In grad_steps = 279, loss = 0.1059151142835617
In grad_steps = 280, loss = 0.10789300501346588
In grad_steps = 281, loss = 0.03772968053817749
In grad_steps = 282, loss = 0.17017996311187744
In grad_steps = 283, loss = 0.3389410674571991
In grad_steps = 284, loss = 0.048117585480213165
In grad_steps = 285, loss = 0.023916104808449745
In grad_steps = 286, loss = 0.21780383586883545
In grad_steps = 287, loss = 0.005843102466315031
In grad_steps = 288, loss = 0.02675764262676239
In grad_steps = 289, loss = 0.0657292976975441
In grad_steps = 290, loss = 0.031609006226062775
In grad_steps = 291, loss = 0.07573062926530838
In grad_steps = 292, loss = 0.004909936338663101
In grad_steps = 293, loss = 0.016570908948779106
In grad_steps = 294, loss = 0.17672668397426605
In grad_steps = 295, loss = 0.015172731131315231
In grad_steps = 296, loss = 0.005766390357166529
In grad_steps = 297, loss = 0.015098600648343563
In grad_steps = 298, loss = 0.20997340977191925
In grad_steps = 299, loss = 0.17744149267673492
In grad_steps = 300, loss = 0.042010657489299774
In grad_steps = 301, loss = 0.14748089015483856
In grad_steps = 302, loss = 0.007088870741426945
In grad_steps = 303, loss = 0.06584861129522324
In grad_steps = 304, loss = 0.6793735027313232
In grad_steps = 305, loss = 0.11215364933013916
In grad_steps = 306, loss = 0.09963469952344894
In grad_steps = 307, loss = 0.012892910279333591
In grad_steps = 308, loss = 0.2508189082145691
In grad_steps = 309, loss = 0.008810112252831459
In grad_steps = 310, loss = 0.0615297369658947
In grad_steps = 311, loss = 0.016492852941155434
In grad_steps = 312, loss = 0.17780987918376923
In grad_steps = 313, loss = 0.04467985779047012
In grad_steps = 314, loss = 0.046842340379953384
Beginning epoch 4
In grad_steps = 315, loss = 0.09830116480588913
In grad_steps = 316, loss = 0.010442049242556095
In grad_steps = 317, loss = 0.009487313218414783
In grad_steps = 318, loss = 0.06492212414741516
In grad_steps = 319, loss = 0.04988492652773857
In grad_steps = 320, loss = 0.0180201455950737
In grad_steps = 321, loss = 0.12626802921295166
In grad_steps = 322, loss = 0.011362157762050629
In grad_steps = 323, loss = 0.02883698046207428
In grad_steps = 324, loss = 0.010978772304952145
In grad_steps = 325, loss = 0.024849623441696167
In grad_steps = 326, loss = 0.02712883986532688
In grad_steps = 327, loss = 0.06464481353759766
In grad_steps = 328, loss = 0.27720507979393005
In grad_steps = 329, loss = 0.0016519054770469666
In grad_steps = 330, loss = 0.0029715928249061108
In grad_steps = 331, loss = 0.00261802040040493
In grad_steps = 332, loss = 0.006244804244488478
In grad_steps = 333, loss = 0.04112819582223892
In grad_steps = 334, loss = 0.015503901988267899
In grad_steps = 335, loss = 0.001515261479653418
In grad_steps = 336, loss = 0.06558401137590408
In grad_steps = 337, loss = 0.004238173831254244
In grad_steps = 338, loss = 0.18631857633590698
In grad_steps = 339, loss = 0.11270540952682495
In grad_steps = 340, loss = 0.004542458336800337
In grad_steps = 341, loss = 0.025645257905125618
In grad_steps = 342, loss = 0.290291965007782
In grad_steps = 343, loss = 0.11413218080997467
In grad_steps = 344, loss = 0.3002889156341553
In grad_steps = 345, loss = 0.046850524842739105
In grad_steps = 346, loss = 0.013795841485261917
In grad_steps = 347, loss = 0.011895603500306606
In grad_steps = 348, loss = 0.015742067247629166
In grad_steps = 349, loss = 0.04274619370698929
In grad_steps = 350, loss = 0.0017984039150178432
In grad_steps = 351, loss = 0.13628003001213074
In grad_steps = 352, loss = 0.09606645256280899
In grad_steps = 353, loss = 0.15490372478961945
In grad_steps = 354, loss = 0.1402018964290619
In grad_steps = 355, loss = 0.08599353581666946
In grad_steps = 356, loss = 0.2310778945684433
In grad_steps = 357, loss = 0.00685854023322463
In grad_steps = 358, loss = 0.0114256851375103
In grad_steps = 359, loss = 0.0018182171043008566
In grad_steps = 360, loss = 0.01693679206073284
In grad_steps = 361, loss = 0.15120799839496613
In grad_steps = 362, loss = 0.16150380671024323
In grad_steps = 363, loss = 0.05960351228713989
In grad_steps = 364, loss = 0.20545677840709686
In grad_steps = 365, loss = 0.003808583365753293
In grad_steps = 366, loss = 0.26392486691474915
In grad_steps = 367, loss = 0.011817478574812412
In grad_steps = 368, loss = 0.027507448568940163
In grad_steps = 369, loss = 0.07676208764314651
In grad_steps = 370, loss = 0.044887393712997437
In grad_steps = 371, loss = 0.11167658865451813
In grad_steps = 372, loss = 0.07575016468763351
In grad_steps = 373, loss = 0.10510552674531937
In grad_steps = 374, loss = 0.009362328797578812
In grad_steps = 375, loss = 0.41989466547966003
In grad_steps = 376, loss = 0.020157432183623314
In grad_steps = 377, loss = 0.07968256622552872
In grad_steps = 378, loss = 0.006069929338991642
In grad_steps = 379, loss = 0.0007813085103407502
In grad_steps = 380, loss = 0.02186626009643078
In grad_steps = 381, loss = 0.02076447755098343
In grad_steps = 382, loss = 0.012835200875997543
In grad_steps = 383, loss = 0.03806808963418007
In grad_steps = 384, loss = 0.02949606068432331
In grad_steps = 385, loss = 0.3161434233188629
In grad_steps = 386, loss = 0.023903219029307365
In grad_steps = 387, loss = 0.05887118726968765
In grad_steps = 388, loss = 0.07068529725074768
In grad_steps = 389, loss = 0.044457897543907166
In grad_steps = 390, loss = 0.13532887399196625
In grad_steps = 391, loss = 0.00862120557576418
In grad_steps = 392, loss = 0.005958851892501116
In grad_steps = 393, loss = 0.001460056402720511
In grad_steps = 394, loss = 0.010839655995368958
In grad_steps = 395, loss = 0.12446539103984833
In grad_steps = 396, loss = 0.1084037572145462
In grad_steps = 397, loss = 0.0006322598783299327
In grad_steps = 398, loss = 0.001320276758633554
In grad_steps = 399, loss = 0.004975014831870794
In grad_steps = 400, loss = 0.0004432767163962126
In grad_steps = 401, loss = 0.0008235989371314645
In grad_steps = 402, loss = 0.07472527027130127
In grad_steps = 403, loss = 0.002282423432916403
In grad_steps = 404, loss = 0.0032156214583665133
In grad_steps = 405, loss = 0.072897769510746
In grad_steps = 406, loss = 0.03298215940594673
In grad_steps = 407, loss = 0.01903924159705639
In grad_steps = 408, loss = 0.028491996228694916
In grad_steps = 409, loss = 0.12809160351753235
In grad_steps = 410, loss = 0.27738848328590393
In grad_steps = 411, loss = 0.09993860125541687
In grad_steps = 412, loss = 0.015683546662330627
In grad_steps = 413, loss = 0.009311899542808533
In grad_steps = 414, loss = 0.002517381915822625
In grad_steps = 415, loss = 0.002191543113440275
In grad_steps = 416, loss = 0.00352636631578207
In grad_steps = 417, loss = 0.10811368376016617
In grad_steps = 418, loss = 0.1549609750509262
In grad_steps = 419, loss = 0.010104993358254433
Beginning epoch 5
In grad_steps = 420, loss = 0.5508284568786621
In grad_steps = 421, loss = 0.002197815803810954
In grad_steps = 422, loss = 0.03274941071867943
In grad_steps = 423, loss = 0.007709701545536518
In grad_steps = 424, loss = 0.01976626180112362
In grad_steps = 425, loss = 0.009890394285321236
In grad_steps = 426, loss = 0.10197924077510834
In grad_steps = 427, loss = 0.05279048904776573
In grad_steps = 428, loss = 0.35462862253189087
In grad_steps = 429, loss = 0.028151147067546844
In grad_steps = 430, loss = 0.04229956492781639
In grad_steps = 431, loss = 0.04109222814440727
In grad_steps = 432, loss = 0.024013454094529152
In grad_steps = 433, loss = 0.031722504645586014
In grad_steps = 434, loss = 0.003749330062419176
In grad_steps = 435, loss = 0.01584501937031746
In grad_steps = 436, loss = 0.004820599220693111
In grad_steps = 437, loss = 0.015552953816950321
In grad_steps = 438, loss = 0.012192915193736553
In grad_steps = 439, loss = 0.0031813052482903004
In grad_steps = 440, loss = 0.008946306072175503
In grad_steps = 441, loss = 0.12984424829483032
In grad_steps = 442, loss = 0.22311419248580933
In grad_steps = 443, loss = 0.0540841668844223
In grad_steps = 444, loss = 0.23004858195781708
In grad_steps = 445, loss = 0.007331884000450373
In grad_steps = 446, loss = 0.022433582693338394
In grad_steps = 447, loss = 0.003924151416867971
In grad_steps = 448, loss = 0.17148949205875397
In grad_steps = 449, loss = 0.06458957493305206
In grad_steps = 450, loss = 0.14276531338691711
In grad_steps = 451, loss = 0.010715179145336151
In grad_steps = 452, loss = 0.0895608440041542
In grad_steps = 453, loss = 0.01632152684032917
In grad_steps = 454, loss = 0.09633409976959229
In grad_steps = 455, loss = 0.01144986879080534
In grad_steps = 456, loss = 0.005571391433477402
In grad_steps = 457, loss = 0.04544007405638695
In grad_steps = 458, loss = 0.04091889411211014
In grad_steps = 459, loss = 0.02964399755001068
In grad_steps = 460, loss = 0.005063141230493784
In grad_steps = 461, loss = 0.057303473353385925
In grad_steps = 462, loss = 0.002663019113242626
In grad_steps = 463, loss = 0.32489773631095886
In grad_steps = 464, loss = 0.007183965295553207
In grad_steps = 465, loss = 0.0036384370177984238
In grad_steps = 466, loss = 0.004582876339554787
In grad_steps = 467, loss = 0.08977032452821732
In grad_steps = 468, loss = 0.0015726867131888866
In grad_steps = 469, loss = 0.001114712911657989
In grad_steps = 470, loss = 0.01753442920744419
In grad_steps = 471, loss = 0.013433163985610008
In grad_steps = 472, loss = 0.0728735402226448
In grad_steps = 473, loss = 0.12342770397663116
In grad_steps = 474, loss = 0.07383754849433899
In grad_steps = 475, loss = 0.19707390666007996
In grad_steps = 476, loss = 0.061274994164705276
In grad_steps = 477, loss = 0.004981410689651966
In grad_steps = 478, loss = 0.002172015141695738
In grad_steps = 479, loss = 0.10622170567512512
In grad_steps = 480, loss = 0.008212639018893242
In grad_steps = 481, loss = 0.008847218938171864
In grad_steps = 482, loss = 0.020602375268936157
In grad_steps = 483, loss = 0.011273108422756195
In grad_steps = 484, loss = 0.004675588104873896
In grad_steps = 485, loss = 0.005083850119262934
In grad_steps = 486, loss = 0.02771669067442417
In grad_steps = 487, loss = 0.2852310240268707
In grad_steps = 488, loss = 0.015210307203233242
In grad_steps = 489, loss = 0.030803153291344643
In grad_steps = 490, loss = 0.011594078503549099
In grad_steps = 491, loss = 0.00759452348574996
In grad_steps = 492, loss = 0.1874413937330246
In grad_steps = 493, loss = 0.19130975008010864
In grad_steps = 494, loss = 0.26545536518096924
In grad_steps = 495, loss = 0.004216248169541359
In grad_steps = 496, loss = 0.02488093078136444
In grad_steps = 497, loss = 0.006311248987913132
In grad_steps = 498, loss = 0.008071144111454487
In grad_steps = 499, loss = 0.013837454840540886
In grad_steps = 500, loss = 0.01468212902545929
In grad_steps = 501, loss = 0.11006944626569748
In grad_steps = 502, loss = 0.009092777967453003
In grad_steps = 503, loss = 0.08059988170862198
In grad_steps = 504, loss = 0.16010534763336182
In grad_steps = 505, loss = 0.005890272092074156
In grad_steps = 506, loss = 0.003989819437265396
In grad_steps = 507, loss = 0.08905063569545746
In grad_steps = 508, loss = 0.0247753094881773
In grad_steps = 509, loss = 0.051031336188316345
In grad_steps = 510, loss = 0.04941883683204651
In grad_steps = 511, loss = 0.08061090856790543
In grad_steps = 512, loss = 0.1161821037530899
In grad_steps = 513, loss = 0.022630155086517334
In grad_steps = 514, loss = 0.018899865448474884
In grad_steps = 515, loss = 0.003074200823903084
In grad_steps = 516, loss = 0.03627419471740723
In grad_steps = 517, loss = 0.004159890580922365
In grad_steps = 518, loss = 0.14699670672416687
In grad_steps = 519, loss = 0.0026157512329518795
In grad_steps = 520, loss = 0.01825634203851223
In grad_steps = 521, loss = 0.007539282087236643
In grad_steps = 522, loss = 0.009405946359038353
In grad_steps = 523, loss = 0.15780003368854523
In grad_steps = 524, loss = 0.003161292290315032
Beginning epoch 6
In grad_steps = 525, loss = 0.03926043584942818
In grad_steps = 526, loss = 0.0033184883650392294
In grad_steps = 527, loss = 0.034155167639255524
In grad_steps = 528, loss = 0.07464955002069473
In grad_steps = 529, loss = 0.11268291622400284
In grad_steps = 530, loss = 0.007595421746373177
In grad_steps = 531, loss = 0.06967464089393616
In grad_steps = 532, loss = 0.004047158639878035
In grad_steps = 533, loss = 0.2411830872297287
In grad_steps = 534, loss = 0.007224541623145342
In grad_steps = 535, loss = 0.015673896297812462
In grad_steps = 536, loss = 0.002632363233715296
In grad_steps = 537, loss = 0.003051907289773226
In grad_steps = 538, loss = 0.09703068435192108
In grad_steps = 539, loss = 0.0030170937534421682
In grad_steps = 540, loss = 0.004173560068011284
In grad_steps = 541, loss = 0.0012671947479248047
In grad_steps = 542, loss = 0.03382554650306702
In grad_steps = 543, loss = 0.005466956179589033
In grad_steps = 544, loss = 0.0012790715554729104
In grad_steps = 545, loss = 0.006357370410114527
In grad_steps = 546, loss = 0.06631020456552505
In grad_steps = 547, loss = 0.004470061976462603
In grad_steps = 548, loss = 0.13597266376018524
In grad_steps = 549, loss = 0.025315089151263237
In grad_steps = 550, loss = 0.0015105330385267735
In grad_steps = 551, loss = 0.11375004053115845
In grad_steps = 552, loss = 0.005375067703425884
In grad_steps = 553, loss = 0.0013380068121477962
In grad_steps = 554, loss = 0.0010758948046714067
In grad_steps = 555, loss = 0.004746157210320234
In grad_steps = 556, loss = 0.00251684058457613
In grad_steps = 557, loss = 0.0011077090166509151
In grad_steps = 558, loss = 0.07171498984098434
In grad_steps = 559, loss = 0.030388951301574707
In grad_steps = 560, loss = 0.0801067054271698
In grad_steps = 561, loss = 0.02760287933051586
In grad_steps = 562, loss = 0.03927747532725334
In grad_steps = 563, loss = 0.008526315912604332
In grad_steps = 564, loss = 0.004362296778708696
In grad_steps = 565, loss = 0.002417271723970771
In grad_steps = 566, loss = 0.02126656100153923
In grad_steps = 567, loss = 0.0006295742350630462
In grad_steps = 568, loss = 0.01482456922531128
In grad_steps = 569, loss = 0.0007310118526220322
In grad_steps = 570, loss = 0.0014975122176110744
In grad_steps = 571, loss = 0.009367583319544792
In grad_steps = 572, loss = 0.0029171884525567293
In grad_steps = 573, loss = 0.002818987937644124
In grad_steps = 574, loss = 9.950060484698042e-05
In grad_steps = 575, loss = 0.11568116396665573
In grad_steps = 576, loss = 0.00036031295894645154
In grad_steps = 577, loss = 0.01378548238426447
In grad_steps = 578, loss = 0.000941033533308655
In grad_steps = 579, loss = 0.0007835744181647897
In grad_steps = 580, loss = 0.0011338208569213748
In grad_steps = 581, loss = 0.00020572722132783383
In grad_steps = 582, loss = 0.0014037223299965262
In grad_steps = 583, loss = 0.05436992645263672
In grad_steps = 584, loss = 0.023265168070793152
In grad_steps = 585, loss = 0.06293286383152008
In grad_steps = 586, loss = 0.001773547730408609
In grad_steps = 587, loss = 0.0005201447056606412
In grad_steps = 588, loss = 0.0011549898190423846
In grad_steps = 589, loss = 6.322846456896514e-05
In grad_steps = 590, loss = 0.00014747738896403462
In grad_steps = 591, loss = 0.0030700387433171272
In grad_steps = 592, loss = 0.0012517522554844618
In grad_steps = 593, loss = 7.595316128572449e-05
In grad_steps = 594, loss = 0.00018727580027189106
In grad_steps = 595, loss = 0.06439273804426193
In grad_steps = 596, loss = 0.000528019736520946
In grad_steps = 597, loss = 0.24164557456970215
In grad_steps = 598, loss = 0.0060202558524906635
In grad_steps = 599, loss = 0.0009266531560570002
In grad_steps = 600, loss = 0.00023558524844702333
In grad_steps = 601, loss = 0.005554378032684326
In grad_steps = 602, loss = 0.01796640455722809
In grad_steps = 603, loss = 0.0009040309232659638
In grad_steps = 604, loss = 0.0646744817495346
In grad_steps = 605, loss = 0.03511550650000572
In grad_steps = 606, loss = 0.06432951986789703
In grad_steps = 607, loss = 0.006887484807521105
In grad_steps = 608, loss = 0.0024504552129656076
In grad_steps = 609, loss = 0.008739868178963661
In grad_steps = 610, loss = 0.0006990100373513997
In grad_steps = 611, loss = 0.00044347019866108894
In grad_steps = 612, loss = 0.023706184700131416
In grad_steps = 613, loss = 0.008919564075767994
In grad_steps = 614, loss = 0.036129243671894073
In grad_steps = 615, loss = 0.0017711451509967446
In grad_steps = 616, loss = 0.03071373887360096
In grad_steps = 617, loss = 0.0008243794436566532
In grad_steps = 618, loss = 0.0015905117616057396
In grad_steps = 619, loss = 0.49122369289398193
In grad_steps = 620, loss = 0.0010150960879400373
In grad_steps = 621, loss = 0.0009836205281317234
In grad_steps = 622, loss = 0.015200728550553322
In grad_steps = 623, loss = 0.03310504928231239
In grad_steps = 624, loss = 0.000756454945076257
In grad_steps = 625, loss = 0.0024719759821891785
In grad_steps = 626, loss = 0.001646926742978394
In grad_steps = 627, loss = 0.010086064226925373
In grad_steps = 628, loss = 0.005256380420178175
In grad_steps = 629, loss = 0.0014566959580406547
Beginning epoch 7
In grad_steps = 630, loss = 0.027824081480503082
In grad_steps = 631, loss = 0.015209141187369823
In grad_steps = 632, loss = 0.05891050025820732
In grad_steps = 633, loss = 0.003750090952962637
In grad_steps = 634, loss = 0.003139534732326865
In grad_steps = 635, loss = 0.002049652859568596
In grad_steps = 636, loss = 0.16552633047103882
In grad_steps = 637, loss = 0.001839043223299086
In grad_steps = 638, loss = 0.28165319561958313
In grad_steps = 639, loss = 0.006861438043415546
In grad_steps = 640, loss = 0.08134940266609192
In grad_steps = 641, loss = 0.006359034683555365
In grad_steps = 642, loss = 0.005395712796598673
In grad_steps = 643, loss = 0.07960028946399689
In grad_steps = 644, loss = 0.0022486255038529634
In grad_steps = 645, loss = 0.011111126281321049
In grad_steps = 646, loss = 0.001602344331331551
In grad_steps = 647, loss = 0.010231902822852135
In grad_steps = 648, loss = 0.07431517541408539
In grad_steps = 649, loss = 0.0017912308685481548
In grad_steps = 650, loss = 0.017139814794063568
In grad_steps = 651, loss = 0.007313638459891081
In grad_steps = 652, loss = 0.023655593395233154
In grad_steps = 653, loss = 0.07015649229288101
In grad_steps = 654, loss = 0.03309723362326622
In grad_steps = 655, loss = 0.0067846388556063175
In grad_steps = 656, loss = 0.025774288922548294
In grad_steps = 657, loss = 0.0018442093860358
In grad_steps = 658, loss = 0.005820940714329481
In grad_steps = 659, loss = 0.02299612946808338
In grad_steps = 660, loss = 0.001471152761951089
In grad_steps = 661, loss = 0.003363827709108591
In grad_steps = 662, loss = 0.0005059160175733268
In grad_steps = 663, loss = 0.0008942304411903024
In grad_steps = 664, loss = 0.003279374912381172
In grad_steps = 665, loss = 0.02481052093207836
In grad_steps = 666, loss = 0.0030880733393132687
In grad_steps = 667, loss = 0.010676762089133263
In grad_steps = 668, loss = 0.0012893563834950328
In grad_steps = 669, loss = 0.03637967258691788
In grad_steps = 670, loss = 0.014510531909763813
In grad_steps = 671, loss = 0.011290407739579678
In grad_steps = 672, loss = 0.001883549033664167
In grad_steps = 673, loss = 0.00468111177906394
In grad_steps = 674, loss = 0.00023828209668863565
In grad_steps = 675, loss = 0.00884015578776598
In grad_steps = 676, loss = 0.005738388746976852
In grad_steps = 677, loss = 0.0005441728280857205
In grad_steps = 678, loss = 0.0016221017576754093
In grad_steps = 679, loss = 0.00019701081328094006
In grad_steps = 680, loss = 0.0011728679528459907
In grad_steps = 681, loss = 0.00048342079389840364
In grad_steps = 682, loss = 0.015597283840179443
In grad_steps = 683, loss = 0.000428069441113621
In grad_steps = 684, loss = 0.0020304834470152855
In grad_steps = 685, loss = 0.0006882917368784547
In grad_steps = 686, loss = 7.968195131979883e-05
In grad_steps = 687, loss = 0.08628557622432709
In grad_steps = 688, loss = 9.562585910316557e-05
In grad_steps = 689, loss = 0.00023999590484891087
In grad_steps = 690, loss = 0.0003024746256414801
In grad_steps = 691, loss = 0.00048560346476733685
In grad_steps = 692, loss = 0.0027574466075748205
In grad_steps = 693, loss = 0.000658911420032382
In grad_steps = 694, loss = 0.00015121222531888634
In grad_steps = 695, loss = 0.00031695447978563607
In grad_steps = 696, loss = 0.00012665760004892945
In grad_steps = 697, loss = 8.26284012873657e-05
In grad_steps = 698, loss = 0.00011440688103903085
In grad_steps = 699, loss = 0.0002480625407770276
In grad_steps = 700, loss = 0.0009788512252271175
In grad_steps = 701, loss = 0.00010363454202888533
In grad_steps = 702, loss = 0.0005818692152388394
In grad_steps = 703, loss = 0.000359930592821911
In grad_steps = 704, loss = 0.1753174364566803
In grad_steps = 705, loss = 0.002112562069669366
In grad_steps = 706, loss = 0.0018613367574289441
In grad_steps = 707, loss = 0.004301778040826321
In grad_steps = 708, loss = 5.282979327603243e-05
In grad_steps = 709, loss = 0.09694976359605789
In grad_steps = 710, loss = 0.0004101563536096364
In grad_steps = 711, loss = 0.0005800931830890477
In grad_steps = 712, loss = 0.00010748997738119215
In grad_steps = 713, loss = 0.0002820714144036174
In grad_steps = 714, loss = 0.0005437268409878016
In grad_steps = 715, loss = 0.00016898727335501462
In grad_steps = 716, loss = 0.00027870407211594284
In grad_steps = 717, loss = 0.05994414538145065
In grad_steps = 718, loss = 0.006381019484251738
In grad_steps = 719, loss = 0.0026318144518882036
In grad_steps = 720, loss = 0.01321137323975563
In grad_steps = 721, loss = 0.1539141684770584
In grad_steps = 722, loss = 0.0002533445949666202
In grad_steps = 723, loss = 0.0011227073846384883
In grad_steps = 724, loss = 0.1838405728340149
In grad_steps = 725, loss = 0.0016192621551454067
In grad_steps = 726, loss = 0.0008432939648628235
In grad_steps = 727, loss = 0.0015937023563310504
In grad_steps = 728, loss = 0.0020524407736957073
In grad_steps = 729, loss = 0.0004822988121304661
In grad_steps = 730, loss = 0.003564572660252452
In grad_steps = 731, loss = 0.0016684627626091242
In grad_steps = 732, loss = 0.011910450644791126
In grad_steps = 733, loss = 0.13674448430538177
In grad_steps = 734, loss = 0.00243312562815845
Beginning epoch 8
In grad_steps = 735, loss = 0.000981043209321797
In grad_steps = 736, loss = 0.0009536620345897973
In grad_steps = 737, loss = 0.001216947566717863
In grad_steps = 738, loss = 0.0025083578657358885
In grad_steps = 739, loss = 0.009573089890182018
In grad_steps = 740, loss = 0.0009864321909844875
In grad_steps = 741, loss = 0.005237548612058163
In grad_steps = 742, loss = 0.004080872517079115
In grad_steps = 743, loss = 0.016951223835349083
In grad_steps = 744, loss = 0.010510940104722977
In grad_steps = 745, loss = 0.008205749094486237
In grad_steps = 746, loss = 0.002149177948012948
In grad_steps = 747, loss = 0.0012790253385901451
In grad_steps = 748, loss = 0.08070414513349533
In grad_steps = 749, loss = 0.0003239987709093839
In grad_steps = 750, loss = 0.001035394612699747
In grad_steps = 751, loss = 0.0002608201466500759
In grad_steps = 752, loss = 0.007809564471244812
In grad_steps = 753, loss = 0.0022317394614219666
In grad_steps = 754, loss = 0.001230451394803822
In grad_steps = 755, loss = 0.0061104074120521545
In grad_steps = 756, loss = 0.0008455808274447918
In grad_steps = 757, loss = 0.02944815717637539
In grad_steps = 758, loss = 0.01343830581754446
In grad_steps = 759, loss = 0.5939875841140747
In grad_steps = 760, loss = 0.001923547824844718
In grad_steps = 761, loss = 0.03284337744116783
In grad_steps = 762, loss = 0.0011190081713721156
In grad_steps = 763, loss = 0.09906335920095444
In grad_steps = 764, loss = 0.0032124994322657585
In grad_steps = 765, loss = 0.0011377505725249648
In grad_steps = 766, loss = 0.01298742275685072
In grad_steps = 767, loss = 0.0005508027970790863
In grad_steps = 768, loss = 0.0008080422412604094
In grad_steps = 769, loss = 0.05656663700938225
In grad_steps = 770, loss = 0.001037752372212708
In grad_steps = 771, loss = 0.0010238006943836808
In grad_steps = 772, loss = 0.002941606566309929
In grad_steps = 773, loss = 0.007378816604614258
In grad_steps = 774, loss = 0.0079153161495924
In grad_steps = 775, loss = 0.021182220429182053
In grad_steps = 776, loss = 0.014317885041236877
In grad_steps = 777, loss = 0.002237194450572133
In grad_steps = 778, loss = 0.0010751622030511498
In grad_steps = 779, loss = 0.001512560062110424
In grad_steps = 780, loss = 0.06685949862003326
In grad_steps = 781, loss = 0.013211989775300026
In grad_steps = 782, loss = 0.0036597352009266615
In grad_steps = 783, loss = 0.0012889866484329104
In grad_steps = 784, loss = 0.0016803396865725517
In grad_steps = 785, loss = 0.0019938612822443247
In grad_steps = 786, loss = 0.010963577777147293
In grad_steps = 787, loss = 0.0015373664209619164
In grad_steps = 788, loss = 0.011317902244627476
In grad_steps = 789, loss = 0.00923610758036375
In grad_steps = 790, loss = 0.0037855315022170544
In grad_steps = 791, loss = 0.000822887581307441
In grad_steps = 792, loss = 0.0003335410146974027
In grad_steps = 793, loss = 0.0008777163457125425
In grad_steps = 794, loss = 0.005010325461626053
In grad_steps = 795, loss = 0.005452960263937712
In grad_steps = 796, loss = 0.0030698375776410103
In grad_steps = 797, loss = 0.0013060275232419372
In grad_steps = 798, loss = 0.0003110051329713315
In grad_steps = 799, loss = 0.0004816959553863853
In grad_steps = 800, loss = 0.0011843094835057855
In grad_steps = 801, loss = 0.0016522801015526056
In grad_steps = 802, loss = 0.005811956711113453
In grad_steps = 803, loss = 0.000252119789365679
In grad_steps = 804, loss = 0.0012468851637095213
In grad_steps = 805, loss = 0.009839704260230064
In grad_steps = 806, loss = 0.0002717145252972841
In grad_steps = 807, loss = 0.0005090879276394844
In grad_steps = 808, loss = 0.04904196411371231
In grad_steps = 809, loss = 0.0003582244971767068
In grad_steps = 810, loss = 0.0009666193509474397
In grad_steps = 811, loss = 0.0004502350930124521
In grad_steps = 812, loss = 7.129621371859685e-05
In grad_steps = 813, loss = 4.116976197110489e-05
In grad_steps = 814, loss = 0.0001262041478184983
In grad_steps = 815, loss = 9.325534483650699e-05
In grad_steps = 816, loss = 0.0007647710735909641
In grad_steps = 817, loss = 7.896716851973906e-05
In grad_steps = 818, loss = 4.348656875663437e-05
In grad_steps = 819, loss = 0.008131518959999084
In grad_steps = 820, loss = 5.49351898371242e-05
In grad_steps = 821, loss = 2.390839836152736e-05
In grad_steps = 822, loss = 0.01442725956439972
In grad_steps = 823, loss = 7.801483297953382e-05
In grad_steps = 824, loss = 0.014665725640952587
In grad_steps = 825, loss = 6.888566713314503e-05
In grad_steps = 826, loss = 0.0009741621906869113
In grad_steps = 827, loss = 0.00011134509259136394
In grad_steps = 828, loss = 2.5182114768540487e-05
In grad_steps = 829, loss = 6.621151987928897e-05
In grad_steps = 830, loss = 1.3716248759010341e-05
In grad_steps = 831, loss = 4.063229789608158e-05
In grad_steps = 832, loss = 9.891742956824601e-05
In grad_steps = 833, loss = 0.06472887843847275
In grad_steps = 834, loss = 1.63312470249366e-05
In grad_steps = 835, loss = 2.725230842770543e-05
In grad_steps = 836, loss = 4.298625572118908e-05
In grad_steps = 837, loss = 0.004329974763095379
In grad_steps = 838, loss = 0.0004476483154576272
In grad_steps = 839, loss = 1.457499365642434e-05
Beginning epoch 9
In grad_steps = 840, loss = 0.0010958954226225615
In grad_steps = 841, loss = 0.00029567661113105714
In grad_steps = 842, loss = 0.12333717197179794
In grad_steps = 843, loss = 0.6071255207061768
In grad_steps = 844, loss = 3.3898955734912306e-05
In grad_steps = 845, loss = 0.0005163681926205754
In grad_steps = 846, loss = 0.0035585013683885336
In grad_steps = 847, loss = 0.0051597668789327145
In grad_steps = 848, loss = 0.17641916871070862
In grad_steps = 849, loss = 0.10431168228387833
In grad_steps = 850, loss = 0.240871399641037
In grad_steps = 851, loss = 0.366422563791275
In grad_steps = 852, loss = 0.0023412941955029964
In grad_steps = 853, loss = 0.14799651503562927
In grad_steps = 854, loss = 0.04352392256259918
In grad_steps = 855, loss = 0.2457270473241806
In grad_steps = 856, loss = 0.09773510694503784
In grad_steps = 857, loss = 0.10725987702608109
In grad_steps = 858, loss = 0.0785718485713005
In grad_steps = 859, loss = 0.011325393803417683
In grad_steps = 860, loss = 0.09314841032028198
In grad_steps = 861, loss = 0.016263136640191078
In grad_steps = 862, loss = 0.006105394102632999
In grad_steps = 863, loss = 0.016523072496056557
In grad_steps = 864, loss = 0.1937011182308197
In grad_steps = 865, loss = 0.19963006675243378
In grad_steps = 866, loss = 0.01275157555937767
In grad_steps = 867, loss = 0.005177602171897888
In grad_steps = 868, loss = 0.0033626509830355644
In grad_steps = 869, loss = 0.0024266454856842756
In grad_steps = 870, loss = 0.013223526068031788
In grad_steps = 871, loss = 0.003956197295337915
In grad_steps = 872, loss = 0.00114161754027009
In grad_steps = 873, loss = 0.0012836118694394827
In grad_steps = 874, loss = 0.4838489592075348
In grad_steps = 875, loss = 0.0014153198571875691
In grad_steps = 876, loss = 0.09183096885681152
In grad_steps = 877, loss = 0.005830581299960613
In grad_steps = 878, loss = 0.07435160130262375
In grad_steps = 879, loss = 0.0060629150830209255
In grad_steps = 880, loss = 0.009110524319112301
In grad_steps = 881, loss = 0.04649659991264343
In grad_steps = 882, loss = 0.016589460894465446
In grad_steps = 883, loss = 0.018110305070877075
In grad_steps = 884, loss = 0.01856525056064129
In grad_steps = 885, loss = 0.027811193838715553
In grad_steps = 886, loss = 0.03668110817670822
In grad_steps = 887, loss = 0.04629557579755783
In grad_steps = 888, loss = 0.017497500404715538
In grad_steps = 889, loss = 0.0066944570280611515
In grad_steps = 890, loss = 0.00471350084990263
In grad_steps = 891, loss = 0.019818469882011414
In grad_steps = 892, loss = 0.0032017980702221394
In grad_steps = 893, loss = 0.009871597401797771
In grad_steps = 894, loss = 0.02679680474102497
In grad_steps = 895, loss = 0.051433462649583817
In grad_steps = 896, loss = 0.06495862454175949
In grad_steps = 897, loss = 0.0006725447019562125
In grad_steps = 898, loss = 0.0028703880961984396
In grad_steps = 899, loss = 0.007189530413597822
In grad_steps = 900, loss = 0.007215466815978289
In grad_steps = 901, loss = 0.020433923229575157
In grad_steps = 902, loss = 0.004085189662873745
In grad_steps = 903, loss = 0.0007925484096631408
In grad_steps = 904, loss = 0.00015863121370784938
In grad_steps = 905, loss = 0.0016769730718806386
In grad_steps = 906, loss = 0.008281699381768703
In grad_steps = 907, loss = 0.0003295567585155368
In grad_steps = 908, loss = 0.0003539917233865708
In grad_steps = 909, loss = 0.0002904265420511365
In grad_steps = 910, loss = 0.00025201524840667844
In grad_steps = 911, loss = 0.0001483564847148955
In grad_steps = 912, loss = 0.01079701166599989
In grad_steps = 913, loss = 0.18454809486865997
In grad_steps = 914, loss = 0.3213239908218384
In grad_steps = 915, loss = 0.00046540499897673726
In grad_steps = 916, loss = 0.005605142563581467
In grad_steps = 917, loss = 0.000723569595720619
In grad_steps = 918, loss = 0.0009378284448757768
In grad_steps = 919, loss = 0.001966552808880806
In grad_steps = 920, loss = 0.002411870053038001
In grad_steps = 921, loss = 0.3322177529335022
In grad_steps = 922, loss = 0.0014784542145207524
In grad_steps = 923, loss = 0.0013118382776156068
In grad_steps = 924, loss = 0.011340667493641376
In grad_steps = 925, loss = 0.004555561114102602
In grad_steps = 926, loss = 0.0017516675870865583
In grad_steps = 927, loss = 0.004291651304811239
In grad_steps = 928, loss = 0.011959117837250233
In grad_steps = 929, loss = 0.07424046844244003
In grad_steps = 930, loss = 0.013068842701613903
In grad_steps = 931, loss = 0.033015549182891846
In grad_steps = 932, loss = 0.0066804648377001286
In grad_steps = 933, loss = 0.04355743154883385
In grad_steps = 934, loss = 0.03674525022506714
In grad_steps = 935, loss = 0.011663427576422691
In grad_steps = 936, loss = 0.009217572398483753
In grad_steps = 937, loss = 0.0032288378570228815
In grad_steps = 938, loss = 0.15075765550136566
In grad_steps = 939, loss = 0.003098823595792055
In grad_steps = 940, loss = 0.010616552084684372
In grad_steps = 941, loss = 0.048531338572502136
In grad_steps = 942, loss = 0.017251402139663696
In grad_steps = 943, loss = 0.006963972933590412
In grad_steps = 944, loss = 0.0018823519349098206
Beginning epoch 10
In grad_steps = 945, loss = 0.010423485189676285
In grad_steps = 946, loss = 0.0023066287394613028
In grad_steps = 947, loss = 0.0014519818359985948
In grad_steps = 948, loss = 0.0012343574780970812
In grad_steps = 949, loss = 0.0014916600193828344
In grad_steps = 950, loss = 0.005864880513399839
In grad_steps = 951, loss = 0.003552226582542062
In grad_steps = 952, loss = 0.0008666290668770671
In grad_steps = 953, loss = 0.004685761407017708
In grad_steps = 954, loss = 0.002333341632038355
In grad_steps = 955, loss = 0.0012460551224648952
In grad_steps = 956, loss = 0.08793912082910538
In grad_steps = 957, loss = 0.0007972548482939601
In grad_steps = 958, loss = 0.001942486152984202
In grad_steps = 959, loss = 0.0004852320998907089
In grad_steps = 960, loss = 0.00399028742685914
In grad_steps = 961, loss = 0.000896724290214479
In grad_steps = 962, loss = 0.3403383195400238
In grad_steps = 963, loss = 0.0008068533497862518
In grad_steps = 964, loss = 0.0006755542126484215
In grad_steps = 965, loss = 0.001287146471440792
In grad_steps = 966, loss = 0.004942291881889105
In grad_steps = 967, loss = 0.0030015252996236086
In grad_steps = 968, loss = 0.047291100025177
In grad_steps = 969, loss = 0.19402188062667847
In grad_steps = 970, loss = 0.0016376666026189923
In grad_steps = 971, loss = 0.003218692960217595
In grad_steps = 972, loss = 0.0015983465127646923
In grad_steps = 973, loss = 0.001286615151911974
In grad_steps = 974, loss = 0.00199519214220345
In grad_steps = 975, loss = 0.0017978850519284606
In grad_steps = 976, loss = 0.00466654310002923
In grad_steps = 977, loss = 0.0014710620744153857
In grad_steps = 978, loss = 0.0014552996726706624
In grad_steps = 979, loss = 0.017028233036398888
In grad_steps = 980, loss = 0.0023070231545716524
In grad_steps = 981, loss = 0.0008952600182965398
In grad_steps = 982, loss = 0.008418526500463486
In grad_steps = 983, loss = 0.0017740355106070638
In grad_steps = 984, loss = 0.0075355032458901405
In grad_steps = 985, loss = 0.0034828048665076494
In grad_steps = 986, loss = 0.025780633091926575
In grad_steps = 987, loss = 0.0018854833906516433
In grad_steps = 988, loss = 0.0011870100861415267
In grad_steps = 989, loss = 0.0006130766123533249
In grad_steps = 990, loss = 0.0010608889861032367
In grad_steps = 991, loss = 0.004429903347045183
In grad_steps = 992, loss = 0.0022699707187712193
In grad_steps = 993, loss = 0.003762681968510151
In grad_steps = 994, loss = 0.0011978178517892957
In grad_steps = 995, loss = 0.0024785753339529037
In grad_steps = 996, loss = 0.0014799037016928196
In grad_steps = 997, loss = 0.0009300688398070633
In grad_steps = 998, loss = 0.2797788381576538
In grad_steps = 999, loss = 0.005325150676071644
In grad_steps = 1000, loss = 0.009692489169538021
In grad_steps = 1001, loss = 0.000823819893412292
In grad_steps = 1002, loss = 0.000610152434092015
In grad_steps = 1003, loss = 0.0031481562182307243
In grad_steps = 1004, loss = 0.0017363062361255288
In grad_steps = 1005, loss = 0.0017588759073987603
In grad_steps = 1006, loss = 0.0008856283966451883
In grad_steps = 1007, loss = 0.004776767920702696
In grad_steps = 1008, loss = 0.002297776285558939
In grad_steps = 1009, loss = 0.0005625333869829774
In grad_steps = 1010, loss = 0.002804656745865941
In grad_steps = 1011, loss = 0.002293118741363287
In grad_steps = 1012, loss = 0.0010434938594698906
In grad_steps = 1013, loss = 0.0016281913267448545
In grad_steps = 1014, loss = 0.0014081831322982907
In grad_steps = 1015, loss = 0.00477232551202178
In grad_steps = 1016, loss = 0.0015174711588770151
In grad_steps = 1017, loss = 0.0012008048361167312
In grad_steps = 1018, loss = 0.012479379773139954
In grad_steps = 1019, loss = 0.002333065029233694
In grad_steps = 1020, loss = 0.00336079066619277
In grad_steps = 1021, loss = 0.0032023126259446144
In grad_steps = 1022, loss = 0.004352968651801348
In grad_steps = 1023, loss = 0.0018803417915478349
In grad_steps = 1024, loss = 0.0012716930359601974
In grad_steps = 1025, loss = 0.0010409981478005648
In grad_steps = 1026, loss = 0.00205306732095778
In grad_steps = 1027, loss = 0.0005270335823297501
In grad_steps = 1028, loss = 0.0006019446882419288
In grad_steps = 1029, loss = 0.013349680230021477
In grad_steps = 1030, loss = 0.0006981970509514213
In grad_steps = 1031, loss = 0.00033991318196058273
In grad_steps = 1032, loss = 0.0024493751116096973
In grad_steps = 1033, loss = 0.002342778956517577
In grad_steps = 1034, loss = 0.001942049479112029
In grad_steps = 1035, loss = 0.008719263598322868
In grad_steps = 1036, loss = 0.010455979034304619
In grad_steps = 1037, loss = 0.00044737281859852374
In grad_steps = 1038, loss = 0.0006885798065923154
In grad_steps = 1039, loss = 0.00043711019679903984
In grad_steps = 1040, loss = 0.0012532315449789166
In grad_steps = 1041, loss = 0.0077759744599461555
In grad_steps = 1042, loss = 0.000752648280467838
In grad_steps = 1043, loss = 0.014825443737208843
In grad_steps = 1044, loss = 0.00027673316071741283
In grad_steps = 1045, loss = 0.0005065792938694358
In grad_steps = 1046, loss = 0.00023198073904495686
In grad_steps = 1047, loss = 0.0017853124300017953
In grad_steps = 1048, loss = 0.0005101298447698355
In grad_steps = 1049, loss = 0.00020822476653847843
Elapsed time: 1625.2095608711243 seconds for ensemble 1 with 10 epochs
LoRA instance 1 evaluation complete. Data saved to /pscratch/sd/t/tianle/lucid/other_source/SURP_2024/results/tmp_lora_ensemble/Llama3-8B-set-6/test_data_instance_1_seed_10094.npz.
lora instance i = 1 Successfully finished.
Training lora instance 2
DEBUG: Total parameters: 8037076992
DEBUG: Trainable parameters: 6815744
Beginning epoch 1
In grad_steps = 0, loss = 0.8738614320755005
In grad_steps = 1, loss = 0.7885340452194214
In grad_steps = 2, loss = 0.7311965823173523
In grad_steps = 3, loss = 0.7009633183479309
In grad_steps = 4, loss = 0.7483982443809509
In grad_steps = 5, loss = 0.6922094821929932
In grad_steps = 6, loss = 0.7121555209159851
In grad_steps = 7, loss = 0.6604786515235901
In grad_steps = 8, loss = 0.7015464901924133
In grad_steps = 9, loss = 0.6539086103439331
In grad_steps = 10, loss = 0.6775633692741394
In grad_steps = 11, loss = 0.5797504186630249
In grad_steps = 12, loss = 0.6798068881034851
In grad_steps = 13, loss = 0.771476149559021
In grad_steps = 14, loss = 0.6136308312416077
In grad_steps = 15, loss = 0.6358680129051208
In grad_steps = 16, loss = 0.6897314786911011
In grad_steps = 17, loss = 0.5361344218254089
In grad_steps = 18, loss = 0.6457014083862305
In grad_steps = 19, loss = 0.49507617950439453
In grad_steps = 20, loss = 0.5904108285903931
In grad_steps = 21, loss = 0.5738888382911682
In grad_steps = 22, loss = 0.563005268573761
In grad_steps = 23, loss = 0.35987919569015503
In grad_steps = 24, loss = 0.5609555244445801
In grad_steps = 25, loss = 0.698114812374115
In grad_steps = 26, loss = 0.5765879154205322
In grad_steps = 27, loss = 0.36779627203941345
In grad_steps = 28, loss = 0.26939457654953003
In grad_steps = 29, loss = 0.423467755317688
In grad_steps = 30, loss = 0.6418097019195557
In grad_steps = 31, loss = 0.33994060754776
In grad_steps = 32, loss = 0.7459666132926941
In grad_steps = 33, loss = 0.5142901539802551
In grad_steps = 34, loss = 0.6192376613616943
In grad_steps = 35, loss = 0.2207232564687729
In grad_steps = 36, loss = 0.35252612829208374
In grad_steps = 37, loss = 0.5680271983146667
In grad_steps = 38, loss = 0.6545916199684143
In grad_steps = 39, loss = 0.2937098741531372
In grad_steps = 40, loss = 0.5430963635444641
In grad_steps = 41, loss = 0.6207259893417358
In grad_steps = 42, loss = 0.3775019645690918
In grad_steps = 43, loss = 0.3505963683128357
In grad_steps = 44, loss = 0.31824854016304016
In grad_steps = 45, loss = 0.48209530115127563
In grad_steps = 46, loss = 0.281816303730011
In grad_steps = 47, loss = 0.3614402711391449
In grad_steps = 48, loss = 0.3028719127178192
In grad_steps = 49, loss = 0.15808412432670593
In grad_steps = 50, loss = 0.7442314028739929
In grad_steps = 51, loss = 0.1247134730219841
In grad_steps = 52, loss = 0.3070444166660309
In grad_steps = 53, loss = 0.5701199173927307
In grad_steps = 54, loss = 0.3404003977775574
In grad_steps = 55, loss = 0.3530430793762207
In grad_steps = 56, loss = 0.2393844574689865
In grad_steps = 57, loss = 0.25186246633529663
In grad_steps = 58, loss = 0.24048520624637604
In grad_steps = 59, loss = 0.2504327893257141
In grad_steps = 60, loss = 0.5425667762756348
In grad_steps = 61, loss = 0.24550704658031464
In grad_steps = 62, loss = 0.2450145184993744
In grad_steps = 63, loss = 0.2587226331233978
In grad_steps = 64, loss = 0.09353244304656982
In grad_steps = 65, loss = 0.2210227996110916
In grad_steps = 66, loss = 0.32370638847351074
In grad_steps = 67, loss = 0.3382434844970703
In grad_steps = 68, loss = 0.3435807228088379
In grad_steps = 69, loss = 0.33418387174606323
In grad_steps = 70, loss = 0.2535772919654846
In grad_steps = 71, loss = 0.3067542016506195
In grad_steps = 72, loss = 0.5370019674301147
In grad_steps = 73, loss = 0.6155862808227539
In grad_steps = 74, loss = 0.3026142418384552
In grad_steps = 75, loss = 0.2176201492547989
In grad_steps = 76, loss = 0.35086971521377563
In grad_steps = 77, loss = 0.1541215479373932
In grad_steps = 78, loss = 0.2080141305923462
In grad_steps = 79, loss = 0.2774577736854553
In grad_steps = 80, loss = 0.46760332584381104
In grad_steps = 81, loss = 0.3233424425125122
In grad_steps = 82, loss = 0.12904900312423706
In grad_steps = 83, loss = 0.33295631408691406
In grad_steps = 84, loss = 0.20033049583435059
In grad_steps = 85, loss = 0.05218984931707382
In grad_steps = 86, loss = 0.2070014774799347
In grad_steps = 87, loss = 0.8042533993721008
In grad_steps = 88, loss = 0.7430564761161804
In grad_steps = 89, loss = 0.30910271406173706
In grad_steps = 90, loss = 0.3402769863605499
In grad_steps = 91, loss = 0.268462210893631
In grad_steps = 92, loss = 0.245209202170372
In grad_steps = 93, loss = 0.2388668954372406
In grad_steps = 94, loss = 0.20928719639778137
In grad_steps = 95, loss = 0.2690725326538086
In grad_steps = 96, loss = 0.38914889097213745
In grad_steps = 97, loss = 0.24553489685058594
In grad_steps = 98, loss = 0.6594103574752808
In grad_steps = 99, loss = 0.11545046418905258
In grad_steps = 100, loss = 0.20844893157482147
In grad_steps = 101, loss = 0.11475807428359985
In grad_steps = 102, loss = 0.15302959084510803
In grad_steps = 103, loss = 0.6650983095169067
In grad_steps = 104, loss = 0.29693087935447693
Beginning epoch 2
In grad_steps = 105, loss = 0.28954699635505676
In grad_steps = 106, loss = 0.22085878252983093
In grad_steps = 107, loss = 0.16750329732894897
In grad_steps = 108, loss = 0.25025710463523865
In grad_steps = 109, loss = 0.4149717390537262
In grad_steps = 110, loss = 0.38046926259994507
In grad_steps = 111, loss = 0.2854284942150116
In grad_steps = 112, loss = 0.12609276175498962
In grad_steps = 113, loss = 0.192018061876297
In grad_steps = 114, loss = 0.1145964190363884
In grad_steps = 115, loss = 0.1713278889656067
In grad_steps = 116, loss = 0.15492132306098938
In grad_steps = 117, loss = 0.25058436393737793
In grad_steps = 118, loss = 0.8354302048683167
In grad_steps = 119, loss = 0.08363435417413712
In grad_steps = 120, loss = 0.17486540973186493
In grad_steps = 121, loss = 0.04938887804746628
In grad_steps = 122, loss = 0.20260293781757355
In grad_steps = 123, loss = 0.0991448238492012
In grad_steps = 124, loss = 0.1408519595861435
In grad_steps = 125, loss = 0.3547895848751068
In grad_steps = 126, loss = 0.12797564268112183
In grad_steps = 127, loss = 0.07914083451032639
In grad_steps = 128, loss = 0.15786100924015045
In grad_steps = 129, loss = 0.21865126490592957
In grad_steps = 130, loss = 0.2584514319896698
In grad_steps = 131, loss = 0.1683134138584137
In grad_steps = 132, loss = 0.1535670906305313
In grad_steps = 133, loss = 0.12951931357383728
In grad_steps = 134, loss = 0.11618900299072266
In grad_steps = 135, loss = 0.13280975818634033
In grad_steps = 136, loss = 0.043303459882736206
In grad_steps = 137, loss = 0.15905998647212982
In grad_steps = 138, loss = 0.10406667739152908
In grad_steps = 139, loss = 0.22763435542583466
In grad_steps = 140, loss = 0.02712538093328476
In grad_steps = 141, loss = 0.08101783692836761
In grad_steps = 142, loss = 0.2095615118741989
In grad_steps = 143, loss = 0.5112642049789429
In grad_steps = 144, loss = 0.1860632300376892
In grad_steps = 145, loss = 0.0904378816485405
In grad_steps = 146, loss = 0.21695378422737122
In grad_steps = 147, loss = 0.014740698970854282
In grad_steps = 148, loss = 0.1529511958360672
In grad_steps = 149, loss = 0.02474377676844597
In grad_steps = 150, loss = 0.29993653297424316
In grad_steps = 151, loss = 0.3380568027496338
In grad_steps = 152, loss = 0.31810352206230164
In grad_steps = 153, loss = 0.04107106477022171
In grad_steps = 154, loss = 0.030931318178772926
In grad_steps = 155, loss = 0.23546075820922852
In grad_steps = 156, loss = 0.22977811098098755
In grad_steps = 157, loss = 0.2888564169406891
In grad_steps = 158, loss = 0.3136519193649292
In grad_steps = 159, loss = 0.21696987748146057
In grad_steps = 160, loss = 0.16303905844688416
In grad_steps = 161, loss = 0.08934536576271057
In grad_steps = 162, loss = 0.13896775245666504
In grad_steps = 163, loss = 0.11431728303432465
In grad_steps = 164, loss = 0.1941775381565094
In grad_steps = 165, loss = 0.19969668984413147
In grad_steps = 166, loss = 0.07025966793298721
In grad_steps = 167, loss = 0.13020899891853333
In grad_steps = 168, loss = 0.12995320558547974
In grad_steps = 169, loss = 0.04007691144943237
In grad_steps = 170, loss = 0.0496385358273983
In grad_steps = 171, loss = 0.15795008838176727
In grad_steps = 172, loss = 0.129508838057518
In grad_steps = 173, loss = 0.03453248739242554
In grad_steps = 174, loss = 0.021892203018069267
In grad_steps = 175, loss = 0.147492453455925
In grad_steps = 176, loss = 0.03614392504096031
In grad_steps = 177, loss = 0.15402354300022125
In grad_steps = 178, loss = 0.5658239126205444
In grad_steps = 179, loss = 0.1560455560684204
In grad_steps = 180, loss = 0.010608624666929245
In grad_steps = 181, loss = 0.15994200110435486
In grad_steps = 182, loss = 0.010038917884230614
In grad_steps = 183, loss = 0.059145938605070114
In grad_steps = 184, loss = 0.05779515951871872
In grad_steps = 185, loss = 0.07017193734645844
In grad_steps = 186, loss = 0.36465901136398315
In grad_steps = 187, loss = 0.008203658275306225
In grad_steps = 188, loss = 0.11477947235107422
In grad_steps = 189, loss = 0.14525216817855835
In grad_steps = 190, loss = 0.03430887311697006
In grad_steps = 191, loss = 0.0056907921098172665
In grad_steps = 192, loss = 0.28519976139068604
In grad_steps = 193, loss = 0.4089105427265167
In grad_steps = 194, loss = 0.07488761097192764
In grad_steps = 195, loss = 0.16250647604465485
In grad_steps = 196, loss = 0.23609276115894318
In grad_steps = 197, loss = 0.06260275095701218
In grad_steps = 198, loss = 0.11224277317523956
In grad_steps = 199, loss = 0.28673800826072693
In grad_steps = 200, loss = 0.15001077950000763
In grad_steps = 201, loss = 0.23765721917152405
In grad_steps = 202, loss = 0.13137276470661163
In grad_steps = 203, loss = 0.28846967220306396
In grad_steps = 204, loss = 0.029295731335878372
In grad_steps = 205, loss = 0.1095058023929596
In grad_steps = 206, loss = 0.017025833949446678
In grad_steps = 207, loss = 0.07041337341070175
In grad_steps = 208, loss = 0.13246974349021912
In grad_steps = 209, loss = 0.029337292537093163
Beginning epoch 3
In grad_steps = 210, loss = 0.13856099545955658
In grad_steps = 211, loss = 0.07129404693841934
In grad_steps = 212, loss = 0.11835121363401413
In grad_steps = 213, loss = 0.18257978558540344
In grad_steps = 214, loss = 0.10844715684652328
In grad_steps = 215, loss = 0.025640182197093964
In grad_steps = 216, loss = 0.25354641675949097
In grad_steps = 217, loss = 0.056521691381931305
In grad_steps = 218, loss = 0.22857879102230072
In grad_steps = 219, loss = 0.01076812669634819
In grad_steps = 220, loss = 0.11202848702669144
In grad_steps = 221, loss = 0.023051472380757332
In grad_steps = 222, loss = 0.25477537512779236
In grad_steps = 223, loss = 0.6938865184783936
In grad_steps = 224, loss = 0.049187641590833664
In grad_steps = 225, loss = 0.11806436628103256
In grad_steps = 226, loss = 0.017822371795773506
In grad_steps = 227, loss = 0.09077973663806915
In grad_steps = 228, loss = 0.10142163932323456
In grad_steps = 229, loss = 0.029749516397714615
In grad_steps = 230, loss = 0.039281927049160004
In grad_steps = 231, loss = 0.025659341365098953
In grad_steps = 232, loss = 0.04979689046740532
In grad_steps = 233, loss = 0.08786202222108841
In grad_steps = 234, loss = 0.4360460937023163
In grad_steps = 235, loss = 0.035448212176561356
In grad_steps = 236, loss = 0.20856842398643494
In grad_steps = 237, loss = 0.16776596009731293
In grad_steps = 238, loss = 0.17890605330467224
In grad_steps = 239, loss = 0.030073432251811028
In grad_steps = 240, loss = 0.030532043427228928
In grad_steps = 241, loss = 0.05261228606104851
In grad_steps = 242, loss = 0.13821808993816376
In grad_steps = 243, loss = 0.09522783756256104
In grad_steps = 244, loss = 0.258994996547699
In grad_steps = 245, loss = 0.002768017817288637
In grad_steps = 246, loss = 0.026788907125592232
In grad_steps = 247, loss = 0.12478426098823547
In grad_steps = 248, loss = 0.11663605272769928
In grad_steps = 249, loss = 0.1442149430513382
In grad_steps = 250, loss = 0.01821337267756462
In grad_steps = 251, loss = 0.2316400408744812
In grad_steps = 252, loss = 0.003932512830942869
In grad_steps = 253, loss = 0.015089689753949642
In grad_steps = 254, loss = 0.014111515134572983
In grad_steps = 255, loss = 0.04487740993499756
In grad_steps = 256, loss = 0.21392031013965607
In grad_steps = 257, loss = 0.07187996059656143
In grad_steps = 258, loss = 0.04497769847512245
In grad_steps = 259, loss = 0.08476418256759644
In grad_steps = 260, loss = 0.0029297147411853075
In grad_steps = 261, loss = 0.15755751729011536
In grad_steps = 262, loss = 0.0715177059173584
In grad_steps = 263, loss = 0.07337834686040878
In grad_steps = 264, loss = 0.04830602928996086
In grad_steps = 265, loss = 0.02585715986788273
In grad_steps = 266, loss = 0.15076687932014465
In grad_steps = 267, loss = 0.0746028795838356
In grad_steps = 268, loss = 0.04287100210785866
In grad_steps = 269, loss = 0.1183835119009018
In grad_steps = 270, loss = 0.4885185658931732
In grad_steps = 271, loss = 0.005652506835758686
In grad_steps = 272, loss = 0.049130264669656754
In grad_steps = 273, loss = 0.01225186139345169
In grad_steps = 274, loss = 0.005032083485275507
In grad_steps = 275, loss = 0.0702529102563858
In grad_steps = 276, loss = 0.06402730941772461
In grad_steps = 277, loss = 0.05330890789628029
In grad_steps = 278, loss = 0.0714464783668518
In grad_steps = 279, loss = 0.042937807738780975
In grad_steps = 280, loss = 0.10386250913143158
In grad_steps = 281, loss = 0.02041143923997879
In grad_steps = 282, loss = 0.07025924324989319
In grad_steps = 283, loss = 0.24835997819900513
In grad_steps = 284, loss = 0.3819652199745178
In grad_steps = 285, loss = 0.028125878423452377
In grad_steps = 286, loss = 0.17545558512210846
In grad_steps = 287, loss = 0.00443427124992013
In grad_steps = 288, loss = 0.005513529293239117
In grad_steps = 289, loss = 0.05189884826540947
In grad_steps = 290, loss = 0.06698516011238098
In grad_steps = 291, loss = 0.29312410950660706
In grad_steps = 292, loss = 0.004937445744872093
In grad_steps = 293, loss = 0.01223142258822918
In grad_steps = 294, loss = 0.07350094616413116
In grad_steps = 295, loss = 0.011624971404671669
In grad_steps = 296, loss = 0.009138623252511024
In grad_steps = 297, loss = 0.053657032549381256
In grad_steps = 298, loss = 0.1336400806903839
In grad_steps = 299, loss = 0.04547338932752609
In grad_steps = 300, loss = 0.20561881363391876
In grad_steps = 301, loss = 0.10275068879127502
In grad_steps = 302, loss = 0.015934469178318977
In grad_steps = 303, loss = 0.016140492632985115
In grad_steps = 304, loss = 0.19550210237503052
In grad_steps = 305, loss = 0.020200815051794052
In grad_steps = 306, loss = 0.1277104914188385
In grad_steps = 307, loss = 0.006898765917867422
In grad_steps = 308, loss = 0.2608150839805603
In grad_steps = 309, loss = 0.00893469713628292
In grad_steps = 310, loss = 0.06404313445091248
In grad_steps = 311, loss = 0.006823105737566948
In grad_steps = 312, loss = 0.1426943689584732
In grad_steps = 313, loss = 0.012351780198514462
In grad_steps = 314, loss = 0.05187508836388588
Beginning epoch 4
In grad_steps = 315, loss = 0.13677337765693665
In grad_steps = 316, loss = 0.0036433611530810595
In grad_steps = 317, loss = 0.005696274805814028
In grad_steps = 318, loss = 0.04783619940280914
In grad_steps = 319, loss = 0.08795743435621262
In grad_steps = 320, loss = 0.05381910875439644
In grad_steps = 321, loss = 0.04869996756315231
In grad_steps = 322, loss = 0.0097074955701828
In grad_steps = 323, loss = 0.0780305489897728
In grad_steps = 324, loss = 0.01779092103242874
In grad_steps = 325, loss = 0.2909105122089386
In grad_steps = 326, loss = 0.016945723444223404
In grad_steps = 327, loss = 0.013467377983033657
In grad_steps = 328, loss = 0.2527155876159668
In grad_steps = 329, loss = 0.0013443081406876445
In grad_steps = 330, loss = 0.04319334030151367
In grad_steps = 331, loss = 0.0039967745542526245
In grad_steps = 332, loss = 0.05204422026872635
In grad_steps = 333, loss = 0.11976345628499985
In grad_steps = 334, loss = 0.0037053939886391163
In grad_steps = 335, loss = 0.04015689715743065
In grad_steps = 336, loss = 0.2453671395778656
In grad_steps = 337, loss = 0.09223441779613495
In grad_steps = 338, loss = 0.1328771710395813
In grad_steps = 339, loss = 0.20893166959285736
In grad_steps = 340, loss = 0.013930777087807655
In grad_steps = 341, loss = 0.05229230597615242
In grad_steps = 342, loss = 0.07435385882854462
In grad_steps = 343, loss = 0.1911287158727646
In grad_steps = 344, loss = 0.05395917221903801
In grad_steps = 345, loss = 0.05232642963528633
In grad_steps = 346, loss = 0.1343066543340683
In grad_steps = 347, loss = 0.048666417598724365
In grad_steps = 348, loss = 0.02563861571252346
In grad_steps = 349, loss = 0.11103568226099014
In grad_steps = 350, loss = 0.005598352290689945
In grad_steps = 351, loss = 0.005284364800900221
In grad_steps = 352, loss = 0.02072320133447647
In grad_steps = 353, loss = 0.08908361196517944
In grad_steps = 354, loss = 0.06955907493829727
In grad_steps = 355, loss = 0.058463677763938904
In grad_steps = 356, loss = 0.27296775579452515
In grad_steps = 357, loss = 0.011954575777053833
In grad_steps = 358, loss = 0.34905585646629333
In grad_steps = 359, loss = 0.003994072321802378
In grad_steps = 360, loss = 0.001967367948964238
In grad_steps = 361, loss = 0.010005882009863853
In grad_steps = 362, loss = 0.03197856619954109
In grad_steps = 363, loss = 0.0556531623005867
In grad_steps = 364, loss = 0.032258205115795135
In grad_steps = 365, loss = 0.005437496583908796
In grad_steps = 366, loss = 0.03975784778594971
In grad_steps = 367, loss = 0.05525537207722664
In grad_steps = 368, loss = 0.06617698818445206
In grad_steps = 369, loss = 0.04142677038908005
In grad_steps = 370, loss = 0.35055261850357056
In grad_steps = 371, loss = 0.003992131911218166
In grad_steps = 372, loss = 0.0005104028969071805
In grad_steps = 373, loss = 0.004539812449365854
In grad_steps = 374, loss = 0.001693320693448186
In grad_steps = 375, loss = 0.012332077138125896
In grad_steps = 376, loss = 0.2634764313697815
In grad_steps = 377, loss = 0.09458783268928528
In grad_steps = 378, loss = 0.01846855878829956
In grad_steps = 379, loss = 0.001989635406062007
In grad_steps = 380, loss = 0.001973605714738369
In grad_steps = 381, loss = 0.17664912343025208
In grad_steps = 382, loss = 0.26364123821258545
In grad_steps = 383, loss = 0.022092605009675026
In grad_steps = 384, loss = 0.01191819366067648
In grad_steps = 385, loss = 0.019105900079011917
In grad_steps = 386, loss = 0.013826148584485054
In grad_steps = 387, loss = 0.07698019593954086
In grad_steps = 388, loss = 0.11150326579809189
In grad_steps = 389, loss = 0.15839913487434387
In grad_steps = 390, loss = 0.05453138053417206
In grad_steps = 391, loss = 0.03375577926635742
In grad_steps = 392, loss = 0.023597847670316696
In grad_steps = 393, loss = 0.007138197310268879
In grad_steps = 394, loss = 0.003425918286666274
In grad_steps = 395, loss = 0.0441526360809803
In grad_steps = 396, loss = 0.026691975072026253
In grad_steps = 397, loss = 0.00037041190080344677
In grad_steps = 398, loss = 0.0013131625019013882
In grad_steps = 399, loss = 0.02224050462245941
In grad_steps = 400, loss = 0.00038874964229762554
In grad_steps = 401, loss = 0.00025778371491469443
In grad_steps = 402, loss = 0.2973584532737732
In grad_steps = 403, loss = 0.15232618153095245
In grad_steps = 404, loss = 0.12619879841804504
In grad_steps = 405, loss = 0.07126404345035553
In grad_steps = 406, loss = 0.10922925174236298
In grad_steps = 407, loss = 0.07372069358825684
In grad_steps = 408, loss = 0.04522571340203285
In grad_steps = 409, loss = 0.007110182661563158
In grad_steps = 410, loss = 0.08473115414381027
In grad_steps = 411, loss = 0.015596684068441391
In grad_steps = 412, loss = 0.02495700865983963
In grad_steps = 413, loss = 0.008659398183226585
In grad_steps = 414, loss = 0.006447046995162964
In grad_steps = 415, loss = 0.009266259148716927
In grad_steps = 416, loss = 0.0010363833280280232
In grad_steps = 417, loss = 0.09092801809310913
In grad_steps = 418, loss = 0.01769043318927288
In grad_steps = 419, loss = 0.007108595687896013
Beginning epoch 5
In grad_steps = 420, loss = 0.14877183735370636
In grad_steps = 421, loss = 0.006258673034608364
In grad_steps = 422, loss = 0.016827190294861794
In grad_steps = 423, loss = 0.48158684372901917
In grad_steps = 424, loss = 0.36606070399284363
In grad_steps = 425, loss = 0.011561271734535694
In grad_steps = 426, loss = 0.22525164484977722
In grad_steps = 427, loss = 0.021687062457203865
In grad_steps = 428, loss = 0.0309454258531332
In grad_steps = 429, loss = 0.0646509900689125
In grad_steps = 430, loss = 0.09334760159254074
In grad_steps = 431, loss = 0.14606329798698425
In grad_steps = 432, loss = 0.06978645920753479
In grad_steps = 433, loss = 0.2006981521844864
In grad_steps = 434, loss = 0.008384889923036098
In grad_steps = 435, loss = 0.05280373990535736
In grad_steps = 436, loss = 0.026634059846401215
In grad_steps = 437, loss = 0.009919240139424801
In grad_steps = 438, loss = 0.0053719328716397285
In grad_steps = 439, loss = 0.005779290106147528
In grad_steps = 440, loss = 0.019902000203728676
In grad_steps = 441, loss = 0.027083560824394226
In grad_steps = 442, loss = 0.01438464317470789
In grad_steps = 443, loss = 0.12775464355945587
In grad_steps = 444, loss = 0.1717914640903473
In grad_steps = 445, loss = 0.005963652860373259
In grad_steps = 446, loss = 0.30034583806991577
In grad_steps = 447, loss = 0.005856139585375786
In grad_steps = 448, loss = 0.01100973691791296
In grad_steps = 449, loss = 0.026029709726572037
In grad_steps = 450, loss = 0.042814891785383224
In grad_steps = 451, loss = 0.011395563371479511
In grad_steps = 452, loss = 0.005080243106931448
In grad_steps = 453, loss = 0.07583902776241302
In grad_steps = 454, loss = 0.09949975460767746
In grad_steps = 455, loss = 0.047067370265722275
In grad_steps = 456, loss = 0.07387901097536087
In grad_steps = 457, loss = 0.11974671483039856
In grad_steps = 458, loss = 0.012007850222289562
In grad_steps = 459, loss = 0.09264808893203735
In grad_steps = 460, loss = 0.012079334817826748
In grad_steps = 461, loss = 0.034302204847335815
In grad_steps = 462, loss = 0.00503299618139863
In grad_steps = 463, loss = 0.08470382541418076
In grad_steps = 464, loss = 0.03932809457182884
In grad_steps = 465, loss = 0.09277113527059555
In grad_steps = 466, loss = 0.1301872581243515
In grad_steps = 467, loss = 0.19587820768356323
In grad_steps = 468, loss = 0.0044442457146942616
In grad_steps = 469, loss = 0.0018894452368840575
In grad_steps = 470, loss = 0.029035689309239388
In grad_steps = 471, loss = 0.006687108892947435
In grad_steps = 472, loss = 0.07397661358118057
In grad_steps = 473, loss = 0.005411090794950724
In grad_steps = 474, loss = 0.09699583053588867
In grad_steps = 475, loss = 0.003580220276489854
In grad_steps = 476, loss = 0.038618460297584534
In grad_steps = 477, loss = 0.0028974951710551977
In grad_steps = 478, loss = 0.05074965953826904
In grad_steps = 479, loss = 0.012466269545257092
In grad_steps = 480, loss = 0.01056333165615797
In grad_steps = 481, loss = 0.26573994755744934
In grad_steps = 482, loss = 0.041981250047683716
In grad_steps = 483, loss = 0.046479202806949615
In grad_steps = 484, loss = 0.003009292995557189
In grad_steps = 485, loss = 0.0011190134100615978
In grad_steps = 486, loss = 0.016047433018684387
In grad_steps = 487, loss = 0.013270681723952293
In grad_steps = 488, loss = 0.004115965683013201
In grad_steps = 489, loss = 0.001470950897783041
In grad_steps = 490, loss = 0.22546352446079254
In grad_steps = 491, loss = 0.001992305740714073
In grad_steps = 492, loss = 0.06219146028161049
In grad_steps = 493, loss = 0.05080794915556908
In grad_steps = 494, loss = 0.00377804902382195
In grad_steps = 495, loss = 0.0005005062557756901
In grad_steps = 496, loss = 0.012632512487471104
In grad_steps = 497, loss = 0.002380520338192582
In grad_steps = 498, loss = 0.000287328235572204
In grad_steps = 499, loss = 0.007772154174745083
In grad_steps = 500, loss = 0.009238187223672867
In grad_steps = 501, loss = 0.004038690123707056
In grad_steps = 502, loss = 0.0004836426815018058
In grad_steps = 503, loss = 0.014482883736491203
In grad_steps = 504, loss = 0.201469287276268
In grad_steps = 505, loss = 0.002304503694176674
In grad_steps = 506, loss = 0.0003059670561924577
In grad_steps = 507, loss = 0.046878259629011154
In grad_steps = 508, loss = 0.0014047924196347594
In grad_steps = 509, loss = 0.0040810867212712765
In grad_steps = 510, loss = 0.004316003993153572
In grad_steps = 511, loss = 0.07243761420249939
In grad_steps = 512, loss = 0.0008638015133328736
In grad_steps = 513, loss = 0.14198420941829681
In grad_steps = 514, loss = 0.05116340517997742
In grad_steps = 515, loss = 0.00041932237218134105
In grad_steps = 516, loss = 0.002487500663846731
In grad_steps = 517, loss = 0.004436721559613943
In grad_steps = 518, loss = 0.0256265327334404
In grad_steps = 519, loss = 0.003424054943025112
In grad_steps = 520, loss = 0.0022873275447636843
In grad_steps = 521, loss = 0.00044492841698229313
In grad_steps = 522, loss = 0.09799966216087341
In grad_steps = 523, loss = 0.02201724611222744
In grad_steps = 524, loss = 0.002018339466303587
Beginning epoch 6
In grad_steps = 525, loss = 0.004543229937553406
In grad_steps = 526, loss = 0.0007559916703030467
In grad_steps = 527, loss = 0.003551493166014552
In grad_steps = 528, loss = 0.05377732217311859
In grad_steps = 529, loss = 0.001820102916099131
In grad_steps = 530, loss = 0.0007303915917873383
In grad_steps = 531, loss = 0.0028967824764549732
In grad_steps = 532, loss = 0.00036083132727071643
In grad_steps = 533, loss = 0.004584712442010641
In grad_steps = 534, loss = 0.001024716068059206
In grad_steps = 535, loss = 0.0030686738900840282
In grad_steps = 536, loss = 0.0028922189958393574
In grad_steps = 537, loss = 0.017466304823756218
In grad_steps = 538, loss = 0.13574020564556122
In grad_steps = 539, loss = 0.0006002120790071785
In grad_steps = 540, loss = 0.014090429060161114
In grad_steps = 541, loss = 8.912278281059116e-05
In grad_steps = 542, loss = 0.10348129272460938
In grad_steps = 543, loss = 0.0002867034636437893
In grad_steps = 544, loss = 0.0007475983002223074
In grad_steps = 545, loss = 0.000935711374040693
In grad_steps = 546, loss = 0.0029040214139968157
In grad_steps = 547, loss = 0.0007011088309809566
In grad_steps = 548, loss = 0.028770171105861664
In grad_steps = 549, loss = 0.0021336162462830544
In grad_steps = 550, loss = 0.001825751387514174
In grad_steps = 551, loss = 0.05142444372177124
In grad_steps = 552, loss = 0.015354625880718231
In grad_steps = 553, loss = 0.0021287184208631516
In grad_steps = 554, loss = 0.00408115005120635
In grad_steps = 555, loss = 0.0005395338521338999
In grad_steps = 556, loss = 0.0022841254249215126
In grad_steps = 557, loss = 0.000806979020126164
In grad_steps = 558, loss = 0.0009452193044126034
In grad_steps = 559, loss = 0.0008297188323922455
In grad_steps = 560, loss = 0.0003663225506898016
In grad_steps = 561, loss = 0.00017640969599597156
In grad_steps = 562, loss = 0.0003862256126012653
In grad_steps = 563, loss = 0.006709609646350145
In grad_steps = 564, loss = 0.01198832131922245
In grad_steps = 565, loss = 0.0032305503264069557
In grad_steps = 566, loss = 0.012325931340456009
In grad_steps = 567, loss = 0.00010756051051430404
In grad_steps = 568, loss = 8.454738417640328e-05
In grad_steps = 569, loss = 7.06806531525217e-05
In grad_steps = 570, loss = 0.00015734299086034298
In grad_steps = 571, loss = 0.014482853934168816
In grad_steps = 572, loss = 0.008763651363551617
In grad_steps = 573, loss = 0.00047419845941476524
In grad_steps = 574, loss = 9.96677263174206e-05
In grad_steps = 575, loss = 0.35984981060028076
In grad_steps = 576, loss = 0.00043407006887719035
In grad_steps = 577, loss = 0.008278354071080685
In grad_steps = 578, loss = 0.009027235209941864
In grad_steps = 579, loss = 0.04851870611310005
In grad_steps = 580, loss = 0.00041750853415578604
In grad_steps = 581, loss = 0.0012915008701384068
In grad_steps = 582, loss = 0.00029475410701707006
In grad_steps = 583, loss = 0.0013106761034578085
In grad_steps = 584, loss = 0.0028938225004822016
In grad_steps = 585, loss = 0.006316032260656357
In grad_steps = 586, loss = 0.015173071064054966
In grad_steps = 587, loss = 0.007657249458134174
In grad_steps = 588, loss = 0.009236258454620838
In grad_steps = 589, loss = 0.006534819025546312
In grad_steps = 590, loss = 0.015210883691906929
In grad_steps = 591, loss = 0.002275899052619934
In grad_steps = 592, loss = 0.006233230698853731
In grad_steps = 593, loss = 0.01650991104543209
In grad_steps = 594, loss = 0.014903745613992214
In grad_steps = 595, loss = 0.007150363642722368
In grad_steps = 596, loss = 0.00038038380444049835
In grad_steps = 597, loss = 0.002925208769738674
In grad_steps = 598, loss = 0.012706760317087173
In grad_steps = 599, loss = 0.0008111635106615722
In grad_steps = 600, loss = 0.002119713695719838
In grad_steps = 601, loss = 0.000986774335615337
In grad_steps = 602, loss = 0.004523167852312326
In grad_steps = 603, loss = 4.503524178289808e-05
In grad_steps = 604, loss = 0.0012191846035420895
In grad_steps = 605, loss = 0.027217574417591095
In grad_steps = 606, loss = 0.0009205921087414026
In grad_steps = 607, loss = 0.000445778074208647
In grad_steps = 608, loss = 0.0007670276099815965
In grad_steps = 609, loss = 0.022270895540714264
In grad_steps = 610, loss = 0.000295772566460073
In grad_steps = 611, loss = 3.183455555699766e-05
In grad_steps = 612, loss = 0.006264456082135439
In grad_steps = 613, loss = 0.0003527943335939199
In grad_steps = 614, loss = 0.0011161071015521884
In grad_steps = 615, loss = 0.00022243348939809948
In grad_steps = 616, loss = 0.000591563293710351
In grad_steps = 617, loss = 2.954020965262316e-05
In grad_steps = 618, loss = 0.0013590629678219557
In grad_steps = 619, loss = 0.0007569027948193252
In grad_steps = 620, loss = 0.0001564210542710498
In grad_steps = 621, loss = 5.536037861020304e-05
In grad_steps = 622, loss = 5.181690721656196e-05
In grad_steps = 623, loss = 0.0014939302345737815
In grad_steps = 624, loss = 4.555579289444722e-05
In grad_steps = 625, loss = 0.09587343782186508
In grad_steps = 626, loss = 3.1143340493144933e-06
In grad_steps = 627, loss = 0.00027999712619930506
In grad_steps = 628, loss = 0.001533537288196385
In grad_steps = 629, loss = 2.6017969503300264e-05
Beginning epoch 7
In grad_steps = 630, loss = 6.389484042301774e-05
In grad_steps = 631, loss = 0.00016504526138305664
In grad_steps = 632, loss = 0.00024764565750956535
In grad_steps = 633, loss = 0.0006563970819115639
In grad_steps = 634, loss = 0.09797666221857071
In grad_steps = 635, loss = 0.00414300337433815
In grad_steps = 636, loss = 0.0010834148852154613
In grad_steps = 637, loss = 0.00015060673467814922
In grad_steps = 638, loss = 0.0004661212442442775
In grad_steps = 639, loss = 0.00010803913028212264
In grad_steps = 640, loss = 0.015567361377179623
In grad_steps = 641, loss = 0.0008190055959858
In grad_steps = 642, loss = 0.07238313555717468
In grad_steps = 643, loss = 0.16646970808506012
In grad_steps = 644, loss = 8.810212602838874e-05
In grad_steps = 645, loss = 0.001905042678117752
In grad_steps = 646, loss = 0.0024882459547370672
In grad_steps = 647, loss = 0.28012144565582275
In grad_steps = 648, loss = 0.000550760654732585
In grad_steps = 649, loss = 0.0006025927141308784
In grad_steps = 650, loss = 0.0023978541139513254
In grad_steps = 651, loss = 0.013050888665020466
In grad_steps = 652, loss = 0.018211867660284042
In grad_steps = 653, loss = 0.015441046096384525
In grad_steps = 654, loss = 0.03621763736009598
In grad_steps = 655, loss = 0.0018569779349491
In grad_steps = 656, loss = 0.03135089948773384
In grad_steps = 657, loss = 0.006494768429547548
In grad_steps = 658, loss = 0.02842133305966854
In grad_steps = 659, loss = 0.002401175443083048
In grad_steps = 660, loss = 0.003209085436537862
In grad_steps = 661, loss = 0.07159903645515442
In grad_steps = 662, loss = 0.0030732708983123302
In grad_steps = 663, loss = 0.0023161752615123987
In grad_steps = 664, loss = 0.014685762114822865
In grad_steps = 665, loss = 0.0016952680889517069
In grad_steps = 666, loss = 0.0016826700884848833
In grad_steps = 667, loss = 0.000897458172403276
In grad_steps = 668, loss = 0.005620197393000126
In grad_steps = 669, loss = 0.06573211401700974
In grad_steps = 670, loss = 0.007691080681979656
In grad_steps = 671, loss = 0.00837022066116333
In grad_steps = 672, loss = 0.000596961472183466
In grad_steps = 673, loss = 0.18978779017925262
In grad_steps = 674, loss = 0.0005637566209770739
In grad_steps = 675, loss = 0.0025424084160476923
In grad_steps = 676, loss = 0.02181825041770935
In grad_steps = 677, loss = 0.08512865751981735
In grad_steps = 678, loss = 0.002958937780931592
In grad_steps = 679, loss = 0.07809127122163773
In grad_steps = 680, loss = 0.0028716607484966516
In grad_steps = 681, loss = 0.0054792254231870174
In grad_steps = 682, loss = 0.006553273182362318
In grad_steps = 683, loss = 0.023493127897381783
In grad_steps = 684, loss = 0.007267678622156382
In grad_steps = 685, loss = 0.001981381792575121
In grad_steps = 686, loss = 0.003335270332172513
In grad_steps = 687, loss = 0.0013481893111020327
In grad_steps = 688, loss = 0.022188253700733185
In grad_steps = 689, loss = 0.000974855967797339
In grad_steps = 690, loss = 0.00314504886046052
In grad_steps = 691, loss = 0.0004153593326918781
In grad_steps = 692, loss = 0.13130350410938263
In grad_steps = 693, loss = 0.0004528412828221917
In grad_steps = 694, loss = 0.0019623455591499805
In grad_steps = 695, loss = 0.002196182496845722
In grad_steps = 696, loss = 0.0031461308244615793
In grad_steps = 697, loss = 0.014219410717487335
In grad_steps = 698, loss = 0.0005880519165657461
In grad_steps = 699, loss = 0.00037850922672078013
In grad_steps = 700, loss = 0.0006865526083856821
In grad_steps = 701, loss = 0.0003875849361065775
In grad_steps = 702, loss = 0.10699595510959625
In grad_steps = 703, loss = 0.3808385729789734
In grad_steps = 704, loss = 0.04085138440132141
In grad_steps = 705, loss = 0.006955738179385662
In grad_steps = 706, loss = 0.04196115583181381
In grad_steps = 707, loss = 0.001233736053109169
In grad_steps = 708, loss = 0.000761835603043437
In grad_steps = 709, loss = 0.1455155313014984
In grad_steps = 710, loss = 0.0028795271646231413
In grad_steps = 711, loss = 0.1540403515100479
In grad_steps = 712, loss = 0.0018258680356666446
In grad_steps = 713, loss = 0.0035844319500029087
In grad_steps = 714, loss = 0.005966681521385908
In grad_steps = 715, loss = 0.0015333149349316955
In grad_steps = 716, loss = 0.00409616157412529
In grad_steps = 717, loss = 0.010222472250461578
In grad_steps = 718, loss = 0.05652395635843277
In grad_steps = 719, loss = 0.061284538358449936
In grad_steps = 720, loss = 0.01319248415529728
In grad_steps = 721, loss = 0.11573317646980286
In grad_steps = 722, loss = 0.05730334296822548
In grad_steps = 723, loss = 0.017850954085588455
In grad_steps = 724, loss = 0.0058967191725969315
In grad_steps = 725, loss = 0.0010375933488830924
In grad_steps = 726, loss = 0.008223638869822025
In grad_steps = 727, loss = 0.0046864841133356094
In grad_steps = 728, loss = 0.07666105031967163
In grad_steps = 729, loss = 0.0294142235070467
In grad_steps = 730, loss = 0.002587702590972185
In grad_steps = 731, loss = 0.00026581468409858644
In grad_steps = 732, loss = 0.01540515385568142
In grad_steps = 733, loss = 0.1357828974723816
In grad_steps = 734, loss = 0.0004957051714882255
Beginning epoch 8
In grad_steps = 735, loss = 0.009467651136219501
In grad_steps = 736, loss = 0.015039535239338875
In grad_steps = 737, loss = 0.0010424806969240308
In grad_steps = 738, loss = 0.004560487344861031
In grad_steps = 739, loss = 0.023083414882421494
In grad_steps = 740, loss = 0.0007468521362170577
In grad_steps = 741, loss = 0.006663657259196043
In grad_steps = 742, loss = 0.004849650431424379
In grad_steps = 743, loss = 0.0209572184830904
In grad_steps = 744, loss = 0.0006978572346270084
In grad_steps = 745, loss = 0.021092094480991364
In grad_steps = 746, loss = 0.004174387082457542
In grad_steps = 747, loss = 0.0028500831685960293
In grad_steps = 748, loss = 0.07088050991296768
In grad_steps = 749, loss = 0.0002773980959318578
In grad_steps = 750, loss = 0.0011893630726262927
In grad_steps = 751, loss = 0.00012405094457790256
In grad_steps = 752, loss = 0.0002612625830806792
In grad_steps = 753, loss = 0.0003249996225349605
In grad_steps = 754, loss = 0.016749560832977295
In grad_steps = 755, loss = 0.0063884020783007145
In grad_steps = 756, loss = 0.0016850660322234035
In grad_steps = 757, loss = 0.00013539484643843025
In grad_steps = 758, loss = 0.006856828462332487
In grad_steps = 759, loss = 0.0006124734063632786
In grad_steps = 760, loss = 0.00037390709621831775
In grad_steps = 761, loss = 0.026004372164607048
In grad_steps = 762, loss = 0.0002567741321399808
In grad_steps = 763, loss = 0.0009722323738969862
In grad_steps = 764, loss = 0.0002697666350286454
In grad_steps = 765, loss = 0.0004473386215977371
In grad_steps = 766, loss = 0.0004997602081857622
In grad_steps = 767, loss = 3.9367056160699576e-05
In grad_steps = 768, loss = 0.0002229836245533079
In grad_steps = 769, loss = 0.0007428265525959432
In grad_steps = 770, loss = 4.782821633853018e-05
In grad_steps = 771, loss = 0.00012444049934856594
In grad_steps = 772, loss = 5.074251748737879e-05
In grad_steps = 773, loss = 0.00010926568938884884
In grad_steps = 774, loss = 0.0011742716887965798
In grad_steps = 775, loss = 0.0007517609628848732
In grad_steps = 776, loss = 0.0009849461494013667
In grad_steps = 777, loss = 0.0003403068403713405
In grad_steps = 778, loss = 6.264165131142363e-05
In grad_steps = 779, loss = 0.000807403412181884
In grad_steps = 780, loss = 6.219443457666785e-05
In grad_steps = 781, loss = 0.0024399575777351856
In grad_steps = 782, loss = 0.0010182153200730681
In grad_steps = 783, loss = 5.226072244113311e-05
In grad_steps = 784, loss = 0.0002536418614909053
In grad_steps = 785, loss = 0.004624336492270231
In grad_steps = 786, loss = 0.00060205691261217
In grad_steps = 787, loss = 0.00013255508383736014
In grad_steps = 788, loss = 0.00018070956866722554
In grad_steps = 789, loss = 0.005373751744627953
In grad_steps = 790, loss = 0.0008576117688789964
In grad_steps = 791, loss = 1.4878373804094736e-05
In grad_steps = 792, loss = 7.316401934076566e-06
In grad_steps = 793, loss = 0.0001865325029939413
In grad_steps = 794, loss = 4.423608334036544e-05
In grad_steps = 795, loss = 0.0002974990929942578
In grad_steps = 796, loss = 1.948081626323983e-05
In grad_steps = 797, loss = 0.0002686066145543009
In grad_steps = 798, loss = 3.419403947191313e-05
In grad_steps = 799, loss = 5.453765879792627e-06
In grad_steps = 800, loss = 1.8111366443918087e-05
In grad_steps = 801, loss = 4.9866430344991386e-05
In grad_steps = 802, loss = 6.976885197218508e-05
In grad_steps = 803, loss = 7.070534138620133e-06
In grad_steps = 804, loss = 6.488389772130176e-05
In grad_steps = 805, loss = 9.417332876182627e-06
In grad_steps = 806, loss = 9.424607924302109e-06
In grad_steps = 807, loss = 0.00018741503299679607
In grad_steps = 808, loss = 0.004466293845325708
In grad_steps = 809, loss = 8.339982741745189e-05
In grad_steps = 810, loss = 1.0400814971944783e-05
In grad_steps = 811, loss = 5.535723175853491e-06
In grad_steps = 812, loss = 3.4247259463882074e-05
In grad_steps = 813, loss = 1.9892995624104515e-06
In grad_steps = 814, loss = 0.0007243198924697936
In grad_steps = 815, loss = 5.371834049583413e-06
In grad_steps = 816, loss = 1.2158960089436732e-05
In grad_steps = 817, loss = 3.300585831311764e-06
In grad_steps = 818, loss = 3.674542676890269e-05
In grad_steps = 819, loss = 4.451976565178484e-05
In grad_steps = 820, loss = 0.000120872791740112
In grad_steps = 821, loss = 2.0414538539625937e-06
In grad_steps = 822, loss = 4.7691628424217924e-05
In grad_steps = 823, loss = 2.34223607549211e-05
In grad_steps = 824, loss = 3.497682700981386e-05
In grad_steps = 825, loss = 9.222580411005765e-05
In grad_steps = 826, loss = 2.290866177645512e-05
In grad_steps = 827, loss = 0.0016709677875041962
In grad_steps = 828, loss = 0.00030637820600531995
In grad_steps = 829, loss = 5.349406819732394e-06
In grad_steps = 830, loss = 2.9280668059072923e-06
In grad_steps = 831, loss = 3.1292324820242357e-06
In grad_steps = 832, loss = 0.0004715139220934361
In grad_steps = 833, loss = 5.9064994275104254e-05
In grad_steps = 834, loss = 4.097791588719701e-06
In grad_steps = 835, loss = 2.7338965082890354e-05
In grad_steps = 836, loss = 9.685744544185582e-07
In grad_steps = 837, loss = 4.6337729145307094e-05
In grad_steps = 838, loss = 3.9860306060290895e-06
In grad_steps = 839, loss = 4.5378392314887606e-06
Beginning epoch 9
In grad_steps = 840, loss = 6.556369953614194e-06
In grad_steps = 841, loss = 9.148867320618592e-06
In grad_steps = 842, loss = 1.5199157132883556e-06
In grad_steps = 843, loss = 3.366946111782454e-05
In grad_steps = 844, loss = 4.939674909110181e-06
In grad_steps = 845, loss = 9.217661136062816e-05
In grad_steps = 846, loss = 2.4040497009991668e-05
In grad_steps = 847, loss = 4.798064310307382e-06
In grad_steps = 848, loss = 1.3872506315237843e-05
In grad_steps = 849, loss = 1.9818485270661768e-06
In grad_steps = 850, loss = 0.004154518246650696
In grad_steps = 851, loss = 1.6129955838550813e-05
In grad_steps = 852, loss = 0.00010741688311100006
In grad_steps = 853, loss = 1.065389733412303e-05
In grad_steps = 854, loss = 1.922244791785488e-06
In grad_steps = 855, loss = 3.3900000744324643e-06
In grad_steps = 856, loss = 1.6093226804514416e-06
In grad_steps = 857, loss = 4.179682491667336e-06
In grad_steps = 858, loss = 2.6076934318552958e-06
In grad_steps = 859, loss = 1.5199141216726275e-06
In grad_steps = 860, loss = 3.578423275030218e-05
In grad_steps = 861, loss = 2.711997240112396e-06
In grad_steps = 862, loss = 8.642446118756197e-06
In grad_steps = 863, loss = 6.8618974182754755e-06
In grad_steps = 864, loss = 1.7447309801355004e-05
In grad_steps = 865, loss = 6.865154136903584e-05
In grad_steps = 866, loss = 1.7499434761703014e-05
In grad_steps = 867, loss = 3.064121847273782e-05
In grad_steps = 868, loss = 0.00018099120643455535
In grad_steps = 869, loss = 5.7919842220144346e-05
In grad_steps = 870, loss = 3.931825631298125e-05
In grad_steps = 871, loss = 3.240463411202654e-05
In grad_steps = 872, loss = 2.4810369723127224e-06
In grad_steps = 873, loss = 1.888573751784861e-05
In grad_steps = 874, loss = 2.514929474273231e-05
In grad_steps = 875, loss = 4.1275948206020985e-06
In grad_steps = 876, loss = 7.7259392128326e-06
In grad_steps = 877, loss = 3.315490175737068e-06
In grad_steps = 878, loss = 4.656582859752234e-06
In grad_steps = 879, loss = 0.00015718999202363193
In grad_steps = 880, loss = 5.202497050049715e-05
In grad_steps = 881, loss = 0.0006676809280179441
In grad_steps = 882, loss = 1.5109115338418633e-05
In grad_steps = 883, loss = 5.446256182040088e-06
In grad_steps = 884, loss = 1.0296048458258156e-05
In grad_steps = 885, loss = 8.441202226094902e-06
In grad_steps = 886, loss = 7.267123146448284e-05
In grad_steps = 887, loss = 8.270540274679661e-05
In grad_steps = 888, loss = 1.4386313523573335e-05
In grad_steps = 889, loss = 6.981033220654353e-05
In grad_steps = 890, loss = 0.00020486867288127542
In grad_steps = 891, loss = 0.00025566614931449294
In grad_steps = 892, loss = 8.441371392109431e-06
In grad_steps = 893, loss = 1.2627935575437732e-05
In grad_steps = 894, loss = 0.00043981499038636684
In grad_steps = 895, loss = 2.4776360078249127e-05
In grad_steps = 896, loss = 2.659848178154789e-06
In grad_steps = 897, loss = 1.5273657254510908e-06
In grad_steps = 898, loss = 0.00012133309792261571
In grad_steps = 899, loss = 0.001346113858744502
In grad_steps = 900, loss = 0.0003852431837003678
In grad_steps = 901, loss = 4.879967946180841e-06
In grad_steps = 902, loss = 5.774149940407369e-06
In grad_steps = 903, loss = 3.0808994779363275e-05
In grad_steps = 904, loss = 1.8477358025847934e-06
In grad_steps = 905, loss = 1.4937121704861056e-05
In grad_steps = 906, loss = 0.0001527958520455286
In grad_steps = 907, loss = 1.3744931493420154e-05
In grad_steps = 908, loss = 1.5720698911536601e-06
In grad_steps = 909, loss = 6.690473128401209e-06
In grad_steps = 910, loss = 5.4760535022069234e-06
In grad_steps = 911, loss = 2.704533244468621e-06
In grad_steps = 912, loss = 0.00020225865591783077
In grad_steps = 913, loss = 1.0385920177213848e-05
In grad_steps = 914, loss = 3.598662078729831e-05
In grad_steps = 915, loss = 4.373458978079725e-06
In grad_steps = 916, loss = 8.493655059282901e-07
In grad_steps = 917, loss = 1.0661548913049046e-05
In grad_steps = 918, loss = 5.960461066933931e-07
In grad_steps = 919, loss = 0.0011437979992479086
In grad_steps = 920, loss = 3.4048691759380745e-06
In grad_steps = 921, loss = 1.6763764278948656e-06
In grad_steps = 922, loss = 2.9727339097007643e-06
In grad_steps = 923, loss = 6.593501439056126e-06
In grad_steps = 924, loss = 7.380797615041956e-05
In grad_steps = 925, loss = 9.711120219435543e-05
In grad_steps = 926, loss = 5.662438411491166e-07
In grad_steps = 927, loss = 1.535407136543654e-05
In grad_steps = 928, loss = 0.0006931052776053548
In grad_steps = 929, loss = 0.0006553312996402383
In grad_steps = 930, loss = 1.759651604515966e-05
In grad_steps = 931, loss = 8.225136298278812e-06
In grad_steps = 932, loss = 2.0121327906963415e-05
In grad_steps = 933, loss = 0.00011952922068303451
In grad_steps = 934, loss = 5.297173629514873e-06
In grad_steps = 935, loss = 7.897603495621297e-07
In grad_steps = 936, loss = 9.462228831580433e-07
In grad_steps = 937, loss = 3.4450615203240886e-05
In grad_steps = 938, loss = 1.7961023331736214e-05
In grad_steps = 939, loss = 4.59694047094672e-06
In grad_steps = 940, loss = 2.146167571481783e-05
In grad_steps = 941, loss = 3.7997943991285865e-07
In grad_steps = 942, loss = 4.616370642906986e-05
In grad_steps = 943, loss = 2.0787012999790022e-06
In grad_steps = 944, loss = 1.1126173831144115e-06
Beginning epoch 10
In grad_steps = 945, loss = 1.497562834629207e-06
In grad_steps = 946, loss = 2.354354819544824e-06
In grad_steps = 947, loss = 5.885951850359561e-07
In grad_steps = 948, loss = 2.0190950635878835e-06
In grad_steps = 949, loss = 2.242603159174905e-06
In grad_steps = 950, loss = 1.981841023734887e-06
In grad_steps = 951, loss = 6.213569577084854e-06
In grad_steps = 952, loss = 1.5944121969369007e-06
In grad_steps = 953, loss = 1.3038487622907269e-06
In grad_steps = 954, loss = 8.791668619778648e-07
In grad_steps = 955, loss = 5.245131433184724e-06
In grad_steps = 956, loss = 5.997619609843241e-06
In grad_steps = 957, loss = 1.2612713362614159e-05
In grad_steps = 958, loss = 1.9296933260193327e-06
In grad_steps = 959, loss = 5.364415187614213e-07
In grad_steps = 960, loss = 8.717169066585484e-07
In grad_steps = 961, loss = 4.991886157768022e-07
In grad_steps = 962, loss = 8.27011945148115e-07
In grad_steps = 963, loss = 6.034966872903169e-07
In grad_steps = 964, loss = 4.3958397100141156e-07
In grad_steps = 965, loss = 2.711975639613229e-06
In grad_steps = 966, loss = 2.0339875845820643e-06
In grad_steps = 967, loss = 1.0952325055768597e-06
In grad_steps = 968, loss = 2.5778845156310126e-06
In grad_steps = 969, loss = 1.639116817386821e-06
In grad_steps = 970, loss = 1.6242189531112672e-06
In grad_steps = 971, loss = 3.449572886893293e-06
In grad_steps = 972, loss = 5.029060957895126e-06
In grad_steps = 973, loss = 4.924779204884544e-06
In grad_steps = 974, loss = 2.5555195861670654e-06
In grad_steps = 975, loss = 3.2931229725363664e-06
In grad_steps = 976, loss = 1.691274064796744e-06
In grad_steps = 977, loss = 6.034967441337358e-07
In grad_steps = 978, loss = 1.8104790342476917e-06
In grad_steps = 979, loss = 1.3634514743898762e-06
In grad_steps = 980, loss = 6.705517421323748e-07
In grad_steps = 981, loss = 1.0728801953518996e-06
In grad_steps = 982, loss = 9.164194807453896e-07
In grad_steps = 983, loss = 1.3262007314551738e-06
In grad_steps = 984, loss = 1.1212649042136036e-05
In grad_steps = 985, loss = 2.883356955862837e-06
In grad_steps = 986, loss = 2.257514097436797e-06
In grad_steps = 987, loss = 1.169739107353962e-06
In grad_steps = 988, loss = 6.481997729679279e-07
In grad_steps = 989, loss = 6.034963462298037e-07
In grad_steps = 990, loss = 9.983763220589026e-07
In grad_steps = 991, loss = 1.6987221442832379e-06
In grad_steps = 992, loss = 3.136644863843685e-06
In grad_steps = 993, loss = 7.450568091371679e-07
In grad_steps = 994, loss = 7.502367225242779e-06
In grad_steps = 995, loss = 1.2442443448890117e-06
In grad_steps = 996, loss = 1.4393121091416106e-05
In grad_steps = 997, loss = 1.4081568906476605e-06
In grad_steps = 998, loss = 1.5050078445710824e-06
In grad_steps = 999, loss = 9.648248123994563e-06
In grad_steps = 1000, loss = 3.2930911402218044e-06
In grad_steps = 1001, loss = 9.909247182804393e-07
In grad_steps = 1002, loss = 4.4703469370688254e-07
In grad_steps = 1003, loss = 1.0058269026558264e-06
In grad_steps = 1004, loss = 1.154837718786439e-06
In grad_steps = 1005, loss = 4.4106468521931674e-06
In grad_steps = 1006, loss = 1.445400243937911e-06
In grad_steps = 1007, loss = 2.212812205470982e-06
In grad_steps = 1008, loss = 2.6225827696180204e-06
In grad_steps = 1009, loss = 5.885952987227938e-07
In grad_steps = 1010, loss = 3.009992724400945e-06
In grad_steps = 1011, loss = 5.550530204345705e-06
In grad_steps = 1012, loss = 4.067902864335338e-06
In grad_steps = 1013, loss = 6.407495902749361e-07
In grad_steps = 1014, loss = 1.534814145998098e-06
In grad_steps = 1015, loss = 2.227706318080891e-06
In grad_steps = 1016, loss = 9.313200166616298e-07
In grad_steps = 1017, loss = 1.92125062312698e-05
In grad_steps = 1018, loss = 7.547188488388201e-06
In grad_steps = 1019, loss = 1.2411797797540203e-05
In grad_steps = 1020, loss = 1.8700892496781307e-06
In grad_steps = 1021, loss = 4.6938635023252573e-07
In grad_steps = 1022, loss = 3.0323697046696907e-06
In grad_steps = 1023, loss = 3.501771743685822e-07
In grad_steps = 1024, loss = 7.860012374294456e-06
In grad_steps = 1025, loss = 1.2889450999864493e-06
In grad_steps = 1026, loss = 8.717169066585484e-07
In grad_steps = 1027, loss = 7.823092573744361e-07
In grad_steps = 1028, loss = 1.9073337398367585e-06
In grad_steps = 1029, loss = 1.1190352779522073e-05
In grad_steps = 1030, loss = 7.077696864143945e-06
In grad_steps = 1031, loss = 3.7252888773764425e-07
In grad_steps = 1032, loss = 9.878805030894e-06
In grad_steps = 1033, loss = 2.6672946660255548e-06
In grad_steps = 1034, loss = 6.899090294609778e-06
In grad_steps = 1035, loss = 1.2501306628109887e-05
In grad_steps = 1036, loss = 2.9237198759801686e-05
In grad_steps = 1037, loss = 5.8410860219737515e-06
In grad_steps = 1038, loss = 2.8463491617003456e-05
In grad_steps = 1039, loss = 2.0488853351707803e-06
In grad_steps = 1040, loss = 5.88595241879375e-07
In grad_steps = 1041, loss = 6.705519695060502e-07
In grad_steps = 1042, loss = 2.3942071493365802e-05
In grad_steps = 1043, loss = 1.1919958524231333e-05
In grad_steps = 1044, loss = 1.1548378324732766e-06
In grad_steps = 1045, loss = 1.2143365893280134e-05
In grad_steps = 1046, loss = 2.607702356272057e-07
In grad_steps = 1047, loss = 1.5987372535164468e-05
In grad_steps = 1048, loss = 1.8551830862634233e-06
In grad_steps = 1049, loss = 1.04109165022237e-06
Elapsed time: 1624.7101109027863 seconds for ensemble 2 with 10 epochs
LoRA instance 2 evaluation complete. Data saved to /pscratch/sd/t/tianle/lucid/other_source/SURP_2024/results/tmp_lora_ensemble/Llama3-8B-set-6/test_data_instance_2_seed_20187.npz.
lora instance i = 2 Successfully finished.
Training lora instance 3
DEBUG: Total parameters: 8037076992
DEBUG: Trainable parameters: 6815744
Beginning epoch 1
In grad_steps = 0, loss = 0.8738614320755005
In grad_steps = 1, loss = 0.786010205745697
In grad_steps = 2, loss = 0.7345541715621948
In grad_steps = 3, loss = 0.7007761001586914
In grad_steps = 4, loss = 0.745729386806488
In grad_steps = 5, loss = 0.6916680932044983
In grad_steps = 6, loss = 0.7121660709381104
In grad_steps = 7, loss = 0.6595573425292969
In grad_steps = 8, loss = 0.7001627087593079
In grad_steps = 9, loss = 0.6516206860542297
In grad_steps = 10, loss = 0.6770730018615723
In grad_steps = 11, loss = 0.5800636410713196
In grad_steps = 12, loss = 0.6773004531860352
In grad_steps = 13, loss = 0.7704132199287415
In grad_steps = 14, loss = 0.6147926449775696
In grad_steps = 15, loss = 0.6340103149414062
In grad_steps = 16, loss = 0.6880850195884705
In grad_steps = 17, loss = 0.532780110836029
In grad_steps = 18, loss = 0.6478118300437927
In grad_steps = 19, loss = 0.4933792054653168
In grad_steps = 20, loss = 0.5890640616416931
In grad_steps = 21, loss = 0.5631271004676819
In grad_steps = 22, loss = 0.5542118549346924
In grad_steps = 23, loss = 0.3603711724281311
In grad_steps = 24, loss = 0.5598992705345154
In grad_steps = 25, loss = 0.7059465646743774
In grad_steps = 26, loss = 0.5878833532333374
In grad_steps = 27, loss = 0.36283618211746216
In grad_steps = 28, loss = 0.26660385727882385
In grad_steps = 29, loss = 0.4106246531009674
In grad_steps = 30, loss = 0.6526228189468384
In grad_steps = 31, loss = 0.32602009177207947
In grad_steps = 32, loss = 0.7083971500396729
In grad_steps = 33, loss = 0.4656076729297638
In grad_steps = 34, loss = 0.654357373714447
In grad_steps = 35, loss = 0.2196437567472458
In grad_steps = 36, loss = 0.35333073139190674
In grad_steps = 37, loss = 0.5470072627067566
In grad_steps = 38, loss = 0.670342206954956
In grad_steps = 39, loss = 0.2936975061893463
In grad_steps = 40, loss = 0.5282154083251953
In grad_steps = 41, loss = 0.6111136674880981
In grad_steps = 42, loss = 0.3686981499195099
In grad_steps = 43, loss = 0.35147181153297424
In grad_steps = 44, loss = 0.3348672389984131
In grad_steps = 45, loss = 0.4877288341522217
In grad_steps = 46, loss = 0.2975066900253296
In grad_steps = 47, loss = 0.3683505952358246
In grad_steps = 48, loss = 0.28078725934028625
In grad_steps = 49, loss = 0.15152497589588165
In grad_steps = 50, loss = 0.7610242962837219
In grad_steps = 51, loss = 0.10682224482297897
In grad_steps = 52, loss = 0.3633551597595215
In grad_steps = 53, loss = 0.5875217318534851
In grad_steps = 54, loss = 0.36726874113082886
In grad_steps = 55, loss = 0.38378530740737915
In grad_steps = 56, loss = 0.24912212789058685
In grad_steps = 57, loss = 0.24558551609516144
In grad_steps = 58, loss = 0.27451565861701965
In grad_steps = 59, loss = 0.2622707784175873
In grad_steps = 60, loss = 0.4971737563610077
In grad_steps = 61, loss = 0.31470993161201477
In grad_steps = 62, loss = 0.23424574732780457
In grad_steps = 63, loss = 0.2927362322807312
In grad_steps = 64, loss = 0.07587064057588577
In grad_steps = 65, loss = 0.2290988266468048
In grad_steps = 66, loss = 0.3538324534893036
In grad_steps = 67, loss = 0.36566436290740967
In grad_steps = 68, loss = 0.17112742364406586
In grad_steps = 69, loss = 0.33620166778564453
In grad_steps = 70, loss = 0.375250905752182
In grad_steps = 71, loss = 0.1433456838130951
In grad_steps = 72, loss = 0.4284607768058777
In grad_steps = 73, loss = 0.5027227997779846
In grad_steps = 74, loss = 0.2671978175640106
In grad_steps = 75, loss = 0.16018424928188324
In grad_steps = 76, loss = 0.41485336422920227
In grad_steps = 77, loss = 0.10343091189861298
In grad_steps = 78, loss = 0.18092475831508636
In grad_steps = 79, loss = 0.24499623477458954
In grad_steps = 80, loss = 0.30356520414352417
In grad_steps = 81, loss = 0.3374326527118683
In grad_steps = 82, loss = 0.11745642870664597
In grad_steps = 83, loss = 0.416402667760849
In grad_steps = 84, loss = 0.22285428643226624
In grad_steps = 85, loss = 0.0997987762093544
In grad_steps = 86, loss = 0.3740552067756653
In grad_steps = 87, loss = 0.9952309131622314
In grad_steps = 88, loss = 0.6989393830299377
In grad_steps = 89, loss = 0.370953232049942
In grad_steps = 90, loss = 0.31357696652412415
In grad_steps = 91, loss = 0.2849481403827667
In grad_steps = 92, loss = 0.3127599358558655
In grad_steps = 93, loss = 0.31439897418022156
In grad_steps = 94, loss = 0.2167585790157318
In grad_steps = 95, loss = 0.34347641468048096
In grad_steps = 96, loss = 0.3994815945625305
In grad_steps = 97, loss = 0.271513432264328
In grad_steps = 98, loss = 0.5608190298080444
In grad_steps = 99, loss = 0.13872882723808289
In grad_steps = 100, loss = 0.2038508653640747
In grad_steps = 101, loss = 0.14496691524982452
In grad_steps = 102, loss = 0.1611466407775879
In grad_steps = 103, loss = 0.5920159816741943
In grad_steps = 104, loss = 0.2559029459953308
Beginning epoch 2
In grad_steps = 105, loss = 0.2711293697357178
In grad_steps = 106, loss = 0.38700395822525024
In grad_steps = 107, loss = 0.2785395681858063
In grad_steps = 108, loss = 0.2392672300338745
In grad_steps = 109, loss = 0.49708321690559387
In grad_steps = 110, loss = 0.30894699692726135
In grad_steps = 111, loss = 0.22869384288787842
In grad_steps = 112, loss = 0.107828289270401
In grad_steps = 113, loss = 0.16648122668266296
In grad_steps = 114, loss = 0.1686127632856369
In grad_steps = 115, loss = 0.1969960331916809
In grad_steps = 116, loss = 0.12168658524751663
In grad_steps = 117, loss = 0.22201576828956604
In grad_steps = 118, loss = 0.8417310118675232
In grad_steps = 119, loss = 0.07716719806194305
In grad_steps = 120, loss = 0.23746830224990845
In grad_steps = 121, loss = 0.04535173997282982
In grad_steps = 122, loss = 0.17147541046142578
In grad_steps = 123, loss = 0.11940224468708038
In grad_steps = 124, loss = 0.19936104118824005
In grad_steps = 125, loss = 0.4040597677230835
In grad_steps = 126, loss = 0.09980937093496323
In grad_steps = 127, loss = 0.11847539246082306
In grad_steps = 128, loss = 0.15714098513126373
In grad_steps = 129, loss = 0.22620172798633575
In grad_steps = 130, loss = 0.1753387153148651
In grad_steps = 131, loss = 0.15312018990516663
In grad_steps = 132, loss = 0.23228007555007935
In grad_steps = 133, loss = 0.11533690989017487
In grad_steps = 134, loss = 0.09447944164276123
In grad_steps = 135, loss = 0.13260646164417267
In grad_steps = 136, loss = 0.03700891509652138
In grad_steps = 137, loss = 0.11460447311401367
In grad_steps = 138, loss = 0.06979535520076752
In grad_steps = 139, loss = 0.237494096159935
In grad_steps = 140, loss = 0.011172689497470856
In grad_steps = 141, loss = 0.07543711364269257
In grad_steps = 142, loss = 0.2595371603965759
In grad_steps = 143, loss = 0.4573580026626587
In grad_steps = 144, loss = 0.18739289045333862
In grad_steps = 145, loss = 0.09738948196172714
In grad_steps = 146, loss = 0.37862429022789
In grad_steps = 147, loss = 0.015718281269073486
In grad_steps = 148, loss = 0.19439272582530975
In grad_steps = 149, loss = 0.04889874532818794
In grad_steps = 150, loss = 0.34686362743377686
In grad_steps = 151, loss = 0.28688904643058777
In grad_steps = 152, loss = 0.32767680287361145
In grad_steps = 153, loss = 0.05077683553099632
In grad_steps = 154, loss = 0.08104567974805832
In grad_steps = 155, loss = 0.19098316133022308
In grad_steps = 156, loss = 0.210160493850708
In grad_steps = 157, loss = 0.3212791979312897
In grad_steps = 158, loss = 0.3212609887123108
In grad_steps = 159, loss = 0.13473035395145416
In grad_steps = 160, loss = 0.14521460235118866
In grad_steps = 161, loss = 0.11960010975599289
In grad_steps = 162, loss = 0.1558634638786316
In grad_steps = 163, loss = 0.0992819219827652
In grad_steps = 164, loss = 0.14160636067390442
In grad_steps = 165, loss = 0.2008744180202484
In grad_steps = 166, loss = 0.07057435810565948
In grad_steps = 167, loss = 0.11212264746427536
In grad_steps = 168, loss = 0.14882978796958923
In grad_steps = 169, loss = 0.023174257948994637
In grad_steps = 170, loss = 0.03723021224141121
In grad_steps = 171, loss = 0.10993368923664093
In grad_steps = 172, loss = 0.13947543501853943
In grad_steps = 173, loss = 0.033042944967746735
In grad_steps = 174, loss = 0.017543047666549683
In grad_steps = 175, loss = 0.03902867063879967
In grad_steps = 176, loss = 0.09293540567159653
In grad_steps = 177, loss = 0.12663573026657104
In grad_steps = 178, loss = 0.4330090582370758
In grad_steps = 179, loss = 0.30154773592948914
In grad_steps = 180, loss = 0.014973154291510582
In grad_steps = 181, loss = 0.2025182545185089
In grad_steps = 182, loss = 0.0068780952133238316
In grad_steps = 183, loss = 0.02842184528708458
In grad_steps = 184, loss = 0.09169143438339233
In grad_steps = 185, loss = 0.058059193193912506
In grad_steps = 186, loss = 0.3650057315826416
In grad_steps = 187, loss = 0.010300294496119022
In grad_steps = 188, loss = 0.04422173649072647
In grad_steps = 189, loss = 0.14522738754749298
In grad_steps = 190, loss = 0.031284842640161514
In grad_steps = 191, loss = 0.007347025442868471
In grad_steps = 192, loss = 0.2839907109737396
In grad_steps = 193, loss = 0.4237518012523651
In grad_steps = 194, loss = 0.09177184104919434
In grad_steps = 195, loss = 0.17890338599681854
In grad_steps = 196, loss = 0.22402900457382202
In grad_steps = 197, loss = 0.05689859017729759
In grad_steps = 198, loss = 0.09352325648069382
In grad_steps = 199, loss = 0.32062721252441406
In grad_steps = 200, loss = 0.15008120238780975
In grad_steps = 201, loss = 0.14143237471580505
In grad_steps = 202, loss = 0.08040928095579147
In grad_steps = 203, loss = 0.29453492164611816
In grad_steps = 204, loss = 0.04194407910108566
In grad_steps = 205, loss = 0.14223024249076843
In grad_steps = 206, loss = 0.014704414643347263
In grad_steps = 207, loss = 0.12963078916072845
In grad_steps = 208, loss = 0.16909858584403992
In grad_steps = 209, loss = 0.028698010370135307
Beginning epoch 3
In grad_steps = 210, loss = 0.2024233490228653
In grad_steps = 211, loss = 0.024201247841119766
In grad_steps = 212, loss = 0.08067069202661514
In grad_steps = 213, loss = 0.17461778223514557
In grad_steps = 214, loss = 0.10664151608943939
In grad_steps = 215, loss = 0.038961123675107956
In grad_steps = 216, loss = 0.2306596040725708
In grad_steps = 217, loss = 0.07593172788619995
In grad_steps = 218, loss = 0.09209413081407547
In grad_steps = 219, loss = 0.010081161744892597
In grad_steps = 220, loss = 0.0831293910741806
In grad_steps = 221, loss = 0.10382142663002014
In grad_steps = 222, loss = 0.15401308238506317
In grad_steps = 223, loss = 0.7482787370681763
In grad_steps = 224, loss = 0.04525478929281235
In grad_steps = 225, loss = 0.09947514533996582
In grad_steps = 226, loss = 0.00965972151607275
In grad_steps = 227, loss = 0.06238143891096115
In grad_steps = 228, loss = 0.014579897746443748
In grad_steps = 229, loss = 0.017877545207738876
In grad_steps = 230, loss = 0.04048169031739235
In grad_steps = 231, loss = 0.044975221157073975
In grad_steps = 232, loss = 0.024143626913428307
In grad_steps = 233, loss = 0.07087458670139313
In grad_steps = 234, loss = 0.4184597134590149
In grad_steps = 235, loss = 0.03263896703720093
In grad_steps = 236, loss = 0.12900717556476593
In grad_steps = 237, loss = 0.14682482182979584
In grad_steps = 238, loss = 0.18584714829921722
In grad_steps = 239, loss = 0.035616882145404816
In grad_steps = 240, loss = 0.031630709767341614
In grad_steps = 241, loss = 0.0872960239648819
In grad_steps = 242, loss = 0.07110697031021118
In grad_steps = 243, loss = 0.0833427906036377
In grad_steps = 244, loss = 0.18174079060554504
In grad_steps = 245, loss = 0.003436450380831957
In grad_steps = 246, loss = 0.03033282235264778
In grad_steps = 247, loss = 0.06033765897154808
In grad_steps = 248, loss = 0.08373376727104187
In grad_steps = 249, loss = 0.14934809505939484
In grad_steps = 250, loss = 0.016864784061908722
In grad_steps = 251, loss = 0.19760993123054504
In grad_steps = 252, loss = 0.007298411801457405
In grad_steps = 253, loss = 0.0046707382425665855
In grad_steps = 254, loss = 0.006577962543815374
In grad_steps = 255, loss = 0.04734168201684952
In grad_steps = 256, loss = 0.2594299614429474
In grad_steps = 257, loss = 0.08240139484405518
In grad_steps = 258, loss = 0.042555518448352814
In grad_steps = 259, loss = 0.09035497158765793
In grad_steps = 260, loss = 0.0023368895053863525
In grad_steps = 261, loss = 0.14981161057949066
In grad_steps = 262, loss = 0.42253661155700684
In grad_steps = 263, loss = 0.1791493147611618
In grad_steps = 264, loss = 0.5273334383964539
In grad_steps = 265, loss = 0.04681564122438431
In grad_steps = 266, loss = 0.19988100230693817
In grad_steps = 267, loss = 0.02976006269454956
In grad_steps = 268, loss = 0.03270743042230606
In grad_steps = 269, loss = 0.108835868537426
In grad_steps = 270, loss = 0.15175814926624298
In grad_steps = 271, loss = 0.03573238477110863
In grad_steps = 272, loss = 0.09252633899450302
In grad_steps = 273, loss = 0.045041050761938095
In grad_steps = 274, loss = 0.044134754687547684
In grad_steps = 275, loss = 0.13254112005233765
In grad_steps = 276, loss = 0.08191803097724915
In grad_steps = 277, loss = 0.027191434055566788
In grad_steps = 278, loss = 0.037162426859140396
In grad_steps = 279, loss = 0.03810632601380348
In grad_steps = 280, loss = 0.048738107085227966
In grad_steps = 281, loss = 0.00562598742544651
In grad_steps = 282, loss = 0.03782472014427185
In grad_steps = 283, loss = 0.19817133247852325
In grad_steps = 284, loss = 0.034799642860889435
In grad_steps = 285, loss = 0.010440237820148468
In grad_steps = 286, loss = 0.01018247939646244
In grad_steps = 287, loss = 0.0017907059518620372
In grad_steps = 288, loss = 0.0002600940060801804
In grad_steps = 289, loss = 0.0021231696009635925
In grad_steps = 290, loss = 0.001003210199996829
In grad_steps = 291, loss = 0.29485005140304565
In grad_steps = 292, loss = 0.00015022246225271374
In grad_steps = 293, loss = 0.0006006502662785351
In grad_steps = 294, loss = 0.007486052345484495
In grad_steps = 295, loss = 0.00026313867419958115
In grad_steps = 296, loss = 0.00016923804651014507
In grad_steps = 297, loss = 0.0176953487098217
In grad_steps = 298, loss = 0.1057528629899025
In grad_steps = 299, loss = 0.0181429423391819
In grad_steps = 300, loss = 0.07742595672607422
In grad_steps = 301, loss = 0.13313163816928864
In grad_steps = 302, loss = 0.005959253292530775
In grad_steps = 303, loss = 0.07079167664051056
In grad_steps = 304, loss = 0.34964969754219055
In grad_steps = 305, loss = 0.01955781877040863
In grad_steps = 306, loss = 0.30055877566337585
In grad_steps = 307, loss = 0.03300319239497185
In grad_steps = 308, loss = 0.3814333975315094
In grad_steps = 309, loss = 0.06797274202108383
In grad_steps = 310, loss = 0.2489677369594574
In grad_steps = 311, loss = 0.02056310325860977
In grad_steps = 312, loss = 0.2348429560661316
In grad_steps = 313, loss = 0.04528413340449333
In grad_steps = 314, loss = 0.026521168649196625
Beginning epoch 4
In grad_steps = 315, loss = 0.11413434892892838
In grad_steps = 316, loss = 0.014654748141765594
In grad_steps = 317, loss = 0.01987176574766636
In grad_steps = 318, loss = 0.11044337600469589
In grad_steps = 319, loss = 0.21584510803222656
In grad_steps = 320, loss = 0.02749631367623806
In grad_steps = 321, loss = 0.05429348349571228
In grad_steps = 322, loss = 0.02241077460348606
In grad_steps = 323, loss = 0.18231521546840668
In grad_steps = 324, loss = 0.04683418571949005
In grad_steps = 325, loss = 0.34539592266082764
In grad_steps = 326, loss = 0.06348805874586105
In grad_steps = 327, loss = 0.03120378777384758
In grad_steps = 328, loss = 0.2831971347332001
In grad_steps = 329, loss = 0.0326937772333622
In grad_steps = 330, loss = 0.22650940716266632
In grad_steps = 331, loss = 0.0037786453031003475
In grad_steps = 332, loss = 0.1630234569311142
In grad_steps = 333, loss = 0.23451407253742218
In grad_steps = 334, loss = 0.019114820286631584
In grad_steps = 335, loss = 0.018328219652175903
In grad_steps = 336, loss = 0.06620834022760391
In grad_steps = 337, loss = 0.029340406879782677
In grad_steps = 338, loss = 0.1298881471157074
In grad_steps = 339, loss = 0.31626754999160767
In grad_steps = 340, loss = 0.06693953275680542
In grad_steps = 341, loss = 0.07292298227548599
In grad_steps = 342, loss = 0.12014180421829224
In grad_steps = 343, loss = 0.17982913553714752
In grad_steps = 344, loss = 0.20558421313762665
In grad_steps = 345, loss = 0.03460513800382614
In grad_steps = 346, loss = 0.007516791578382254
In grad_steps = 347, loss = 0.011788589879870415
In grad_steps = 348, loss = 0.018150243908166885
In grad_steps = 349, loss = 0.31798574328422546
In grad_steps = 350, loss = 0.002413394395262003
In grad_steps = 351, loss = 0.05561816692352295
In grad_steps = 352, loss = 0.09836331754922867
In grad_steps = 353, loss = 0.2679913341999054
In grad_steps = 354, loss = 0.129333034157753
In grad_steps = 355, loss = 0.031158464029431343
In grad_steps = 356, loss = 0.2257346361875534
In grad_steps = 357, loss = 0.018245715647935867
In grad_steps = 358, loss = 0.059133365750312805
In grad_steps = 359, loss = 0.0035263679455965757
In grad_steps = 360, loss = 0.01840021461248398
In grad_steps = 361, loss = 0.05557599663734436
In grad_steps = 362, loss = 0.032621730118989944
In grad_steps = 363, loss = 0.04309992864727974
In grad_steps = 364, loss = 0.07861781865358353
In grad_steps = 365, loss = 0.010156212374567986
In grad_steps = 366, loss = 0.27692800760269165
In grad_steps = 367, loss = 0.10924408584833145
In grad_steps = 368, loss = 0.16238251328468323
In grad_steps = 369, loss = 0.022960631176829338
In grad_steps = 370, loss = 0.045597970485687256
In grad_steps = 371, loss = 0.016424080356955528
In grad_steps = 372, loss = 0.005855404306203127
In grad_steps = 373, loss = 0.010668668895959854
In grad_steps = 374, loss = 0.0027891029603779316
In grad_steps = 375, loss = 0.32399964332580566
In grad_steps = 376, loss = 0.1732085645198822
In grad_steps = 377, loss = 0.19101482629776
In grad_steps = 378, loss = 0.0051336330361664295
In grad_steps = 379, loss = 0.004423218779265881
In grad_steps = 380, loss = 0.005643585696816444
In grad_steps = 381, loss = 0.015635818243026733
In grad_steps = 382, loss = 0.1071152314543724
In grad_steps = 383, loss = 0.01853557862341404
In grad_steps = 384, loss = 0.052659813314676285
In grad_steps = 385, loss = 0.04616031050682068
In grad_steps = 386, loss = 0.030201004818081856
In grad_steps = 387, loss = 0.05423642694950104
In grad_steps = 388, loss = 0.11811619251966476
In grad_steps = 389, loss = 0.15600986778736115
In grad_steps = 390, loss = 0.035219814628362656
In grad_steps = 391, loss = 0.008612287230789661
In grad_steps = 392, loss = 0.007866226136684418
In grad_steps = 393, loss = 0.0010323961032554507
In grad_steps = 394, loss = 0.0037735598161816597
In grad_steps = 395, loss = 0.012334048748016357
In grad_steps = 396, loss = 0.09016410261392593
In grad_steps = 397, loss = 0.0006356253288686275
In grad_steps = 398, loss = 0.0013243237044662237
In grad_steps = 399, loss = 0.01121873315423727
In grad_steps = 400, loss = 0.0005158846033737063
In grad_steps = 401, loss = 0.0010937071638181806
In grad_steps = 402, loss = 0.13411343097686768
In grad_steps = 403, loss = 0.2553563416004181
In grad_steps = 404, loss = 0.011008829809725285
In grad_steps = 405, loss = 0.10033948719501495
In grad_steps = 406, loss = 0.044996995478868484
In grad_steps = 407, loss = 0.07269181311130524
In grad_steps = 408, loss = 0.016694018617272377
In grad_steps = 409, loss = 0.01316869631409645
In grad_steps = 410, loss = 0.052139781415462494
In grad_steps = 411, loss = 0.04887925460934639
In grad_steps = 412, loss = 0.019798509776592255
In grad_steps = 413, loss = 0.03507998213171959
In grad_steps = 414, loss = 0.005706602707505226
In grad_steps = 415, loss = 0.10187430679798126
In grad_steps = 416, loss = 0.004591812379658222
In grad_steps = 417, loss = 0.021372081711888313
In grad_steps = 418, loss = 0.05123737081885338
In grad_steps = 419, loss = 0.0027739705983549356
Beginning epoch 5
In grad_steps = 420, loss = 0.27905359864234924
In grad_steps = 421, loss = 0.0009039730648510158
In grad_steps = 422, loss = 0.013814978301525116
In grad_steps = 423, loss = 0.11842338740825653
In grad_steps = 424, loss = 0.1516263633966446
In grad_steps = 425, loss = 0.00617893086746335
In grad_steps = 426, loss = 0.0475807823240757
In grad_steps = 427, loss = 0.008238991722464561
In grad_steps = 428, loss = 0.02765316516160965
In grad_steps = 429, loss = 0.027655821293592453
In grad_steps = 430, loss = 0.06769459694623947
In grad_steps = 431, loss = 0.0651191771030426
In grad_steps = 432, loss = 0.015944812446832657
In grad_steps = 433, loss = 0.2638857960700989
In grad_steps = 434, loss = 0.0025029396638274193
In grad_steps = 435, loss = 0.0319824293255806
In grad_steps = 436, loss = 0.0012627852847799659
In grad_steps = 437, loss = 0.027735937386751175
In grad_steps = 438, loss = 0.0016037375899031758
In grad_steps = 439, loss = 0.0033815379720181227
In grad_steps = 440, loss = 0.019255870953202248
In grad_steps = 441, loss = 0.06840387731790543
In grad_steps = 442, loss = 0.14581525325775146
In grad_steps = 443, loss = 0.11176545172929764
In grad_steps = 444, loss = 0.14051124453544617
In grad_steps = 445, loss = 0.0028259060345590115
In grad_steps = 446, loss = 0.04480206221342087
In grad_steps = 447, loss = 0.02050306461751461
In grad_steps = 448, loss = 0.009364889934659004
In grad_steps = 449, loss = 0.01717918924987316
In grad_steps = 450, loss = 0.01895695924758911
In grad_steps = 451, loss = 0.2679906189441681
In grad_steps = 452, loss = 0.0551290363073349
In grad_steps = 453, loss = 0.06804849207401276
In grad_steps = 454, loss = 0.12196437269449234
In grad_steps = 455, loss = 0.01713157258927822
In grad_steps = 456, loss = 0.00638413242995739
In grad_steps = 457, loss = 0.05270550772547722
In grad_steps = 458, loss = 0.0029627380426973104
In grad_steps = 459, loss = 0.0066706398501992226
In grad_steps = 460, loss = 0.016889097169041634
In grad_steps = 461, loss = 0.025519590824842453
In grad_steps = 462, loss = 0.005050344858318567
In grad_steps = 463, loss = 0.1683170646429062
In grad_steps = 464, loss = 0.009808537550270557
In grad_steps = 465, loss = 0.005469021387398243
In grad_steps = 466, loss = 0.001251264358870685
In grad_steps = 467, loss = 0.06956356763839722
In grad_steps = 468, loss = 0.0012880212161689997
In grad_steps = 469, loss = 0.00032242381712421775
In grad_steps = 470, loss = 0.1396573930978775
In grad_steps = 471, loss = 0.0016705216839909554
In grad_steps = 472, loss = 0.06266290694475174
In grad_steps = 473, loss = 0.002951549831777811
In grad_steps = 474, loss = 0.13211119174957275
In grad_steps = 475, loss = 0.19734913110733032
In grad_steps = 476, loss = 0.10160897672176361
In grad_steps = 477, loss = 0.018354518339037895
In grad_steps = 478, loss = 0.003232632763683796
In grad_steps = 479, loss = 0.005204681772738695
In grad_steps = 480, loss = 0.013669944368302822
In grad_steps = 481, loss = 0.0010381854372099042
In grad_steps = 482, loss = 0.03916668891906738
In grad_steps = 483, loss = 0.0053364611230790615
In grad_steps = 484, loss = 0.0023980350233614445
In grad_steps = 485, loss = 0.0014814694877713919
In grad_steps = 486, loss = 0.2737264335155487
In grad_steps = 487, loss = 0.03160322085022926
In grad_steps = 488, loss = 0.002106656786054373
In grad_steps = 489, loss = 0.00852414220571518
In grad_steps = 490, loss = 0.024858370423316956
In grad_steps = 491, loss = 0.026057135313749313
In grad_steps = 492, loss = 0.07507302612066269
In grad_steps = 493, loss = 0.06295851618051529
In grad_steps = 494, loss = 0.014988436363637447
In grad_steps = 495, loss = 0.016982588917016983
In grad_steps = 496, loss = 0.041549451649188995
In grad_steps = 497, loss = 0.06816684454679489
In grad_steps = 498, loss = 0.05289209634065628
In grad_steps = 499, loss = 0.011633248068392277
In grad_steps = 500, loss = 0.08576133102178574
In grad_steps = 501, loss = 0.003571190172806382
In grad_steps = 502, loss = 0.00019238024833612144
In grad_steps = 503, loss = 0.0025679159443825483
In grad_steps = 504, loss = 0.030509041622281075
In grad_steps = 505, loss = 0.0003811695205513388
In grad_steps = 506, loss = 0.00025116102187894285
In grad_steps = 507, loss = 0.22135284543037415
In grad_steps = 508, loss = 0.0007426274823956192
In grad_steps = 509, loss = 0.03166435286402702
In grad_steps = 510, loss = 0.0014719758182764053
In grad_steps = 511, loss = 0.18080328404903412
In grad_steps = 512, loss = 0.000742589880246669
In grad_steps = 513, loss = 0.0016546655679121614
In grad_steps = 514, loss = 0.05538075417280197
In grad_steps = 515, loss = 0.007382781244814396
In grad_steps = 516, loss = 0.005271160509437323
In grad_steps = 517, loss = 0.011628556065261364
In grad_steps = 518, loss = 0.03437052667140961
In grad_steps = 519, loss = 0.00581725686788559
In grad_steps = 520, loss = 0.005041655618697405
In grad_steps = 521, loss = 0.0019345238106325269
In grad_steps = 522, loss = 0.08429616689682007
In grad_steps = 523, loss = 0.11411722749471664
In grad_steps = 524, loss = 0.027373338118195534
Beginning epoch 6
In grad_steps = 525, loss = 0.010694117285311222
In grad_steps = 526, loss = 0.0011395324254408479
In grad_steps = 527, loss = 0.0017162536969408393
In grad_steps = 528, loss = 0.005426866002380848
In grad_steps = 529, loss = 0.002237150212749839
In grad_steps = 530, loss = 0.0008580178255215287
In grad_steps = 531, loss = 0.0038584887515753508
In grad_steps = 532, loss = 0.0006413075607270002
In grad_steps = 533, loss = 0.06299526989459991
In grad_steps = 534, loss = 0.00046262648538686335
In grad_steps = 535, loss = 0.0010143271647393703
In grad_steps = 536, loss = 0.0023285155184566975
In grad_steps = 537, loss = 0.010872789658606052
In grad_steps = 538, loss = 0.14816789329051971
In grad_steps = 539, loss = 0.00013086626131553203
In grad_steps = 540, loss = 0.002215152606368065
In grad_steps = 541, loss = 0.00012183183571323752
In grad_steps = 542, loss = 0.004933625925332308
In grad_steps = 543, loss = 0.0004710430803243071
In grad_steps = 544, loss = 0.0005061163101345301
In grad_steps = 545, loss = 0.00035319311427883804
In grad_steps = 546, loss = 0.04319886863231659
In grad_steps = 547, loss = 0.004562005400657654
In grad_steps = 548, loss = 0.07157770544290543
In grad_steps = 549, loss = 0.4700475037097931
In grad_steps = 550, loss = 0.08959464728832245
In grad_steps = 551, loss = 0.006164067890495062
In grad_steps = 552, loss = 0.0054726190865039825
In grad_steps = 553, loss = 0.02503282204270363
In grad_steps = 554, loss = 0.024802878499031067
In grad_steps = 555, loss = 0.009405064396560192
In grad_steps = 556, loss = 0.008760623633861542
In grad_steps = 557, loss = 0.001776253804564476
In grad_steps = 558, loss = 0.0020934329368174076
In grad_steps = 559, loss = 0.03423650562763214
In grad_steps = 560, loss = 0.0032125581055879593
In grad_steps = 561, loss = 0.004559663590043783
In grad_steps = 562, loss = 0.017040453851222992
In grad_steps = 563, loss = 0.05445330590009689
In grad_steps = 564, loss = 0.008021069690585136
In grad_steps = 565, loss = 0.01118664350360632
In grad_steps = 566, loss = 0.031781937927007675
In grad_steps = 567, loss = 0.0016489101108163595
In grad_steps = 568, loss = 0.11927809566259384
In grad_steps = 569, loss = 0.1242431029677391
In grad_steps = 570, loss = 0.0022531868889927864
In grad_steps = 571, loss = 0.005552803631871939
In grad_steps = 572, loss = 0.003935478162020445
In grad_steps = 573, loss = 0.004130316432565451
In grad_steps = 574, loss = 0.004104630555957556
In grad_steps = 575, loss = 0.006657568737864494
In grad_steps = 576, loss = 0.027598965913057327
In grad_steps = 577, loss = 0.010257847607135773
In grad_steps = 578, loss = 0.13436971604824066
In grad_steps = 579, loss = 0.1695820838212967
In grad_steps = 580, loss = 0.011692900210618973
In grad_steps = 581, loss = 0.0060110236518085
In grad_steps = 582, loss = 0.00622539222240448
In grad_steps = 583, loss = 0.05395673215389252
In grad_steps = 584, loss = 0.07207118719816208
In grad_steps = 585, loss = 0.005788595415651798
In grad_steps = 586, loss = 0.004400040488690138
In grad_steps = 587, loss = 0.029786797240376472
In grad_steps = 588, loss = 0.030987892299890518
In grad_steps = 589, loss = 0.02029389888048172
In grad_steps = 590, loss = 0.0057370332069695
In grad_steps = 591, loss = 0.10048716515302658
In grad_steps = 592, loss = 0.12135522067546844
In grad_steps = 593, loss = 0.0018770865863189101
In grad_steps = 594, loss = 0.0032442566007375717
In grad_steps = 595, loss = 0.001610848237760365
In grad_steps = 596, loss = 0.0008031595498323441
In grad_steps = 597, loss = 0.010643534362316132
In grad_steps = 598, loss = 0.025547342374920845
In grad_steps = 599, loss = 0.023092882707715034
In grad_steps = 600, loss = 0.0020662040915340185
In grad_steps = 601, loss = 0.016545649617910385
In grad_steps = 602, loss = 0.006205440033227205
In grad_steps = 603, loss = 0.0003024860634468496
In grad_steps = 604, loss = 0.03483647108078003
In grad_steps = 605, loss = 0.296480268239975
In grad_steps = 606, loss = 0.0020108504686504602
In grad_steps = 607, loss = 0.0003375437227077782
In grad_steps = 608, loss = 0.018967948853969574
In grad_steps = 609, loss = 0.20581115782260895
In grad_steps = 610, loss = 0.00016911461716517806
In grad_steps = 611, loss = 0.0003003108431585133
In grad_steps = 612, loss = 0.23547960817813873
In grad_steps = 613, loss = 0.03052329272031784
In grad_steps = 614, loss = 0.022025607526302338
In grad_steps = 615, loss = 0.003566575003787875
In grad_steps = 616, loss = 0.07023074477910995
In grad_steps = 617, loss = 0.009188564494252205
In grad_steps = 618, loss = 0.012008601799607277
In grad_steps = 619, loss = 0.31008514761924744
In grad_steps = 620, loss = 0.1140492632985115
In grad_steps = 621, loss = 0.00402570515871048
In grad_steps = 622, loss = 0.022151796147227287
In grad_steps = 623, loss = 0.025330504402518272
In grad_steps = 624, loss = 0.015258660539984703
In grad_steps = 625, loss = 0.0024373847991228104
In grad_steps = 626, loss = 0.003263607621192932
In grad_steps = 627, loss = 0.04353620111942291
In grad_steps = 628, loss = 0.020617542788386345
In grad_steps = 629, loss = 0.044980503618717194
Beginning epoch 7
In grad_steps = 630, loss = 0.049200884997844696
In grad_steps = 631, loss = 0.004654301330447197
In grad_steps = 632, loss = 0.003405066905543208
In grad_steps = 633, loss = 0.012207366526126862
In grad_steps = 634, loss = 0.02659592777490616
In grad_steps = 635, loss = 0.016486024484038353
In grad_steps = 636, loss = 0.006069845519959927
In grad_steps = 637, loss = 0.0029400326311588287
In grad_steps = 638, loss = 0.0032651307992637157
In grad_steps = 639, loss = 0.0005876216455362737
In grad_steps = 640, loss = 0.0017577112885192037
In grad_steps = 641, loss = 0.01884395442903042
In grad_steps = 642, loss = 0.0005558541161008179
In grad_steps = 643, loss = 0.03119460493326187
In grad_steps = 644, loss = 0.0007130260346457362
In grad_steps = 645, loss = 0.051703788340091705
In grad_steps = 646, loss = 0.0015856785466894507
In grad_steps = 647, loss = 0.00010259247210342437
In grad_steps = 648, loss = 0.10519977658987045
In grad_steps = 649, loss = 0.0030493177473545074
In grad_steps = 650, loss = 0.00020039356604684144
In grad_steps = 651, loss = 0.00046495263813994825
In grad_steps = 652, loss = 0.003006960730999708
In grad_steps = 653, loss = 0.0007676996756345034
In grad_steps = 654, loss = 0.08115030825138092
In grad_steps = 655, loss = 0.000504673516843468
In grad_steps = 656, loss = 0.0007154846098273993
In grad_steps = 657, loss = 0.012845327146351337
In grad_steps = 658, loss = 0.0022967769764363766
In grad_steps = 659, loss = 0.0006097647710703313
In grad_steps = 660, loss = 0.012490712106227875
In grad_steps = 661, loss = 0.0012544329511001706
In grad_steps = 662, loss = 0.11056442558765411
In grad_steps = 663, loss = 0.0006036601844243705
In grad_steps = 664, loss = 0.00034266989678144455
In grad_steps = 665, loss = 0.00012822187272831798
In grad_steps = 666, loss = 0.0006522020557895303
In grad_steps = 667, loss = 0.007049765437841415
In grad_steps = 668, loss = 0.000386297469958663
In grad_steps = 669, loss = 0.03282349556684494
In grad_steps = 670, loss = 0.0017239013686776161
In grad_steps = 671, loss = 0.016584474593400955
In grad_steps = 672, loss = 0.0005676786531694233
In grad_steps = 673, loss = 0.004272271879017353
In grad_steps = 674, loss = 0.0033251475542783737
In grad_steps = 675, loss = 0.015540585853159428
In grad_steps = 676, loss = 0.000767954159528017
In grad_steps = 677, loss = 0.008757773786783218
In grad_steps = 678, loss = 0.0002277419698657468
In grad_steps = 679, loss = 0.00022209234884940088
In grad_steps = 680, loss = 0.0069729965180158615
In grad_steps = 681, loss = 0.00011337864270899445
In grad_steps = 682, loss = 0.2327067106962204
In grad_steps = 683, loss = 0.009173684753477573
In grad_steps = 684, loss = 0.0008985686581581831
In grad_steps = 685, loss = 0.0007348314975388348
In grad_steps = 686, loss = 0.0003708612348418683
In grad_steps = 687, loss = 0.00010917311010416597
In grad_steps = 688, loss = 0.009784660302102566
In grad_steps = 689, loss = 0.00027717382181435823
In grad_steps = 690, loss = 0.0007355876732617617
In grad_steps = 691, loss = 0.02415303699672222
In grad_steps = 692, loss = 0.0010067394468933344
In grad_steps = 693, loss = 0.00398316839709878
In grad_steps = 694, loss = 0.0076826573349535465
In grad_steps = 695, loss = 0.00042663118802011013
In grad_steps = 696, loss = 0.01946639083325863
In grad_steps = 697, loss = 0.00013626313011627644
In grad_steps = 698, loss = 0.00034859534935094416
In grad_steps = 699, loss = 0.004032572731375694
In grad_steps = 700, loss = 0.006006066687405109
In grad_steps = 701, loss = 0.0003287126019131392
In grad_steps = 702, loss = 0.0006445279577746987
In grad_steps = 703, loss = 0.005944081582129002
In grad_steps = 704, loss = 0.23574785888195038
In grad_steps = 705, loss = 0.0039025284349918365
In grad_steps = 706, loss = 0.00853358767926693
In grad_steps = 707, loss = 0.0008110300987027586
In grad_steps = 708, loss = 0.002906461711972952
In grad_steps = 709, loss = 0.0009024403989315033
In grad_steps = 710, loss = 0.005091254133731127
In grad_steps = 711, loss = 0.01708238013088703
In grad_steps = 712, loss = 0.000330571667291224
In grad_steps = 713, loss = 0.00474548852071166
In grad_steps = 714, loss = 0.019041633233428
In grad_steps = 715, loss = 0.0002729608677327633
In grad_steps = 716, loss = 0.0002621656167320907
In grad_steps = 717, loss = 0.010213286615908146
In grad_steps = 718, loss = 0.0010506489779800177
In grad_steps = 719, loss = 0.00041521250386722386
In grad_steps = 720, loss = 0.009491228498518467
In grad_steps = 721, loss = 0.007659330032765865
In grad_steps = 722, loss = 0.0005029255407862365
In grad_steps = 723, loss = 0.0009244016255252063
In grad_steps = 724, loss = 0.2273273915052414
In grad_steps = 725, loss = 0.0026138315442949533
In grad_steps = 726, loss = 0.0008253330015577376
In grad_steps = 727, loss = 0.0009037980926223099
In grad_steps = 728, loss = 0.06016002967953682
In grad_steps = 729, loss = 0.0006383134750649333
In grad_steps = 730, loss = 0.00017378486518282443
In grad_steps = 731, loss = 0.0002213504194514826
In grad_steps = 732, loss = 0.002623267937451601
In grad_steps = 733, loss = 0.0019532418809831142
In grad_steps = 734, loss = 0.009463305585086346
Beginning epoch 8
In grad_steps = 735, loss = 0.0015584001084789634
In grad_steps = 736, loss = 0.009250077418982983
In grad_steps = 737, loss = 0.00044969242298975587
In grad_steps = 738, loss = 0.002909300848841667
In grad_steps = 739, loss = 0.024157864972949028
In grad_steps = 740, loss = 0.0014431595336645842
In grad_steps = 741, loss = 0.012908296659588814
In grad_steps = 742, loss = 0.0074993278831243515
In grad_steps = 743, loss = 0.0025289026089012623
In grad_steps = 744, loss = 0.0008200297597795725
In grad_steps = 745, loss = 0.04059943929314613
In grad_steps = 746, loss = 0.022206490859389305
In grad_steps = 747, loss = 0.021709807217121124
In grad_steps = 748, loss = 0.0010796112474054098
In grad_steps = 749, loss = 0.00010167103027924895
In grad_steps = 750, loss = 0.00017181341536343098
In grad_steps = 751, loss = 0.0007249420159496367
In grad_steps = 752, loss = 0.00016435969155281782
In grad_steps = 753, loss = 0.00024138968728948385
In grad_steps = 754, loss = 0.0006133884890004992
In grad_steps = 755, loss = 0.00077380909351632
In grad_steps = 756, loss = 0.001676943269558251
In grad_steps = 757, loss = 0.2949363887310028
In grad_steps = 758, loss = 0.006437898147851229
In grad_steps = 759, loss = 0.0029612702783197165
In grad_steps = 760, loss = 0.006219310220330954
In grad_steps = 761, loss = 0.004589627962559462
In grad_steps = 762, loss = 0.000614259741269052
In grad_steps = 763, loss = 0.0005620886222459376
In grad_steps = 764, loss = 0.0006730447639711201
In grad_steps = 765, loss = 0.0008911677869036794
In grad_steps = 766, loss = 0.0033966165501624346
In grad_steps = 767, loss = 0.0005005890852771699
In grad_steps = 768, loss = 0.00139124586712569
In grad_steps = 769, loss = 0.0019028685055673122
In grad_steps = 770, loss = 0.0017679533921182156
In grad_steps = 771, loss = 0.04149085283279419
In grad_steps = 772, loss = 0.09238314628601074
In grad_steps = 773, loss = 0.000698049203492701
In grad_steps = 774, loss = 0.007696371991187334
In grad_steps = 775, loss = 0.0025545076932758093
In grad_steps = 776, loss = 0.004549144301563501
In grad_steps = 777, loss = 0.0007078748312778771
In grad_steps = 778, loss = 0.0056075360625982285
In grad_steps = 779, loss = 0.0012599510373547673
In grad_steps = 780, loss = 0.07427729666233063
In grad_steps = 781, loss = 0.037667907774448395
In grad_steps = 782, loss = 0.004609301220625639
In grad_steps = 783, loss = 0.0007133020553737879
In grad_steps = 784, loss = 0.0011530477786436677
In grad_steps = 785, loss = 0.0050148749724030495
In grad_steps = 786, loss = 0.00047108891885727644
In grad_steps = 787, loss = 0.01252881158143282
In grad_steps = 788, loss = 0.06350502371788025
In grad_steps = 789, loss = 0.0010824231430888176
In grad_steps = 790, loss = 0.0010080677457153797
In grad_steps = 791, loss = 0.00030633973074145615
In grad_steps = 792, loss = 0.0004481119685806334
In grad_steps = 793, loss = 0.00019642083498183638
In grad_steps = 794, loss = 0.00030837670783512294
In grad_steps = 795, loss = 0.0004586238064803183
In grad_steps = 796, loss = 0.00042511732317507267
In grad_steps = 797, loss = 0.0005357307964004576
In grad_steps = 798, loss = 0.05903853476047516
In grad_steps = 799, loss = 0.003282734425738454
In grad_steps = 800, loss = 0.0017579467967152596
In grad_steps = 801, loss = 0.0002093318908009678
In grad_steps = 802, loss = 0.00018780730897560716
In grad_steps = 803, loss = 0.001281184726394713
In grad_steps = 804, loss = 0.002412921516224742
In grad_steps = 805, loss = 0.01375897042453289
In grad_steps = 806, loss = 0.00031523974030278623
In grad_steps = 807, loss = 0.00044489678111858666
In grad_steps = 808, loss = 0.0007154093473218381
In grad_steps = 809, loss = 0.0011824890971183777
In grad_steps = 810, loss = 0.0006974368588998914
In grad_steps = 811, loss = 0.00011098017421318218
In grad_steps = 812, loss = 0.0005520166014321148
In grad_steps = 813, loss = 0.00010167426808038726
In grad_steps = 814, loss = 0.0009312746697105467
In grad_steps = 815, loss = 0.0038146597798913717
In grad_steps = 816, loss = 0.0010867643868550658
In grad_steps = 817, loss = 0.00016259943367913365
In grad_steps = 818, loss = 0.0004853752616327256
In grad_steps = 819, loss = 0.02428944781422615
In grad_steps = 820, loss = 4.843121860176325e-05
In grad_steps = 821, loss = 2.2164807887747884e-05
In grad_steps = 822, loss = 0.00018715478654485196
In grad_steps = 823, loss = 0.004697150085121393
In grad_steps = 824, loss = 0.03159269317984581
In grad_steps = 825, loss = 0.0011428860016167164
In grad_steps = 826, loss = 0.00015449043712578714
In grad_steps = 827, loss = 8.468031592201442e-05
In grad_steps = 828, loss = 7.546781853307039e-05
In grad_steps = 829, loss = 0.09761609882116318
In grad_steps = 830, loss = 5.58035790163558e-05
In grad_steps = 831, loss = 5.482131746248342e-05
In grad_steps = 832, loss = 6.67940839775838e-05
In grad_steps = 833, loss = 0.010615886189043522
In grad_steps = 834, loss = 0.0002198907604906708
In grad_steps = 835, loss = 5.5742337281117216e-05
In grad_steps = 836, loss = 9.827248504734598e-06
In grad_steps = 837, loss = 9.410643542651087e-05
In grad_steps = 838, loss = 0.00042414426570758224
In grad_steps = 839, loss = 2.4628003302495927e-05
Beginning epoch 9
In grad_steps = 840, loss = 0.0025306050665676594
In grad_steps = 841, loss = 0.3080480992794037
In grad_steps = 842, loss = 5.939570473856293e-05
In grad_steps = 843, loss = 0.0003726775466930121
In grad_steps = 844, loss = 0.0006091776303946972
In grad_steps = 845, loss = 0.0001486488908994943
In grad_steps = 846, loss = 0.0016583524411544204
In grad_steps = 847, loss = 0.0015140397008508444
In grad_steps = 848, loss = 0.3100106418132782
In grad_steps = 849, loss = 0.0005338646587915719
In grad_steps = 850, loss = 0.0006750604952685535
In grad_steps = 851, loss = 0.005982088390737772
In grad_steps = 852, loss = 0.0012238066410645843
In grad_steps = 853, loss = 0.0024026301689445972
In grad_steps = 854, loss = 0.015014657750725746
In grad_steps = 855, loss = 0.0029548644088208675
In grad_steps = 856, loss = 0.026264015585184097
In grad_steps = 857, loss = 0.0041410308331251144
In grad_steps = 858, loss = 0.0024001121055334806
In grad_steps = 859, loss = 0.002897776896134019
In grad_steps = 860, loss = 0.00643926952034235
In grad_steps = 861, loss = 0.0036158813163638115
In grad_steps = 862, loss = 0.026720847934484482
In grad_steps = 863, loss = 0.013773571699857712
In grad_steps = 864, loss = 0.002033964730799198
In grad_steps = 865, loss = 0.0005780995707027614
In grad_steps = 866, loss = 0.01256530825048685
In grad_steps = 867, loss = 0.01675155572593212
In grad_steps = 868, loss = 0.0027452369686216116
In grad_steps = 869, loss = 0.006723084952682257
In grad_steps = 870, loss = 0.0022446338552981615
In grad_steps = 871, loss = 0.0011965689482167363
In grad_steps = 872, loss = 0.0004018433392047882
In grad_steps = 873, loss = 0.001729633193463087
In grad_steps = 874, loss = 0.0019224088173359632
In grad_steps = 875, loss = 0.0005989559576846659
In grad_steps = 876, loss = 0.0004857740714214742
In grad_steps = 877, loss = 0.001984489383175969
In grad_steps = 878, loss = 0.0007603602716699243
In grad_steps = 879, loss = 0.03753863647580147
In grad_steps = 880, loss = 0.011027916334569454
In grad_steps = 881, loss = 0.017833834514021873
In grad_steps = 882, loss = 0.0005017805378884077
In grad_steps = 883, loss = 0.00018560848548077047
In grad_steps = 884, loss = 0.00034127553226426244
In grad_steps = 885, loss = 9.389907063450664e-05
In grad_steps = 886, loss = 0.0016977973282337189
In grad_steps = 887, loss = 0.0005991164362058043
In grad_steps = 888, loss = 0.0005092482315376401
In grad_steps = 889, loss = 0.0008274646243080497
In grad_steps = 890, loss = 0.005604354664683342
In grad_steps = 891, loss = 0.00046803223085589707
In grad_steps = 892, loss = 9.868200868368149e-05
In grad_steps = 893, loss = 0.0004745587648358196
In grad_steps = 894, loss = 0.005609015002846718
In grad_steps = 895, loss = 0.0004376122960820794
In grad_steps = 896, loss = 7.622179691679776e-05
In grad_steps = 897, loss = 0.00016125230467878282
In grad_steps = 898, loss = 0.00015613279538229108
In grad_steps = 899, loss = 0.00023034910554997623
In grad_steps = 900, loss = 0.0003017768613062799
In grad_steps = 901, loss = 6.363672582665458e-05
In grad_steps = 902, loss = 0.0006004767492413521
In grad_steps = 903, loss = 0.0001254396338481456
In grad_steps = 904, loss = 8.583562885178253e-05
In grad_steps = 905, loss = 7.535351323895156e-05
In grad_steps = 906, loss = 0.00010937814658973366
In grad_steps = 907, loss = 0.006807798985391855
In grad_steps = 908, loss = 0.0022154110483825207
In grad_steps = 909, loss = 8.542677824152634e-05
In grad_steps = 910, loss = 0.00010399590973975137
In grad_steps = 911, loss = 0.00014926154108252376
In grad_steps = 912, loss = 0.005762880202382803
In grad_steps = 913, loss = 0.0019692203495651484
In grad_steps = 914, loss = 8.332217112183571e-05
In grad_steps = 915, loss = 0.00018230231944471598
In grad_steps = 916, loss = 0.00014473004557657987
In grad_steps = 917, loss = 0.00023149811022449285
In grad_steps = 918, loss = 2.9033151804469526e-05
In grad_steps = 919, loss = 0.000129469990497455
In grad_steps = 920, loss = 0.0041862851940095425
In grad_steps = 921, loss = 0.0006684054969809949
In grad_steps = 922, loss = 2.6433712264406495e-05
In grad_steps = 923, loss = 9.339870302937925e-05
In grad_steps = 924, loss = 0.06845422089099884
In grad_steps = 925, loss = 0.00016000817413441837
In grad_steps = 926, loss = 5.434372360468842e-05
In grad_steps = 927, loss = 0.0023461030796170235
In grad_steps = 928, loss = 0.00048482322017662227
In grad_steps = 929, loss = 5.644359043799341e-05
In grad_steps = 930, loss = 0.00040208784048445523
In grad_steps = 931, loss = 6.0474059864645824e-05
In grad_steps = 932, loss = 2.40944282268174e-05
In grad_steps = 933, loss = 0.03402981162071228
In grad_steps = 934, loss = 4.291882214602083e-05
In grad_steps = 935, loss = 3.3190783142345026e-05
In grad_steps = 936, loss = 2.2232117771636695e-05
In grad_steps = 937, loss = 0.0002311365242348984
In grad_steps = 938, loss = 0.001581790391355753
In grad_steps = 939, loss = 0.0015759181696921587
In grad_steps = 940, loss = 8.377614722121507e-05
In grad_steps = 941, loss = 1.4565732271876186e-05
In grad_steps = 942, loss = 7.176939834607765e-05
In grad_steps = 943, loss = 0.0027930480428040028
In grad_steps = 944, loss = 3.838334305328317e-05
Beginning epoch 10
In grad_steps = 945, loss = 0.00014455406926572323
In grad_steps = 946, loss = 0.00011699055903591216
In grad_steps = 947, loss = 6.567160016857088e-05
In grad_steps = 948, loss = 0.15358231961727142
In grad_steps = 949, loss = 0.00012672228331211954
In grad_steps = 950, loss = 9.560689068166539e-05
In grad_steps = 951, loss = 0.010969357565045357
In grad_steps = 952, loss = 0.10059141367673874
In grad_steps = 953, loss = 0.0017536422237753868
In grad_steps = 954, loss = 0.000516468717250973
In grad_steps = 955, loss = 0.0022908649407327175
In grad_steps = 956, loss = 0.0002577636914793402
In grad_steps = 957, loss = 0.00022444887144956738
In grad_steps = 958, loss = 0.0012163174105808139
In grad_steps = 959, loss = 0.001114086015149951
In grad_steps = 960, loss = 0.00020547375606838614
In grad_steps = 961, loss = 0.00048180558951571584
In grad_steps = 962, loss = 0.0004928573034703732
In grad_steps = 963, loss = 0.0006575504667125642
In grad_steps = 964, loss = 0.00043747565359808505
In grad_steps = 965, loss = 0.000985459191724658
In grad_steps = 966, loss = 0.019945358857512474
In grad_steps = 967, loss = 0.013974829576909542
In grad_steps = 968, loss = 0.004800985101610422
In grad_steps = 969, loss = 0.0033305317629128695
In grad_steps = 970, loss = 0.00028503392240963876
In grad_steps = 971, loss = 0.012910781428217888
In grad_steps = 972, loss = 0.0008459267555736005
In grad_steps = 973, loss = 0.00160321278963238
In grad_steps = 974, loss = 0.00213932478800416
In grad_steps = 975, loss = 0.0008374049211852252
In grad_steps = 976, loss = 0.0004882958601228893
In grad_steps = 977, loss = 0.00047200292465277016
In grad_steps = 978, loss = 0.0004973098984919488
In grad_steps = 979, loss = 0.0003596062015276402
In grad_steps = 980, loss = 0.019054850563406944
In grad_steps = 981, loss = 7.77011809987016e-05
In grad_steps = 982, loss = 0.00016580824740231037
In grad_steps = 983, loss = 0.00029985958826728165
In grad_steps = 984, loss = 0.0001587723527336493
In grad_steps = 985, loss = 0.0620141327381134
In grad_steps = 986, loss = 0.0008754301816225052
In grad_steps = 987, loss = 0.00028538648621179163
In grad_steps = 988, loss = 0.017922498285770416
In grad_steps = 989, loss = 0.0003251088201068342
In grad_steps = 990, loss = 4.044693923788145e-05
In grad_steps = 991, loss = 8.452951442450285e-05
In grad_steps = 992, loss = 0.42436230182647705
In grad_steps = 993, loss = 0.022210286930203438
In grad_steps = 994, loss = 0.030324941501021385
In grad_steps = 995, loss = 0.0006522334297187626
In grad_steps = 996, loss = 0.0014145751483738422
In grad_steps = 997, loss = 0.0004356538993306458
In grad_steps = 998, loss = 0.002082115039229393
In grad_steps = 999, loss = 0.008548722602427006
In grad_steps = 1000, loss = 0.2405935376882553
In grad_steps = 1001, loss = 0.0005709377583116293
In grad_steps = 1002, loss = 0.0006570884143002331
In grad_steps = 1003, loss = 0.0036846501752734184
In grad_steps = 1004, loss = 0.0021605268120765686
In grad_steps = 1005, loss = 0.003318392438814044
In grad_steps = 1006, loss = 0.0010099775390699506
In grad_steps = 1007, loss = 0.07582130283117294
In grad_steps = 1008, loss = 0.0016146489651873708
In grad_steps = 1009, loss = 0.0013896175660192966
In grad_steps = 1010, loss = 0.0026594935916364193
In grad_steps = 1011, loss = 0.0040033659897744656
In grad_steps = 1012, loss = 0.01466505415737629
In grad_steps = 1013, loss = 0.0021488950587809086
In grad_steps = 1014, loss = 0.001974071841686964
In grad_steps = 1015, loss = 0.0024997552391141653
In grad_steps = 1016, loss = 0.005667334888130426
In grad_steps = 1017, loss = 0.01099884882569313
In grad_steps = 1018, loss = 0.04316025227308273
In grad_steps = 1019, loss = 0.02798311971127987
In grad_steps = 1020, loss = 0.0023222481831908226
In grad_steps = 1021, loss = 0.0016602218383923173
In grad_steps = 1022, loss = 0.0012869524070993066
In grad_steps = 1023, loss = 0.010951553471386433
In grad_steps = 1024, loss = 0.0008235953282564878
In grad_steps = 1025, loss = 0.005110984202474356
In grad_steps = 1026, loss = 0.02134108543395996
In grad_steps = 1027, loss = 0.00031914966530166566
In grad_steps = 1028, loss = 0.0005778312915936112
In grad_steps = 1029, loss = 0.0002785645774565637
In grad_steps = 1030, loss = 0.00016090934514068067
In grad_steps = 1031, loss = 9.32123075472191e-05
In grad_steps = 1032, loss = 0.0022970964200794697
In grad_steps = 1033, loss = 0.2645723223686218
In grad_steps = 1034, loss = 0.0008619138970971107
In grad_steps = 1035, loss = 0.07146985083818436
In grad_steps = 1036, loss = 0.0002929173642769456
In grad_steps = 1037, loss = 0.013009650632739067
In grad_steps = 1038, loss = 0.0009339496027678251
In grad_steps = 1039, loss = 0.0022317736875265837
In grad_steps = 1040, loss = 0.06858940422534943
In grad_steps = 1041, loss = 0.16719216108322144
In grad_steps = 1042, loss = 0.0004888857365585864
In grad_steps = 1043, loss = 0.0012703214306384325
In grad_steps = 1044, loss = 0.0006397668039426208
In grad_steps = 1045, loss = 0.0004719352291431278
In grad_steps = 1046, loss = 0.001124927424825728
In grad_steps = 1047, loss = 0.08397381752729416
In grad_steps = 1048, loss = 0.3163018524646759
In grad_steps = 1049, loss = 0.02994181402027607
Elapsed time: 1624.7262761592865 seconds for ensemble 3 with 10 epochs
LoRA instance 3 evaluation complete. Data saved to /pscratch/sd/t/tianle/lucid/other_source/SURP_2024/results/tmp_lora_ensemble/Llama3-8B-set-6/test_data_instance_3_seed_30280.npz.
lora instance i = 3 Successfully finished.
Training lora instance 4
DEBUG: Total parameters: 8037076992
DEBUG: Trainable parameters: 6815744
Beginning epoch 1
In grad_steps = 0, loss = 0.8738614320755005
In grad_steps = 1, loss = 0.7901961803436279
In grad_steps = 2, loss = 0.7301238775253296
In grad_steps = 3, loss = 0.6996442675590515
In grad_steps = 4, loss = 0.7477620840072632
In grad_steps = 5, loss = 0.6931903958320618
In grad_steps = 6, loss = 0.7128002643585205
In grad_steps = 7, loss = 0.6600545048713684
In grad_steps = 8, loss = 0.6995223164558411
In grad_steps = 9, loss = 0.6521085500717163
In grad_steps = 10, loss = 0.6762880086898804
In grad_steps = 11, loss = 0.5774399638175964
In grad_steps = 12, loss = 0.6774662137031555
In grad_steps = 13, loss = 0.7681235671043396
In grad_steps = 14, loss = 0.6120443940162659
In grad_steps = 15, loss = 0.6367741823196411
In grad_steps = 16, loss = 0.6877180337905884
In grad_steps = 17, loss = 0.5353509783744812
In grad_steps = 18, loss = 0.6474846005439758
In grad_steps = 19, loss = 0.49212324619293213
In grad_steps = 20, loss = 0.5861311554908752
In grad_steps = 21, loss = 0.5753618478775024
In grad_steps = 22, loss = 0.5500434637069702
In grad_steps = 23, loss = 0.3587305545806885
In grad_steps = 24, loss = 0.5584850311279297
In grad_steps = 25, loss = 0.711238443851471
In grad_steps = 26, loss = 0.5888930559158325
In grad_steps = 27, loss = 0.36217543482780457
In grad_steps = 28, loss = 0.2650355100631714
In grad_steps = 29, loss = 0.39497220516204834
In grad_steps = 30, loss = 0.6524456739425659
In grad_steps = 31, loss = 0.3147725760936737
In grad_steps = 32, loss = 0.7050597071647644
In grad_steps = 33, loss = 0.4485016465187073
In grad_steps = 34, loss = 0.730500340461731
In grad_steps = 35, loss = 0.21330298483371735
In grad_steps = 36, loss = 0.352680504322052
In grad_steps = 37, loss = 0.5729250907897949
In grad_steps = 38, loss = 0.7019139528274536
In grad_steps = 39, loss = 0.29033032059669495
In grad_steps = 40, loss = 0.4932387173175812
In grad_steps = 41, loss = 0.6168959736824036
In grad_steps = 42, loss = 0.38058972358703613
In grad_steps = 43, loss = 0.3285679817199707
In grad_steps = 44, loss = 0.2927304208278656
In grad_steps = 45, loss = 0.6029576063156128
In grad_steps = 46, loss = 0.3013952672481537
In grad_steps = 47, loss = 0.3822558522224426
In grad_steps = 48, loss = 0.3787529170513153
In grad_steps = 49, loss = 0.14219854772090912
In grad_steps = 50, loss = 0.7175433039665222
In grad_steps = 51, loss = 0.16051384806632996
In grad_steps = 52, loss = 0.2975604236125946
In grad_steps = 53, loss = 0.5040909051895142
In grad_steps = 54, loss = 0.3468760550022125
In grad_steps = 55, loss = 0.3776426613330841
In grad_steps = 56, loss = 0.18810324370861053
In grad_steps = 57, loss = 0.26219287514686584
In grad_steps = 58, loss = 0.2446921467781067
In grad_steps = 59, loss = 0.2545604407787323
In grad_steps = 60, loss = 0.5673184394836426
In grad_steps = 61, loss = 0.3274356424808502
In grad_steps = 62, loss = 0.25636982917785645
In grad_steps = 63, loss = 0.30824053287506104
In grad_steps = 64, loss = 0.19739291071891785
In grad_steps = 65, loss = 0.21394556760787964
In grad_steps = 66, loss = 0.3666839897632599
In grad_steps = 67, loss = 0.34950709342956543
In grad_steps = 68, loss = 0.2967855632305145
In grad_steps = 69, loss = 0.3990987539291382
In grad_steps = 70, loss = 0.3903237283229828
In grad_steps = 71, loss = 0.11205140501260757
In grad_steps = 72, loss = 0.4094952344894409
In grad_steps = 73, loss = 0.4683482348918915
In grad_steps = 74, loss = 0.22743280231952667
In grad_steps = 75, loss = 0.14198055863380432
In grad_steps = 76, loss = 0.47820550203323364
In grad_steps = 77, loss = 0.11182251572608948
In grad_steps = 78, loss = 0.16493330895900726
In grad_steps = 79, loss = 0.2246672362089157
In grad_steps = 80, loss = 0.26390108466148376
In grad_steps = 81, loss = 0.32968252897262573
In grad_steps = 82, loss = 0.18327100574970245
In grad_steps = 83, loss = 0.4814361035823822
In grad_steps = 84, loss = 0.31475818157196045
In grad_steps = 85, loss = 0.04927387088537216
In grad_steps = 86, loss = 0.31200653314590454
In grad_steps = 87, loss = 0.8920128345489502
In grad_steps = 88, loss = 0.7000226378440857
In grad_steps = 89, loss = 0.3907043933868408
In grad_steps = 90, loss = 0.360409140586853
In grad_steps = 91, loss = 0.34409260749816895
In grad_steps = 92, loss = 0.27927133440971375
In grad_steps = 93, loss = 0.3131226599216461
In grad_steps = 94, loss = 0.2660043239593506
In grad_steps = 95, loss = 0.31345564126968384
In grad_steps = 96, loss = 0.4861891567707062
In grad_steps = 97, loss = 0.35502809286117554
In grad_steps = 98, loss = 0.5335702300071716
In grad_steps = 99, loss = 0.1876874715089798
In grad_steps = 100, loss = 0.1921246349811554
In grad_steps = 101, loss = 0.1058788001537323
In grad_steps = 102, loss = 0.16782963275909424
In grad_steps = 103, loss = 0.619966983795166
In grad_steps = 104, loss = 0.28355199098587036
Beginning epoch 2
In grad_steps = 105, loss = 0.32038289308547974
In grad_steps = 106, loss = 0.29032254219055176
In grad_steps = 107, loss = 0.14023219048976898
In grad_steps = 108, loss = 0.24214155972003937
In grad_steps = 109, loss = 0.5215592980384827
In grad_steps = 110, loss = 0.47240206599235535
In grad_steps = 111, loss = 0.3957827389240265
In grad_steps = 112, loss = 0.13554337620735168
In grad_steps = 113, loss = 0.16516855359077454
In grad_steps = 114, loss = 0.1443953514099121
In grad_steps = 115, loss = 0.20880867540836334
In grad_steps = 116, loss = 0.136095330119133
In grad_steps = 117, loss = 0.22624149918556213
In grad_steps = 118, loss = 1.008638858795166
In grad_steps = 119, loss = 0.15584900975227356
In grad_steps = 120, loss = 0.3029586374759674
In grad_steps = 121, loss = 0.06334029138088226
In grad_steps = 122, loss = 0.2332388460636139
In grad_steps = 123, loss = 0.15159553289413452
In grad_steps = 124, loss = 0.11838964372873306
In grad_steps = 125, loss = 0.3119768500328064
In grad_steps = 126, loss = 0.13841816782951355
In grad_steps = 127, loss = 0.14980259537696838
In grad_steps = 128, loss = 0.1437990665435791
In grad_steps = 129, loss = 0.23293671011924744
In grad_steps = 130, loss = 0.1795559972524643
In grad_steps = 131, loss = 0.22437019646167755
In grad_steps = 132, loss = 0.2709091901779175
In grad_steps = 133, loss = 0.16238880157470703
In grad_steps = 134, loss = 0.12219748646020889
In grad_steps = 135, loss = 0.12986724078655243
In grad_steps = 136, loss = 0.026802802458405495
In grad_steps = 137, loss = 0.19391340017318726
In grad_steps = 138, loss = 0.16176654398441315
In grad_steps = 139, loss = 0.2624715566635132
In grad_steps = 140, loss = 0.007103097625076771
In grad_steps = 141, loss = 0.1414562165737152
In grad_steps = 142, loss = 0.2346634566783905
In grad_steps = 143, loss = 0.4816717505455017
In grad_steps = 144, loss = 0.23309031128883362
In grad_steps = 145, loss = 0.08186442404985428
In grad_steps = 146, loss = 0.34368598461151123
In grad_steps = 147, loss = 0.024179965257644653
In grad_steps = 148, loss = 0.16608281433582306
In grad_steps = 149, loss = 0.036822590976953506
In grad_steps = 150, loss = 0.4342546761035919
In grad_steps = 151, loss = 0.3272828459739685
In grad_steps = 152, loss = 0.2960886061191559
In grad_steps = 153, loss = 0.07994846254587173
In grad_steps = 154, loss = 0.05803515017032623
In grad_steps = 155, loss = 0.16422072052955627
In grad_steps = 156, loss = 0.32521629333496094
In grad_steps = 157, loss = 0.28951239585876465
In grad_steps = 158, loss = 0.30513715744018555
In grad_steps = 159, loss = 0.15685583651065826
In grad_steps = 160, loss = 0.13784664869308472
In grad_steps = 161, loss = 0.12765100598335266
In grad_steps = 162, loss = 0.11677262932062149
In grad_steps = 163, loss = 0.11387155950069427
In grad_steps = 164, loss = 0.1227903813123703
In grad_steps = 165, loss = 0.16221460700035095
In grad_steps = 166, loss = 0.07365310192108154
In grad_steps = 167, loss = 0.1097419261932373
In grad_steps = 168, loss = 0.1339036226272583
In grad_steps = 169, loss = 0.06950005888938904
In grad_steps = 170, loss = 0.048600852489471436
In grad_steps = 171, loss = 0.10117820650339127
In grad_steps = 172, loss = 0.018409544602036476
In grad_steps = 173, loss = 0.02451574057340622
In grad_steps = 174, loss = 0.035434700548648834
In grad_steps = 175, loss = 0.1284809559583664
In grad_steps = 176, loss = 0.025219853967428207
In grad_steps = 177, loss = 0.18342602252960205
In grad_steps = 178, loss = 0.5440295338630676
In grad_steps = 179, loss = 0.0846719890832901
In grad_steps = 180, loss = 0.06248914822936058
In grad_steps = 181, loss = 0.15405073761940002
In grad_steps = 182, loss = 0.014373861253261566
In grad_steps = 183, loss = 0.01173961628228426
In grad_steps = 184, loss = 0.05628308653831482
In grad_steps = 185, loss = 0.055762000381946564
In grad_steps = 186, loss = 0.19437117874622345
In grad_steps = 187, loss = 0.02111046016216278
In grad_steps = 188, loss = 0.02463032305240631
In grad_steps = 189, loss = 0.28464266657829285
In grad_steps = 190, loss = 0.04738810285925865
In grad_steps = 191, loss = 0.003824666840955615
In grad_steps = 192, loss = 0.2654375433921814
In grad_steps = 193, loss = 0.5065898299217224
In grad_steps = 194, loss = 0.19115766882896423
In grad_steps = 195, loss = 0.12117616832256317
In grad_steps = 196, loss = 0.3402738571166992
In grad_steps = 197, loss = 0.17454367876052856
In grad_steps = 198, loss = 0.18402643501758575
In grad_steps = 199, loss = 0.36890074610710144
In grad_steps = 200, loss = 0.19114714860916138
In grad_steps = 201, loss = 0.28086474537849426
In grad_steps = 202, loss = 0.07403987646102905
In grad_steps = 203, loss = 0.39404720067977905
In grad_steps = 204, loss = 0.19418810307979584
In grad_steps = 205, loss = 0.28096863627433777
In grad_steps = 206, loss = 0.055542875081300735
In grad_steps = 207, loss = 0.15247368812561035
In grad_steps = 208, loss = 0.20289579033851624
In grad_steps = 209, loss = 0.11626610159873962
Beginning epoch 3
In grad_steps = 210, loss = 0.14522720873355865
In grad_steps = 211, loss = 0.04183223098516464
In grad_steps = 212, loss = 0.10683231055736542
In grad_steps = 213, loss = 0.1975145787000656
In grad_steps = 214, loss = 0.22637484967708588
In grad_steps = 215, loss = 0.053299978375434875
In grad_steps = 216, loss = 0.09492355585098267
In grad_steps = 217, loss = 0.03404565528035164
In grad_steps = 218, loss = 0.07672684639692307
In grad_steps = 219, loss = 0.03694124147295952
In grad_steps = 220, loss = 0.1826089471578598
In grad_steps = 221, loss = 0.08465134352445602
In grad_steps = 222, loss = 0.14944925904273987
In grad_steps = 223, loss = 0.2778874337673187
In grad_steps = 224, loss = 0.0027125005144625902
In grad_steps = 225, loss = 0.05192803218960762
In grad_steps = 226, loss = 0.0036765511613339186
In grad_steps = 227, loss = 0.06749874353408813
In grad_steps = 228, loss = 0.010612507350742817
In grad_steps = 229, loss = 0.003844138467684388
In grad_steps = 230, loss = 0.012777664698660374
In grad_steps = 231, loss = 0.11998159438371658
In grad_steps = 232, loss = 0.015705173835158348
In grad_steps = 233, loss = 0.18305344879627228
In grad_steps = 234, loss = 0.26713690161705017
In grad_steps = 235, loss = 0.019563863053917885
In grad_steps = 236, loss = 0.07894818484783173
In grad_steps = 237, loss = 0.03983538597822189
In grad_steps = 238, loss = 0.06361071020364761
In grad_steps = 239, loss = 0.12518738210201263
In grad_steps = 240, loss = 0.00939104799181223
In grad_steps = 241, loss = 0.027746230363845825
In grad_steps = 242, loss = 0.013224197551608086
In grad_steps = 243, loss = 0.02619892917573452
In grad_steps = 244, loss = 0.10567685216665268
In grad_steps = 245, loss = 0.0012653960147872567
In grad_steps = 246, loss = 0.012429558672010899
In grad_steps = 247, loss = 0.05660748854279518
In grad_steps = 248, loss = 0.2253209799528122
In grad_steps = 249, loss = 0.21632449328899384
In grad_steps = 250, loss = 0.019596902653574944
In grad_steps = 251, loss = 0.28194427490234375
In grad_steps = 252, loss = 0.02299654111266136
In grad_steps = 253, loss = 0.04818078875541687
In grad_steps = 254, loss = 0.0022237254306674004
In grad_steps = 255, loss = 0.028092380613088608
In grad_steps = 256, loss = 0.26167139410972595
In grad_steps = 257, loss = 0.30468836426734924
In grad_steps = 258, loss = 0.04926115274429321
In grad_steps = 259, loss = 0.09891108423471451
In grad_steps = 260, loss = 0.008613456040620804
In grad_steps = 261, loss = 0.16139233112335205
In grad_steps = 262, loss = 0.05411252751946449
In grad_steps = 263, loss = 0.13165511190891266
In grad_steps = 264, loss = 0.11207319051027298
In grad_steps = 265, loss = 0.14007005095481873
In grad_steps = 266, loss = 0.11191113293170929
In grad_steps = 267, loss = 0.02299388125538826
In grad_steps = 268, loss = 0.0667463093996048
In grad_steps = 269, loss = 0.13072456419467926
In grad_steps = 270, loss = 0.2088877111673355
In grad_steps = 271, loss = 0.036174360662698746
In grad_steps = 272, loss = 0.15381047129631042
In grad_steps = 273, loss = 0.022820081561803818
In grad_steps = 274, loss = 0.011717312969267368
In grad_steps = 275, loss = 0.032585736364126205
In grad_steps = 276, loss = 0.01959921233355999
In grad_steps = 277, loss = 0.07694341242313385
In grad_steps = 278, loss = 0.01650381088256836
In grad_steps = 279, loss = 0.011932313442230225
In grad_steps = 280, loss = 0.07185254245996475
In grad_steps = 281, loss = 0.023143649101257324
In grad_steps = 282, loss = 0.07348868250846863
In grad_steps = 283, loss = 0.11102867871522903
In grad_steps = 284, loss = 0.19491024315357208
In grad_steps = 285, loss = 0.00291977240704
In grad_steps = 286, loss = 0.0323784239590168
In grad_steps = 287, loss = 0.0019475192530080676
In grad_steps = 288, loss = 0.0007936529582366347
In grad_steps = 289, loss = 0.017127390950918198
In grad_steps = 290, loss = 0.03622129186987877
In grad_steps = 291, loss = 0.2525656521320343
In grad_steps = 292, loss = 0.00016806242638267577
In grad_steps = 293, loss = 0.00018056987028103322
In grad_steps = 294, loss = 0.0028927072416990995
In grad_steps = 295, loss = 0.0002958276600111276
In grad_steps = 296, loss = 0.00013481650967150927
In grad_steps = 297, loss = 0.07434704899787903
In grad_steps = 298, loss = 0.011945052072405815
In grad_steps = 299, loss = 0.0006482973694801331
In grad_steps = 300, loss = 0.16345112025737762
In grad_steps = 301, loss = 0.1510896384716034
In grad_steps = 302, loss = 0.01747894659638405
In grad_steps = 303, loss = 0.03724595159292221
In grad_steps = 304, loss = 0.08294776827096939
In grad_steps = 305, loss = 0.08866836875677109
In grad_steps = 306, loss = 0.09813322126865387
In grad_steps = 307, loss = 0.021616153419017792
In grad_steps = 308, loss = 0.2749643921852112
In grad_steps = 309, loss = 0.0012131552211940289
In grad_steps = 310, loss = 0.00783559400588274
In grad_steps = 311, loss = 0.0006421211292035878
In grad_steps = 312, loss = 0.12802352011203766
In grad_steps = 313, loss = 0.19957031309604645
In grad_steps = 314, loss = 0.0019198621157556772
Beginning epoch 4
In grad_steps = 315, loss = 0.14773064851760864
In grad_steps = 316, loss = 0.013582635670900345
In grad_steps = 317, loss = 0.008783087134361267
In grad_steps = 318, loss = 0.16703234612941742
In grad_steps = 319, loss = 0.05017770081758499
In grad_steps = 320, loss = 0.025260545313358307
In grad_steps = 321, loss = 0.1618455946445465
In grad_steps = 322, loss = 0.04544555023312569
In grad_steps = 323, loss = 0.2962951362133026
In grad_steps = 324, loss = 0.08132077008485794
In grad_steps = 325, loss = 0.0755344033241272
In grad_steps = 326, loss = 0.06215304136276245
In grad_steps = 327, loss = 0.05758153274655342
In grad_steps = 328, loss = 0.5596028566360474
In grad_steps = 329, loss = 0.06291273236274719
In grad_steps = 330, loss = 0.017229698598384857
In grad_steps = 331, loss = 0.014606707729399204
In grad_steps = 332, loss = 0.028437521308660507
In grad_steps = 333, loss = 0.008595326915383339
In grad_steps = 334, loss = 0.02935197204351425
In grad_steps = 335, loss = 0.049321841448545456
In grad_steps = 336, loss = 0.06989935785531998
In grad_steps = 337, loss = 0.061498820781707764
In grad_steps = 338, loss = 0.11992911249399185
In grad_steps = 339, loss = 0.17563970386981964
In grad_steps = 340, loss = 0.05515598505735397
In grad_steps = 341, loss = 0.03211584314703941
In grad_steps = 342, loss = 0.19116266071796417
In grad_steps = 343, loss = 0.05141803249716759
In grad_steps = 344, loss = 0.015454836189746857
In grad_steps = 345, loss = 0.034792207181453705
In grad_steps = 346, loss = 0.010927530005574226
In grad_steps = 347, loss = 0.022893279790878296
In grad_steps = 348, loss = 0.011793330311775208
In grad_steps = 349, loss = 0.11621105670928955
In grad_steps = 350, loss = 0.001073480467312038
In grad_steps = 351, loss = 0.04485030099749565
In grad_steps = 352, loss = 0.06129952892661095
In grad_steps = 353, loss = 0.07968925684690475
In grad_steps = 354, loss = 0.09856582432985306
In grad_steps = 355, loss = 0.012041698209941387
In grad_steps = 356, loss = 0.08491995185613632
In grad_steps = 357, loss = 0.005259102210402489
In grad_steps = 358, loss = 0.13905960321426392
In grad_steps = 359, loss = 0.00024617399321869016
In grad_steps = 360, loss = 0.0011362863006070256
In grad_steps = 361, loss = 0.005218572448939085
In grad_steps = 362, loss = 0.022662637755274773
In grad_steps = 363, loss = 0.01320970430970192
In grad_steps = 364, loss = 0.004708435852080584
In grad_steps = 365, loss = 0.0017289220122620463
In grad_steps = 366, loss = 0.15781772136688232
In grad_steps = 367, loss = 0.007993129082024097
In grad_steps = 368, loss = 0.12771473824977875
In grad_steps = 369, loss = 0.03184742480516434
In grad_steps = 370, loss = 0.01508874911814928
In grad_steps = 371, loss = 0.07164920121431351
In grad_steps = 372, loss = 0.0009359454270452261
In grad_steps = 373, loss = 0.003493179101496935
In grad_steps = 374, loss = 0.004760772455483675
In grad_steps = 375, loss = 0.0013132203603163362
In grad_steps = 376, loss = 0.00063126883469522
In grad_steps = 377, loss = 0.060456883162260056
In grad_steps = 378, loss = 0.0026133463252335787
In grad_steps = 379, loss = 0.001077925437130034
In grad_steps = 380, loss = 0.009392980486154556
In grad_steps = 381, loss = 0.01708202250301838
In grad_steps = 382, loss = 0.2274460643529892
In grad_steps = 383, loss = 0.0016626502620056272
In grad_steps = 384, loss = 0.002739697229117155
In grad_steps = 385, loss = 0.027237148955464363
In grad_steps = 386, loss = 0.0010498829651623964
In grad_steps = 387, loss = 0.11197908967733383
In grad_steps = 388, loss = 0.23057901859283447
In grad_steps = 389, loss = 0.012474123388528824
In grad_steps = 390, loss = 0.0041239517740905285
In grad_steps = 391, loss = 0.011501208879053593
In grad_steps = 392, loss = 0.0034386569168418646
In grad_steps = 393, loss = 0.0019896836020052433
In grad_steps = 394, loss = 0.003115515923127532
In grad_steps = 395, loss = 0.030639661476016045
In grad_steps = 396, loss = 0.1292848140001297
In grad_steps = 397, loss = 0.0009911691304296255
In grad_steps = 398, loss = 0.040757372975349426
In grad_steps = 399, loss = 0.02848917990922928
In grad_steps = 400, loss = 0.004570578224956989
In grad_steps = 401, loss = 0.006843613460659981
In grad_steps = 402, loss = 0.0018155841389670968
In grad_steps = 403, loss = 0.16644135117530823
In grad_steps = 404, loss = 0.0055709462612867355
In grad_steps = 405, loss = 0.010783717967569828
In grad_steps = 406, loss = 0.011901878751814365
In grad_steps = 407, loss = 0.0016171373426914215
In grad_steps = 408, loss = 0.0030817606020718813
In grad_steps = 409, loss = 0.009754974395036697
In grad_steps = 410, loss = 0.0005711594712920487
In grad_steps = 411, loss = 0.30716580152511597
In grad_steps = 412, loss = 0.0072387633845210075
In grad_steps = 413, loss = 0.0067777372896671295
In grad_steps = 414, loss = 0.0006967916269786656
In grad_steps = 415, loss = 0.0364418663084507
In grad_steps = 416, loss = 0.0024512908421456814
In grad_steps = 417, loss = 0.006289836950600147
In grad_steps = 418, loss = 0.004031587392091751
In grad_steps = 419, loss = 0.027514932677149773
Beginning epoch 5
In grad_steps = 420, loss = 0.10468678921461105
In grad_steps = 421, loss = 0.0010030291741713881
In grad_steps = 422, loss = 0.0016582357930019498
In grad_steps = 423, loss = 0.026715697720646858
In grad_steps = 424, loss = 0.14429549872875214
In grad_steps = 425, loss = 0.07149418443441391
In grad_steps = 426, loss = 0.03352193906903267
In grad_steps = 427, loss = 0.00560377910733223
In grad_steps = 428, loss = 0.01958085224032402
In grad_steps = 429, loss = 0.03020915389060974
In grad_steps = 430, loss = 0.08221868425607681
In grad_steps = 431, loss = 0.14434516429901123
In grad_steps = 432, loss = 0.009680821560323238
In grad_steps = 433, loss = 0.14984196424484253
In grad_steps = 434, loss = 0.0027695049066096544
In grad_steps = 435, loss = 0.01023252122104168
In grad_steps = 436, loss = 0.005971110425889492
In grad_steps = 437, loss = 0.10433892905712128
In grad_steps = 438, loss = 0.18479178845882416
In grad_steps = 439, loss = 0.004910331219434738
In grad_steps = 440, loss = 0.43935105204582214
In grad_steps = 441, loss = 0.1349828839302063
In grad_steps = 442, loss = 0.008948540315032005
In grad_steps = 443, loss = 0.22079166769981384
In grad_steps = 444, loss = 0.03272952139377594
In grad_steps = 445, loss = 0.09071247279644012
In grad_steps = 446, loss = 0.05386800318956375
In grad_steps = 447, loss = 0.14402469992637634
In grad_steps = 448, loss = 0.387927770614624
In grad_steps = 449, loss = 0.11963474005460739
In grad_steps = 450, loss = 0.052453573793172836
In grad_steps = 451, loss = 0.02597159519791603
In grad_steps = 452, loss = 0.008652857504785061
In grad_steps = 453, loss = 0.017804168164730072
In grad_steps = 454, loss = 0.03524111583828926
In grad_steps = 455, loss = 0.004045846406370401
In grad_steps = 456, loss = 0.031552355736494064
In grad_steps = 457, loss = 0.023591944947838783
In grad_steps = 458, loss = 0.09240724891424179
In grad_steps = 459, loss = 0.029903298243880272
In grad_steps = 460, loss = 0.006172077730298042
In grad_steps = 461, loss = 0.36832550168037415
In grad_steps = 462, loss = 0.044675350189208984
In grad_steps = 463, loss = 0.028083454817533493
In grad_steps = 464, loss = 0.0010368784423917532
In grad_steps = 465, loss = 0.0012628318509086967
In grad_steps = 466, loss = 0.0031566820107400417
In grad_steps = 467, loss = 0.17641408741474152
In grad_steps = 468, loss = 0.008222783915698528
In grad_steps = 469, loss = 0.017666367813944817
In grad_steps = 470, loss = 0.002687256783246994
In grad_steps = 471, loss = 0.01924705319106579
In grad_steps = 472, loss = 0.007349325343966484
In grad_steps = 473, loss = 0.024463143199682236
In grad_steps = 474, loss = 0.1676332950592041
In grad_steps = 475, loss = 0.049586571753025055
In grad_steps = 476, loss = 0.0035633768420666456
In grad_steps = 477, loss = 0.0013418023008853197
In grad_steps = 478, loss = 0.0013600587844848633
In grad_steps = 479, loss = 0.00308310822583735
In grad_steps = 480, loss = 0.007102276664227247
In grad_steps = 481, loss = 0.0003529030364006758
In grad_steps = 482, loss = 0.0011377413757145405
In grad_steps = 483, loss = 0.0019346406916156411
In grad_steps = 484, loss = 0.00012609096302185208
In grad_steps = 485, loss = 0.0004431795096024871
In grad_steps = 486, loss = 0.10077407956123352
In grad_steps = 487, loss = 0.0046182540245354176
In grad_steps = 488, loss = 0.0007186509319581091
In grad_steps = 489, loss = 0.00024170672986656427
In grad_steps = 490, loss = 0.0007830813992768526
In grad_steps = 491, loss = 0.012410621158778667
In grad_steps = 492, loss = 0.10095734894275665
In grad_steps = 493, loss = 0.05459068715572357
In grad_steps = 494, loss = 0.0003639449132606387
In grad_steps = 495, loss = 0.0022425646893680096
In grad_steps = 496, loss = 0.010988851077854633
In grad_steps = 497, loss = 0.0019080362981185317
In grad_steps = 498, loss = 0.0003225017280783504
In grad_steps = 499, loss = 0.008368413895368576
In grad_steps = 500, loss = 0.199336975812912
In grad_steps = 501, loss = 0.022612731903791428
In grad_steps = 502, loss = 0.009763650596141815
In grad_steps = 503, loss = 0.0033361290115863085
In grad_steps = 504, loss = 0.009048606269061565
In grad_steps = 505, loss = 0.0004146061837673187
In grad_steps = 506, loss = 0.0011560545535758138
In grad_steps = 507, loss = 0.29717516899108887
In grad_steps = 508, loss = 0.294792503118515
In grad_steps = 509, loss = 0.08105957508087158
In grad_steps = 510, loss = 0.005460423417389393
In grad_steps = 511, loss = 0.14282193779945374
In grad_steps = 512, loss = 0.11055691540241241
In grad_steps = 513, loss = 0.015401008538901806
In grad_steps = 514, loss = 0.008127209730446339
In grad_steps = 515, loss = 0.1740771234035492
In grad_steps = 516, loss = 0.1607285439968109
In grad_steps = 517, loss = 0.0440349355340004
In grad_steps = 518, loss = 0.16921868920326233
In grad_steps = 519, loss = 0.01854574866592884
In grad_steps = 520, loss = 0.03798786923289299
In grad_steps = 521, loss = 0.00829643290489912
In grad_steps = 522, loss = 0.056386735290288925
In grad_steps = 523, loss = 0.09794996678829193
In grad_steps = 524, loss = 0.027612656354904175
Beginning epoch 6
In grad_steps = 525, loss = 0.09866133332252502
In grad_steps = 526, loss = 0.011138761416077614
In grad_steps = 527, loss = 0.08995608985424042
In grad_steps = 528, loss = 0.12134058773517609
In grad_steps = 529, loss = 0.03739762678742409
In grad_steps = 530, loss = 0.021565141156315804
In grad_steps = 531, loss = 0.03068610280752182
In grad_steps = 532, loss = 0.022779911756515503
In grad_steps = 533, loss = 0.17548561096191406
In grad_steps = 534, loss = 0.015015137381851673
In grad_steps = 535, loss = 0.042977094650268555
In grad_steps = 536, loss = 0.009687069803476334
In grad_steps = 537, loss = 0.0037578814662992954
In grad_steps = 538, loss = 0.3008837103843689
In grad_steps = 539, loss = 0.0015193721046671271
In grad_steps = 540, loss = 0.0046578687615692616
In grad_steps = 541, loss = 0.0006636754260398448
In grad_steps = 542, loss = 0.15620630979537964
In grad_steps = 543, loss = 0.0005057081580162048
In grad_steps = 544, loss = 0.06347524374723434
In grad_steps = 545, loss = 0.0014466849388554692
In grad_steps = 546, loss = 0.003595754038542509
In grad_steps = 547, loss = 0.11086034029722214
In grad_steps = 548, loss = 0.1726938784122467
In grad_steps = 549, loss = 0.04722670465707779
In grad_steps = 550, loss = 0.005534046329557896
In grad_steps = 551, loss = 0.010360486805438995
In grad_steps = 552, loss = 0.005294021684676409
In grad_steps = 553, loss = 0.04029393568634987
In grad_steps = 554, loss = 0.05579541623592377
In grad_steps = 555, loss = 0.05353203043341637
In grad_steps = 556, loss = 0.054881613701581955
In grad_steps = 557, loss = 0.019230881705880165
In grad_steps = 558, loss = 0.5331528782844543
In grad_steps = 559, loss = 0.03270985931158066
In grad_steps = 560, loss = 0.0019382236059755087
In grad_steps = 561, loss = 0.0013593845069408417
In grad_steps = 562, loss = 0.039164312183856964
In grad_steps = 563, loss = 0.0025406002532690763
In grad_steps = 564, loss = 0.015629563480615616
In grad_steps = 565, loss = 0.008111409842967987
In grad_steps = 566, loss = 0.06312032788991928
In grad_steps = 567, loss = 0.0014171428047120571
In grad_steps = 568, loss = 0.041813552379608154
In grad_steps = 569, loss = 0.002345465822145343
In grad_steps = 570, loss = 0.03420191630721092
In grad_steps = 571, loss = 0.0067000119015574455
In grad_steps = 572, loss = 0.019441215321421623
In grad_steps = 573, loss = 0.006520689465105534
In grad_steps = 574, loss = 0.0012719972291961312
In grad_steps = 575, loss = 0.14428424835205078
In grad_steps = 576, loss = 0.0015173604479059577
In grad_steps = 577, loss = 0.2970737814903259
In grad_steps = 578, loss = 0.003213706659153104
In grad_steps = 579, loss = 0.022599834948778152
In grad_steps = 580, loss = 0.011455632746219635
In grad_steps = 581, loss = 0.0020643926691263914
In grad_steps = 582, loss = 0.001687687705270946
In grad_steps = 583, loss = 0.003591405227780342
In grad_steps = 584, loss = 0.36411207914352417
In grad_steps = 585, loss = 0.060824569314718246
In grad_steps = 586, loss = 0.14585228264331818
In grad_steps = 587, loss = 0.055363062769174576
In grad_steps = 588, loss = 0.01784387230873108
In grad_steps = 589, loss = 0.025338822975754738
In grad_steps = 590, loss = 0.014850813895463943
In grad_steps = 591, loss = 0.003128302050754428
In grad_steps = 592, loss = 0.005913027096539736
In grad_steps = 593, loss = 0.019092155620455742
In grad_steps = 594, loss = 0.030978046357631683
In grad_steps = 595, loss = 0.1313549131155014
In grad_steps = 596, loss = 0.03092922829091549
In grad_steps = 597, loss = 0.015397963114082813
In grad_steps = 598, loss = 0.006616335827857256
In grad_steps = 599, loss = 0.009829293936491013
In grad_steps = 600, loss = 0.0015934235416352749
In grad_steps = 601, loss = 0.030421540141105652
In grad_steps = 602, loss = 0.01436765119433403
In grad_steps = 603, loss = 0.0014270106330513954
In grad_steps = 604, loss = 0.0021404444705694914
In grad_steps = 605, loss = 0.0034666513092815876
In grad_steps = 606, loss = 0.018947675824165344
In grad_steps = 607, loss = 0.002586561255156994
In grad_steps = 608, loss = 0.0027690082788467407
In grad_steps = 609, loss = 0.0054405550472438335
In grad_steps = 610, loss = 0.00482599250972271
In grad_steps = 611, loss = 0.00041486474219709635
In grad_steps = 612, loss = 0.003779822029173374
In grad_steps = 613, loss = 0.000559926382265985
In grad_steps = 614, loss = 0.0032945876009762287
In grad_steps = 615, loss = 0.003515449585393071
In grad_steps = 616, loss = 0.0050940499641001225
In grad_steps = 617, loss = 0.01211750227957964
In grad_steps = 618, loss = 0.000303323264233768
In grad_steps = 619, loss = 0.010670059360563755
In grad_steps = 620, loss = 0.00028263794956728816
In grad_steps = 621, loss = 0.002287279348820448
In grad_steps = 622, loss = 0.003521800972521305
In grad_steps = 623, loss = 0.19751791656017303
In grad_steps = 624, loss = 0.000349583599017933
In grad_steps = 625, loss = 0.000223511757212691
In grad_steps = 626, loss = 0.00013851151743438095
In grad_steps = 627, loss = 0.002338952152058482
In grad_steps = 628, loss = 0.0008586098556406796
In grad_steps = 629, loss = 0.00029365436057560146
Beginning epoch 7
In grad_steps = 630, loss = 0.04619379714131355
In grad_steps = 631, loss = 0.14710140228271484
In grad_steps = 632, loss = 0.012583394534885883
In grad_steps = 633, loss = 0.0008111525094136596
In grad_steps = 634, loss = 0.0035101736430078745
In grad_steps = 635, loss = 0.0022052216809242964
In grad_steps = 636, loss = 0.12057195603847504
In grad_steps = 637, loss = 0.040834855288267136
In grad_steps = 638, loss = 0.07421012967824936
In grad_steps = 639, loss = 0.0036575135309249163
In grad_steps = 640, loss = 0.020805615931749344
In grad_steps = 641, loss = 0.42601531744003296
In grad_steps = 642, loss = 0.0007929729763418436
In grad_steps = 643, loss = 0.13909311592578888
In grad_steps = 644, loss = 0.005652769934386015
In grad_steps = 645, loss = 0.006215381436049938
In grad_steps = 646, loss = 0.002099819714203477
In grad_steps = 647, loss = 0.010389742441475391
In grad_steps = 648, loss = 0.02083565667271614
In grad_steps = 649, loss = 0.021212942898273468
In grad_steps = 650, loss = 0.012222522869706154
In grad_steps = 651, loss = 0.061512239277362823
In grad_steps = 652, loss = 0.019769450649619102
In grad_steps = 653, loss = 0.12832458317279816
In grad_steps = 654, loss = 0.3613559901714325
In grad_steps = 655, loss = 0.016592491418123245
In grad_steps = 656, loss = 0.2821715772151947
In grad_steps = 657, loss = 0.005935973487794399
In grad_steps = 658, loss = 0.09829597175121307
In grad_steps = 659, loss = 0.018718469887971878
In grad_steps = 660, loss = 0.22706450521945953
In grad_steps = 661, loss = 0.18505091965198517
In grad_steps = 662, loss = 0.04667811840772629
In grad_steps = 663, loss = 0.057012803852558136
In grad_steps = 664, loss = 0.03559290990233421
In grad_steps = 665, loss = 0.016320837661623955
In grad_steps = 666, loss = 0.00820055790245533
In grad_steps = 667, loss = 0.03698970004916191
In grad_steps = 668, loss = 0.020256975665688515
In grad_steps = 669, loss = 0.06293636560440063
In grad_steps = 670, loss = 0.008838839828968048
In grad_steps = 671, loss = 0.09478077292442322
In grad_steps = 672, loss = 0.016344577074050903
In grad_steps = 673, loss = 0.0908493623137474
In grad_steps = 674, loss = 0.0017619837308302522
In grad_steps = 675, loss = 0.018848901614546776
In grad_steps = 676, loss = 0.0018194327130913734
In grad_steps = 677, loss = 0.012195853516459465
In grad_steps = 678, loss = 0.005408947356045246
In grad_steps = 679, loss = 0.012175701558589935
In grad_steps = 680, loss = 0.03180079534649849
In grad_steps = 681, loss = 0.0063346391543745995
In grad_steps = 682, loss = 0.0021179646719247103
In grad_steps = 683, loss = 0.003814233699813485
In grad_steps = 684, loss = 0.11080357432365417
In grad_steps = 685, loss = 0.08471222966909409
In grad_steps = 686, loss = 0.0003777080855797976
In grad_steps = 687, loss = 0.0027580552268773317
In grad_steps = 688, loss = 0.06146242469549179
In grad_steps = 689, loss = 0.055688828229904175
In grad_steps = 690, loss = 0.0017284026835113764
In grad_steps = 691, loss = 0.00039452724740840495
In grad_steps = 692, loss = 0.03086954541504383
In grad_steps = 693, loss = 0.0051568238995969296
In grad_steps = 694, loss = 7.960687071317807e-05
In grad_steps = 695, loss = 0.00036523956805467606
In grad_steps = 696, loss = 0.0005828863941133022
In grad_steps = 697, loss = 3.5858443879988045e-05
In grad_steps = 698, loss = 0.26632294058799744
In grad_steps = 699, loss = 0.003101249923929572
In grad_steps = 700, loss = 0.0003090364043600857
In grad_steps = 701, loss = 0.00020273475092835724
In grad_steps = 702, loss = 0.0008243878255598247
In grad_steps = 703, loss = 0.21069277822971344
In grad_steps = 704, loss = 8.151771180564538e-05
In grad_steps = 705, loss = 0.00036290351999923587
In grad_steps = 706, loss = 0.098969005048275
In grad_steps = 707, loss = 0.0002864645794034004
In grad_steps = 708, loss = 0.0009329805616289377
In grad_steps = 709, loss = 0.07972769439220428
In grad_steps = 710, loss = 0.0016272572102025151
In grad_steps = 711, loss = 0.07593927532434464
In grad_steps = 712, loss = 0.0005716503947041929
In grad_steps = 713, loss = 0.00334509601816535
In grad_steps = 714, loss = 0.09783683717250824
In grad_steps = 715, loss = 0.011424226686358452
In grad_steps = 716, loss = 0.0007891984423622489
In grad_steps = 717, loss = 0.0034572600852698088
In grad_steps = 718, loss = 0.0018493179231882095
In grad_steps = 719, loss = 0.3426488935947418
In grad_steps = 720, loss = 0.013206650502979755
In grad_steps = 721, loss = 0.01330217719078064
In grad_steps = 722, loss = 0.06342833489179611
In grad_steps = 723, loss = 0.009055654518306255
In grad_steps = 724, loss = 0.3830156624317169
In grad_steps = 725, loss = 0.004889717325568199
In grad_steps = 726, loss = 0.004601238761097193
In grad_steps = 727, loss = 0.03008795715868473
In grad_steps = 728, loss = 0.06688102334737778
In grad_steps = 729, loss = 0.008237426169216633
In grad_steps = 730, loss = 0.01192017737776041
In grad_steps = 731, loss = 0.004321929533034563
In grad_steps = 732, loss = 0.0858645886182785
In grad_steps = 733, loss = 0.030518706887960434
In grad_steps = 734, loss = 0.005955866072326899
Beginning epoch 8
In grad_steps = 735, loss = 0.05187059938907623
In grad_steps = 736, loss = 0.11305846273899078
In grad_steps = 737, loss = 0.00829851534217596
In grad_steps = 738, loss = 0.03732084110379219
In grad_steps = 739, loss = 0.1019645407795906
In grad_steps = 740, loss = 0.008298274129629135
In grad_steps = 741, loss = 0.0058771600015461445
In grad_steps = 742, loss = 0.004920514300465584
In grad_steps = 743, loss = 0.021647345274686813
In grad_steps = 744, loss = 0.008439329452812672
In grad_steps = 745, loss = 0.03080923855304718
In grad_steps = 746, loss = 0.0084049878641963
In grad_steps = 747, loss = 0.013921272940933704
In grad_steps = 748, loss = 0.004227310419082642
In grad_steps = 749, loss = 0.002826167270541191
In grad_steps = 750, loss = 0.004182466305792332
In grad_steps = 751, loss = 0.0015464097959920764
In grad_steps = 752, loss = 0.11947767436504364
In grad_steps = 753, loss = 0.0007929793791845441
In grad_steps = 754, loss = 0.004009305965155363
In grad_steps = 755, loss = 0.010527653619647026
In grad_steps = 756, loss = 0.017749913036823273
In grad_steps = 757, loss = 0.0026591920759528875
In grad_steps = 758, loss = 0.0103575075045228
In grad_steps = 759, loss = 0.0018482677405700088
In grad_steps = 760, loss = 0.0013742888113483787
In grad_steps = 761, loss = 0.01364406943321228
In grad_steps = 762, loss = 0.3154357373714447
In grad_steps = 763, loss = 0.0004166713042650372
In grad_steps = 764, loss = 0.019167693331837654
In grad_steps = 765, loss = 0.0003520202590152621
In grad_steps = 766, loss = 0.0009694942273199558
In grad_steps = 767, loss = 0.0012739209923893213
In grad_steps = 768, loss = 0.002204430988058448
In grad_steps = 769, loss = 0.0016679723048582673
In grad_steps = 770, loss = 0.001647696364670992
In grad_steps = 771, loss = 0.0010352515382692218
In grad_steps = 772, loss = 0.10597574710845947
In grad_steps = 773, loss = 0.0019232503836974502
In grad_steps = 774, loss = 0.028441529721021652
In grad_steps = 775, loss = 0.012218025512993336
In grad_steps = 776, loss = 0.02103058621287346
In grad_steps = 777, loss = 0.00877862237393856
In grad_steps = 778, loss = 0.0013125311816111207
In grad_steps = 779, loss = 0.0011730489786714315
In grad_steps = 780, loss = 0.001721792621538043
In grad_steps = 781, loss = 0.0014854021137580276
In grad_steps = 782, loss = 0.0053708902560174465
In grad_steps = 783, loss = 0.0039356546476483345
In grad_steps = 784, loss = 0.004546079784631729
In grad_steps = 785, loss = 0.00632511219009757
In grad_steps = 786, loss = 0.0026849096175283194
In grad_steps = 787, loss = 0.004109176807105541
In grad_steps = 788, loss = 0.08625027537345886
In grad_steps = 789, loss = 0.010767309926450253
In grad_steps = 790, loss = 0.005461452528834343
In grad_steps = 791, loss = 0.006039078813046217
In grad_steps = 792, loss = 0.0009731611935421824
In grad_steps = 793, loss = 0.013092665933072567
In grad_steps = 794, loss = 0.0010631962213665247
In grad_steps = 795, loss = 0.006302861962467432
In grad_steps = 796, loss = 0.00018516699492465705
In grad_steps = 797, loss = 0.0015673058805987239
In grad_steps = 798, loss = 0.004435206763446331
In grad_steps = 799, loss = 0.00018129803356714547
In grad_steps = 800, loss = 0.0009782740380614996
In grad_steps = 801, loss = 0.0002753630396910012
In grad_steps = 802, loss = 0.00023071627947501838
In grad_steps = 803, loss = 0.0014250311069190502
In grad_steps = 804, loss = 0.0887133926153183
In grad_steps = 805, loss = 0.012681939639151096
In grad_steps = 806, loss = 0.0009777512168511748
In grad_steps = 807, loss = 0.0017857459606602788
In grad_steps = 808, loss = 0.0006902592722326517
In grad_steps = 809, loss = 0.0001773493568180129
In grad_steps = 810, loss = 0.0003915137785952538
In grad_steps = 811, loss = 0.0002160304138669744
In grad_steps = 812, loss = 0.0006087264628149569
In grad_steps = 813, loss = 0.00020036492787767202
In grad_steps = 814, loss = 0.000834094185847789
In grad_steps = 815, loss = 0.0019224256975576282
In grad_steps = 816, loss = 0.01248803362250328
In grad_steps = 817, loss = 7.713856757618487e-05
In grad_steps = 818, loss = 0.0002918180252891034
In grad_steps = 819, loss = 0.04899829626083374
In grad_steps = 820, loss = 0.0009651280124671757
In grad_steps = 821, loss = 0.00010392256081104279
In grad_steps = 822, loss = 0.014105132780969143
In grad_steps = 823, loss = 0.0006190618732944131
In grad_steps = 824, loss = 0.0012835192028433084
In grad_steps = 825, loss = 0.010265209712088108
In grad_steps = 826, loss = 0.005254059098660946
In grad_steps = 827, loss = 7.593729969812557e-05
In grad_steps = 828, loss = 0.027289221063256264
In grad_steps = 829, loss = 0.23874053359031677
In grad_steps = 830, loss = 8.718034223420545e-05
In grad_steps = 831, loss = 0.0007735511171631515
In grad_steps = 832, loss = 0.00029881461523473263
In grad_steps = 833, loss = 0.0006260679801926017
In grad_steps = 834, loss = 0.00036066744360141456
In grad_steps = 835, loss = 0.00015376691590063274
In grad_steps = 836, loss = 0.00010762993770185858
In grad_steps = 837, loss = 0.0005525262677110732
In grad_steps = 838, loss = 0.0005376037443056703
In grad_steps = 839, loss = 0.0003538577875588089
Beginning epoch 9
In grad_steps = 840, loss = 0.0204031839966774
In grad_steps = 841, loss = 0.0003651281585916877
In grad_steps = 842, loss = 0.00022520484344568104
In grad_steps = 843, loss = 0.0008690318209119141
In grad_steps = 844, loss = 0.001222970662638545
In grad_steps = 845, loss = 0.0017566613387316465
In grad_steps = 846, loss = 0.0006401980062946677
In grad_steps = 847, loss = 0.00024656736059114337
In grad_steps = 848, loss = 0.003315049223601818
In grad_steps = 849, loss = 0.0002705029328353703
In grad_steps = 850, loss = 0.0019544456154108047
In grad_steps = 851, loss = 0.3671918511390686
In grad_steps = 852, loss = 0.0007095685577951372
In grad_steps = 853, loss = 0.06629011780023575
In grad_steps = 854, loss = 0.0025894970167428255
In grad_steps = 855, loss = 0.0012301228707656264
In grad_steps = 856, loss = 0.0022014924325048923
In grad_steps = 857, loss = 0.0002761390060186386
In grad_steps = 858, loss = 0.007228439673781395
In grad_steps = 859, loss = 0.001577607705257833
In grad_steps = 860, loss = 0.00462324870750308
In grad_steps = 861, loss = 0.013581843115389347
In grad_steps = 862, loss = 0.009467152878642082
In grad_steps = 863, loss = 0.009858939796686172
In grad_steps = 864, loss = 0.03391772136092186
In grad_steps = 865, loss = 0.003579770913347602
In grad_steps = 866, loss = 0.013373281806707382
In grad_steps = 867, loss = 0.00250961328856647
In grad_steps = 868, loss = 0.0021944118198007345
In grad_steps = 869, loss = 0.0012837584363296628
In grad_steps = 870, loss = 0.0004195011861156672
In grad_steps = 871, loss = 0.00044685392640531063
In grad_steps = 872, loss = 0.0008036487270146608
In grad_steps = 873, loss = 0.0014921191614121199
In grad_steps = 874, loss = 0.0813932940363884
In grad_steps = 875, loss = 0.0005501939449459314
In grad_steps = 876, loss = 0.0005351219442673028
In grad_steps = 877, loss = 0.007684854790568352
In grad_steps = 878, loss = 0.009815121069550514
In grad_steps = 879, loss = 0.0395096093416214
In grad_steps = 880, loss = 0.00496782036498189
In grad_steps = 881, loss = 0.00409962423145771
In grad_steps = 882, loss = 0.004538621753454208
In grad_steps = 883, loss = 0.00046373711666092277
In grad_steps = 884, loss = 0.000758972077164799
In grad_steps = 885, loss = 0.001529415836557746
In grad_steps = 886, loss = 0.007555664516985416
In grad_steps = 887, loss = 0.004521739203482866
In grad_steps = 888, loss = 0.0004886933020316064
In grad_steps = 889, loss = 0.008356593549251556
In grad_steps = 890, loss = 0.0009019657736644149
In grad_steps = 891, loss = 0.0070172809064388275
In grad_steps = 892, loss = 0.0015580219915136695
In grad_steps = 893, loss = 0.5692703127861023
In grad_steps = 894, loss = 0.0023080131504684687
In grad_steps = 895, loss = 0.0013499213382601738
In grad_steps = 896, loss = 0.0006229194696061313
In grad_steps = 897, loss = 0.0017768609104678035
In grad_steps = 898, loss = 0.0026524572167545557
In grad_steps = 899, loss = 0.00120684748981148
In grad_steps = 900, loss = 0.002840937115252018
In grad_steps = 901, loss = 0.0004623973509296775
In grad_steps = 902, loss = 0.013016733340919018
In grad_steps = 903, loss = 0.0026340861804783344
In grad_steps = 904, loss = 0.001302237855270505
In grad_steps = 905, loss = 0.001277309376746416
In grad_steps = 906, loss = 0.0006542232003994286
In grad_steps = 907, loss = 0.0008438592194579542
In grad_steps = 908, loss = 0.0014002884272485971
In grad_steps = 909, loss = 0.0010343539761379361
In grad_steps = 910, loss = 0.001320171868428588
In grad_steps = 911, loss = 0.002857908373698592
In grad_steps = 912, loss = 0.010189865715801716
In grad_steps = 913, loss = 0.07741570472717285
In grad_steps = 914, loss = 0.0017924823332577944
In grad_steps = 915, loss = 0.0007395005086436868
In grad_steps = 916, loss = 0.001136797945946455
In grad_steps = 917, loss = 0.01858373172581196
In grad_steps = 918, loss = 0.0006158357136882842
In grad_steps = 919, loss = 0.0175496693700552
In grad_steps = 920, loss = 0.0009856270626187325
In grad_steps = 921, loss = 0.001753831165842712
In grad_steps = 922, loss = 0.0002131341607309878
In grad_steps = 923, loss = 0.0007422565831802785
In grad_steps = 924, loss = 0.0009380636038258672
In grad_steps = 925, loss = 0.0032489332370460033
In grad_steps = 926, loss = 0.00018723917310126126
In grad_steps = 927, loss = 0.005639113485813141
In grad_steps = 928, loss = 0.0006737626390531659
In grad_steps = 929, loss = 0.0021469886414706707
In grad_steps = 930, loss = 0.000975415634457022
In grad_steps = 931, loss = 0.0027139231096953154
In grad_steps = 932, loss = 0.00021331937750801444
In grad_steps = 933, loss = 0.0002988514897879213
In grad_steps = 934, loss = 0.0004647844471037388
In grad_steps = 935, loss = 0.0001533444446977228
In grad_steps = 936, loss = 0.0005000357632525265
In grad_steps = 937, loss = 0.0003554931899998337
In grad_steps = 938, loss = 0.0017644194886088371
In grad_steps = 939, loss = 0.00042304658563807607
In grad_steps = 940, loss = 0.00021838968677911907
In grad_steps = 941, loss = 6.812497304053977e-05
In grad_steps = 942, loss = 0.00030861690174788237
In grad_steps = 943, loss = 0.00028071439010091126
In grad_steps = 944, loss = 0.00033352532773278654
Beginning epoch 10
In grad_steps = 945, loss = 0.03635102137923241
In grad_steps = 946, loss = 0.00023791207058820873
In grad_steps = 947, loss = 0.00017680411110632122
In grad_steps = 948, loss = 0.0010659191757440567
In grad_steps = 949, loss = 0.0004627225862350315
In grad_steps = 950, loss = 0.001264003338292241
In grad_steps = 951, loss = 0.00011272924166405573
In grad_steps = 952, loss = 0.00013576983474195004
In grad_steps = 953, loss = 0.0009810832561925054
In grad_steps = 954, loss = 7.357625872828066e-05
In grad_steps = 955, loss = 0.001672318670898676
In grad_steps = 956, loss = 0.0005790426512248814
In grad_steps = 957, loss = 0.00045448989840224385
In grad_steps = 958, loss = 0.0010902300709858537
In grad_steps = 959, loss = 0.00017792485596146435
In grad_steps = 960, loss = 0.0001726087211864069
In grad_steps = 961, loss = 8.624858310213313e-05
In grad_steps = 962, loss = 4.824729330721311e-05
In grad_steps = 963, loss = 5.0623639253899455e-05
In grad_steps = 964, loss = 8.608668576925993e-05
In grad_steps = 965, loss = 6.390721682691947e-05
In grad_steps = 966, loss = 0.00033321190858259797
In grad_steps = 967, loss = 0.0001856042945291847
In grad_steps = 968, loss = 0.005047152284532785
In grad_steps = 969, loss = 0.009840712882578373
In grad_steps = 970, loss = 0.0008593437960371375
In grad_steps = 971, loss = 0.00016490511188749224
In grad_steps = 972, loss = 0.00012913905084133148
In grad_steps = 973, loss = 0.00011885947606060654
In grad_steps = 974, loss = 7.507865666411817e-05
In grad_steps = 975, loss = 2.828932338161394e-05
In grad_steps = 976, loss = 3.209595161024481e-05
In grad_steps = 977, loss = 4.296553379390389e-05
In grad_steps = 978, loss = 6.0305123042780906e-05
In grad_steps = 979, loss = 0.0001927152625285089
In grad_steps = 980, loss = 7.191312033683062e-05
In grad_steps = 981, loss = 3.619322887971066e-05
In grad_steps = 982, loss = 0.00016712266369722784
In grad_steps = 983, loss = 4.6472501708194613e-05
In grad_steps = 984, loss = 0.0003788416797760874
In grad_steps = 985, loss = 9.78801108431071e-05
In grad_steps = 986, loss = 0.00016430013056378812
In grad_steps = 987, loss = 0.00017935044888872653
In grad_steps = 988, loss = 2.1755264242528938e-05
In grad_steps = 989, loss = 2.8317195756244473e-05
In grad_steps = 990, loss = 0.00018432969227433205
In grad_steps = 991, loss = 0.00017289792594965547
In grad_steps = 992, loss = 0.00031820882577449083
In grad_steps = 993, loss = 5.142407098901458e-05
In grad_steps = 994, loss = 0.00015142641495913267
In grad_steps = 995, loss = 8.88405556906946e-05
In grad_steps = 996, loss = 0.0004614763311110437
In grad_steps = 997, loss = 3.5224198654759675e-05
In grad_steps = 998, loss = 0.21245773136615753
In grad_steps = 999, loss = 0.00012748580775223672
In grad_steps = 1000, loss = 0.00015039902064017951
In grad_steps = 1001, loss = 4.835998697672039e-05
In grad_steps = 1002, loss = 0.000398416566895321
In grad_steps = 1003, loss = 0.0004410992260091007
In grad_steps = 1004, loss = 0.00035863573430106044
In grad_steps = 1005, loss = 0.00042977335397154093
In grad_steps = 1006, loss = 9.907957428367808e-05
In grad_steps = 1007, loss = 0.0008932395139709115
In grad_steps = 1008, loss = 0.0003523878112901002
In grad_steps = 1009, loss = 0.0016641704132780433
In grad_steps = 1010, loss = 0.000727460253983736
In grad_steps = 1011, loss = 0.00021622130589094013
In grad_steps = 1012, loss = 0.00028888191445730627
In grad_steps = 1013, loss = 0.00036252441350370646
In grad_steps = 1014, loss = 0.07215375453233719
In grad_steps = 1015, loss = 0.00020477721409406513
In grad_steps = 1016, loss = 0.0003556647861842066
In grad_steps = 1017, loss = 0.0005525166052393615
In grad_steps = 1018, loss = 0.0007155961939133704
In grad_steps = 1019, loss = 0.0002037578815361485
In grad_steps = 1020, loss = 0.00020280595344956964
In grad_steps = 1021, loss = 0.00023789299302734435
In grad_steps = 1022, loss = 0.00042742269579321146
In grad_steps = 1023, loss = 0.004171229433268309
In grad_steps = 1024, loss = 0.02604093961417675
In grad_steps = 1025, loss = 0.0005227046785876155
In grad_steps = 1026, loss = 0.0004875777813140303
In grad_steps = 1027, loss = 9.199306077789515e-05
In grad_steps = 1028, loss = 0.003431806806474924
In grad_steps = 1029, loss = 0.0013009246904402971
In grad_steps = 1030, loss = 0.002404386643320322
In grad_steps = 1031, loss = 0.00016775446420069784
In grad_steps = 1032, loss = 0.000821659981738776
In grad_steps = 1033, loss = 0.0010833153501152992
In grad_steps = 1034, loss = 0.002747713588178158
In grad_steps = 1035, loss = 0.0005514752119779587
In grad_steps = 1036, loss = 0.0012413421645760536
In grad_steps = 1037, loss = 0.0007264114101417363
In grad_steps = 1038, loss = 0.00043134891893714666
In grad_steps = 1039, loss = 0.0004946491098962724
In grad_steps = 1040, loss = 0.00024366742582060397
In grad_steps = 1041, loss = 0.009726366028189659
In grad_steps = 1042, loss = 0.000736550020519644
In grad_steps = 1043, loss = 0.0034482073970139027
In grad_steps = 1044, loss = 0.001373221748508513
In grad_steps = 1045, loss = 0.0010622848058119416
In grad_steps = 1046, loss = 6.940619641682133e-05
In grad_steps = 1047, loss = 0.0004896182799711823
In grad_steps = 1048, loss = 0.14075766503810883
In grad_steps = 1049, loss = 0.0003319761308375746
Elapsed time: 1624.7848353385925 seconds for ensemble 4 with 10 epochs
LoRA instance 4 evaluation complete. Data saved to /pscratch/sd/t/tianle/lucid/other_source/SURP_2024/results/tmp_lora_ensemble/Llama3-8B-set-6/test_data_instance_4_seed_40373.npz.
lora instance i = 4 Successfully finished.
Final, Test average ensemble probabilities = 
[[9.99971688e-01 2.83258560e-05]
 [9.99843776e-01 1.56233655e-04]
 [9.99378979e-01 6.20989711e-04]
 [9.99823213e-01 1.76822097e-04]
 [5.42273156e-05 9.99945819e-01]
 [9.99921203e-01 7.88007746e-05]
 [9.99952912e-01 4.70820160e-05]
 [3.45220184e-03 9.96547818e-01]
 [9.99953449e-01 4.65459816e-05]
 [7.19747186e-05 9.99927998e-01]
 [8.12407970e-01 1.87592044e-01]
 [9.99592781e-01 4.07204323e-04]
 [9.99905765e-01 9.42093175e-05]
 [4.63336997e-04 9.99536693e-01]
 [2.16565866e-04 9.99783397e-01]
 [3.24743829e-04 9.99675155e-01]
 [9.99444485e-01 5.55542880e-04]
 [9.99832153e-01 1.67917504e-04]
 [6.98591322e-02 9.30140853e-01]
 [9.99968648e-01 3.13212731e-05]
 [1.03267244e-04 9.99896824e-01]
 [9.99952793e-01 4.72030370e-05]
 [1.33958180e-03 9.98660386e-01]
 [3.74413736e-04 9.99625564e-01]
 [6.05625683e-05 9.99939442e-01]
 [9.99966502e-01 3.34672914e-05]
 [3.41185165e-04 9.99658883e-01]
 [9.93701339e-01 6.29869243e-03]
 [9.99735534e-01 2.64433678e-04]
 [3.36637022e-05 9.99966323e-01]
 [9.99965966e-01 3.39978906e-05]
 [4.62252507e-03 9.95377421e-01]
 [3.20672733e-03 9.96793270e-01]
 [9.98776615e-01 1.22329895e-03]
 [4.76154019e-05 9.99952316e-01]
 [9.99814689e-01 1.85289260e-04]
 [9.99679565e-01 3.20447114e-04]
 [1.41300133e-03 9.98586953e-01]
 [9.99968827e-01 3.11998774e-05]
 [9.99973893e-01 2.61393234e-05]
 [9.98882294e-01 1.11772609e-03]
 [5.12197381e-04 9.99487758e-01]
 [9.97465312e-01 2.53471732e-03]
 [6.43670675e-04 9.99356270e-01]
 [5.70968477e-05 9.99942780e-01]
 [9.99639332e-01 3.60727019e-04]
 [9.99838531e-01 1.61414282e-04]
 [2.51790945e-04 9.99748230e-01]
 [9.99961495e-01 3.85539206e-05]
 [9.99900162e-01 9.98481555e-05]
 [9.49446869e-04 9.99050498e-01]
 [2.16312619e-04 9.99783695e-01]
 [9.99891639e-01 1.08287422e-04]
 [9.99888420e-01 1.11597670e-04]
 [1.98773385e-04 9.99801278e-01]
 [1.66464844e-04 9.99833465e-01]
 [2.84433656e-04 9.99715626e-01]
 [1.62384033e-01 8.37615967e-01]
 [1.83330446e-01 8.16669583e-01]
 [9.99953628e-01 4.63241995e-05]
 [9.95534539e-01 4.46544681e-03]
 [9.99962211e-01 3.77407996e-05]
 [4.74474713e-04 9.99525547e-01]
 [9.52799455e-05 9.99904752e-01]
 [8.82093787e-01 1.17906176e-01]
 [9.99789059e-01 2.10900645e-04]
 [9.99962687e-01 3.72678696e-05]
 [2.48031982e-04 9.99751925e-01]
 [9.99935329e-01 6.46633998e-05]
 [1.68982486e-03 9.98310208e-01]
 [9.76129413e-01 2.38706004e-02]
 [9.99949157e-01 5.08434205e-05]
 [9.99933898e-01 6.60625010e-05]
 [6.41674997e-05 9.99935806e-01]
 [1.10873814e-04 9.99889195e-01]
 [9.99955356e-01 4.46697486e-05]
 [1.66624886e-04 9.99833405e-01]
 [9.99935746e-01 6.42124069e-05]
 [2.52016395e-01 7.47983634e-01]
 [9.99952316e-01 4.76619243e-05]
 [6.39065052e-04 9.99360919e-01]
 [9.77790177e-01 2.22098175e-02]
 [9.69039142e-01 3.09608541e-02]
 [1.72479165e-04 9.99827504e-01]
 [9.99921918e-01 7.80793343e-05]
 [9.99959290e-01 4.06940744e-05]
 [8.62488747e-01 1.37511164e-01]
 [2.27287688e-04 9.99772727e-01]
 [9.95656133e-01 4.34390735e-03]
 [4.23512831e-02 9.57648635e-01]
 [6.89162916e-05 9.99931037e-01]
 [2.54443294e-04 9.99745548e-01]
 [1.64377189e-03 9.98356223e-01]
 [4.36682130e-05 9.99956310e-01]
 [9.99852180e-01 1.47862796e-04]
 [9.99923825e-01 7.61716219e-05]
 [9.07695759e-03 9.90923047e-01]
 [9.99457538e-01 5.42455644e-04]
 [9.99879360e-01 1.20712000e-04]
 [1.05772773e-03 9.98942256e-01]
 [9.99886811e-01 1.13210619e-04]
 [9.99911129e-01 8.88547802e-05]
 [9.99834180e-01 1.65800564e-04]
 [4.24360187e-05 9.99957561e-01]
 [7.32467597e-05 9.99926746e-01]
 [9.96543586e-01 3.45643610e-03]
 [8.19134439e-05 9.99918103e-01]
 [9.69558537e-01 3.04414071e-02]
 [4.63461547e-05 9.99953628e-01]
 [1.56625742e-04 9.99843419e-01]
 [9.99851406e-01 1.48554478e-04]
 [2.01452896e-02 9.79854703e-01]
 [9.99889016e-01 1.10946363e-04]
 [3.46618581e-05 9.99965370e-01]
 [9.99670327e-01 3.29650706e-04]
 [9.99948025e-01 5.19389396e-05]
 [7.05058932e-01 2.94941038e-01]
 [9.99910474e-01 8.95142875e-05]
 [7.22950208e-05 9.99927640e-01]
 [9.99874592e-01 1.25486302e-04]
 [1.65101810e-04 9.99834895e-01]
 [3.03225213e-04 9.99696732e-01]
 [8.17408290e-05 9.99918282e-01]
 [9.99956906e-01 4.30971886e-05]
 [9.99765873e-01 2.34110004e-04]
 [9.99960244e-01 3.98243428e-05]
 [7.34175992e-05 9.99926567e-01]
 [9.99874949e-01 1.25071034e-04]
 [1.19073351e-03 9.98809218e-01]
 [9.99834180e-01 1.65880236e-04]
 [7.71549006e-04 9.99228477e-01]
 [9.99919534e-01 8.04252777e-05]
 [3.80028105e-05 9.99961972e-01]
 [9.42927072e-05 9.99905705e-01]
 [1.36952367e-04 9.99863029e-01]
 [4.41845885e-04 9.99558151e-01]
 [9.99937236e-01 6.27756672e-05]
 [2.48177210e-04 9.99751747e-01]
 [9.99946892e-01 5.30972320e-05]
 [2.57693529e-02 9.74230587e-01]
 [7.46422520e-05 9.99925435e-01]
 [9.99942183e-01 5.78379368e-05]
 [1.26557902e-03 9.98734474e-01]
 [5.05300814e-05 9.99949455e-01]
 [9.99382377e-01 6.17559883e-04]
 [3.35388988e-01 6.64611042e-01]
 [9.99852002e-01 1.48127569e-04]
 [9.99484241e-01 5.15730993e-04]
 [1.90572992e-01 8.09426963e-01]
 [9.95579839e-01 4.42018034e-03]
 [9.99962211e-01 3.77213364e-05]
 [5.67885654e-05 9.99943256e-01]
 [9.99930084e-01 6.99248412e-05]
 [5.84165791e-05 9.99941647e-01]
 [4.86932024e-02 9.51306820e-01]
 [2.73563825e-02 9.72643673e-01]
 [8.99947464e-01 1.00052610e-01]
 [9.99876797e-01 1.23170597e-04]
 [9.99869704e-01 1.30282337e-04]
 [9.94670510e-01 5.32959495e-03]
 [1.34522139e-04 9.99865532e-01]
 [9.99963164e-01 3.67853172e-05]
 [6.04067091e-03 9.93959308e-01]
 [9.99167740e-01 8.32330144e-04]
 [9.99896884e-01 1.03110353e-04]
 [8.83792236e-04 9.99116242e-01]
 [1.18992869e-02 9.88100708e-01]
 [5.73939542e-05 9.99942601e-01]
 [2.08074620e-04 9.99791920e-01]
 [4.26367745e-02 9.57363248e-01]
 [5.93754594e-05 9.99940693e-01]
 [9.98112321e-01 1.88772695e-03]
 [5.34911924e-05 9.99946475e-01]
 [6.50767148e-01 3.49232882e-01]
 [9.91022885e-01 8.97710398e-03]
 [9.99616027e-01 3.83931620e-04]
 [9.99811828e-01 1.88153732e-04]
 [3.92730959e-04 9.99607265e-01]
 [8.11956227e-01 1.88043743e-01]
 [4.38643066e-04 9.99561310e-01]
 [6.18080841e-03 9.93819118e-01]
 [6.19248226e-02 9.38075066e-01]
 [5.86319424e-04 9.99413669e-01]
 [9.97387588e-01 2.61234841e-03]
 [1.04202470e-03 9.98957932e-01]
 [5.28788951e-04 9.99471307e-01]
 [9.99814689e-01 1.85279001e-04]]
Accuracy: 0.9893
MCC: 0.9788
AUC: 0.9994
Confusion Matrix:
tensor([[96,  2],
        [ 0, 89]])
Specificity: 0.9796
Precision (Macro): 0.9890
F1 Score (Macro): 0.9893
Expected Calibration Error (ECE): 0.0237
NLL loss: 0.0381
Ensemble evaluation complete.
