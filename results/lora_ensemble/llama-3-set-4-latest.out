Job is running on partition: gpu_ss11
Job is running under account: m2616_g
/pscratch/sd/t/tianle/conda/envs/lucid_surp_2024/lib/python3.11/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:11<00:33, 11.02s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:21<00:21, 10.82s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:32<00:10, 10.72s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:34<00:00,  7.44s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:34<00:00,  8.68s/it]
Llama3 has been loaded successfully.
Namespace(model_name='Llama3', config='/pscratch/sd/t/tianle/lucid/other_source/SURP_2024/src/config.json', repo_dir='/pscratch/sd/t/tianle/lucid/other_source/SURP_2024/', dataset='4', n_ensemble=5, seed=1, use_model_snapshot=False)
self.num_epochs = 2, self.batch_size = 4, self.max_length = 50
Model:  LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(128256, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)
          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)
          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)
          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)
)
Train dataset looks like:  Dataset({
    features: ['question', 'answer'],
    num_rows: 9409
})
Test dataset looks like:  Dataset({
    features: ['question', 'answer'],
    num_rows: 2353
})
Training lora instance 0
DEBUG: Total parameters: 8037076992
DEBUG: Trainable parameters: 6815744
Beginning epoch 1
In grad_steps = 0, loss = 0.5826846361160278
In grad_steps = 1, loss = 0.48125219345092773
In grad_steps = 2, loss = 1.732238531112671
In grad_steps = 3, loss = 0.7374504804611206
In grad_steps = 4, loss = 0.9249066710472107
In grad_steps = 5, loss = 0.733489990234375
In grad_steps = 6, loss = 1.0994107723236084
In grad_steps = 7, loss = 0.821294903755188
In grad_steps = 8, loss = 0.44713079929351807
In grad_steps = 9, loss = 0.6969436407089233
In grad_steps = 10, loss = 0.7558619379997253
In grad_steps = 11, loss = 1.016089677810669
In grad_steps = 12, loss = 1.0309756994247437
In grad_steps = 13, loss = 0.6844143867492676
In grad_steps = 14, loss = 0.6538245677947998
In grad_steps = 15, loss = 0.7559962868690491
In grad_steps = 16, loss = 0.7299110889434814
In grad_steps = 17, loss = 0.563302218914032
In grad_steps = 18, loss = 1.1379791498184204
In grad_steps = 19, loss = 1.0272618532180786
In grad_steps = 20, loss = 1.0088059902191162
In grad_steps = 21, loss = 0.574245810508728
In grad_steps = 22, loss = 0.7082914113998413
In grad_steps = 23, loss = 0.5565446019172668
In grad_steps = 24, loss = 0.7660064697265625
In grad_steps = 25, loss = 0.6730362176895142
In grad_steps = 26, loss = 0.6545864343643188
In grad_steps = 27, loss = 0.6780094504356384
In grad_steps = 28, loss = 0.7073342800140381
In grad_steps = 29, loss = 0.7214572429656982
In grad_steps = 30, loss = 0.7172548770904541
In grad_steps = 31, loss = 0.6795670390129089
In grad_steps = 32, loss = 0.6893446445465088
In grad_steps = 33, loss = 0.7291399240493774
In grad_steps = 34, loss = 0.7235915660858154
In grad_steps = 35, loss = 0.5997815728187561
In grad_steps = 36, loss = 0.686369776725769
In grad_steps = 37, loss = 0.6837239265441895
In grad_steps = 38, loss = 0.7353872060775757
In grad_steps = 39, loss = 0.6393399238586426
In grad_steps = 40, loss = 0.643092930316925
In grad_steps = 41, loss = 0.5658956170082092
In grad_steps = 42, loss = 0.5276768207550049
In grad_steps = 43, loss = 0.5228410959243774
In grad_steps = 44, loss = 0.7565401792526245
In grad_steps = 45, loss = 0.7575207948684692
In grad_steps = 46, loss = 0.9374021291732788
In grad_steps = 47, loss = 0.6092580556869507
In grad_steps = 48, loss = 0.9397339820861816
In grad_steps = 49, loss = 0.5181597471237183
In grad_steps = 50, loss = 1.007837176322937
In grad_steps = 51, loss = 1.0034972429275513
In grad_steps = 52, loss = 0.7058267593383789
In grad_steps = 53, loss = 0.6823381185531616
In grad_steps = 54, loss = 0.7026196718215942
In grad_steps = 55, loss = 0.6230267882347107
In grad_steps = 56, loss = 0.6210970878601074
In grad_steps = 57, loss = 2.9867424964904785
In grad_steps = 58, loss = 0.6468371748924255
In grad_steps = 59, loss = 0.7066657543182373
In grad_steps = 60, loss = 0.6851977109909058
In grad_steps = 61, loss = 0.7445310354232788
In grad_steps = 62, loss = 0.8020876049995422
In grad_steps = 63, loss = 0.6297577023506165
In grad_steps = 64, loss = 0.7049880623817444
In grad_steps = 65, loss = 0.744245707988739
In grad_steps = 66, loss = 0.6780110597610474
In grad_steps = 67, loss = 0.6930360198020935
In grad_steps = 68, loss = 0.684368908405304
In grad_steps = 69, loss = 0.7937676906585693
In grad_steps = 70, loss = 0.6796552538871765
In grad_steps = 71, loss = 0.6783121824264526
In grad_steps = 72, loss = 0.6323211789131165
In grad_steps = 73, loss = 0.7229664325714111
In grad_steps = 74, loss = 1.8317738771438599
In grad_steps = 75, loss = 0.5808694958686829
In grad_steps = 76, loss = 0.7213419675827026
In grad_steps = 77, loss = 0.7827169895172119
In grad_steps = 78, loss = 0.6537548303604126
In grad_steps = 79, loss = 0.606305718421936
In grad_steps = 80, loss = 0.6940914392471313
In grad_steps = 81, loss = 0.5960415601730347
In grad_steps = 82, loss = 0.6760823130607605
In grad_steps = 83, loss = 0.7085328698158264
In grad_steps = 84, loss = 0.6684139966964722
In grad_steps = 85, loss = 0.7346978187561035
In grad_steps = 86, loss = 0.601544976234436
In grad_steps = 87, loss = 0.597194254398346
In grad_steps = 88, loss = 0.6586662530899048
In grad_steps = 89, loss = 0.6992244124412537
In grad_steps = 90, loss = 0.662710964679718
In grad_steps = 91, loss = 0.7157844305038452
In grad_steps = 92, loss = 0.7222286462783813
In grad_steps = 93, loss = 0.5787379741668701
In grad_steps = 94, loss = 0.6081258058547974
In grad_steps = 95, loss = 0.5766912698745728
In grad_steps = 96, loss = 0.7754907608032227
In grad_steps = 97, loss = 0.6593406200408936
In grad_steps = 98, loss = 0.5469740629196167
In grad_steps = 99, loss = 0.6028292775154114
In grad_steps = 100, loss = 0.5395349264144897
In grad_steps = 101, loss = 0.5179482102394104
In grad_steps = 102, loss = 0.8161521553993225
In grad_steps = 103, loss = 0.6741945743560791
In grad_steps = 104, loss = 0.7350465059280396
In grad_steps = 105, loss = 0.7061272859573364
In grad_steps = 106, loss = 0.540249764919281
In grad_steps = 107, loss = 0.8483665585517883
In grad_steps = 108, loss = 0.4351172149181366
In grad_steps = 109, loss = 0.5518630743026733
In grad_steps = 110, loss = 0.5127707123756409
In grad_steps = 111, loss = 0.47684094309806824
In grad_steps = 112, loss = 0.8312985897064209
In grad_steps = 113, loss = 0.8815141916275024
In grad_steps = 114, loss = 0.5266603231430054
In grad_steps = 115, loss = 0.3725663125514984
In grad_steps = 116, loss = 0.4026235342025757
In grad_steps = 117, loss = 0.5758130550384521
In grad_steps = 118, loss = 1.358353853225708
In grad_steps = 119, loss = 1.4017536640167236
In grad_steps = 120, loss = 0.887284517288208
In grad_steps = 121, loss = 0.9055271148681641
In grad_steps = 122, loss = 0.7675983905792236
In grad_steps = 123, loss = 0.6472539305686951
In grad_steps = 124, loss = 0.6347535252571106
In grad_steps = 125, loss = 0.7546137571334839
In grad_steps = 126, loss = 0.43039587140083313
In grad_steps = 127, loss = 0.7629095315933228
In grad_steps = 128, loss = 0.6865240335464478
In grad_steps = 129, loss = 0.8045287132263184
In grad_steps = 130, loss = 0.5746499300003052
In grad_steps = 131, loss = 0.8295741677284241
In grad_steps = 132, loss = 0.6371203064918518
In grad_steps = 133, loss = 0.71176677942276
In grad_steps = 134, loss = 0.7100387811660767
In grad_steps = 135, loss = 0.6612412333488464
In grad_steps = 136, loss = 0.6926794648170471
In grad_steps = 137, loss = 0.7043564319610596
In grad_steps = 138, loss = 0.652478039264679
In grad_steps = 139, loss = 0.5696144104003906
In grad_steps = 140, loss = 0.6195836663246155
In grad_steps = 141, loss = 0.6641225218772888
In grad_steps = 142, loss = 0.6984447836875916
In grad_steps = 143, loss = 0.6175817251205444
In grad_steps = 144, loss = 0.6171295642852783
In grad_steps = 145, loss = 0.785749077796936
In grad_steps = 146, loss = 0.5987898707389832
In grad_steps = 147, loss = 0.6249980926513672
In grad_steps = 148, loss = 0.6443317532539368
In grad_steps = 149, loss = 0.4461972415447235
In grad_steps = 150, loss = 0.451281875371933
In grad_steps = 151, loss = 0.7975854277610779
In grad_steps = 152, loss = 0.18029490113258362
In grad_steps = 153, loss = 1.049393653869629
In grad_steps = 154, loss = 1.2633392810821533
In grad_steps = 155, loss = 0.5396882891654968
In grad_steps = 156, loss = 0.579866886138916
In grad_steps = 157, loss = 0.6945236921310425
In grad_steps = 158, loss = 0.5650801658630371
In grad_steps = 159, loss = 0.6388983726501465
In grad_steps = 160, loss = 0.6142811179161072
In grad_steps = 161, loss = 0.39508548378944397
In grad_steps = 162, loss = 0.9596872329711914
In grad_steps = 163, loss = 0.4763159155845642
In grad_steps = 164, loss = 1.0188113451004028
In grad_steps = 165, loss = 0.5837883353233337
In grad_steps = 166, loss = 0.6869696974754333
In grad_steps = 167, loss = 0.4085652530193329
In grad_steps = 168, loss = 0.4478839337825775
In grad_steps = 169, loss = 0.7392911911010742
In grad_steps = 170, loss = 0.623167872428894
In grad_steps = 171, loss = 0.7007136344909668
In grad_steps = 172, loss = 0.6593106985092163
In grad_steps = 173, loss = 0.4843575060367584
In grad_steps = 174, loss = 0.8630415201187134
In grad_steps = 175, loss = 0.5776596665382385
In grad_steps = 176, loss = 0.46558162569999695
In grad_steps = 177, loss = 0.5127426385879517
In grad_steps = 178, loss = 0.6262563467025757
In grad_steps = 179, loss = 0.5461221933364868
In grad_steps = 180, loss = 0.5324525833129883
In grad_steps = 181, loss = 0.5082076787948608
In grad_steps = 182, loss = 0.5650003552436829
In grad_steps = 183, loss = 0.4401458501815796
In grad_steps = 184, loss = 0.4824453592300415
In grad_steps = 185, loss = 0.5048308968544006
In grad_steps = 186, loss = 1.068586826324463
In grad_steps = 187, loss = 0.2626070976257324
In grad_steps = 188, loss = 0.7772648334503174
In grad_steps = 189, loss = 0.17805007100105286
In grad_steps = 190, loss = 0.6557015180587769
In grad_steps = 191, loss = 0.6898257732391357
In grad_steps = 192, loss = 0.3571023643016815
In grad_steps = 193, loss = 0.29645535349845886
In grad_steps = 194, loss = 0.6652773022651672
In grad_steps = 195, loss = 0.2599831819534302
In grad_steps = 196, loss = 0.3215343654155731
In grad_steps = 197, loss = 0.1647438257932663
In grad_steps = 198, loss = 0.708834707736969
In grad_steps = 199, loss = 0.7248948812484741
In grad_steps = 200, loss = 0.21972735226154327
In grad_steps = 201, loss = 0.9179593324661255
In grad_steps = 202, loss = 0.14832286536693573
In grad_steps = 203, loss = 0.304175466299057
In grad_steps = 204, loss = 0.41973376274108887
In grad_steps = 205, loss = 0.8239850997924805
In grad_steps = 206, loss = 1.1956487894058228
In grad_steps = 207, loss = 0.5388519167900085
In grad_steps = 208, loss = 0.37204861640930176
In grad_steps = 209, loss = 0.6633723974227905
In grad_steps = 210, loss = 0.6152122616767883
In grad_steps = 211, loss = 0.4206957519054413
In grad_steps = 212, loss = 0.35654953122138977
In grad_steps = 213, loss = 0.5566923022270203
In grad_steps = 214, loss = 0.5202075242996216
In grad_steps = 215, loss = 0.4958939254283905
In grad_steps = 216, loss = 0.5656487941741943
In grad_steps = 217, loss = 0.5756427645683289
In grad_steps = 218, loss = 0.5577095150947571
In grad_steps = 219, loss = 0.3347674310207367
In grad_steps = 220, loss = 0.23668597638607025
In grad_steps = 221, loss = 0.359056681394577
In grad_steps = 222, loss = 0.35809046030044556
In grad_steps = 223, loss = 0.48619914054870605
In grad_steps = 224, loss = 0.2944066822528839
In grad_steps = 225, loss = 1.0094534158706665
In grad_steps = 226, loss = 0.45038747787475586
In grad_steps = 227, loss = 1.1127066612243652
In grad_steps = 228, loss = 0.3422049582004547
In grad_steps = 229, loss = 0.47789373993873596
In grad_steps = 230, loss = 0.6124542951583862
In grad_steps = 231, loss = 0.33502262830734253
In grad_steps = 232, loss = 0.16449879109859467
In grad_steps = 233, loss = 0.7612258195877075
In grad_steps = 234, loss = 0.6062721014022827
In grad_steps = 235, loss = 0.42799586057662964
In grad_steps = 236, loss = 0.619279682636261
In grad_steps = 237, loss = 0.524929404258728
In grad_steps = 238, loss = 0.2586962580680847
In grad_steps = 239, loss = 0.3960241675376892
In grad_steps = 240, loss = 0.806647777557373
In grad_steps = 241, loss = 0.2919182777404785
In grad_steps = 242, loss = 0.5778961777687073
In grad_steps = 243, loss = 0.5268995761871338
In grad_steps = 244, loss = 0.28849896788597107
In grad_steps = 245, loss = 0.2191985547542572
In grad_steps = 246, loss = 0.20539569854736328
In grad_steps = 247, loss = 0.6547335386276245
In grad_steps = 248, loss = 0.1458132565021515
In grad_steps = 249, loss = 0.3057239055633545
In grad_steps = 250, loss = 0.35570070147514343
In grad_steps = 251, loss = 0.6080052852630615
In grad_steps = 252, loss = 1.3065083026885986
In grad_steps = 253, loss = 0.21616503596305847
In grad_steps = 254, loss = 0.8859333395957947
In grad_steps = 255, loss = 0.5765026807785034
In grad_steps = 256, loss = 0.4048982262611389
In grad_steps = 257, loss = 0.6033205389976501
In grad_steps = 258, loss = 0.19776248931884766
In grad_steps = 259, loss = 0.7086811065673828
In grad_steps = 260, loss = 0.7889249324798584
In grad_steps = 261, loss = 0.6533225774765015
In grad_steps = 262, loss = 0.34322434663772583
In grad_steps = 263, loss = 0.7821954488754272
In grad_steps = 264, loss = 0.2865881025791168
In grad_steps = 265, loss = 0.37496593594551086
In grad_steps = 266, loss = 0.5650457739830017
In grad_steps = 267, loss = 0.4056296944618225
In grad_steps = 268, loss = 0.49102985858917236
In grad_steps = 269, loss = 0.4079267382621765
In grad_steps = 270, loss = 0.32585158944129944
In grad_steps = 271, loss = 0.33190393447875977
In grad_steps = 272, loss = 0.2113284021615982
In grad_steps = 273, loss = 0.5567132830619812
In grad_steps = 274, loss = 0.15291357040405273
In grad_steps = 275, loss = 0.2587093412876129
In grad_steps = 276, loss = 0.4106901288032532
In grad_steps = 277, loss = 0.6748281717300415
In grad_steps = 278, loss = 0.1237993836402893
In grad_steps = 279, loss = 0.9793233275413513
In grad_steps = 280, loss = 0.27321895956993103
In grad_steps = 281, loss = 0.5099729299545288
In grad_steps = 282, loss = 0.4718431234359741
In grad_steps = 283, loss = 0.8999897241592407
In grad_steps = 284, loss = 0.2832343578338623
In grad_steps = 285, loss = 0.5976510047912598
In grad_steps = 286, loss = 0.6771042346954346
In grad_steps = 287, loss = 0.06602783501148224
In grad_steps = 288, loss = 0.058382295072078705
In grad_steps = 289, loss = 0.5875062942504883
In grad_steps = 290, loss = 0.07836495339870453
In grad_steps = 291, loss = 0.41268211603164673
In grad_steps = 292, loss = 0.7145590782165527
In grad_steps = 293, loss = 0.48006361722946167
In grad_steps = 294, loss = 0.339851975440979
In grad_steps = 295, loss = 0.23997658491134644
In grad_steps = 296, loss = 0.18082764744758606
In grad_steps = 297, loss = 0.3524719774723053
In grad_steps = 298, loss = 0.35907816886901855
In grad_steps = 299, loss = 0.5262777805328369
In grad_steps = 300, loss = 0.5593893527984619
In grad_steps = 301, loss = 0.3381420373916626
In grad_steps = 302, loss = 0.4307151436805725
In grad_steps = 303, loss = 0.25370118021965027
In grad_steps = 304, loss = 0.43634650111198425
In grad_steps = 305, loss = 1.5560506582260132
In grad_steps = 306, loss = 0.10112322866916656
In grad_steps = 307, loss = 0.3287286162376404
In grad_steps = 308, loss = 0.5051336288452148
In grad_steps = 309, loss = 0.7793603539466858
In grad_steps = 310, loss = 0.4969491958618164
In grad_steps = 311, loss = 0.8147186636924744
In grad_steps = 312, loss = 0.5358428359031677
In grad_steps = 313, loss = 0.7758967876434326
In grad_steps = 314, loss = 0.6307369470596313
In grad_steps = 315, loss = 0.30340051651000977
In grad_steps = 316, loss = 0.3788793087005615
In grad_steps = 317, loss = 0.4369673728942871
In grad_steps = 318, loss = 0.2875239849090576
In grad_steps = 319, loss = 0.4309813678264618
In grad_steps = 320, loss = 0.19492870569229126
In grad_steps = 321, loss = 0.3946453332901001
In grad_steps = 322, loss = 0.7759464979171753
In grad_steps = 323, loss = 0.5754770636558533
In grad_steps = 324, loss = 0.2851566672325134
In grad_steps = 325, loss = 0.19819597899913788
In grad_steps = 326, loss = 0.1442602574825287
In grad_steps = 327, loss = 0.26946157217025757
In grad_steps = 328, loss = 0.18142428994178772
In grad_steps = 329, loss = 0.1557394117116928
In grad_steps = 330, loss = 0.7506779432296753
In grad_steps = 331, loss = 0.17909619212150574
In grad_steps = 332, loss = 0.14952269196510315
In grad_steps = 333, loss = 0.3769545257091522
In grad_steps = 334, loss = 0.07187939435243607
In grad_steps = 335, loss = 0.22306546568870544
In grad_steps = 336, loss = 0.6643279194831848
In grad_steps = 337, loss = 0.07747206836938858
In grad_steps = 338, loss = 0.2661113739013672
In grad_steps = 339, loss = 0.7664823532104492
In grad_steps = 340, loss = 0.4588778614997864
In grad_steps = 341, loss = 0.04232454672455788
In grad_steps = 342, loss = 1.1133909225463867
In grad_steps = 343, loss = 1.028828740119934
In grad_steps = 344, loss = 0.06360077112913132
In grad_steps = 345, loss = 0.07199586182832718
In grad_steps = 346, loss = 0.38313671946525574
In grad_steps = 347, loss = 0.8366646766662598
In grad_steps = 348, loss = 0.7661850452423096
In grad_steps = 349, loss = 0.9760388731956482
In grad_steps = 350, loss = 0.7764888405799866
In grad_steps = 351, loss = 0.46852192282676697
In grad_steps = 352, loss = 0.6182119846343994
In grad_steps = 353, loss = 0.30398130416870117
In grad_steps = 354, loss = 0.4790686368942261
In grad_steps = 355, loss = 1.2471137046813965
In grad_steps = 356, loss = 0.13567110896110535
In grad_steps = 357, loss = 0.5595751404762268
In grad_steps = 358, loss = 0.16184306144714355
In grad_steps = 359, loss = 0.7053614854812622
In grad_steps = 360, loss = 0.7485980987548828
In grad_steps = 361, loss = 0.5744432210922241
In grad_steps = 362, loss = 0.24294792115688324
In grad_steps = 363, loss = 0.38416147232055664
In grad_steps = 364, loss = 0.47865045070648193
In grad_steps = 365, loss = 0.34868648648262024
In grad_steps = 366, loss = 0.3140724301338196
In grad_steps = 367, loss = 0.48950904607772827
In grad_steps = 368, loss = 0.295878529548645
In grad_steps = 369, loss = 0.33045434951782227
In grad_steps = 370, loss = 0.5906760692596436
In grad_steps = 371, loss = 0.32404959201812744
In grad_steps = 372, loss = 0.1686905026435852
In grad_steps = 373, loss = 0.4429839551448822
In grad_steps = 374, loss = 0.288897305727005
In grad_steps = 375, loss = 0.4821043908596039
In grad_steps = 376, loss = 0.08598814904689789
In grad_steps = 377, loss = 0.616054117679596
In grad_steps = 378, loss = 0.43607473373413086
In grad_steps = 379, loss = 0.18407224118709564
In grad_steps = 380, loss = 0.28405269980430603
In grad_steps = 381, loss = 0.7655878663063049
In grad_steps = 382, loss = 0.22295887768268585
In grad_steps = 383, loss = 0.4033823311328888
In grad_steps = 384, loss = 0.2891342043876648
In grad_steps = 385, loss = 0.221039280295372
In grad_steps = 386, loss = 2.0091874599456787
In grad_steps = 387, loss = 0.48438143730163574
In grad_steps = 388, loss = 0.6112082600593567
In grad_steps = 389, loss = 0.5766527652740479
In grad_steps = 390, loss = 0.34485170245170593
In grad_steps = 391, loss = 0.23968258500099182
In grad_steps = 392, loss = 0.298470139503479
In grad_steps = 393, loss = 0.6201122403144836
In grad_steps = 394, loss = 0.27895089983940125
In grad_steps = 395, loss = 0.46914589405059814
In grad_steps = 396, loss = 0.4620665907859802
In grad_steps = 397, loss = 0.6301982402801514
In grad_steps = 398, loss = 0.2566290497779846
In grad_steps = 399, loss = 0.41903185844421387
In grad_steps = 400, loss = 0.40988877415657043
In grad_steps = 401, loss = 0.18498149514198303
In grad_steps = 402, loss = 0.43696409463882446
In grad_steps = 403, loss = 0.9912342429161072
In grad_steps = 404, loss = 0.1587623953819275
In grad_steps = 405, loss = 0.25056982040405273
In grad_steps = 406, loss = 0.8599401116371155
In grad_steps = 407, loss = 0.36369678378105164
In grad_steps = 408, loss = 0.08295438438653946
In grad_steps = 409, loss = 0.45390188694000244
In grad_steps = 410, loss = 0.26771238446235657
In grad_steps = 411, loss = 0.8472159504890442
In grad_steps = 412, loss = 0.32194018363952637
In grad_steps = 413, loss = 0.1892835944890976
In grad_steps = 414, loss = 0.6210514307022095
In grad_steps = 415, loss = 0.4600885510444641
In grad_steps = 416, loss = 0.19327084720134735
In grad_steps = 417, loss = 0.23880435526371002
In grad_steps = 418, loss = 0.40823447704315186
In grad_steps = 419, loss = 0.25965607166290283
In grad_steps = 420, loss = 0.07659430056810379
In grad_steps = 421, loss = 1.086643934249878
In grad_steps = 422, loss = 0.6083136796951294
In grad_steps = 423, loss = 0.17663031816482544
In grad_steps = 424, loss = 0.5674082636833191
In grad_steps = 425, loss = 0.06892619282007217
In grad_steps = 426, loss = 0.6049536466598511
In grad_steps = 427, loss = 0.2158050835132599
In grad_steps = 428, loss = 0.09439225494861603
In grad_steps = 429, loss = 0.7257631421089172
In grad_steps = 430, loss = 0.5656661987304688
In grad_steps = 431, loss = 0.3484112322330475
In grad_steps = 432, loss = 0.08789995312690735
In grad_steps = 433, loss = 0.35582441091537476
In grad_steps = 434, loss = 1.022731065750122
In grad_steps = 435, loss = 0.10321460664272308
In grad_steps = 436, loss = 0.23718714714050293
In grad_steps = 437, loss = 0.3028748631477356
In grad_steps = 438, loss = 0.9871494770050049
In grad_steps = 439, loss = 0.20042547583580017
In grad_steps = 440, loss = 0.10187528282403946
In grad_steps = 441, loss = 0.49324703216552734
In grad_steps = 442, loss = 0.570093035697937
In grad_steps = 443, loss = 0.11417588591575623
In grad_steps = 444, loss = 0.6312792897224426
In grad_steps = 445, loss = 0.17843838036060333
In grad_steps = 446, loss = 1.0023826360702515
In grad_steps = 447, loss = 0.71113121509552
In grad_steps = 448, loss = 0.6444695591926575
In grad_steps = 449, loss = 0.15904687345027924
In grad_steps = 450, loss = 0.23854340612888336
In grad_steps = 451, loss = 0.3077967166900635
In grad_steps = 452, loss = 0.31072425842285156
In grad_steps = 453, loss = 0.29668495059013367
In grad_steps = 454, loss = 0.49787041544914246
In grad_steps = 455, loss = 0.5935189723968506
In grad_steps = 456, loss = 0.6868974566459656
In grad_steps = 457, loss = 0.312745064496994
In grad_steps = 458, loss = 0.6002882719039917
In grad_steps = 459, loss = 0.34347569942474365
In grad_steps = 460, loss = 0.46307846903800964
In grad_steps = 461, loss = 0.10690838098526001
In grad_steps = 462, loss = 0.622840166091919
In grad_steps = 463, loss = 0.2574308514595032
In grad_steps = 464, loss = 0.5864936113357544
In grad_steps = 465, loss = 0.6202816963195801
In grad_steps = 466, loss = 0.36893296241760254
In grad_steps = 467, loss = 0.5754274725914001
In grad_steps = 468, loss = 0.8878296613693237
In grad_steps = 469, loss = 0.43802428245544434
In grad_steps = 470, loss = 0.4570266604423523
In grad_steps = 471, loss = 0.1228654533624649
In grad_steps = 472, loss = 0.17929500341415405
In grad_steps = 473, loss = 0.2407250702381134
In grad_steps = 474, loss = 0.8706390261650085
In grad_steps = 475, loss = 0.7503405213356018
In grad_steps = 476, loss = 0.17990607023239136
In grad_steps = 477, loss = 0.2420407235622406
In grad_steps = 478, loss = 0.19576317071914673
In grad_steps = 479, loss = 0.5129055976867676
In grad_steps = 480, loss = 0.13829778134822845
In grad_steps = 481, loss = 0.7467116713523865
In grad_steps = 482, loss = 0.10557753592729568
In grad_steps = 483, loss = 0.23153699934482574
In grad_steps = 484, loss = 0.21303291618824005
In grad_steps = 485, loss = 0.49089881777763367
In grad_steps = 486, loss = 0.3258479833602905
In grad_steps = 487, loss = 0.4812049865722656
In grad_steps = 488, loss = 1.0071364641189575
In grad_steps = 489, loss = 0.9759708046913147
In grad_steps = 490, loss = 0.14306440949440002
In grad_steps = 491, loss = 0.4164268374443054
In grad_steps = 492, loss = 0.1995781660079956
In grad_steps = 493, loss = 0.05431060120463371
In grad_steps = 494, loss = 1.1147280931472778
In grad_steps = 495, loss = 0.1274314522743225
In grad_steps = 496, loss = 0.6886367797851562
In grad_steps = 497, loss = 0.14481861889362335
In grad_steps = 498, loss = 0.5056071877479553
In grad_steps = 499, loss = 0.1938174068927765
In grad_steps = 500, loss = 0.1174052357673645
In grad_steps = 501, loss = 0.4646332561969757
In grad_steps = 502, loss = 0.7255640625953674
In grad_steps = 503, loss = 0.577804446220398
In grad_steps = 504, loss = 0.5126984715461731
In grad_steps = 505, loss = 0.6264258623123169
In grad_steps = 506, loss = 0.36799946427345276
In grad_steps = 507, loss = 0.4913339614868164
In grad_steps = 508, loss = 0.6860628128051758
In grad_steps = 509, loss = 0.3819558024406433
In grad_steps = 510, loss = 0.11419929563999176
In grad_steps = 511, loss = 0.2664039134979248
In grad_steps = 512, loss = 0.21151618659496307
In grad_steps = 513, loss = 0.5770426988601685
In grad_steps = 514, loss = 0.2796168923377991
In grad_steps = 515, loss = 0.45375898480415344
In grad_steps = 516, loss = 0.48644647002220154
In grad_steps = 517, loss = 0.4612922966480255
In grad_steps = 518, loss = 0.5175763368606567
In grad_steps = 519, loss = 0.34790685772895813
In grad_steps = 520, loss = 0.13405297696590424
In grad_steps = 521, loss = 0.4552406966686249
In grad_steps = 522, loss = 0.14463770389556885
In grad_steps = 523, loss = 0.2654034495353699
In grad_steps = 524, loss = 0.36406606435775757
In grad_steps = 525, loss = 0.40571099519729614
In grad_steps = 526, loss = 0.15084883570671082
In grad_steps = 527, loss = 0.5225117206573486
In grad_steps = 528, loss = 0.19243378937244415
In grad_steps = 529, loss = 0.07717525959014893
In grad_steps = 530, loss = 0.08114033192396164
In grad_steps = 531, loss = 0.6337037086486816
In grad_steps = 532, loss = 0.22210657596588135
In grad_steps = 533, loss = 0.34608739614486694
In grad_steps = 534, loss = 1.047336459159851
In grad_steps = 535, loss = 0.558720588684082
In grad_steps = 536, loss = 0.22987046837806702
In grad_steps = 537, loss = 0.47402557730674744
In grad_steps = 538, loss = 1.5595613718032837
In grad_steps = 539, loss = 0.2736304998397827
In grad_steps = 540, loss = 0.14525221288204193
In grad_steps = 541, loss = 0.5032111406326294
In grad_steps = 542, loss = 0.6972800493240356
In grad_steps = 543, loss = 0.11711021512746811
In grad_steps = 544, loss = 0.7360177636146545
In grad_steps = 545, loss = 1.009822130203247
In grad_steps = 546, loss = 0.26028913259506226
In grad_steps = 547, loss = 0.3552943766117096
In grad_steps = 548, loss = 0.29792261123657227
In grad_steps = 549, loss = 0.5146940350532532
In grad_steps = 550, loss = 0.7373822331428528
In grad_steps = 551, loss = 0.24967655539512634
In grad_steps = 552, loss = 0.2799583375453949
In grad_steps = 553, loss = 0.3946727216243744
In grad_steps = 554, loss = 0.6004759073257446
In grad_steps = 555, loss = 0.5800949931144714
In grad_steps = 556, loss = 0.81097412109375
In grad_steps = 557, loss = 0.318952351808548
In grad_steps = 558, loss = 0.43978309631347656
In grad_steps = 559, loss = 0.2651965320110321
In grad_steps = 560, loss = 0.8867142200469971
In grad_steps = 561, loss = 0.38043591380119324
In grad_steps = 562, loss = 0.4364449977874756
In grad_steps = 563, loss = 0.362601101398468
In grad_steps = 564, loss = 0.8424423336982727
In grad_steps = 565, loss = 0.371972918510437
In grad_steps = 566, loss = 0.23464342951774597
In grad_steps = 567, loss = 0.7700284719467163
In grad_steps = 568, loss = 0.7287839651107788
In grad_steps = 569, loss = 0.147255539894104
In grad_steps = 570, loss = 0.4046442210674286
In grad_steps = 571, loss = 0.15039530396461487
In grad_steps = 572, loss = 0.31551167368888855
In grad_steps = 573, loss = 0.21191710233688354
In grad_steps = 574, loss = 0.09236063063144684
In grad_steps = 575, loss = 0.20004241168498993
In grad_steps = 576, loss = 0.21685315668582916
In grad_steps = 577, loss = 0.40608805418014526
In grad_steps = 578, loss = 0.4328860938549042
In grad_steps = 579, loss = 0.3152538537979126
In grad_steps = 580, loss = 1.0343376398086548
In grad_steps = 581, loss = 0.642816960811615
In grad_steps = 582, loss = 0.7291675209999084
In grad_steps = 583, loss = 0.08084353804588318
In grad_steps = 584, loss = 0.05887597054243088
In grad_steps = 585, loss = 0.0998506247997284
In grad_steps = 586, loss = 0.5149945020675659
In grad_steps = 587, loss = 0.25379908084869385
In grad_steps = 588, loss = 0.49043163657188416
In grad_steps = 589, loss = 1.0311317443847656
In grad_steps = 590, loss = 0.2768514156341553
In grad_steps = 591, loss = 0.32293251156806946
In grad_steps = 592, loss = 1.552402377128601
In grad_steps = 593, loss = 0.08971575647592545
In grad_steps = 594, loss = 0.11078491061925888
In grad_steps = 595, loss = 0.47794198989868164
In grad_steps = 596, loss = 0.5507800579071045
In grad_steps = 597, loss = 0.5727677345275879
In grad_steps = 598, loss = 0.27454397082328796
In grad_steps = 599, loss = 0.4585116505622864
In grad_steps = 600, loss = 0.6079762578010559
In grad_steps = 601, loss = 0.16616302728652954
In grad_steps = 602, loss = 0.3365078866481781
In grad_steps = 603, loss = 0.3571145534515381
In grad_steps = 604, loss = 0.10337826609611511
In grad_steps = 605, loss = 0.4920412302017212
In grad_steps = 606, loss = 0.17108836770057678
In grad_steps = 607, loss = 0.22656522691249847
In grad_steps = 608, loss = 0.40439265966415405
In grad_steps = 609, loss = 0.4366895258426666
In grad_steps = 610, loss = 0.5401529669761658
In grad_steps = 611, loss = 0.545556902885437
In grad_steps = 612, loss = 0.4009926915168762
In grad_steps = 613, loss = 0.8023216128349304
In grad_steps = 614, loss = 0.2562715411186218
In grad_steps = 615, loss = 0.7510858178138733
In grad_steps = 616, loss = 0.2988431751728058
In grad_steps = 617, loss = 0.13421860337257385
In grad_steps = 618, loss = 0.2514592707157135
In grad_steps = 619, loss = 0.09908801317214966
In grad_steps = 620, loss = 0.2125082165002823
In grad_steps = 621, loss = 1.2127556800842285
In grad_steps = 622, loss = 0.2954367697238922
In grad_steps = 623, loss = 0.1817694753408432
In grad_steps = 624, loss = 0.11462247371673584
In grad_steps = 625, loss = 0.6000240445137024
In grad_steps = 626, loss = 0.3292219340801239
In grad_steps = 627, loss = 0.06413832306861877
In grad_steps = 628, loss = 0.1727532595396042
In grad_steps = 629, loss = 0.12832382321357727
In grad_steps = 630, loss = 0.14081960916519165
In grad_steps = 631, loss = 0.09296724200248718
In grad_steps = 632, loss = 0.12680499255657196
In grad_steps = 633, loss = 0.8222852349281311
In grad_steps = 634, loss = 0.2514743506908417
In grad_steps = 635, loss = 0.42143023014068604
In grad_steps = 636, loss = 0.3242759108543396
In grad_steps = 637, loss = 0.44677528738975525
In grad_steps = 638, loss = 0.035617709159851074
In grad_steps = 639, loss = 0.5965656042098999
In grad_steps = 640, loss = 0.1744246929883957
In grad_steps = 641, loss = 0.17773756384849548
In grad_steps = 642, loss = 0.493757963180542
In grad_steps = 643, loss = 0.6989188194274902
In grad_steps = 644, loss = 1.0331310033798218
In grad_steps = 645, loss = 1.0629725456237793
In grad_steps = 646, loss = 0.14584892988204956
In grad_steps = 647, loss = 0.3375374972820282
In grad_steps = 648, loss = 0.14386336505413055
In grad_steps = 649, loss = 0.5013159513473511
In grad_steps = 650, loss = 0.6823285818099976
In grad_steps = 651, loss = 0.3449512720108032
In grad_steps = 652, loss = 0.9110329151153564
In grad_steps = 653, loss = 0.44462963938713074
In grad_steps = 654, loss = 0.6484825611114502
In grad_steps = 655, loss = 0.6130169630050659
In grad_steps = 656, loss = 0.16382990777492523
In grad_steps = 657, loss = 0.1682853102684021
In grad_steps = 658, loss = 0.4336962103843689
In grad_steps = 659, loss = 0.383634090423584
In grad_steps = 660, loss = 0.3010147213935852
In grad_steps = 661, loss = 0.2173043191432953
In grad_steps = 662, loss = 0.2696946859359741
In grad_steps = 663, loss = 0.19653630256652832
In grad_steps = 664, loss = 0.10679519176483154
In grad_steps = 665, loss = 0.2106894701719284
In grad_steps = 666, loss = 0.2997700870037079
In grad_steps = 667, loss = 0.5255089998245239
In grad_steps = 668, loss = 0.10454965382814407
In grad_steps = 669, loss = 0.3146132826805115
In grad_steps = 670, loss = 0.7005017399787903
In grad_steps = 671, loss = 0.7175725698471069
In grad_steps = 672, loss = 0.08047030866146088
In grad_steps = 673, loss = 1.0199499130249023
In grad_steps = 674, loss = 0.06541119515895844
In grad_steps = 675, loss = 0.8412657976150513
In grad_steps = 676, loss = 0.08762244880199432
In grad_steps = 677, loss = 0.563564121723175
In grad_steps = 678, loss = 0.03214196860790253
In grad_steps = 679, loss = 0.0980047956109047
In grad_steps = 680, loss = 0.035618856549263
In grad_steps = 681, loss = 1.1274285316467285
In grad_steps = 682, loss = 0.635092556476593
In grad_steps = 683, loss = 0.21773259341716766
In grad_steps = 684, loss = 0.1797754317522049
In grad_steps = 685, loss = 0.27956312894821167
In grad_steps = 686, loss = 0.37140586972236633
In grad_steps = 687, loss = 0.3877061903476715
In grad_steps = 688, loss = 0.6440984606742859
In grad_steps = 689, loss = 0.14696942269802094
In grad_steps = 690, loss = 0.2296409010887146
In grad_steps = 691, loss = 0.25801923871040344
In grad_steps = 692, loss = 0.2580944895744324
In grad_steps = 693, loss = 0.4205981492996216
In grad_steps = 694, loss = 0.3229600191116333
In grad_steps = 695, loss = 0.3379460573196411
In grad_steps = 696, loss = 0.698921263217926
In grad_steps = 697, loss = 0.3607861399650574
In grad_steps = 698, loss = 0.282462477684021
In grad_steps = 699, loss = 0.6432895064353943
In grad_steps = 700, loss = 0.5876151919364929
In grad_steps = 701, loss = 0.08328387141227722
In grad_steps = 702, loss = 0.17014853656291962
In grad_steps = 703, loss = 0.17527461051940918
In grad_steps = 704, loss = 0.25312983989715576
In grad_steps = 705, loss = 0.09572524577379227
In grad_steps = 706, loss = 0.45166605710983276
In grad_steps = 707, loss = 0.08467419445514679
In grad_steps = 708, loss = 0.024397574365139008
In grad_steps = 709, loss = 0.3102076053619385
In grad_steps = 710, loss = 0.4026890993118286
In grad_steps = 711, loss = 1.4850072860717773
In grad_steps = 712, loss = 0.600189208984375
In grad_steps = 713, loss = 1.6025739908218384
In grad_steps = 714, loss = 0.34752511978149414
In grad_steps = 715, loss = 1.0459837913513184
In grad_steps = 716, loss = 0.2873300611972809
In grad_steps = 717, loss = 0.18898190557956696
In grad_steps = 718, loss = 0.2714380621910095
In grad_steps = 719, loss = 0.059476882219314575
In grad_steps = 720, loss = 0.06483667343854904
In grad_steps = 721, loss = 1.3132703304290771
In grad_steps = 722, loss = 0.33812814950942993
In grad_steps = 723, loss = 0.5379466414451599
In grad_steps = 724, loss = 0.10879747569561005
In grad_steps = 725, loss = 0.6151835918426514
In grad_steps = 726, loss = 0.29954925179481506
In grad_steps = 727, loss = 0.4231007695198059
In grad_steps = 728, loss = 0.11516408622264862
In grad_steps = 729, loss = 0.11568562686443329
In grad_steps = 730, loss = 0.3067672848701477
In grad_steps = 731, loss = 0.6574942469596863
In grad_steps = 732, loss = 0.45032361149787903
In grad_steps = 733, loss = 1.0187864303588867
In grad_steps = 734, loss = 0.34013688564300537
In grad_steps = 735, loss = 0.29552626609802246
In grad_steps = 736, loss = 0.23585927486419678
In grad_steps = 737, loss = 0.15441465377807617
In grad_steps = 738, loss = 0.7352806329727173
In grad_steps = 739, loss = 0.0973481610417366
In grad_steps = 740, loss = 0.18899935483932495
In grad_steps = 741, loss = 0.5052345991134644
In grad_steps = 742, loss = 0.13035711646080017
In grad_steps = 743, loss = 0.11026594787836075
In grad_steps = 744, loss = 0.9063401818275452
In grad_steps = 745, loss = 0.6989473104476929
In grad_steps = 746, loss = 0.240154430270195
In grad_steps = 747, loss = 0.44464483857154846
In grad_steps = 748, loss = 0.07971390336751938
In grad_steps = 749, loss = 0.768836259841919
In grad_steps = 750, loss = 0.8386311531066895
In grad_steps = 751, loss = 0.12271228432655334
In grad_steps = 752, loss = 0.08593827486038208
In grad_steps = 753, loss = 0.13710075616836548
In grad_steps = 754, loss = 0.6849783658981323
In grad_steps = 755, loss = 0.8259965181350708
In grad_steps = 756, loss = 0.13027271628379822
In grad_steps = 757, loss = 0.14250469207763672
In grad_steps = 758, loss = 0.8243576288223267
In grad_steps = 759, loss = 0.5857145190238953
In grad_steps = 760, loss = 0.24116067588329315
In grad_steps = 761, loss = 0.31397148966789246
In grad_steps = 762, loss = 0.17944234609603882
In grad_steps = 763, loss = 0.5073810815811157
In grad_steps = 764, loss = 0.6425243616104126
In grad_steps = 765, loss = 0.5943295955657959
In grad_steps = 766, loss = 0.10282419621944427
In grad_steps = 767, loss = 0.2905488908290863
In grad_steps = 768, loss = 0.2134927660226822
In grad_steps = 769, loss = 1.0801130533218384
In grad_steps = 770, loss = 0.1895381659269333
In grad_steps = 771, loss = 0.15470904111862183
In grad_steps = 772, loss = 0.36786606907844543
In grad_steps = 773, loss = 1.0214366912841797
In grad_steps = 774, loss = 0.3818410038948059
In grad_steps = 775, loss = 0.14148229360580444
In grad_steps = 776, loss = 0.2648417055606842
In grad_steps = 777, loss = 0.5379111766815186
In grad_steps = 778, loss = 0.14919841289520264
In grad_steps = 779, loss = 0.20256470143795013
In grad_steps = 780, loss = 0.4698687195777893
In grad_steps = 781, loss = 0.4537104368209839
In grad_steps = 782, loss = 0.21020643413066864
In grad_steps = 783, loss = 0.9620442986488342
In grad_steps = 784, loss = 0.734452486038208
In grad_steps = 785, loss = 0.07803092896938324
In grad_steps = 786, loss = 0.2852451801300049
In grad_steps = 787, loss = 0.308641642332077
In grad_steps = 788, loss = 0.5635846853256226
In grad_steps = 789, loss = 0.2597300708293915
In grad_steps = 790, loss = 0.4284624457359314
In grad_steps = 791, loss = 0.11677300930023193
In grad_steps = 792, loss = 0.09106448292732239
In grad_steps = 793, loss = 0.6410988569259644
In grad_steps = 794, loss = 1.6715279817581177
In grad_steps = 795, loss = 0.703406572341919
In grad_steps = 796, loss = 0.13236956298351288
In grad_steps = 797, loss = 0.6001701354980469
In grad_steps = 798, loss = 0.2258014678955078
In grad_steps = 799, loss = 0.44332653284072876
In grad_steps = 800, loss = 0.33165499567985535
In grad_steps = 801, loss = 0.30284374952316284
In grad_steps = 802, loss = 0.27405253052711487
In grad_steps = 803, loss = 0.1831866353750229
In grad_steps = 804, loss = 0.0948830395936966
In grad_steps = 805, loss = 0.5818591713905334
In grad_steps = 806, loss = 0.11332345008850098
In grad_steps = 807, loss = 0.2231321930885315
In grad_steps = 808, loss = 0.25201690196990967
In grad_steps = 809, loss = 0.11620397865772247
In grad_steps = 810, loss = 0.7564409971237183
In grad_steps = 811, loss = 0.1749400794506073
In grad_steps = 812, loss = 0.305816113948822
In grad_steps = 813, loss = 0.32721948623657227
In grad_steps = 814, loss = 0.09945069998502731
In grad_steps = 815, loss = 0.10189661383628845
In grad_steps = 816, loss = 0.8210660815238953
In grad_steps = 817, loss = 0.06237039342522621
In grad_steps = 818, loss = 0.3675910532474518
In grad_steps = 819, loss = 0.05182715505361557
In grad_steps = 820, loss = 0.2854243516921997
In grad_steps = 821, loss = 0.19347728788852692
In grad_steps = 822, loss = 0.4980558454990387
In grad_steps = 823, loss = 0.30084335803985596
In grad_steps = 824, loss = 0.04102535545825958
In grad_steps = 825, loss = 0.09208428859710693
In grad_steps = 826, loss = 0.9613887667655945
In grad_steps = 827, loss = 0.11851989477872849
In grad_steps = 828, loss = 1.063110589981079
In grad_steps = 829, loss = 0.26051414012908936
In grad_steps = 830, loss = 0.10464678704738617
In grad_steps = 831, loss = 0.03847985342144966
In grad_steps = 832, loss = 0.8945119380950928
In grad_steps = 833, loss = 0.500152587890625
In grad_steps = 834, loss = 0.5158110857009888
In grad_steps = 835, loss = 1.6782548427581787
In grad_steps = 836, loss = 0.6923502087593079
In grad_steps = 837, loss = 0.7427470088005066
In grad_steps = 838, loss = 1.3632186651229858
In grad_steps = 839, loss = 0.7683346271514893
In grad_steps = 840, loss = 0.16570748388767242
In grad_steps = 841, loss = 1.0020091533660889
In grad_steps = 842, loss = 0.19324450194835663
In grad_steps = 843, loss = 0.7603427171707153
In grad_steps = 844, loss = 0.5751078128814697
In grad_steps = 845, loss = 0.32545337080955505
In grad_steps = 846, loss = 0.878777027130127
In grad_steps = 847, loss = 0.5819178819656372
In grad_steps = 848, loss = 0.5638537406921387
In grad_steps = 849, loss = 0.8733240962028503
In grad_steps = 850, loss = 0.6589053869247437
In grad_steps = 851, loss = 0.5785830616950989
In grad_steps = 852, loss = 0.4511142671108246
In grad_steps = 853, loss = 0.34917059540748596
In grad_steps = 854, loss = 0.3621956408023834
In grad_steps = 855, loss = 0.5948886275291443
In grad_steps = 856, loss = 0.41859835386276245
In grad_steps = 857, loss = 0.2762860953807831
In grad_steps = 858, loss = 0.6065040826797485
In grad_steps = 859, loss = 0.4123298227787018
In grad_steps = 860, loss = 0.346749871969223
In grad_steps = 861, loss = 0.332423597574234
In grad_steps = 862, loss = 0.1789160668849945
In grad_steps = 863, loss = 0.22413738071918488
In grad_steps = 864, loss = 0.3037716746330261
In grad_steps = 865, loss = 0.4498255252838135
In grad_steps = 866, loss = 0.19598491489887238
In grad_steps = 867, loss = 0.1734965741634369
In grad_steps = 868, loss = 0.07643869519233704
In grad_steps = 869, loss = 0.22667455673217773
In grad_steps = 870, loss = 0.04195304214954376
In grad_steps = 871, loss = 0.17712710797786713
In grad_steps = 872, loss = 0.21774610877037048
In grad_steps = 873, loss = 0.2049407660961151
In grad_steps = 874, loss = 0.026886239647865295
In grad_steps = 875, loss = 0.005215823650360107
In grad_steps = 876, loss = 0.020203951746225357
In grad_steps = 877, loss = 0.09289435297250748
In grad_steps = 878, loss = 0.006129422225058079
In grad_steps = 879, loss = 0.006107237655669451
In grad_steps = 880, loss = 1.1764248609542847
In grad_steps = 881, loss = 1.226142406463623
In grad_steps = 882, loss = 1.3512519598007202
In grad_steps = 883, loss = 0.7044073939323425
In grad_steps = 884, loss = 0.03903663158416748
In grad_steps = 885, loss = 0.5412601828575134
In grad_steps = 886, loss = 0.4743928015232086
In grad_steps = 887, loss = 0.29417556524276733
In grad_steps = 888, loss = 0.2478446215391159
In grad_steps = 889, loss = 0.9111781120300293
In grad_steps = 890, loss = 0.3471256494522095
In grad_steps = 891, loss = 0.28172382712364197
In grad_steps = 892, loss = 0.9358360171318054
In grad_steps = 893, loss = 0.32939770817756653
In grad_steps = 894, loss = 0.25435253977775574
In grad_steps = 895, loss = 0.3929068148136139
In grad_steps = 896, loss = 0.6236960887908936
In grad_steps = 897, loss = 0.5425646305084229
In grad_steps = 898, loss = 0.7294220924377441
In grad_steps = 899, loss = 0.2905886173248291
In grad_steps = 900, loss = 0.5199390053749084
In grad_steps = 901, loss = 0.47218257188796997
In grad_steps = 902, loss = 0.3308863639831543
In grad_steps = 903, loss = 0.26478344202041626
In grad_steps = 904, loss = 0.3927730321884155
In grad_steps = 905, loss = 0.4414476752281189
In grad_steps = 906, loss = 0.26058176159858704
In grad_steps = 907, loss = 0.28639164566993713
In grad_steps = 908, loss = 0.1776912808418274
In grad_steps = 909, loss = 0.6117687225341797
In grad_steps = 910, loss = 0.08996031433343887
In grad_steps = 911, loss = 0.18919529020786285
In grad_steps = 912, loss = 0.20191600918769836
In grad_steps = 913, loss = 0.7206645607948303
In grad_steps = 914, loss = 0.231084406375885
In grad_steps = 915, loss = 0.30078205466270447
In grad_steps = 916, loss = 0.12986920773983002
In grad_steps = 917, loss = 0.03514654189348221
In grad_steps = 918, loss = 1.041344165802002
In grad_steps = 919, loss = 1.8954628705978394
In grad_steps = 920, loss = 0.8337982892990112
In grad_steps = 921, loss = 0.5851500630378723
In grad_steps = 922, loss = 0.07564809918403625
In grad_steps = 923, loss = 1.0500246286392212
In grad_steps = 924, loss = 0.8874996304512024
In grad_steps = 925, loss = 0.29143276810646057
In grad_steps = 926, loss = 0.2616526484489441
In grad_steps = 927, loss = 0.419830322265625
In grad_steps = 928, loss = 0.8512840270996094
In grad_steps = 929, loss = 0.22176314890384674
In grad_steps = 930, loss = 0.3686465620994568
In grad_steps = 931, loss = 0.5237531065940857
In grad_steps = 932, loss = 0.5079481601715088
In grad_steps = 933, loss = 0.5300450325012207
In grad_steps = 934, loss = 0.24131813645362854
In grad_steps = 935, loss = 0.30281198024749756
In grad_steps = 936, loss = 0.27014750242233276
In grad_steps = 937, loss = 0.5709667801856995
In grad_steps = 938, loss = 0.42487648129463196
In grad_steps = 939, loss = 0.5112739205360413
In grad_steps = 940, loss = 0.6261106133460999
In grad_steps = 941, loss = 0.25868353247642517
In grad_steps = 942, loss = 0.26413995027542114
In grad_steps = 943, loss = 0.4852849245071411
In grad_steps = 944, loss = 0.41317281126976013
In grad_steps = 945, loss = 0.1750292032957077
In grad_steps = 946, loss = 0.2500082850456238
In grad_steps = 947, loss = 0.7307202816009521
In grad_steps = 948, loss = 0.3101532459259033
In grad_steps = 949, loss = 0.7013623118400574
In grad_steps = 950, loss = 0.2672326862812042
In grad_steps = 951, loss = 0.7213270664215088
In grad_steps = 952, loss = 0.21461598575115204
In grad_steps = 953, loss = 0.8250691890716553
In grad_steps = 954, loss = 0.5450095534324646
In grad_steps = 955, loss = 0.465506374835968
In grad_steps = 956, loss = 0.8853082060813904
In grad_steps = 957, loss = 0.7478599548339844
In grad_steps = 958, loss = 0.4720267355442047
In grad_steps = 959, loss = 0.2501200735569
In grad_steps = 960, loss = 0.3800166845321655
In grad_steps = 961, loss = 0.47717130184173584
In grad_steps = 962, loss = 0.6741541624069214
In grad_steps = 963, loss = 0.9008765816688538
In grad_steps = 964, loss = 0.29348286986351013
In grad_steps = 965, loss = 0.6932263374328613
In grad_steps = 966, loss = 0.4332064092159271
In grad_steps = 967, loss = 0.719551682472229
In grad_steps = 968, loss = 0.2807058095932007
In grad_steps = 969, loss = 0.3523075580596924
In grad_steps = 970, loss = 0.33827245235443115
In grad_steps = 971, loss = 0.25033414363861084
In grad_steps = 972, loss = 0.15482431650161743
In grad_steps = 973, loss = 0.3816305100917816
In grad_steps = 974, loss = 0.13225044310092926
In grad_steps = 975, loss = 0.3319762647151947
In grad_steps = 976, loss = 0.3051472306251526
In grad_steps = 977, loss = 0.19148463010787964
In grad_steps = 978, loss = 0.15537258982658386
In grad_steps = 979, loss = 0.47441577911376953
In grad_steps = 980, loss = 0.296856552362442
In grad_steps = 981, loss = 0.7033820152282715
In grad_steps = 982, loss = 0.24854230880737305
In grad_steps = 983, loss = 0.3686569035053253
In grad_steps = 984, loss = 0.04894498735666275
In grad_steps = 985, loss = 0.20716039836406708
In grad_steps = 986, loss = 1.2512600421905518
In grad_steps = 987, loss = 0.23133520781993866
In grad_steps = 988, loss = 0.05428963154554367
In grad_steps = 989, loss = 0.04259499907493591
In grad_steps = 990, loss = 0.3305513858795166
In grad_steps = 991, loss = 0.3501262664794922
In grad_steps = 992, loss = 1.0699958801269531
In grad_steps = 993, loss = 0.6641201972961426
In grad_steps = 994, loss = 0.47496703267097473
In grad_steps = 995, loss = 0.07603341341018677
In grad_steps = 996, loss = 0.14800159633159637
In grad_steps = 997, loss = 0.19004398584365845
In grad_steps = 998, loss = 0.1146184578537941
In grad_steps = 999, loss = 0.776360034942627
In grad_steps = 1000, loss = 0.46678730845451355
In grad_steps = 1001, loss = 0.2388812005519867
In grad_steps = 1002, loss = 0.07645741105079651
In grad_steps = 1003, loss = 0.5963453054428101
In grad_steps = 1004, loss = 0.46867552399635315
In grad_steps = 1005, loss = 0.14288535714149475
In grad_steps = 1006, loss = 0.32222050428390503
In grad_steps = 1007, loss = 0.23527562618255615
In grad_steps = 1008, loss = 0.08310411125421524
In grad_steps = 1009, loss = 0.06928099691867828
In grad_steps = 1010, loss = 0.0324944332242012
In grad_steps = 1011, loss = 0.37289702892303467
In grad_steps = 1012, loss = 1.162062168121338
In grad_steps = 1013, loss = 0.10590977966785431
In grad_steps = 1014, loss = 0.6653500199317932
In grad_steps = 1015, loss = 0.5304278135299683
In grad_steps = 1016, loss = 0.21746939420700073
In grad_steps = 1017, loss = 0.8762667179107666
In grad_steps = 1018, loss = 0.2358173131942749
In grad_steps = 1019, loss = 0.8584228754043579
In grad_steps = 1020, loss = 0.28888487815856934
In grad_steps = 1021, loss = 0.20779624581336975
In grad_steps = 1022, loss = 0.7875667810440063
In grad_steps = 1023, loss = 0.19093069434165955
In grad_steps = 1024, loss = 0.7154409289360046
In grad_steps = 1025, loss = 0.7019798159599304
In grad_steps = 1026, loss = 1.1334387063980103
In grad_steps = 1027, loss = 0.3490419387817383
In grad_steps = 1028, loss = 0.37382543087005615
In grad_steps = 1029, loss = 0.632029116153717
In grad_steps = 1030, loss = 0.4912486970424652
In grad_steps = 1031, loss = 0.7004266977310181
In grad_steps = 1032, loss = 0.1825374811887741
In grad_steps = 1033, loss = 0.6549645066261292
In grad_steps = 1034, loss = 0.3790116012096405
In grad_steps = 1035, loss = 0.49522697925567627
In grad_steps = 1036, loss = 0.32175499200820923
In grad_steps = 1037, loss = 0.36912792921066284
In grad_steps = 1038, loss = 0.4030117392539978
In grad_steps = 1039, loss = 0.2762901484966278
In grad_steps = 1040, loss = 0.3866564631462097
In grad_steps = 1041, loss = 0.0813225731253624
In grad_steps = 1042, loss = 0.20793357491493225
In grad_steps = 1043, loss = 0.29635360836982727
In grad_steps = 1044, loss = 0.7937325835227966
In grad_steps = 1045, loss = 0.06801512837409973
In grad_steps = 1046, loss = 0.5402878522872925
In grad_steps = 1047, loss = 0.045957840979099274
In grad_steps = 1048, loss = 0.2623470425605774
In grad_steps = 1049, loss = 0.1570882350206375
In grad_steps = 1050, loss = 0.8829993605613708
In grad_steps = 1051, loss = 0.48836448788642883
In grad_steps = 1052, loss = 0.7860194444656372
In grad_steps = 1053, loss = 0.056409820914268494
In grad_steps = 1054, loss = 0.46051326394081116
In grad_steps = 1055, loss = 0.7103814482688904
In grad_steps = 1056, loss = 0.21150869131088257
In grad_steps = 1057, loss = 0.2674959599971771
In grad_steps = 1058, loss = 0.3101123571395874
In grad_steps = 1059, loss = 0.7415138483047485
In grad_steps = 1060, loss = 0.09355442970991135
In grad_steps = 1061, loss = 0.2940504848957062
In grad_steps = 1062, loss = 0.049628857523202896
In grad_steps = 1063, loss = 0.6939850449562073
In grad_steps = 1064, loss = 0.26848936080932617
In grad_steps = 1065, loss = 0.2273389995098114
In grad_steps = 1066, loss = 0.5528386235237122
In grad_steps = 1067, loss = 0.2996504008769989
In grad_steps = 1068, loss = 0.03502149507403374
In grad_steps = 1069, loss = 0.6608469486236572
In grad_steps = 1070, loss = 0.49237579107284546
In grad_steps = 1071, loss = 0.1238175630569458
In grad_steps = 1072, loss = 0.19224144518375397
In grad_steps = 1073, loss = 0.9536837935447693
In grad_steps = 1074, loss = 0.10513480752706528
In grad_steps = 1075, loss = 0.05077172815799713
In grad_steps = 1076, loss = 0.3471900522708893
In grad_steps = 1077, loss = 0.8870893716812134
In grad_steps = 1078, loss = 0.3263767659664154
In grad_steps = 1079, loss = 0.9093512892723083
In grad_steps = 1080, loss = 0.11669670790433884
In grad_steps = 1081, loss = 0.18167029321193695
In grad_steps = 1082, loss = 0.8285062313079834
In grad_steps = 1083, loss = 0.1667022556066513
In grad_steps = 1084, loss = 0.2300652116537094
In grad_steps = 1085, loss = 0.25453436374664307
In grad_steps = 1086, loss = 0.21226215362548828
In grad_steps = 1087, loss = 0.22810912132263184
In grad_steps = 1088, loss = 0.63318932056427
In grad_steps = 1089, loss = 0.1946462094783783
In grad_steps = 1090, loss = 0.2768121063709259
In grad_steps = 1091, loss = 0.44800394773483276
In grad_steps = 1092, loss = 0.36454930901527405
In grad_steps = 1093, loss = 0.19143715500831604
In grad_steps = 1094, loss = 0.2267509400844574
In grad_steps = 1095, loss = 0.24845081567764282
In grad_steps = 1096, loss = 0.12562796473503113
In grad_steps = 1097, loss = 0.04540349170565605
In grad_steps = 1098, loss = 0.15410573780536652
In grad_steps = 1099, loss = 0.9441367983818054
In grad_steps = 1100, loss = 0.37049371004104614
In grad_steps = 1101, loss = 0.16771532595157623
In grad_steps = 1102, loss = 0.45713934302330017
In grad_steps = 1103, loss = 0.601008951663971
In grad_steps = 1104, loss = 0.11210723221302032
In grad_steps = 1105, loss = 0.05354759842157364
In grad_steps = 1106, loss = 0.37950921058654785
In grad_steps = 1107, loss = 0.44076305627822876
In grad_steps = 1108, loss = 0.4184006154537201
In grad_steps = 1109, loss = 0.10237603634595871
In grad_steps = 1110, loss = 0.38591060042381287
In grad_steps = 1111, loss = 0.21277455985546112
In grad_steps = 1112, loss = 1.239874005317688
In grad_steps = 1113, loss = 0.8345648050308228
In grad_steps = 1114, loss = 1.488999605178833
In grad_steps = 1115, loss = 0.08482633531093597
In grad_steps = 1116, loss = 0.20933020114898682
In grad_steps = 1117, loss = 0.3701234459877014
In grad_steps = 1118, loss = 0.12689197063446045
In grad_steps = 1119, loss = 0.2180711030960083
In grad_steps = 1120, loss = 0.10203740000724792
In grad_steps = 1121, loss = 0.11970005929470062
In grad_steps = 1122, loss = 0.258222758769989
In grad_steps = 1123, loss = 0.40344515442848206
In grad_steps = 1124, loss = 0.6629629135131836
In grad_steps = 1125, loss = 0.21414868533611298
In grad_steps = 1126, loss = 0.18860900402069092
In grad_steps = 1127, loss = 0.23202168941497803
In grad_steps = 1128, loss = 0.10015764087438583
In grad_steps = 1129, loss = 0.4995964765548706
In grad_steps = 1130, loss = 0.840726912021637
In grad_steps = 1131, loss = 0.6639256477355957
In grad_steps = 1132, loss = 0.19588366150856018
In grad_steps = 1133, loss = 0.05354132503271103
In grad_steps = 1134, loss = 0.5110504031181335
In grad_steps = 1135, loss = 0.7911912798881531
In grad_steps = 1136, loss = 0.12896330654621124
In grad_steps = 1137, loss = 0.3942740857601166
In grad_steps = 1138, loss = 0.4113120436668396
In grad_steps = 1139, loss = 0.24789534509181976
In grad_steps = 1140, loss = 0.11353059113025665
In grad_steps = 1141, loss = 0.32247743010520935
In grad_steps = 1142, loss = 0.5832834839820862
In grad_steps = 1143, loss = 0.5350548624992371
In grad_steps = 1144, loss = 0.24607615172863007
In grad_steps = 1145, loss = 0.10854992270469666
In grad_steps = 1146, loss = 0.3901198208332062
In grad_steps = 1147, loss = 0.34650129079818726
In grad_steps = 1148, loss = 0.4154461622238159
In grad_steps = 1149, loss = 0.24223089218139648
In grad_steps = 1150, loss = 0.12463865429162979
In grad_steps = 1151, loss = 0.865462601184845
In grad_steps = 1152, loss = 0.044807303696870804
In grad_steps = 1153, loss = 0.31979480385780334
In grad_steps = 1154, loss = 0.049153298139572144
In grad_steps = 1155, loss = 0.6322331428527832
In grad_steps = 1156, loss = 0.7316302061080933
In grad_steps = 1157, loss = 0.052419669926166534
In grad_steps = 1158, loss = 0.10239758342504501
In grad_steps = 1159, loss = 0.0740455687046051
In grad_steps = 1160, loss = 0.1661779284477234
In grad_steps = 1161, loss = 0.6689019799232483
In grad_steps = 1162, loss = 0.1121826171875
In grad_steps = 1163, loss = 0.3688039779663086
In grad_steps = 1164, loss = 0.1581130176782608
In grad_steps = 1165, loss = 0.6245113015174866
In grad_steps = 1166, loss = 0.4814639091491699
In grad_steps = 1167, loss = 0.17555385828018188
In grad_steps = 1168, loss = 0.5426665544509888
In grad_steps = 1169, loss = 1.2564812898635864
In grad_steps = 1170, loss = 0.3055981397628784
In grad_steps = 1171, loss = 0.3889005184173584
In grad_steps = 1172, loss = 0.09945667535066605
In grad_steps = 1173, loss = 0.26262587308883667
In grad_steps = 1174, loss = 1.154908537864685
In grad_steps = 1175, loss = 0.976042628288269
In grad_steps = 1176, loss = 0.7968600988388062
In grad_steps = 1177, loss = 0.3255601227283478
In grad_steps = 1178, loss = 0.11082926392555237
In grad_steps = 1179, loss = 0.30454057455062866
In grad_steps = 1180, loss = 0.35355573892593384
In grad_steps = 1181, loss = 0.5327616930007935
In grad_steps = 1182, loss = 0.4842738211154938
In grad_steps = 1183, loss = 0.40935662388801575
In grad_steps = 1184, loss = 0.18591465055942535
In grad_steps = 1185, loss = 0.564139723777771
In grad_steps = 1186, loss = 0.46443694829940796
In grad_steps = 1187, loss = 0.7587565183639526
In grad_steps = 1188, loss = 0.09474842250347137
In grad_steps = 1189, loss = 0.7497979402542114
In grad_steps = 1190, loss = 0.37433353066444397
In grad_steps = 1191, loss = 0.2567015290260315
In grad_steps = 1192, loss = 0.22144798934459686
In grad_steps = 1193, loss = 0.2078363448381424
In grad_steps = 1194, loss = 0.13583596050739288
In grad_steps = 1195, loss = 0.6714943647384644
In grad_steps = 1196, loss = 0.7889355421066284
In grad_steps = 1197, loss = 0.29951000213623047
In grad_steps = 1198, loss = 0.18660977482795715
In grad_steps = 1199, loss = 0.5479850172996521
In grad_steps = 1200, loss = 0.45815855264663696
In grad_steps = 1201, loss = 0.3576897382736206
In grad_steps = 1202, loss = 0.19520722329616547
In grad_steps = 1203, loss = 0.3565555810928345
In grad_steps = 1204, loss = 0.15266065299510956
In grad_steps = 1205, loss = 0.09114496409893036
In grad_steps = 1206, loss = 0.7693504691123962
In grad_steps = 1207, loss = 0.34788385033607483
In grad_steps = 1208, loss = 0.12141141295433044
In grad_steps = 1209, loss = 0.4075152575969696
In grad_steps = 1210, loss = 0.04392198473215103
In grad_steps = 1211, loss = 0.2598564624786377
In grad_steps = 1212, loss = 0.02056053839623928
In grad_steps = 1213, loss = 0.507809042930603
In grad_steps = 1214, loss = 0.23137082159519196
In grad_steps = 1215, loss = 0.4189439117908478
In grad_steps = 1216, loss = 0.037957027554512024
In grad_steps = 1217, loss = 0.02417534962296486
In grad_steps = 1218, loss = 0.029412586241960526
In grad_steps = 1219, loss = 0.026570999994874
In grad_steps = 1220, loss = 1.1958954334259033
In grad_steps = 1221, loss = 0.719860851764679
In grad_steps = 1222, loss = 0.09958595782518387
In grad_steps = 1223, loss = 0.8661142587661743
In grad_steps = 1224, loss = 0.01860469952225685
In grad_steps = 1225, loss = 0.5123612880706787
In grad_steps = 1226, loss = 0.2963014245033264
In grad_steps = 1227, loss = 0.06401827931404114
In grad_steps = 1228, loss = 0.4093424081802368
In grad_steps = 1229, loss = 0.2004733383655548
In grad_steps = 1230, loss = 0.08547921478748322
In grad_steps = 1231, loss = 0.4691821336746216
In grad_steps = 1232, loss = 0.6365866661071777
In grad_steps = 1233, loss = 0.08706960827112198
In grad_steps = 1234, loss = 0.19353464245796204
In grad_steps = 1235, loss = 1.0865799188613892
In grad_steps = 1236, loss = 0.29148629307746887
In grad_steps = 1237, loss = 0.758594810962677
In grad_steps = 1238, loss = 0.5214518308639526
In grad_steps = 1239, loss = 0.5057122707366943
In grad_steps = 1240, loss = 0.6445218324661255
In grad_steps = 1241, loss = 0.5074599385261536
In grad_steps = 1242, loss = 0.2462477833032608
In grad_steps = 1243, loss = 1.2133877277374268
In grad_steps = 1244, loss = 0.4650905728340149
In grad_steps = 1245, loss = 0.18838323652744293
In grad_steps = 1246, loss = 0.4000442326068878
In grad_steps = 1247, loss = 0.5007414221763611
In grad_steps = 1248, loss = 0.8735969066619873
In grad_steps = 1249, loss = 0.2442440539598465
In grad_steps = 1250, loss = 0.27806976437568665
In grad_steps = 1251, loss = 0.3325311243534088
In grad_steps = 1252, loss = 0.49338841438293457
In grad_steps = 1253, loss = 0.7527270317077637
In grad_steps = 1254, loss = 0.2047603875398636
In grad_steps = 1255, loss = 0.2510783076286316
In grad_steps = 1256, loss = 0.8093167543411255
In grad_steps = 1257, loss = 0.0679362565279007
In grad_steps = 1258, loss = 0.663699746131897
In grad_steps = 1259, loss = 0.1836071014404297
In grad_steps = 1260, loss = 0.43846726417541504
In grad_steps = 1261, loss = 0.20856189727783203
In grad_steps = 1262, loss = 0.06561136245727539
In grad_steps = 1263, loss = 0.5846990942955017
In grad_steps = 1264, loss = 0.7852575778961182
In grad_steps = 1265, loss = 0.23541180789470673
In grad_steps = 1266, loss = 0.7069926261901855
In grad_steps = 1267, loss = 0.39806169271469116
In grad_steps = 1268, loss = 0.37616387009620667
In grad_steps = 1269, loss = 0.37248390913009644
In grad_steps = 1270, loss = 0.47014614939689636
In grad_steps = 1271, loss = 0.5404229164123535
In grad_steps = 1272, loss = 0.055598802864551544
In grad_steps = 1273, loss = 0.2145862579345703
In grad_steps = 1274, loss = 0.31774264574050903
In grad_steps = 1275, loss = 0.06643983721733093
In grad_steps = 1276, loss = 0.3406791090965271
In grad_steps = 1277, loss = 0.1434190720319748
In grad_steps = 1278, loss = 0.07272899895906448
In grad_steps = 1279, loss = 0.03653872758150101
In grad_steps = 1280, loss = 0.06545840203762054
In grad_steps = 1281, loss = 0.05783842131495476
In grad_steps = 1282, loss = 0.2782100737094879
In grad_steps = 1283, loss = 0.08296758681535721
In grad_steps = 1284, loss = 0.027851808816194534
In grad_steps = 1285, loss = 0.5160771012306213
In grad_steps = 1286, loss = 0.34882545471191406
In grad_steps = 1287, loss = 0.4141414761543274
In grad_steps = 1288, loss = 0.22795526683330536
In grad_steps = 1289, loss = 0.11041350662708282
In grad_steps = 1290, loss = 0.2926170527935028
In grad_steps = 1291, loss = 0.07300844043493271
In grad_steps = 1292, loss = 0.5201846361160278
In grad_steps = 1293, loss = 0.03337083011865616
In grad_steps = 1294, loss = 0.6200903058052063
In grad_steps = 1295, loss = 0.14545971155166626
In grad_steps = 1296, loss = 0.45665064454078674
In grad_steps = 1297, loss = 0.3376413881778717
In grad_steps = 1298, loss = 0.12278134375810623
In grad_steps = 1299, loss = 0.7150406241416931
In grad_steps = 1300, loss = 0.5815824866294861
In grad_steps = 1301, loss = 0.012589934282004833
In grad_steps = 1302, loss = 0.953708827495575
In grad_steps = 1303, loss = 0.06882654130458832
In grad_steps = 1304, loss = 0.5901106595993042
In grad_steps = 1305, loss = 0.04889512434601784
In grad_steps = 1306, loss = 0.17693613469600677
In grad_steps = 1307, loss = 0.14038494229316711
In grad_steps = 1308, loss = 0.3680347204208374
In grad_steps = 1309, loss = 0.2558112144470215
In grad_steps = 1310, loss = 0.29211586713790894
In grad_steps = 1311, loss = 0.06262502074241638
In grad_steps = 1312, loss = 0.733731746673584
In grad_steps = 1313, loss = 0.03181660175323486
In grad_steps = 1314, loss = 0.4553186297416687
In grad_steps = 1315, loss = 0.09078826010227203
In grad_steps = 1316, loss = 0.8493407964706421
In grad_steps = 1317, loss = 0.28229257464408875
In grad_steps = 1318, loss = 0.6305513381958008
In grad_steps = 1319, loss = 0.3949269652366638
In grad_steps = 1320, loss = 0.2905072569847107
In grad_steps = 1321, loss = 0.23629602789878845
In grad_steps = 1322, loss = 0.4850289821624756
In grad_steps = 1323, loss = 0.05649208277463913
In grad_steps = 1324, loss = 0.5758827924728394
In grad_steps = 1325, loss = 0.42095738649368286
In grad_steps = 1326, loss = 0.6914219856262207
In grad_steps = 1327, loss = 0.3518589437007904
In grad_steps = 1328, loss = 1.1795604228973389
In grad_steps = 1329, loss = 0.09798586368560791
In grad_steps = 1330, loss = 0.27198487520217896
In grad_steps = 1331, loss = 0.1722967028617859
In grad_steps = 1332, loss = 0.543316662311554
In grad_steps = 1333, loss = 0.05178479850292206
In grad_steps = 1334, loss = 0.2663353681564331
In grad_steps = 1335, loss = 0.2365139126777649
In grad_steps = 1336, loss = 0.12349823862314224
In grad_steps = 1337, loss = 0.396291583776474
In grad_steps = 1338, loss = 0.971570611000061
In grad_steps = 1339, loss = 0.5569647550582886
In grad_steps = 1340, loss = 0.6373293995857239
In grad_steps = 1341, loss = 0.2810937464237213
In grad_steps = 1342, loss = 0.16204187273979187
In grad_steps = 1343, loss = 0.17854920029640198
In grad_steps = 1344, loss = 0.22593136131763458
In grad_steps = 1345, loss = 0.4586648941040039
In grad_steps = 1346, loss = 0.22572460770606995
In grad_steps = 1347, loss = 0.1997622400522232
In grad_steps = 1348, loss = 0.5931500792503357
In grad_steps = 1349, loss = 0.21423527598381042
In grad_steps = 1350, loss = 0.08831633627414703
In grad_steps = 1351, loss = 0.12898710370063782
In grad_steps = 1352, loss = 0.16111508011817932
In grad_steps = 1353, loss = 0.1728733777999878
In grad_steps = 1354, loss = 0.2926369905471802
In grad_steps = 1355, loss = 0.2635788917541504
In grad_steps = 1356, loss = 0.03821215778589249
In grad_steps = 1357, loss = 0.28443723917007446
In grad_steps = 1358, loss = 0.5431787371635437
In grad_steps = 1359, loss = 0.13588064908981323
In grad_steps = 1360, loss = 0.36473342776298523
In grad_steps = 1361, loss = 0.12608924508094788
In grad_steps = 1362, loss = 0.01961221918463707
In grad_steps = 1363, loss = 0.047551415860652924
In grad_steps = 1364, loss = 0.1512736678123474
In grad_steps = 1365, loss = 0.2653999626636505
In grad_steps = 1366, loss = 0.24076169729232788
In grad_steps = 1367, loss = 1.7606135606765747
In grad_steps = 1368, loss = 0.07835929840803146
In grad_steps = 1369, loss = 0.31182894110679626
In grad_steps = 1370, loss = 0.511696994304657
In grad_steps = 1371, loss = 0.195938378572464
In grad_steps = 1372, loss = 1.225573182106018
In grad_steps = 1373, loss = 0.6831426620483398
In grad_steps = 1374, loss = 0.03676456958055496
In grad_steps = 1375, loss = 0.24545085430145264
In grad_steps = 1376, loss = 0.07460974901914597
In grad_steps = 1377, loss = 0.5931955575942993
In grad_steps = 1378, loss = 0.05674082413315773
In grad_steps = 1379, loss = 0.6126692891120911
In grad_steps = 1380, loss = 0.1048535704612732
In grad_steps = 1381, loss = 0.2564087510108948
In grad_steps = 1382, loss = 1.1258901357650757
In grad_steps = 1383, loss = 0.21479666233062744
In grad_steps = 1384, loss = 0.03405652195215225
In grad_steps = 1385, loss = 0.5060789585113525
In grad_steps = 1386, loss = 0.11091795563697815
In grad_steps = 1387, loss = 0.25250473618507385
In grad_steps = 1388, loss = 0.6095016598701477
In grad_steps = 1389, loss = 0.05471979081630707
In grad_steps = 1390, loss = 0.07548045367002487
In grad_steps = 1391, loss = 0.07030387967824936
In grad_steps = 1392, loss = 0.06768614053726196
In grad_steps = 1393, loss = 0.5528029799461365
In grad_steps = 1394, loss = 0.15338197350502014
In grad_steps = 1395, loss = 0.17828920483589172
In grad_steps = 1396, loss = 0.12034580111503601
In grad_steps = 1397, loss = 1.0484561920166016
In grad_steps = 1398, loss = 0.17597605288028717
In grad_steps = 1399, loss = 0.7500244975090027
In grad_steps = 1400, loss = 0.26200857758522034
In grad_steps = 1401, loss = 0.46862873435020447
In grad_steps = 1402, loss = 0.09551623463630676
In grad_steps = 1403, loss = 0.15099960565567017
In grad_steps = 1404, loss = 0.16361741721630096
In grad_steps = 1405, loss = 0.04146673157811165
In grad_steps = 1406, loss = 0.4016476273536682
In grad_steps = 1407, loss = 0.2921190559864044
In grad_steps = 1408, loss = 0.384201318025589
In grad_steps = 1409, loss = 0.26582348346710205
In grad_steps = 1410, loss = 0.6301933526992798
In grad_steps = 1411, loss = 0.07215441018342972
In grad_steps = 1412, loss = 0.2694447636604309
In grad_steps = 1413, loss = 0.1993304193019867
In grad_steps = 1414, loss = 0.037311166524887085
In grad_steps = 1415, loss = 0.06719555705785751
In grad_steps = 1416, loss = 0.16488924622535706
In grad_steps = 1417, loss = 0.6792779564857483
In grad_steps = 1418, loss = 0.05934520065784454
In grad_steps = 1419, loss = 0.18003499507904053
In grad_steps = 1420, loss = 0.13313695788383484
In grad_steps = 1421, loss = 0.6270801424980164
In grad_steps = 1422, loss = 0.117707759141922
In grad_steps = 1423, loss = 0.04605123773217201
In grad_steps = 1424, loss = 0.5300657153129578
In grad_steps = 1425, loss = 0.32080015540122986
In grad_steps = 1426, loss = 0.0784880593419075
In grad_steps = 1427, loss = 0.05294596031308174
In grad_steps = 1428, loss = 0.008023517206311226
In grad_steps = 1429, loss = 1.3427618741989136
In grad_steps = 1430, loss = 1.3543152809143066
In grad_steps = 1431, loss = 0.06566066294908524
In grad_steps = 1432, loss = 0.4546341896057129
In grad_steps = 1433, loss = 0.471097469329834
In grad_steps = 1434, loss = 0.7732742428779602
In grad_steps = 1435, loss = 0.07202000916004181
In grad_steps = 1436, loss = 1.8050217628479004
In grad_steps = 1437, loss = 0.7632611393928528
In grad_steps = 1438, loss = 0.10803993046283722
In grad_steps = 1439, loss = 0.21043305099010468
In grad_steps = 1440, loss = 0.615439772605896
In grad_steps = 1441, loss = 0.5835807919502258
In grad_steps = 1442, loss = 0.0744168609380722
In grad_steps = 1443, loss = 0.18845234811306
In grad_steps = 1444, loss = 0.3908126950263977
In grad_steps = 1445, loss = 0.47275233268737793
In grad_steps = 1446, loss = 0.17330579459667206
In grad_steps = 1447, loss = 0.15908314287662506
In grad_steps = 1448, loss = 0.22259332239627838
In grad_steps = 1449, loss = 0.33275189995765686
In grad_steps = 1450, loss = 0.1679173856973648
In grad_steps = 1451, loss = 0.3693500757217407
In grad_steps = 1452, loss = 0.610935628414154
In grad_steps = 1453, loss = 0.03678605332970619
In grad_steps = 1454, loss = 0.532410740852356
In grad_steps = 1455, loss = 0.5295251607894897
In grad_steps = 1456, loss = 0.291792094707489
In grad_steps = 1457, loss = 0.7520654201507568
In grad_steps = 1458, loss = 0.266105592250824
In grad_steps = 1459, loss = 0.33354973793029785
In grad_steps = 1460, loss = 0.06642981618642807
In grad_steps = 1461, loss = 0.2014250010251999
In grad_steps = 1462, loss = 0.14437852799892426
In grad_steps = 1463, loss = 0.6012331247329712
In grad_steps = 1464, loss = 0.28260597586631775
In grad_steps = 1465, loss = 0.8997488021850586
In grad_steps = 1466, loss = 0.5731966495513916
In grad_steps = 1467, loss = 0.05540934205055237
In grad_steps = 1468, loss = 0.11290562897920609
In grad_steps = 1469, loss = 0.6133796572685242
In grad_steps = 1470, loss = 0.09538420289754868
In grad_steps = 1471, loss = 0.3783947825431824
In grad_steps = 1472, loss = 1.0578173398971558
In grad_steps = 1473, loss = 0.08022217452526093
In grad_steps = 1474, loss = 0.40421193838119507
In grad_steps = 1475, loss = 1.1637907028198242
In grad_steps = 1476, loss = 0.23124180734157562
In grad_steps = 1477, loss = 0.3363894522190094
In grad_steps = 1478, loss = 0.5988999009132385
In grad_steps = 1479, loss = 0.2535862624645233
In grad_steps = 1480, loss = 0.2564900517463684
In grad_steps = 1481, loss = 0.22814065217971802
In grad_steps = 1482, loss = 0.10020890831947327
In grad_steps = 1483, loss = 0.4943571090698242
In grad_steps = 1484, loss = 0.11869364976882935
In grad_steps = 1485, loss = 0.12189575284719467
In grad_steps = 1486, loss = 0.5713288187980652
In grad_steps = 1487, loss = 0.5004172325134277
In grad_steps = 1488, loss = 0.14647512137889862
In grad_steps = 1489, loss = 0.32407402992248535
In grad_steps = 1490, loss = 0.1371702253818512
In grad_steps = 1491, loss = 0.5918890833854675
In grad_steps = 1492, loss = 0.09898171573877335
In grad_steps = 1493, loss = 0.28037676215171814
In grad_steps = 1494, loss = 0.6304868459701538
In grad_steps = 1495, loss = 0.13677653670310974
In grad_steps = 1496, loss = 0.26743265986442566
In grad_steps = 1497, loss = 0.2458149641752243
In grad_steps = 1498, loss = 0.4394596219062805
In grad_steps = 1499, loss = 0.07242162525653839
In grad_steps = 1500, loss = 0.20210954546928406
In grad_steps = 1501, loss = 0.5288349390029907
In grad_steps = 1502, loss = 0.05750998854637146
In grad_steps = 1503, loss = 0.693420946598053
In grad_steps = 1504, loss = 0.16890427470207214
In grad_steps = 1505, loss = 0.10266990959644318
In grad_steps = 1506, loss = 0.06484881043434143
In grad_steps = 1507, loss = 0.23150095343589783
In grad_steps = 1508, loss = 0.6759259104728699
In grad_steps = 1509, loss = 0.8297803997993469
In grad_steps = 1510, loss = 0.6211974620819092
In grad_steps = 1511, loss = 0.0698518231511116
In grad_steps = 1512, loss = 0.04633452370762825
In grad_steps = 1513, loss = 1.1979554891586304
In grad_steps = 1514, loss = 0.2898949086666107
In grad_steps = 1515, loss = 0.20286764204502106
In grad_steps = 1516, loss = 0.10334305465221405
In grad_steps = 1517, loss = 0.7568817138671875
In grad_steps = 1518, loss = 0.25351613759994507
In grad_steps = 1519, loss = 0.6125237941741943
In grad_steps = 1520, loss = 0.09163971990346909
In grad_steps = 1521, loss = 0.5897057056427002
In grad_steps = 1522, loss = 0.21570280194282532
In grad_steps = 1523, loss = 0.2711377739906311
In grad_steps = 1524, loss = 0.07831981033086777
In grad_steps = 1525, loss = 1.0399253368377686
In grad_steps = 1526, loss = 0.07343541830778122
In grad_steps = 1527, loss = 0.04232630133628845
In grad_steps = 1528, loss = 0.2658127546310425
In grad_steps = 1529, loss = 0.15146858990192413
In grad_steps = 1530, loss = 0.1605709344148636
In grad_steps = 1531, loss = 0.09977822750806808
In grad_steps = 1532, loss = 0.3185621201992035
In grad_steps = 1533, loss = 1.5978236198425293
In grad_steps = 1534, loss = 0.26196813583374023
In grad_steps = 1535, loss = 0.042289022356271744
In grad_steps = 1536, loss = 0.2388642579317093
In grad_steps = 1537, loss = 0.20403330028057098
In grad_steps = 1538, loss = 0.5674799680709839
In grad_steps = 1539, loss = 0.04703627526760101
In grad_steps = 1540, loss = 0.09179598093032837
In grad_steps = 1541, loss = 0.3478066921234131
In grad_steps = 1542, loss = 0.18602418899536133
In grad_steps = 1543, loss = 0.4352681636810303
In grad_steps = 1544, loss = 0.41315385699272156
In grad_steps = 1545, loss = 0.024620475247502327
In grad_steps = 1546, loss = 0.966130793094635
In grad_steps = 1547, loss = 0.03454814851284027
In grad_steps = 1548, loss = 0.17080780863761902
In grad_steps = 1549, loss = 0.08082757145166397
In grad_steps = 1550, loss = 0.08617404103279114
In grad_steps = 1551, loss = 0.1760883778333664
In grad_steps = 1552, loss = 0.5577395558357239
In grad_steps = 1553, loss = 0.08008214831352234
In grad_steps = 1554, loss = 0.7570240497589111
In grad_steps = 1555, loss = 0.04751312732696533
In grad_steps = 1556, loss = 0.05945616960525513
In grad_steps = 1557, loss = 0.9146385192871094
In grad_steps = 1558, loss = 0.3633354604244232
In grad_steps = 1559, loss = 0.594083309173584
In grad_steps = 1560, loss = 0.7591201663017273
In grad_steps = 1561, loss = 0.14143253862857819
In grad_steps = 1562, loss = 0.07013010233640671
In grad_steps = 1563, loss = 0.3397047221660614
In grad_steps = 1564, loss = 0.09871436655521393
In grad_steps = 1565, loss = 0.1223028153181076
In grad_steps = 1566, loss = 0.6606307625770569
In grad_steps = 1567, loss = 0.12200897932052612
In grad_steps = 1568, loss = 0.5291305184364319
In grad_steps = 1569, loss = 0.2813822329044342
In grad_steps = 1570, loss = 0.5982272028923035
In grad_steps = 1571, loss = 0.3301377594470978
In grad_steps = 1572, loss = 0.5890630483627319
In grad_steps = 1573, loss = 0.20986340939998627
In grad_steps = 1574, loss = 0.3848287761211395
In grad_steps = 1575, loss = 0.15155869722366333
In grad_steps = 1576, loss = 0.09778988361358643
In grad_steps = 1577, loss = 0.35728010535240173
In grad_steps = 1578, loss = 0.8435818552970886
In grad_steps = 1579, loss = 0.10194361209869385
In grad_steps = 1580, loss = 0.0835193544626236
In grad_steps = 1581, loss = 0.22917844355106354
In grad_steps = 1582, loss = 0.08683927357196808
In grad_steps = 1583, loss = 0.773783802986145
In grad_steps = 1584, loss = 0.3765040338039398
In grad_steps = 1585, loss = 1.2057774066925049
In grad_steps = 1586, loss = 0.7936366200447083
In grad_steps = 1587, loss = 0.5157093405723572
In grad_steps = 1588, loss = 0.24166840314865112
In grad_steps = 1589, loss = 0.11096570640802383
In grad_steps = 1590, loss = 0.21960000693798065
In grad_steps = 1591, loss = 0.041748546063899994
In grad_steps = 1592, loss = 0.5902230739593506
In grad_steps = 1593, loss = 0.07987625151872635
In grad_steps = 1594, loss = 0.11654908955097198
In grad_steps = 1595, loss = 0.2948462963104248
In grad_steps = 1596, loss = 0.4504924416542053
In grad_steps = 1597, loss = 0.36288756132125854
In grad_steps = 1598, loss = 0.16812942922115326
In grad_steps = 1599, loss = 0.38884595036506653
In grad_steps = 1600, loss = 0.44450587034225464
In grad_steps = 1601, loss = 0.371031254529953
In grad_steps = 1602, loss = 0.49823063611984253
In grad_steps = 1603, loss = 0.09682421386241913
In grad_steps = 1604, loss = 0.27460840344429016
In grad_steps = 1605, loss = 0.11696571111679077
In grad_steps = 1606, loss = 0.7452536225318909
In grad_steps = 1607, loss = 0.19054749608039856
In grad_steps = 1608, loss = 0.32576343417167664
In grad_steps = 1609, loss = 0.5005947947502136
In grad_steps = 1610, loss = 0.1703333556652069
In grad_steps = 1611, loss = 0.115408755838871
In grad_steps = 1612, loss = 0.6166526079177856
In grad_steps = 1613, loss = 0.2950672507286072
In grad_steps = 1614, loss = 0.16030538082122803
In grad_steps = 1615, loss = 0.16110014915466309
In grad_steps = 1616, loss = 0.7251387238502502
In grad_steps = 1617, loss = 0.017443818971514702
In grad_steps = 1618, loss = 0.17746701836585999
In grad_steps = 1619, loss = 0.35988613963127136
In grad_steps = 1620, loss = 0.02041545882821083
In grad_steps = 1621, loss = 0.17649288475513458
In grad_steps = 1622, loss = 0.6926232576370239
In grad_steps = 1623, loss = 0.1789427399635315
In grad_steps = 1624, loss = 0.11486034840345383
In grad_steps = 1625, loss = 0.09642825275659561
In grad_steps = 1626, loss = 0.019501570612192154
In grad_steps = 1627, loss = 0.043538857251405716
In grad_steps = 1628, loss = 0.22650504112243652
In grad_steps = 1629, loss = 0.03791816905140877
In grad_steps = 1630, loss = 0.4549138844013214
In grad_steps = 1631, loss = 0.20872652530670166
In grad_steps = 1632, loss = 0.701902449131012
In grad_steps = 1633, loss = 0.06268270313739777
In grad_steps = 1634, loss = 0.025356575846672058
In grad_steps = 1635, loss = 0.3303576111793518
In grad_steps = 1636, loss = 0.6671668887138367
In grad_steps = 1637, loss = 0.018193868920207024
In grad_steps = 1638, loss = 0.09418532997369766
In grad_steps = 1639, loss = 2.196824073791504
In grad_steps = 1640, loss = 0.23420505225658417
In grad_steps = 1641, loss = 0.040244124829769135
In grad_steps = 1642, loss = 0.9278703331947327
In grad_steps = 1643, loss = 0.15868540108203888
In grad_steps = 1644, loss = 0.7114869952201843
In grad_steps = 1645, loss = 0.1344071924686432
In grad_steps = 1646, loss = 0.3856131434440613
In grad_steps = 1647, loss = 0.5181020498275757
In grad_steps = 1648, loss = 0.05408180505037308
In grad_steps = 1649, loss = 0.07966440916061401
In grad_steps = 1650, loss = 1.0068917274475098
In grad_steps = 1651, loss = 0.24632826447486877
In grad_steps = 1652, loss = 0.27701008319854736
In grad_steps = 1653, loss = 0.32087060809135437
In grad_steps = 1654, loss = 0.5593048334121704
In grad_steps = 1655, loss = 0.3535112142562866
In grad_steps = 1656, loss = 0.4943171441555023
In grad_steps = 1657, loss = 0.2236512303352356
In grad_steps = 1658, loss = 0.13875964283943176
In grad_steps = 1659, loss = 1.1314300298690796
In grad_steps = 1660, loss = 0.3947864770889282
In grad_steps = 1661, loss = 0.3216789960861206
In grad_steps = 1662, loss = 1.1154860258102417
In grad_steps = 1663, loss = 0.47287341952323914
In grad_steps = 1664, loss = 0.22868852317333221
In grad_steps = 1665, loss = 0.42890384793281555
In grad_steps = 1666, loss = 0.08223767578601837
In grad_steps = 1667, loss = 0.16917695105075836
In grad_steps = 1668, loss = 0.9162670969963074
In grad_steps = 1669, loss = 0.43065395951271057
In grad_steps = 1670, loss = 0.11431616544723511
In grad_steps = 1671, loss = 0.2127111554145813
In grad_steps = 1672, loss = 0.622087299823761
In grad_steps = 1673, loss = 0.4095562696456909
In grad_steps = 1674, loss = 0.20086777210235596
In grad_steps = 1675, loss = 0.6881053447723389
In grad_steps = 1676, loss = 0.5164852142333984
In grad_steps = 1677, loss = 0.4028339385986328
In grad_steps = 1678, loss = 0.2670825719833374
In grad_steps = 1679, loss = 0.48381856083869934
In grad_steps = 1680, loss = 0.17513436079025269
In grad_steps = 1681, loss = 0.4932151138782501
In grad_steps = 1682, loss = 0.18802109360694885
In grad_steps = 1683, loss = 0.17712661623954773
In grad_steps = 1684, loss = 0.5059985518455505
In grad_steps = 1685, loss = 0.17558667063713074
In grad_steps = 1686, loss = 0.5275938510894775
In grad_steps = 1687, loss = 0.32541733980178833
In grad_steps = 1688, loss = 0.5191148519515991
In grad_steps = 1689, loss = 0.26057249307632446
In grad_steps = 1690, loss = 0.10083745419979095
In grad_steps = 1691, loss = 0.19144350290298462
In grad_steps = 1692, loss = 0.2919950485229492
In grad_steps = 1693, loss = 0.1530715525150299
In grad_steps = 1694, loss = 0.36429303884506226
In grad_steps = 1695, loss = 0.13530704379081726
In grad_steps = 1696, loss = 0.1801280677318573
In grad_steps = 1697, loss = 0.12980155646800995
In grad_steps = 1698, loss = 0.03720790147781372
In grad_steps = 1699, loss = 0.14396311342716217
In grad_steps = 1700, loss = 0.1217838004231453
In grad_steps = 1701, loss = 0.7892171144485474
In grad_steps = 1702, loss = 0.06751176714897156
In grad_steps = 1703, loss = 0.15881243348121643
In grad_steps = 1704, loss = 0.06991973519325256
In grad_steps = 1705, loss = 0.3228724002838135
In grad_steps = 1706, loss = 0.07061690837144852
In grad_steps = 1707, loss = 0.11013208329677582
In grad_steps = 1708, loss = 0.8796656727790833
In grad_steps = 1709, loss = 0.05421535670757294
In grad_steps = 1710, loss = 0.13796395063400269
In grad_steps = 1711, loss = 0.03112262859940529
In grad_steps = 1712, loss = 0.39254069328308105
In grad_steps = 1713, loss = 0.8667371273040771
In grad_steps = 1714, loss = 0.03547980636358261
In grad_steps = 1715, loss = 0.4496162533760071
In grad_steps = 1716, loss = 0.9544749855995178
In grad_steps = 1717, loss = 0.7757143378257751
In grad_steps = 1718, loss = 0.01763191446661949
In grad_steps = 1719, loss = 0.1717473715543747
In grad_steps = 1720, loss = 0.22151723504066467
In grad_steps = 1721, loss = 0.033223025500774384
In grad_steps = 1722, loss = 0.22175545990467072
In grad_steps = 1723, loss = 0.07345330715179443
In grad_steps = 1724, loss = 1.2702220678329468
In grad_steps = 1725, loss = 0.12499594688415527
In grad_steps = 1726, loss = 0.6379178166389465
In grad_steps = 1727, loss = 0.29363977909088135
In grad_steps = 1728, loss = 0.1547490656375885
In grad_steps = 1729, loss = 0.6853972673416138
In grad_steps = 1730, loss = 0.08631651848554611
In grad_steps = 1731, loss = 0.42469143867492676
In grad_steps = 1732, loss = 0.049426592886447906
In grad_steps = 1733, loss = 0.04586738348007202
In grad_steps = 1734, loss = 0.20094498991966248
In grad_steps = 1735, loss = 0.47902539372444153
In grad_steps = 1736, loss = 0.4034283459186554
In grad_steps = 1737, loss = 0.7017061710357666
In grad_steps = 1738, loss = 0.10437176376581192
In grad_steps = 1739, loss = 0.046484965831041336
In grad_steps = 1740, loss = 0.048187024891376495
In grad_steps = 1741, loss = 0.1834908276796341
In grad_steps = 1742, loss = 0.3871001899242401
In grad_steps = 1743, loss = 0.0411042794585228
In grad_steps = 1744, loss = 0.5439704656600952
In grad_steps = 1745, loss = 0.45897141098976135
In grad_steps = 1746, loss = 0.4950072169303894
In grad_steps = 1747, loss = 0.049052201211452484
In grad_steps = 1748, loss = 0.26865702867507935
In grad_steps = 1749, loss = 0.03722109645605087
In grad_steps = 1750, loss = 0.8956305980682373
In grad_steps = 1751, loss = 0.259390264749527
In grad_steps = 1752, loss = 0.10099820047616959
In grad_steps = 1753, loss = 0.1583680510520935
In grad_steps = 1754, loss = 0.11837108433246613
In grad_steps = 1755, loss = 0.05352361127734184
In grad_steps = 1756, loss = 0.1404806673526764
In grad_steps = 1757, loss = 1.0037941932678223
In grad_steps = 1758, loss = 0.04285261780023575
In grad_steps = 1759, loss = 0.13622796535491943
In grad_steps = 1760, loss = 0.20657360553741455
In grad_steps = 1761, loss = 0.1652372032403946
In grad_steps = 1762, loss = 0.4347945749759674
In grad_steps = 1763, loss = 0.049565013498067856
In grad_steps = 1764, loss = 0.03623809292912483
In grad_steps = 1765, loss = 0.08322270959615707
In grad_steps = 1766, loss = 0.01068955473601818
In grad_steps = 1767, loss = 0.19763894379138947
In grad_steps = 1768, loss = 0.7174801230430603
In grad_steps = 1769, loss = 0.10228356719017029
In grad_steps = 1770, loss = 0.6816537380218506
In grad_steps = 1771, loss = 0.7493242025375366
In grad_steps = 1772, loss = 0.05880127102136612
In grad_steps = 1773, loss = 0.03163234889507294
In grad_steps = 1774, loss = 0.03481685370206833
In grad_steps = 1775, loss = 0.8465614914894104
In grad_steps = 1776, loss = 0.6265258193016052
In grad_steps = 1777, loss = 0.13400830328464508
In grad_steps = 1778, loss = 0.5579453706741333
In grad_steps = 1779, loss = 0.7220600843429565
In grad_steps = 1780, loss = 0.040299564599990845
In grad_steps = 1781, loss = 0.08081145584583282
In grad_steps = 1782, loss = 0.6380622982978821
In grad_steps = 1783, loss = 0.18891876935958862
In grad_steps = 1784, loss = 1.1730259656906128
In grad_steps = 1785, loss = 0.18897844851016998
In grad_steps = 1786, loss = 0.24772685766220093
In grad_steps = 1787, loss = 0.20245468616485596
In grad_steps = 1788, loss = 0.1123356819152832
In grad_steps = 1789, loss = 0.3831532597541809
In grad_steps = 1790, loss = 0.258087694644928
In grad_steps = 1791, loss = 0.524763822555542
In grad_steps = 1792, loss = 0.15309950709342957
In grad_steps = 1793, loss = 0.25584980845451355
In grad_steps = 1794, loss = 0.15197712182998657
In grad_steps = 1795, loss = 0.07046378403902054
In grad_steps = 1796, loss = 0.11661089956760406
In grad_steps = 1797, loss = 0.27953481674194336
In grad_steps = 1798, loss = 0.12254814803600311
In grad_steps = 1799, loss = 0.6415472030639648
In grad_steps = 1800, loss = 0.5526098608970642
In grad_steps = 1801, loss = 0.17537304759025574
In grad_steps = 1802, loss = 0.6130502820014954
In grad_steps = 1803, loss = 0.121894970536232
In grad_steps = 1804, loss = 0.7088868618011475
In grad_steps = 1805, loss = 0.311148077249527
In grad_steps = 1806, loss = 0.1023545116186142
In grad_steps = 1807, loss = 0.162114679813385
In grad_steps = 1808, loss = 0.8954189419746399
In grad_steps = 1809, loss = 0.49825620651245117
In grad_steps = 1810, loss = 0.053657591342926025
In grad_steps = 1811, loss = 0.045366521924734116
In grad_steps = 1812, loss = 0.20653566718101501
In grad_steps = 1813, loss = 0.2731805443763733
In grad_steps = 1814, loss = 0.08465253561735153
In grad_steps = 1815, loss = 0.4191349744796753
In grad_steps = 1816, loss = 0.07610961049795151
In grad_steps = 1817, loss = 0.7318974137306213
In grad_steps = 1818, loss = 0.13551855087280273
In grad_steps = 1819, loss = 0.3885326087474823
In grad_steps = 1820, loss = 0.8479605913162231
In grad_steps = 1821, loss = 0.42775556445121765
In grad_steps = 1822, loss = 0.10254015773534775
In grad_steps = 1823, loss = 1.0926322937011719
In grad_steps = 1824, loss = 0.15017861127853394
In grad_steps = 1825, loss = 0.09808821976184845
In grad_steps = 1826, loss = 0.7320886850357056
In grad_steps = 1827, loss = 1.2195611000061035
In grad_steps = 1828, loss = 0.799390971660614
In grad_steps = 1829, loss = 0.7471163272857666
In grad_steps = 1830, loss = 0.5130807161331177
In grad_steps = 1831, loss = 0.36519190669059753
In grad_steps = 1832, loss = 0.6504594683647156
In grad_steps = 1833, loss = 1.3495492935180664
In grad_steps = 1834, loss = 0.14766182005405426
In grad_steps = 1835, loss = 0.5459657311439514
In grad_steps = 1836, loss = 0.5023902058601379
In grad_steps = 1837, loss = 0.6313963532447815
In grad_steps = 1838, loss = 0.45841655135154724
In grad_steps = 1839, loss = 0.30629345774650574
In grad_steps = 1840, loss = 0.19046135246753693
In grad_steps = 1841, loss = 0.28814834356307983
In grad_steps = 1842, loss = 0.43797779083251953
In grad_steps = 1843, loss = 0.2825530767440796
In grad_steps = 1844, loss = 0.882909893989563
In grad_steps = 1845, loss = 0.3030809164047241
In grad_steps = 1846, loss = 0.42889541387557983
In grad_steps = 1847, loss = 0.44622302055358887
In grad_steps = 1848, loss = 0.20512589812278748
In grad_steps = 1849, loss = 0.24222317337989807
In grad_steps = 1850, loss = 0.3195662498474121
In grad_steps = 1851, loss = 0.4864022731781006
In grad_steps = 1852, loss = 0.18090887367725372
In grad_steps = 1853, loss = 0.3909415602684021
In grad_steps = 1854, loss = 0.28422266244888306
In grad_steps = 1855, loss = 0.07561434060335159
In grad_steps = 1856, loss = 0.11253821849822998
In grad_steps = 1857, loss = 0.3047000765800476
In grad_steps = 1858, loss = 0.27338317036628723
In grad_steps = 1859, loss = 0.8188380002975464
In grad_steps = 1860, loss = 0.5421004891395569
In grad_steps = 1861, loss = 0.08714397996664047
In grad_steps = 1862, loss = 0.3732185959815979
In grad_steps = 1863, loss = 0.2937197983264923
In grad_steps = 1864, loss = 0.1511795073747635
In grad_steps = 1865, loss = 0.28658047318458557
In grad_steps = 1866, loss = 0.23712390661239624
In grad_steps = 1867, loss = 0.9775056838989258
In grad_steps = 1868, loss = 0.07501649111509323
In grad_steps = 1869, loss = 0.07205429673194885
In grad_steps = 1870, loss = 0.04788191616535187
In grad_steps = 1871, loss = 0.22133636474609375
In grad_steps = 1872, loss = 0.055589571595191956
In grad_steps = 1873, loss = 0.07752543687820435
In grad_steps = 1874, loss = 0.2032245695590973
In grad_steps = 1875, loss = 0.11619356274604797
In grad_steps = 1876, loss = 0.0653655156493187
In grad_steps = 1877, loss = 0.7438158988952637
In grad_steps = 1878, loss = 0.08735425770282745
In grad_steps = 1879, loss = 1.1036581993103027
In grad_steps = 1880, loss = 0.06238177418708801
In grad_steps = 1881, loss = 0.3097492456436157
In grad_steps = 1882, loss = 0.15341025590896606
In grad_steps = 1883, loss = 0.4956183433532715
In grad_steps = 1884, loss = 0.06519553810358047
In grad_steps = 1885, loss = 0.42345938086509705
In grad_steps = 1886, loss = 0.1513015478849411
In grad_steps = 1887, loss = 0.0985521450638771
In grad_steps = 1888, loss = 0.63570237159729
In grad_steps = 1889, loss = 0.06403181701898575
In grad_steps = 1890, loss = 0.03832487016916275
In grad_steps = 1891, loss = 0.34289830923080444
In grad_steps = 1892, loss = 0.5837882161140442
In grad_steps = 1893, loss = 0.05844903737306595
In grad_steps = 1894, loss = 0.03899054229259491
In grad_steps = 1895, loss = 0.5328431129455566
In grad_steps = 1896, loss = 0.2933780252933502
In grad_steps = 1897, loss = 0.07612882554531097
In grad_steps = 1898, loss = 0.0904763713479042
In grad_steps = 1899, loss = 0.13074913620948792
In grad_steps = 1900, loss = 0.0822020098567009
In grad_steps = 1901, loss = 0.21402078866958618
In grad_steps = 1902, loss = 0.10087068378925323
In grad_steps = 1903, loss = 0.21431522071361542
In grad_steps = 1904, loss = 0.2689899504184723
In grad_steps = 1905, loss = 0.05587294325232506
In grad_steps = 1906, loss = 0.2573070526123047
In grad_steps = 1907, loss = 1.3938792943954468
In grad_steps = 1908, loss = 0.17684565484523773
In grad_steps = 1909, loss = 0.05667559430003166
In grad_steps = 1910, loss = 1.121075987815857
In grad_steps = 1911, loss = 0.3742619752883911
In grad_steps = 1912, loss = 0.15624511241912842
In grad_steps = 1913, loss = 0.07563010603189468
In grad_steps = 1914, loss = 0.05991123989224434
In grad_steps = 1915, loss = 0.3718666136264801
In grad_steps = 1916, loss = 0.579517126083374
In grad_steps = 1917, loss = 0.17714542150497437
In grad_steps = 1918, loss = 0.2829010486602783
In grad_steps = 1919, loss = 0.45676565170288086
In grad_steps = 1920, loss = 0.2082868069410324
In grad_steps = 1921, loss = 0.6284068822860718
In grad_steps = 1922, loss = 0.2002231478691101
In grad_steps = 1923, loss = 0.023324396461248398
In grad_steps = 1924, loss = 0.3802297115325928
In grad_steps = 1925, loss = 0.37105414271354675
In grad_steps = 1926, loss = 0.8278928399085999
In grad_steps = 1927, loss = 0.32166776061058044
In grad_steps = 1928, loss = 1.2029118537902832
In grad_steps = 1929, loss = 0.3846832811832428
In grad_steps = 1930, loss = 0.36196020245552063
In grad_steps = 1931, loss = 1.145402431488037
In grad_steps = 1932, loss = 0.1612531989812851
In grad_steps = 1933, loss = 0.6243550181388855
In grad_steps = 1934, loss = 0.36398422718048096
In grad_steps = 1935, loss = 0.15087704360485077
In grad_steps = 1936, loss = 0.10414143651723862
In grad_steps = 1937, loss = 0.10379327088594437
In grad_steps = 1938, loss = 0.3633688688278198
In grad_steps = 1939, loss = 0.4783552885055542
In grad_steps = 1940, loss = 0.3358190357685089
In grad_steps = 1941, loss = 0.14800918102264404
In grad_steps = 1942, loss = 0.25286978483200073
In grad_steps = 1943, loss = 0.649173378944397
In grad_steps = 1944, loss = 0.14070911705493927
In grad_steps = 1945, loss = 0.14524240791797638
In grad_steps = 1946, loss = 0.1263081431388855
In grad_steps = 1947, loss = 0.23670226335525513
In grad_steps = 1948, loss = 0.06919533759355545
In grad_steps = 1949, loss = 0.21868811547756195
In grad_steps = 1950, loss = 0.6681839823722839
In grad_steps = 1951, loss = 0.9355782866477966
In grad_steps = 1952, loss = 0.0814162865281105
In grad_steps = 1953, loss = 0.6176539063453674
In grad_steps = 1954, loss = 0.7393950819969177
In grad_steps = 1955, loss = 0.029634598642587662
In grad_steps = 1956, loss = 0.2321196347475052
In grad_steps = 1957, loss = 0.042561158537864685
In grad_steps = 1958, loss = 0.07796558737754822
In grad_steps = 1959, loss = 0.10449691116809845
In grad_steps = 1960, loss = 0.07787561416625977
In grad_steps = 1961, loss = 0.7081040143966675
In grad_steps = 1962, loss = 0.045930683612823486
In grad_steps = 1963, loss = 0.5526590943336487
In grad_steps = 1964, loss = 0.736555814743042
In grad_steps = 1965, loss = 0.20660066604614258
In grad_steps = 1966, loss = 1.2591619491577148
In grad_steps = 1967, loss = 0.7335929870605469
In grad_steps = 1968, loss = 0.4019659161567688
In grad_steps = 1969, loss = 0.1353042721748352
In grad_steps = 1970, loss = 0.2448907196521759
In grad_steps = 1971, loss = 0.36644846200942993
In grad_steps = 1972, loss = 0.6490291953086853
In grad_steps = 1973, loss = 0.6114097237586975
In grad_steps = 1974, loss = 0.5644062161445618
In grad_steps = 1975, loss = 0.2541418969631195
In grad_steps = 1976, loss = 0.28072619438171387
In grad_steps = 1977, loss = 0.6475282907485962
In grad_steps = 1978, loss = 0.33252862095832825
In grad_steps = 1979, loss = 0.1566111445426941
In grad_steps = 1980, loss = 0.18607714772224426
In grad_steps = 1981, loss = 0.5419033765792847
In grad_steps = 1982, loss = 0.11406005173921585
In grad_steps = 1983, loss = 0.15622301399707794
In grad_steps = 1984, loss = 0.1688898205757141
In grad_steps = 1985, loss = 0.22536516189575195
In grad_steps = 1986, loss = 0.37816300988197327
In grad_steps = 1987, loss = 0.1573375016450882
In grad_steps = 1988, loss = 0.6897388696670532
In grad_steps = 1989, loss = 0.20842722058296204
In grad_steps = 1990, loss = 0.5197048187255859
In grad_steps = 1991, loss = 0.08101674169301987
In grad_steps = 1992, loss = 0.5559939742088318
In grad_steps = 1993, loss = 0.1296873837709427
In grad_steps = 1994, loss = 0.2339506894350052
In grad_steps = 1995, loss = 0.6460773348808289
In grad_steps = 1996, loss = 0.03641770780086517
In grad_steps = 1997, loss = 0.10584515333175659
In grad_steps = 1998, loss = 0.3330965042114258
In grad_steps = 1999, loss = 0.35192862153053284
In grad_steps = 2000, loss = 0.05376451462507248
In grad_steps = 2001, loss = 0.15431955456733704
In grad_steps = 2002, loss = 0.5608397722244263
In grad_steps = 2003, loss = 0.09122823178768158
In grad_steps = 2004, loss = 0.09626038372516632
In grad_steps = 2005, loss = 0.04211953282356262
In grad_steps = 2006, loss = 0.4330154061317444
In grad_steps = 2007, loss = 0.15835851430892944
In grad_steps = 2008, loss = 0.09333626925945282
In grad_steps = 2009, loss = 0.6076048612594604
In grad_steps = 2010, loss = 0.07024233788251877
In grad_steps = 2011, loss = 0.4865821897983551
In grad_steps = 2012, loss = 0.07967323064804077
In grad_steps = 2013, loss = 0.04463865980505943
In grad_steps = 2014, loss = 0.20297545194625854
In grad_steps = 2015, loss = 0.01241364423185587
In grad_steps = 2016, loss = 0.062342651188373566
In grad_steps = 2017, loss = 0.05845755711197853
In grad_steps = 2018, loss = 0.07036823034286499
In grad_steps = 2019, loss = 1.3386499881744385
In grad_steps = 2020, loss = 0.11625602841377258
In grad_steps = 2021, loss = 0.06002877280116081
In grad_steps = 2022, loss = 0.8729889988899231
In grad_steps = 2023, loss = 0.41877275705337524
In grad_steps = 2024, loss = 0.06817293167114258
In grad_steps = 2025, loss = 0.07569291442632675
In grad_steps = 2026, loss = 0.05282183736562729
In grad_steps = 2027, loss = 0.7566568851470947
In grad_steps = 2028, loss = 0.24852900207042694
In grad_steps = 2029, loss = 0.45123493671417236
In grad_steps = 2030, loss = 0.5339080095291138
In grad_steps = 2031, loss = 0.05006915703415871
In grad_steps = 2032, loss = 0.01705336570739746
In grad_steps = 2033, loss = 0.051534250378608704
In grad_steps = 2034, loss = 0.0746702253818512
In grad_steps = 2035, loss = 0.15940150618553162
In grad_steps = 2036, loss = 0.0439240038394928
In grad_steps = 2037, loss = 0.23693212866783142
In grad_steps = 2038, loss = 0.45203468203544617
In grad_steps = 2039, loss = 0.653171718120575
In grad_steps = 2040, loss = 0.29774007201194763
In grad_steps = 2041, loss = 0.09777091443538666
In grad_steps = 2042, loss = 0.3206136226654053
In grad_steps = 2043, loss = 0.28006669878959656
In grad_steps = 2044, loss = 0.09821313619613647
In grad_steps = 2045, loss = 0.7660412192344666
In grad_steps = 2046, loss = 0.6319754123687744
In grad_steps = 2047, loss = 0.04529009759426117
In grad_steps = 2048, loss = 0.22393256425857544
In grad_steps = 2049, loss = 2.0746335983276367
In grad_steps = 2050, loss = 0.48522013425827026
In grad_steps = 2051, loss = 0.4947618246078491
In grad_steps = 2052, loss = 0.08156456798315048
In grad_steps = 2053, loss = 0.9551517963409424
In grad_steps = 2054, loss = 0.3271816372871399
In grad_steps = 2055, loss = 0.15217794477939606
In grad_steps = 2056, loss = 0.08499570190906525
In grad_steps = 2057, loss = 0.10003475844860077
In grad_steps = 2058, loss = 0.30022525787353516
In grad_steps = 2059, loss = 0.13656258583068848
In grad_steps = 2060, loss = 0.5046001672744751
In grad_steps = 2061, loss = 0.30281224846839905
In grad_steps = 2062, loss = 0.135131374001503
In grad_steps = 2063, loss = 0.12116502970457077
In grad_steps = 2064, loss = 0.2634807229042053
In grad_steps = 2065, loss = 0.09002520143985748
In grad_steps = 2066, loss = 0.1140844076871872
In grad_steps = 2067, loss = 0.05996706336736679
In grad_steps = 2068, loss = 0.11247299611568451
In grad_steps = 2069, loss = 0.20896001160144806
In grad_steps = 2070, loss = 0.5140985250473022
In grad_steps = 2071, loss = 0.4756155014038086
In grad_steps = 2072, loss = 0.04085233062505722
In grad_steps = 2073, loss = 0.7702952027320862
In grad_steps = 2074, loss = 0.11319653689861298
In grad_steps = 2075, loss = 0.07352524250745773
In grad_steps = 2076, loss = 0.04537348449230194
In grad_steps = 2077, loss = 0.14062169194221497
In grad_steps = 2078, loss = 0.6812254786491394
In grad_steps = 2079, loss = 0.47541865706443787
In grad_steps = 2080, loss = 0.46299147605895996
In grad_steps = 2081, loss = 0.021422814577817917
In grad_steps = 2082, loss = 0.07142310589551926
In grad_steps = 2083, loss = 0.18474382162094116
In grad_steps = 2084, loss = 0.08215705305337906
In grad_steps = 2085, loss = 0.15438063442707062
In grad_steps = 2086, loss = 0.2111208140850067
In grad_steps = 2087, loss = 0.08815811574459076
In grad_steps = 2088, loss = 0.0208927933126688
In grad_steps = 2089, loss = 0.029379984363913536
In grad_steps = 2090, loss = 0.04640904814004898
In grad_steps = 2091, loss = 0.050691843032836914
In grad_steps = 2092, loss = 0.4812026023864746
In grad_steps = 2093, loss = 0.06641651690006256
In grad_steps = 2094, loss = 0.10908398032188416
In grad_steps = 2095, loss = 0.0788194090127945
In grad_steps = 2096, loss = 0.03147558122873306
In grad_steps = 2097, loss = 1.8880102634429932
In grad_steps = 2098, loss = 0.01676097698509693
In grad_steps = 2099, loss = 0.023793397471308708
In grad_steps = 2100, loss = 0.07544402033090591
In grad_steps = 2101, loss = 0.1144690215587616
In grad_steps = 2102, loss = 0.03844822943210602
In grad_steps = 2103, loss = 0.17838361859321594
In grad_steps = 2104, loss = 0.7477397918701172
In grad_steps = 2105, loss = 0.7782790660858154
In grad_steps = 2106, loss = 0.14167392253875732
In grad_steps = 2107, loss = 1.4552773237228394
In grad_steps = 2108, loss = 0.20881269872188568
In grad_steps = 2109, loss = 0.1440337598323822
In grad_steps = 2110, loss = 0.22687789797782898
In grad_steps = 2111, loss = 0.32115182280540466
In grad_steps = 2112, loss = 0.6995292901992798
In grad_steps = 2113, loss = 0.5063337087631226
In grad_steps = 2114, loss = 0.13063140213489532
In grad_steps = 2115, loss = 0.06878190487623215
In grad_steps = 2116, loss = 0.08150351047515869
In grad_steps = 2117, loss = 0.3302048444747925
In grad_steps = 2118, loss = 0.22120776772499084
In grad_steps = 2119, loss = 0.03196579962968826
In grad_steps = 2120, loss = 0.022292954847216606
In grad_steps = 2121, loss = 0.18245527148246765
In grad_steps = 2122, loss = 0.1127394512295723
In grad_steps = 2123, loss = 0.47505438327789307
In grad_steps = 2124, loss = 0.09881236404180527
In grad_steps = 2125, loss = 0.035211700946092606
In grad_steps = 2126, loss = 1.1989467144012451
In grad_steps = 2127, loss = 0.154695525765419
In grad_steps = 2128, loss = 0.12805655598640442
In grad_steps = 2129, loss = 0.38642144203186035
In grad_steps = 2130, loss = 0.12586139142513275
In grad_steps = 2131, loss = 0.44548720121383667
In grad_steps = 2132, loss = 0.4004262089729309
In grad_steps = 2133, loss = 0.03914007544517517
In grad_steps = 2134, loss = 0.1804531365633011
In grad_steps = 2135, loss = 0.3938734233379364
In grad_steps = 2136, loss = 0.4387756288051605
In grad_steps = 2137, loss = 0.21578828990459442
In grad_steps = 2138, loss = 0.519700288772583
In grad_steps = 2139, loss = 0.0843399167060852
In grad_steps = 2140, loss = 0.3874264061450958
In grad_steps = 2141, loss = 0.16215156018733978
In grad_steps = 2142, loss = 0.031067099422216415
In grad_steps = 2143, loss = 0.27702319622039795
In grad_steps = 2144, loss = 0.9453532099723816
In grad_steps = 2145, loss = 0.1574292927980423
In grad_steps = 2146, loss = 0.2792545258998871
In grad_steps = 2147, loss = 0.5227041840553284
In grad_steps = 2148, loss = 0.80535489320755
In grad_steps = 2149, loss = 0.03305075317621231
In grad_steps = 2150, loss = 0.18191413581371307
In grad_steps = 2151, loss = 0.5584526062011719
In grad_steps = 2152, loss = 0.2747619152069092
In grad_steps = 2153, loss = 0.14923705160617828
In grad_steps = 2154, loss = 0.08481328934431076
In grad_steps = 2155, loss = 0.37745440006256104
In grad_steps = 2156, loss = 0.1735401153564453
In grad_steps = 2157, loss = 0.04310103505849838
In grad_steps = 2158, loss = 0.2269737720489502
In grad_steps = 2159, loss = 0.8296054601669312
In grad_steps = 2160, loss = 0.17814627289772034
In grad_steps = 2161, loss = 0.018872467800974846
In grad_steps = 2162, loss = 0.8765246272087097
In grad_steps = 2163, loss = 0.051082588732242584
In grad_steps = 2164, loss = 0.07982314378023148
In grad_steps = 2165, loss = 0.20913362503051758
In grad_steps = 2166, loss = 0.5999958515167236
In grad_steps = 2167, loss = 0.07446187734603882
In grad_steps = 2168, loss = 0.12506026029586792
In grad_steps = 2169, loss = 0.6142733097076416
In grad_steps = 2170, loss = 0.018732625991106033
In grad_steps = 2171, loss = 0.7894960641860962
In grad_steps = 2172, loss = 0.8313701748847961
In grad_steps = 2173, loss = 0.00818590633571148
In grad_steps = 2174, loss = 0.27440139651298523
In grad_steps = 2175, loss = 0.3705783486366272
In grad_steps = 2176, loss = 0.1868152916431427
In grad_steps = 2177, loss = 0.09903152287006378
In grad_steps = 2178, loss = 0.10219034552574158
In grad_steps = 2179, loss = 0.32326623797416687
In grad_steps = 2180, loss = 0.06024756655097008
In grad_steps = 2181, loss = 0.05438758432865143
In grad_steps = 2182, loss = 0.14440304040908813
In grad_steps = 2183, loss = 0.19212815165519714
In grad_steps = 2184, loss = 0.11420778930187225
In grad_steps = 2185, loss = 0.180095836520195
In grad_steps = 2186, loss = 0.26163652539253235
In grad_steps = 2187, loss = 0.31566038727760315
In grad_steps = 2188, loss = 0.04090540111064911
In grad_steps = 2189, loss = 0.14636550843715668
In grad_steps = 2190, loss = 0.053086258471012115
In grad_steps = 2191, loss = 0.26046764850616455
In grad_steps = 2192, loss = 0.6088982820510864
In grad_steps = 2193, loss = 0.2470681220293045
In grad_steps = 2194, loss = 0.04242316633462906
In grad_steps = 2195, loss = 0.022793971002101898
In grad_steps = 2196, loss = 0.12615276873111725
In grad_steps = 2197, loss = 0.019797159358859062
In grad_steps = 2198, loss = 1.2326229810714722
In grad_steps = 2199, loss = 0.056374259293079376
In grad_steps = 2200, loss = 0.3873840570449829
In grad_steps = 2201, loss = 0.29247793555259705
In grad_steps = 2202, loss = 0.44759228825569153
In grad_steps = 2203, loss = 0.09635695815086365
In grad_steps = 2204, loss = 0.21555590629577637
In grad_steps = 2205, loss = 0.016174551099538803
In grad_steps = 2206, loss = 0.14642629027366638
In grad_steps = 2207, loss = 0.058897338807582855
In grad_steps = 2208, loss = 0.1358439177274704
In grad_steps = 2209, loss = 0.06594686955213547
In grad_steps = 2210, loss = 0.42956438660621643
In grad_steps = 2211, loss = 0.10285307466983795
In grad_steps = 2212, loss = 1.4064750671386719
In grad_steps = 2213, loss = 1.3502287864685059
In grad_steps = 2214, loss = 0.08835574239492416
In grad_steps = 2215, loss = 0.13271573185920715
In grad_steps = 2216, loss = 0.18941478431224823
In grad_steps = 2217, loss = 1.2863807678222656
In grad_steps = 2218, loss = 0.33742696046829224
In grad_steps = 2219, loss = 0.49880728125572205
In grad_steps = 2220, loss = 0.18281510472297668
In grad_steps = 2221, loss = 0.12079225480556488
In grad_steps = 2222, loss = 0.2510550618171692
In grad_steps = 2223, loss = 0.4245806932449341
In grad_steps = 2224, loss = 0.06331051141023636
In grad_steps = 2225, loss = 0.4027472138404846
In grad_steps = 2226, loss = 0.2427750676870346
In grad_steps = 2227, loss = 0.3904455602169037
In grad_steps = 2228, loss = 0.5882435441017151
In grad_steps = 2229, loss = 0.4756958782672882
In grad_steps = 2230, loss = 0.13948500156402588
In grad_steps = 2231, loss = 0.07754062116146088
In grad_steps = 2232, loss = 0.08695753663778305
In grad_steps = 2233, loss = 0.12906844913959503
In grad_steps = 2234, loss = 0.4304691553115845
In grad_steps = 2235, loss = 0.017993377521634102
In grad_steps = 2236, loss = 0.39152660965919495
In grad_steps = 2237, loss = 0.6965397000312805
In grad_steps = 2238, loss = 0.38214191794395447
In grad_steps = 2239, loss = 0.5051450729370117
In grad_steps = 2240, loss = 0.2214740514755249
In grad_steps = 2241, loss = 0.20645540952682495
In grad_steps = 2242, loss = 0.5266525745391846
In grad_steps = 2243, loss = 0.09929324686527252
In grad_steps = 2244, loss = 0.08555184304714203
In grad_steps = 2245, loss = 1.041048288345337
In grad_steps = 2246, loss = 0.49863603711128235
In grad_steps = 2247, loss = 0.08317945152521133
In grad_steps = 2248, loss = 0.8199396729469299
In grad_steps = 2249, loss = 0.5178827047348022
In grad_steps = 2250, loss = 0.1821145862340927
In grad_steps = 2251, loss = 0.12500205636024475
In grad_steps = 2252, loss = 0.7447558641433716
In grad_steps = 2253, loss = 0.5111485123634338
In grad_steps = 2254, loss = 0.3768676519393921
In grad_steps = 2255, loss = 0.13662567734718323
In grad_steps = 2256, loss = 0.4397179186344147
In grad_steps = 2257, loss = 0.1397417038679123
In grad_steps = 2258, loss = 0.14036819338798523
In grad_steps = 2259, loss = 0.5125638246536255
In grad_steps = 2260, loss = 0.21717578172683716
In grad_steps = 2261, loss = 0.1615254282951355
In grad_steps = 2262, loss = 0.20814430713653564
In grad_steps = 2263, loss = 0.36634430289268494
In grad_steps = 2264, loss = 0.40792572498321533
In grad_steps = 2265, loss = 0.07513013482093811
In grad_steps = 2266, loss = 0.4686664640903473
In grad_steps = 2267, loss = 0.3064516484737396
In grad_steps = 2268, loss = 0.03293788805603981
In grad_steps = 2269, loss = 0.8933718204498291
In grad_steps = 2270, loss = 0.07906343042850494
In grad_steps = 2271, loss = 0.2375010848045349
In grad_steps = 2272, loss = 0.050788238644599915
In grad_steps = 2273, loss = 0.25440359115600586
In grad_steps = 2274, loss = 0.09684079885482788
In grad_steps = 2275, loss = 0.08656713366508484
In grad_steps = 2276, loss = 0.07897040992975235
In grad_steps = 2277, loss = 0.32598423957824707
In grad_steps = 2278, loss = 0.12540094554424286
In grad_steps = 2279, loss = 0.016653692349791527
In grad_steps = 2280, loss = 0.03473108634352684
In grad_steps = 2281, loss = 0.3195742964744568
In grad_steps = 2282, loss = 0.1703750193119049
In grad_steps = 2283, loss = 0.8679451942443848
In grad_steps = 2284, loss = 0.1071891337633133
In grad_steps = 2285, loss = 0.036171186715364456
In grad_steps = 2286, loss = 0.21216586232185364
In grad_steps = 2287, loss = 0.8122283220291138
In grad_steps = 2288, loss = 0.2603999078273773
In grad_steps = 2289, loss = 0.02059772238135338
In grad_steps = 2290, loss = 0.2606149911880493
In grad_steps = 2291, loss = 0.03888171166181564
In grad_steps = 2292, loss = 0.5644315481185913
In grad_steps = 2293, loss = 0.14140290021896362
In grad_steps = 2294, loss = 0.03555484861135483
In grad_steps = 2295, loss = 0.12056851387023926
In grad_steps = 2296, loss = 0.08374176174402237
In grad_steps = 2297, loss = 0.13022197782993317
In grad_steps = 2298, loss = 0.1430159956216812
In grad_steps = 2299, loss = 0.196551114320755
In grad_steps = 2300, loss = 0.057080864906311035
In grad_steps = 2301, loss = 0.08847720921039581
In grad_steps = 2302, loss = 0.4479086399078369
In grad_steps = 2303, loss = 0.009374506771564484
In grad_steps = 2304, loss = 0.017350656911730766
In grad_steps = 2305, loss = 0.11813025921583176
In grad_steps = 2306, loss = 0.020159512758255005
In grad_steps = 2307, loss = 0.18555137515068054
In grad_steps = 2308, loss = 0.4672272503376007
In grad_steps = 2309, loss = 0.006194639950990677
In grad_steps = 2310, loss = 0.852220356464386
In grad_steps = 2311, loss = 0.5866604447364807
In grad_steps = 2312, loss = 0.7870345115661621
In grad_steps = 2313, loss = 1.3432526588439941
In grad_steps = 2314, loss = 0.07954293489456177
In grad_steps = 2315, loss = 0.27441880106925964
In grad_steps = 2316, loss = 0.0713982805609703
In grad_steps = 2317, loss = 0.11283204704523087
In grad_steps = 2318, loss = 0.5509802103042603
In grad_steps = 2319, loss = 1.2062530517578125
In grad_steps = 2320, loss = 0.10501439869403839
In grad_steps = 2321, loss = 0.08457353711128235
In grad_steps = 2322, loss = 0.1665697544813156
In grad_steps = 2323, loss = 0.06045757234096527
In grad_steps = 2324, loss = 0.29233136773109436
In grad_steps = 2325, loss = 0.1343652904033661
In grad_steps = 2326, loss = 0.06304324418306351
In grad_steps = 2327, loss = 0.9642250537872314
In grad_steps = 2328, loss = 0.6341259479522705
In grad_steps = 2329, loss = 0.1788870096206665
In grad_steps = 2330, loss = 0.12849795818328857
In grad_steps = 2331, loss = 1.0613524913787842
In grad_steps = 2332, loss = 0.19005617499351501
In grad_steps = 2333, loss = 0.7756108045578003
In grad_steps = 2334, loss = 0.15953943133354187
In grad_steps = 2335, loss = 0.5023358464241028
In grad_steps = 2336, loss = 0.272306889295578
In grad_steps = 2337, loss = 0.17918537557125092
In grad_steps = 2338, loss = 0.1235327497124672
In grad_steps = 2339, loss = 0.1801782250404358
In grad_steps = 2340, loss = 0.28578412532806396
In grad_steps = 2341, loss = 0.25210750102996826
In grad_steps = 2342, loss = 1.116269588470459
In grad_steps = 2343, loss = 0.13577166199684143
In grad_steps = 2344, loss = 0.6595366597175598
In grad_steps = 2345, loss = 0.2408563792705536
In grad_steps = 2346, loss = 0.2264097034931183
In grad_steps = 2347, loss = 0.2508499026298523
In grad_steps = 2348, loss = 0.07451815903186798
In grad_steps = 2349, loss = 0.13883161544799805
In grad_steps = 2350, loss = 0.19531239569187164
In grad_steps = 2351, loss = 0.6239416599273682
In grad_steps = 2352, loss = 0.04238514229655266
Beginning epoch 2
In grad_steps = 2353, loss = 0.13460521399974823
In grad_steps = 2354, loss = 1.5992419719696045
In grad_steps = 2355, loss = 0.4228020906448364
In grad_steps = 2356, loss = 0.32821276783943176
In grad_steps = 2357, loss = 0.14206109941005707
In grad_steps = 2358, loss = 0.30686724185943604
In grad_steps = 2359, loss = 0.17707625031471252
In grad_steps = 2360, loss = 0.15240272879600525
In grad_steps = 2361, loss = 0.07375073432922363
In grad_steps = 2362, loss = 0.7800753116607666
In grad_steps = 2363, loss = 0.21837672591209412
In grad_steps = 2364, loss = 0.4699638783931732
In grad_steps = 2365, loss = 0.1965601146221161
In grad_steps = 2366, loss = 0.10475543141365051
In grad_steps = 2367, loss = 0.189533069729805
In grad_steps = 2368, loss = 0.1191675066947937
In grad_steps = 2369, loss = 0.12512801587581635
In grad_steps = 2370, loss = 0.31696805357933044
In grad_steps = 2371, loss = 0.57927006483078
In grad_steps = 2372, loss = 0.2828321158885956
In grad_steps = 2373, loss = 0.23924660682678223
In grad_steps = 2374, loss = 0.27528342604637146
In grad_steps = 2375, loss = 0.205976665019989
In grad_steps = 2376, loss = 0.5503309965133667
In grad_steps = 2377, loss = 0.7002299427986145
In grad_steps = 2378, loss = 0.07989145070314407
In grad_steps = 2379, loss = 0.03887120634317398
In grad_steps = 2380, loss = 0.015311012044548988
In grad_steps = 2381, loss = 0.07189976423978806
In grad_steps = 2382, loss = 0.038342684507369995
In grad_steps = 2383, loss = 0.10031545162200928
In grad_steps = 2384, loss = 0.8213418126106262
In grad_steps = 2385, loss = 0.10419642180204391
In grad_steps = 2386, loss = 0.03902139514684677
In grad_steps = 2387, loss = 0.3314269781112671
In grad_steps = 2388, loss = 0.2938915491104126
In grad_steps = 2389, loss = 0.03649410605430603
In grad_steps = 2390, loss = 0.02327144704759121
In grad_steps = 2391, loss = 0.5396026372909546
In grad_steps = 2392, loss = 0.8359952569007874
In grad_steps = 2393, loss = 0.024434808641672134
In grad_steps = 2394, loss = 0.24970915913581848
In grad_steps = 2395, loss = 0.22773537039756775
In grad_steps = 2396, loss = 0.1574559211730957
In grad_steps = 2397, loss = 0.3798612356185913
In grad_steps = 2398, loss = 0.6235305666923523
In grad_steps = 2399, loss = 0.11080189049243927
In grad_steps = 2400, loss = 0.7655367255210876
In grad_steps = 2401, loss = 0.06095796823501587
In grad_steps = 2402, loss = 0.3434993028640747
In grad_steps = 2403, loss = 0.4178437888622284
In grad_steps = 2404, loss = 0.9504377841949463
In grad_steps = 2405, loss = 0.08133170008659363
In grad_steps = 2406, loss = 0.21637758612632751
In grad_steps = 2407, loss = 0.05908534675836563
In grad_steps = 2408, loss = 0.05770576000213623
In grad_steps = 2409, loss = 0.47146886587142944
In grad_steps = 2410, loss = 0.05725936219096184
In grad_steps = 2411, loss = 0.5646262168884277
In grad_steps = 2412, loss = 0.058485209941864014
In grad_steps = 2413, loss = 0.4829169511795044
In grad_steps = 2414, loss = 0.1289454847574234
In grad_steps = 2415, loss = 0.5233181715011597
In grad_steps = 2416, loss = 0.6748055219650269
In grad_steps = 2417, loss = 0.24644668400287628
In grad_steps = 2418, loss = 0.06681889295578003
In grad_steps = 2419, loss = 0.10358375310897827
In grad_steps = 2420, loss = 0.06190815940499306
In grad_steps = 2421, loss = 0.06238121911883354
In grad_steps = 2422, loss = 1.0956615209579468
In grad_steps = 2423, loss = 0.2604711353778839
In grad_steps = 2424, loss = 0.053557537496089935
In grad_steps = 2425, loss = 1.2426515817642212
In grad_steps = 2426, loss = 0.06319589912891388
In grad_steps = 2427, loss = 0.07263554632663727
In grad_steps = 2428, loss = 0.06468424201011658
In grad_steps = 2429, loss = 0.2846721112728119
In grad_steps = 2430, loss = 0.5143347978591919
In grad_steps = 2431, loss = 0.36003240942955017
In grad_steps = 2432, loss = 0.07807337492704391
In grad_steps = 2433, loss = 0.2667652368545532
In grad_steps = 2434, loss = 0.03322506695985794
In grad_steps = 2435, loss = 0.15111303329467773
In grad_steps = 2436, loss = 0.22854894399642944
In grad_steps = 2437, loss = 0.08512597531080246
In grad_steps = 2438, loss = 0.45777806639671326
In grad_steps = 2439, loss = 1.0452654361724854
In grad_steps = 2440, loss = 0.07998862862586975
In grad_steps = 2441, loss = 0.2979978621006012
In grad_steps = 2442, loss = 0.35883986949920654
In grad_steps = 2443, loss = 1.0888599157333374
In grad_steps = 2444, loss = 0.5017355680465698
In grad_steps = 2445, loss = 0.2302553802728653
In grad_steps = 2446, loss = 0.1794755458831787
In grad_steps = 2447, loss = 0.08409041911363602
In grad_steps = 2448, loss = 0.7732325792312622
In grad_steps = 2449, loss = 0.12749052047729492
In grad_steps = 2450, loss = 0.07213462889194489
In grad_steps = 2451, loss = 0.29855459928512573
In grad_steps = 2452, loss = 0.2836763262748718
In grad_steps = 2453, loss = 0.5426725149154663
In grad_steps = 2454, loss = 0.16551180183887482
In grad_steps = 2455, loss = 0.578632652759552
In grad_steps = 2456, loss = 0.5447934865951538
In grad_steps = 2457, loss = 0.42225977778434753
In grad_steps = 2458, loss = 0.35779932141304016
In grad_steps = 2459, loss = 0.05461599677801132
In grad_steps = 2460, loss = 0.05231938511133194
In grad_steps = 2461, loss = 0.17534112930297852
In grad_steps = 2462, loss = 0.15917804837226868
In grad_steps = 2463, loss = 0.17297419905662537
In grad_steps = 2464, loss = 0.09995055943727493
In grad_steps = 2465, loss = 0.3843570649623871
In grad_steps = 2466, loss = 0.18189027905464172
In grad_steps = 2467, loss = 0.6549643278121948
In grad_steps = 2468, loss = 0.21975144743919373
In grad_steps = 2469, loss = 0.8030979633331299
In grad_steps = 2470, loss = 0.15663772821426392
In grad_steps = 2471, loss = 0.29126033186912537
In grad_steps = 2472, loss = 0.4858817160129547
In grad_steps = 2473, loss = 0.7470766305923462
In grad_steps = 2474, loss = 0.14376795291900635
In grad_steps = 2475, loss = 0.3948846459388733
In grad_steps = 2476, loss = 0.1020621657371521
In grad_steps = 2477, loss = 0.3441469669342041
In grad_steps = 2478, loss = 0.9143637418746948
In grad_steps = 2479, loss = 0.2838019132614136
In grad_steps = 2480, loss = 0.20387354493141174
In grad_steps = 2481, loss = 0.4417291283607483
In grad_steps = 2482, loss = 0.11191217601299286
In grad_steps = 2483, loss = 0.3503681421279907
In grad_steps = 2484, loss = 0.13871446251869202
In grad_steps = 2485, loss = 0.16175305843353271
In grad_steps = 2486, loss = 0.0675085261464119
In grad_steps = 2487, loss = 0.7208334803581238
In grad_steps = 2488, loss = 0.07013516128063202
In grad_steps = 2489, loss = 0.16048182547092438
In grad_steps = 2490, loss = 0.12815771996974945
In grad_steps = 2491, loss = 0.3241400122642517
In grad_steps = 2492, loss = 0.08939798176288605
In grad_steps = 2493, loss = 0.033452119678258896
In grad_steps = 2494, loss = 0.17914456129074097
In grad_steps = 2495, loss = 0.3805381655693054
In grad_steps = 2496, loss = 0.3445175290107727
In grad_steps = 2497, loss = 0.13147036731243134
In grad_steps = 2498, loss = 0.2617926597595215
In grad_steps = 2499, loss = 0.12159282714128494
In grad_steps = 2500, loss = 0.6657899618148804
In grad_steps = 2501, loss = 0.9598633050918579
In grad_steps = 2502, loss = 1.38718843460083
In grad_steps = 2503, loss = 0.059099242091178894
In grad_steps = 2504, loss = 0.15561097860336304
In grad_steps = 2505, loss = 0.09034080803394318
In grad_steps = 2506, loss = 0.6500001549720764
In grad_steps = 2507, loss = 1.2714365720748901
In grad_steps = 2508, loss = 0.1646658033132553
In grad_steps = 2509, loss = 0.3060897886753082
In grad_steps = 2510, loss = 0.5107088685035706
In grad_steps = 2511, loss = 0.19337522983551025
In grad_steps = 2512, loss = 0.3354172110557556
In grad_steps = 2513, loss = 0.408478319644928
In grad_steps = 2514, loss = 0.06115760654211044
In grad_steps = 2515, loss = 0.3132724165916443
In grad_steps = 2516, loss = 0.16725733876228333
In grad_steps = 2517, loss = 0.16684108972549438
In grad_steps = 2518, loss = 0.1818615049123764
In grad_steps = 2519, loss = 0.39593732357025146
In grad_steps = 2520, loss = 0.051661569625139236
In grad_steps = 2521, loss = 0.056273430585861206
In grad_steps = 2522, loss = 0.632946789264679
In grad_steps = 2523, loss = 0.062495581805706024
In grad_steps = 2524, loss = 0.33987244963645935
In grad_steps = 2525, loss = 0.08842748403549194
In grad_steps = 2526, loss = 0.2876121699810028
In grad_steps = 2527, loss = 0.8644884824752808
In grad_steps = 2528, loss = 0.09356775134801865
In grad_steps = 2529, loss = 0.056646645069122314
In grad_steps = 2530, loss = 0.03734966367483139
In grad_steps = 2531, loss = 0.3839706778526306
In grad_steps = 2532, loss = 0.11562081426382065
In grad_steps = 2533, loss = 0.16714298725128174
In grad_steps = 2534, loss = 0.03393976017832756
In grad_steps = 2535, loss = 0.17330627143383026
In grad_steps = 2536, loss = 0.04837793484330177
In grad_steps = 2537, loss = 0.14098942279815674
In grad_steps = 2538, loss = 0.19203805923461914
In grad_steps = 2539, loss = 0.14721471071243286
In grad_steps = 2540, loss = 0.027392733842134476
In grad_steps = 2541, loss = 0.38363873958587646
In grad_steps = 2542, loss = 0.010776892304420471
In grad_steps = 2543, loss = 0.06803163141012192
In grad_steps = 2544, loss = 0.23999185860157013
In grad_steps = 2545, loss = 0.10315980017185211
In grad_steps = 2546, loss = 0.019365228712558746
In grad_steps = 2547, loss = 0.43392249941825867
In grad_steps = 2548, loss = 0.016581477597355843
In grad_steps = 2549, loss = 0.8384105563163757
In grad_steps = 2550, loss = 0.007909506559371948
In grad_steps = 2551, loss = 0.018555719405412674
In grad_steps = 2552, loss = 0.01992654800415039
In grad_steps = 2553, loss = 0.02695486508309841
In grad_steps = 2554, loss = 1.5628881454467773
In grad_steps = 2555, loss = 0.7645456194877625
In grad_steps = 2556, loss = 0.018422968685626984
In grad_steps = 2557, loss = 0.025123311206698418
In grad_steps = 2558, loss = 0.24338014423847198
In grad_steps = 2559, loss = 0.6729910969734192
In grad_steps = 2560, loss = 0.045758191496133804
In grad_steps = 2561, loss = 0.06877172738313675
In grad_steps = 2562, loss = 0.7573376893997192
In grad_steps = 2563, loss = 0.04309950768947601
In grad_steps = 2564, loss = 0.10320015251636505
In grad_steps = 2565, loss = 0.1242256835103035
In grad_steps = 2566, loss = 0.0666487067937851
In grad_steps = 2567, loss = 0.4513413906097412
In grad_steps = 2568, loss = 0.43633538484573364
In grad_steps = 2569, loss = 0.043087076395750046
In grad_steps = 2570, loss = 0.4585747718811035
In grad_steps = 2571, loss = 0.44066083431243896
In grad_steps = 2572, loss = 0.08005582541227341
In grad_steps = 2573, loss = 0.07187820971012115
In grad_steps = 2574, loss = 0.30034586787223816
In grad_steps = 2575, loss = 0.019707288593053818
In grad_steps = 2576, loss = 0.15085099637508392
In grad_steps = 2577, loss = 0.27940768003463745
In grad_steps = 2578, loss = 0.8630611300468445
In grad_steps = 2579, loss = 0.17578013241291046
In grad_steps = 2580, loss = 0.551006555557251
In grad_steps = 2581, loss = 0.23835042119026184
In grad_steps = 2582, loss = 0.46904322504997253
In grad_steps = 2583, loss = 0.1818109154701233
In grad_steps = 2584, loss = 0.1272105574607849
In grad_steps = 2585, loss = 0.05323319137096405
In grad_steps = 2586, loss = 0.9612323045730591
In grad_steps = 2587, loss = 0.1840963214635849
In grad_steps = 2588, loss = 0.2060234397649765
In grad_steps = 2589, loss = 0.23919528722763062
In grad_steps = 2590, loss = 0.3131408095359802
In grad_steps = 2591, loss = 0.1173754408955574
In grad_steps = 2592, loss = 0.22867512702941895
In grad_steps = 2593, loss = 0.09508444368839264
In grad_steps = 2594, loss = 0.12798629701137543
In grad_steps = 2595, loss = 0.030574234202504158
In grad_steps = 2596, loss = 0.025573549792170525
In grad_steps = 2597, loss = 0.2595043182373047
In grad_steps = 2598, loss = 0.06912804394960403
In grad_steps = 2599, loss = 0.0329323336482048
In grad_steps = 2600, loss = 0.7319985628128052
In grad_steps = 2601, loss = 0.02327435463666916
In grad_steps = 2602, loss = 0.05531257390975952
In grad_steps = 2603, loss = 0.11678672581911087
In grad_steps = 2604, loss = 0.5442818403244019
In grad_steps = 2605, loss = 1.0507973432540894
In grad_steps = 2606, loss = 0.07258636504411697
In grad_steps = 2607, loss = 0.22132840752601624
In grad_steps = 2608, loss = 0.8159707188606262
In grad_steps = 2609, loss = 0.18300914764404297
In grad_steps = 2610, loss = 0.5947432518005371
In grad_steps = 2611, loss = 0.1509018987417221
In grad_steps = 2612, loss = 0.11883632093667984
In grad_steps = 2613, loss = 0.456646203994751
In grad_steps = 2614, loss = 0.4898040294647217
In grad_steps = 2615, loss = 0.16566240787506104
In grad_steps = 2616, loss = 0.4081048369407654
In grad_steps = 2617, loss = 0.10867264866828918
In grad_steps = 2618, loss = 0.09100833535194397
In grad_steps = 2619, loss = 0.2758793830871582
In grad_steps = 2620, loss = 0.08022577315568924
In grad_steps = 2621, loss = 0.44869381189346313
In grad_steps = 2622, loss = 0.19065937399864197
In grad_steps = 2623, loss = 0.1236373633146286
In grad_steps = 2624, loss = 0.14372220635414124
In grad_steps = 2625, loss = 0.069271981716156
In grad_steps = 2626, loss = 0.5412021279335022
In grad_steps = 2627, loss = 0.04813215881586075
In grad_steps = 2628, loss = 0.05285197123885155
In grad_steps = 2629, loss = 0.44803154468536377
In grad_steps = 2630, loss = 0.416843056678772
In grad_steps = 2631, loss = 0.01875683106482029
In grad_steps = 2632, loss = 1.0057395696640015
In grad_steps = 2633, loss = 0.43506282567977905
In grad_steps = 2634, loss = 0.8291698694229126
In grad_steps = 2635, loss = 0.18336309492588043
In grad_steps = 2636, loss = 0.5263922810554504
In grad_steps = 2637, loss = 0.09010929614305496
In grad_steps = 2638, loss = 0.7807323932647705
In grad_steps = 2639, loss = 0.5889023542404175
In grad_steps = 2640, loss = 0.04847566410899162
In grad_steps = 2641, loss = 0.02139296941459179
In grad_steps = 2642, loss = 0.6739715337753296
In grad_steps = 2643, loss = 0.1412225067615509
In grad_steps = 2644, loss = 0.21055728197097778
In grad_steps = 2645, loss = 0.22808250784873962
In grad_steps = 2646, loss = 0.2100253850221634
In grad_steps = 2647, loss = 0.11246030032634735
In grad_steps = 2648, loss = 0.16555117070674896
In grad_steps = 2649, loss = 0.04537045583128929
In grad_steps = 2650, loss = 0.3256760835647583
In grad_steps = 2651, loss = 0.2754468023777008
In grad_steps = 2652, loss = 0.03755948320031166
In grad_steps = 2653, loss = 0.10132038593292236
In grad_steps = 2654, loss = 0.1747446358203888
In grad_steps = 2655, loss = 0.009452019818127155
In grad_steps = 2656, loss = 0.31447726488113403
In grad_steps = 2657, loss = 0.28224238753318787
In grad_steps = 2658, loss = 0.8665199875831604
In grad_steps = 2659, loss = 0.0545552521944046
In grad_steps = 2660, loss = 0.524775505065918
In grad_steps = 2661, loss = 0.20954185724258423
In grad_steps = 2662, loss = 0.43080466985702515
In grad_steps = 2663, loss = 0.14205603301525116
In grad_steps = 2664, loss = 0.5830290913581848
In grad_steps = 2665, loss = 0.19742150604724884
In grad_steps = 2666, loss = 0.19951538741588593
In grad_steps = 2667, loss = 0.6096097230911255
In grad_steps = 2668, loss = 0.2601228952407837
In grad_steps = 2669, loss = 0.0832589790225029
In grad_steps = 2670, loss = 0.11629657447338104
In grad_steps = 2671, loss = 0.5631473660469055
In grad_steps = 2672, loss = 0.08378121256828308
In grad_steps = 2673, loss = 0.13826076686382294
In grad_steps = 2674, loss = 0.1444132924079895
In grad_steps = 2675, loss = 0.06447877734899521
In grad_steps = 2676, loss = 0.45706331729888916
In grad_steps = 2677, loss = 0.07902706414461136
In grad_steps = 2678, loss = 0.09351985156536102
In grad_steps = 2679, loss = 0.035753875970840454
In grad_steps = 2680, loss = 0.02185414358973503
In grad_steps = 2681, loss = 0.060701191425323486
In grad_steps = 2682, loss = 0.007365017663687468
In grad_steps = 2683, loss = 0.06269743293523788
In grad_steps = 2684, loss = 0.07276248186826706
In grad_steps = 2685, loss = 0.09170065820217133
In grad_steps = 2686, loss = 0.4630547761917114
In grad_steps = 2687, loss = 0.030604315921664238
In grad_steps = 2688, loss = 0.025989960879087448
In grad_steps = 2689, loss = 0.15239961445331573
In grad_steps = 2690, loss = 0.048553165048360825
In grad_steps = 2691, loss = 0.05031801015138626
In grad_steps = 2692, loss = 0.04931364580988884
In grad_steps = 2693, loss = 0.06968288868665695
In grad_steps = 2694, loss = 0.03377489373087883
In grad_steps = 2695, loss = 1.162635087966919
In grad_steps = 2696, loss = 0.3968788683414459
In grad_steps = 2697, loss = 0.23230800032615662
In grad_steps = 2698, loss = 0.08246999233961105
In grad_steps = 2699, loss = 0.03541838377714157
In grad_steps = 2700, loss = 0.03354714810848236
In grad_steps = 2701, loss = 1.0737565755844116
In grad_steps = 2702, loss = 0.9634571671485901
In grad_steps = 2703, loss = 0.4862785339355469
In grad_steps = 2704, loss = 0.1005321741104126
In grad_steps = 2705, loss = 0.13483279943466187
In grad_steps = 2706, loss = 0.04115491360425949
In grad_steps = 2707, loss = 0.18032899498939514
In grad_steps = 2708, loss = 0.7384061217308044
In grad_steps = 2709, loss = 0.22893112897872925
In grad_steps = 2710, loss = 0.4915558397769928
In grad_steps = 2711, loss = 0.06522725522518158
In grad_steps = 2712, loss = 0.4076940417289734
In grad_steps = 2713, loss = 0.4643447995185852
In grad_steps = 2714, loss = 0.11763627827167511
In grad_steps = 2715, loss = 0.20133855938911438
In grad_steps = 2716, loss = 0.06549417972564697
In grad_steps = 2717, loss = 0.12182678282260895
In grad_steps = 2718, loss = 0.11736364662647247
In grad_steps = 2719, loss = 0.08564186841249466
In grad_steps = 2720, loss = 0.13693565130233765
In grad_steps = 2721, loss = 0.1759575456380844
In grad_steps = 2722, loss = 0.07074910402297974
In grad_steps = 2723, loss = 0.8344224691390991
In grad_steps = 2724, loss = 0.03706936165690422
In grad_steps = 2725, loss = 0.052692804485559464
In grad_steps = 2726, loss = 0.11719224601984024
In grad_steps = 2727, loss = 0.07950443029403687
In grad_steps = 2728, loss = 0.029290419071912766
In grad_steps = 2729, loss = 0.02294863387942314
In grad_steps = 2730, loss = 0.1624574065208435
In grad_steps = 2731, loss = 0.20390352606773376
In grad_steps = 2732, loss = 0.09206312894821167
In grad_steps = 2733, loss = 0.04716735705733299
In grad_steps = 2734, loss = 0.5096816420555115
In grad_steps = 2735, loss = 0.1511266529560089
In grad_steps = 2736, loss = 0.015934616327285767
In grad_steps = 2737, loss = 0.026389148086309433
In grad_steps = 2738, loss = 0.08774520456790924
In grad_steps = 2739, loss = 1.0131851434707642
In grad_steps = 2740, loss = 0.023481791839003563
In grad_steps = 2741, loss = 0.9761555790901184
In grad_steps = 2742, loss = 1.201350212097168
In grad_steps = 2743, loss = 0.1213127076625824
In grad_steps = 2744, loss = 0.10467758774757385
In grad_steps = 2745, loss = 0.030027680099010468
In grad_steps = 2746, loss = 0.1898087114095688
In grad_steps = 2747, loss = 0.024780478328466415
In grad_steps = 2748, loss = 0.2658660411834717
In grad_steps = 2749, loss = 0.05655360594391823
In grad_steps = 2750, loss = 0.2044338434934616
In grad_steps = 2751, loss = 0.07769334316253662
In grad_steps = 2752, loss = 0.3667682707309723
In grad_steps = 2753, loss = 0.24474172294139862
In grad_steps = 2754, loss = 0.084940604865551
In grad_steps = 2755, loss = 0.05918281525373459
In grad_steps = 2756, loss = 0.5976276993751526
In grad_steps = 2757, loss = 0.2719555199146271
In grad_steps = 2758, loss = 0.04836486652493477
In grad_steps = 2759, loss = 1.318344235420227
In grad_steps = 2760, loss = 0.12282595783472061
In grad_steps = 2761, loss = 0.049173012375831604
In grad_steps = 2762, loss = 0.39672011137008667
In grad_steps = 2763, loss = 0.0641850009560585
In grad_steps = 2764, loss = 0.9001408219337463
In grad_steps = 2765, loss = 0.22971926629543304
In grad_steps = 2766, loss = 0.18930089473724365
In grad_steps = 2767, loss = 0.3901882469654083
In grad_steps = 2768, loss = 0.19584380090236664
In grad_steps = 2769, loss = 0.08900833874940872
In grad_steps = 2770, loss = 0.5415300726890564
In grad_steps = 2771, loss = 0.16608500480651855
In grad_steps = 2772, loss = 0.08098270744085312
In grad_steps = 2773, loss = 0.016756124794483185
In grad_steps = 2774, loss = 0.2832099497318268
In grad_steps = 2775, loss = 0.1759680211544037
In grad_steps = 2776, loss = 0.15683911740779877
In grad_steps = 2777, loss = 0.24247339367866516
In grad_steps = 2778, loss = 0.01807226426899433
In grad_steps = 2779, loss = 0.12537124752998352
In grad_steps = 2780, loss = 0.05516285449266434
In grad_steps = 2781, loss = 0.03180263191461563
In grad_steps = 2782, loss = 0.8043804168701172
In grad_steps = 2783, loss = 0.46841371059417725
In grad_steps = 2784, loss = 0.5591415762901306
In grad_steps = 2785, loss = 0.04309459775686264
In grad_steps = 2786, loss = 0.30293339490890503
In grad_steps = 2787, loss = 0.7121351957321167
In grad_steps = 2788, loss = 0.045002590864896774
In grad_steps = 2789, loss = 0.28182291984558105
In grad_steps = 2790, loss = 0.9249231815338135
In grad_steps = 2791, loss = 0.9607846736907959
In grad_steps = 2792, loss = 0.05755722522735596
In grad_steps = 2793, loss = 0.024952929466962814
In grad_steps = 2794, loss = 0.41436296701431274
In grad_steps = 2795, loss = 0.567123532295227
In grad_steps = 2796, loss = 0.14642684161663055
In grad_steps = 2797, loss = 0.12975171208381653
In grad_steps = 2798, loss = 0.0517553985118866
In grad_steps = 2799, loss = 0.07823961973190308
In grad_steps = 2800, loss = 0.06462282687425613
In grad_steps = 2801, loss = 0.26740211248397827
In grad_steps = 2802, loss = 0.1846521496772766
In grad_steps = 2803, loss = 0.06663957238197327
In grad_steps = 2804, loss = 0.0966024100780487
In grad_steps = 2805, loss = 0.2904326319694519
In grad_steps = 2806, loss = 0.1525149941444397
In grad_steps = 2807, loss = 0.2939346730709076
In grad_steps = 2808, loss = 0.02386809140443802
In grad_steps = 2809, loss = 0.895901083946228
In grad_steps = 2810, loss = 0.06909825652837753
In grad_steps = 2811, loss = 0.2707691192626953
In grad_steps = 2812, loss = 0.18624767661094666
In grad_steps = 2813, loss = 0.11319339275360107
In grad_steps = 2814, loss = 0.018177885562181473
In grad_steps = 2815, loss = 0.7027488350868225
In grad_steps = 2816, loss = 0.11200475692749023
In grad_steps = 2817, loss = 0.1792321801185608
In grad_steps = 2818, loss = 0.6467202305793762
In grad_steps = 2819, loss = 0.12505090236663818
In grad_steps = 2820, loss = 0.24832360446453094
In grad_steps = 2821, loss = 0.1986372172832489
In grad_steps = 2822, loss = 0.15822798013687134
In grad_steps = 2823, loss = 0.15689261257648468
In grad_steps = 2824, loss = 0.017291564494371414
In grad_steps = 2825, loss = 0.07317949831485748
In grad_steps = 2826, loss = 0.05670182406902313
In grad_steps = 2827, loss = 0.26699307560920715
In grad_steps = 2828, loss = 0.36051055788993835
In grad_steps = 2829, loss = 0.009014149196445942
In grad_steps = 2830, loss = 0.01993746869266033
In grad_steps = 2831, loss = 0.03296104073524475
In grad_steps = 2832, loss = 0.5383565425872803
In grad_steps = 2833, loss = 0.022125067189335823
In grad_steps = 2834, loss = 0.28682780265808105
In grad_steps = 2835, loss = 0.025325024500489235
In grad_steps = 2836, loss = 0.04379341006278992
In grad_steps = 2837, loss = 0.21845772862434387
In grad_steps = 2838, loss = 0.39860495924949646
In grad_steps = 2839, loss = 0.014577401801943779
In grad_steps = 2840, loss = 0.6259355545043945
In grad_steps = 2841, loss = 1.599900245666504
In grad_steps = 2842, loss = 0.7026489973068237
In grad_steps = 2843, loss = 0.26792287826538086
In grad_steps = 2844, loss = 0.33940619230270386
In grad_steps = 2845, loss = 0.07436177134513855
In grad_steps = 2846, loss = 0.035469118505716324
In grad_steps = 2847, loss = 0.6614401936531067
In grad_steps = 2848, loss = 0.04202422872185707
In grad_steps = 2849, loss = 0.24042381346225739
In grad_steps = 2850, loss = 0.05521438643336296
In grad_steps = 2851, loss = 0.4771210551261902
In grad_steps = 2852, loss = 0.08946135640144348
In grad_steps = 2853, loss = 0.0636131539940834
In grad_steps = 2854, loss = 0.3124447166919708
In grad_steps = 2855, loss = 0.46445953845977783
In grad_steps = 2856, loss = 0.4011598825454712
In grad_steps = 2857, loss = 0.3130713105201721
In grad_steps = 2858, loss = 0.4295114278793335
In grad_steps = 2859, loss = 0.12490826100111008
In grad_steps = 2860, loss = 0.46960213780403137
In grad_steps = 2861, loss = 0.7821091413497925
In grad_steps = 2862, loss = 0.3939947783946991
In grad_steps = 2863, loss = 0.03762824460864067
In grad_steps = 2864, loss = 0.3742208480834961
In grad_steps = 2865, loss = 0.17673251032829285
In grad_steps = 2866, loss = 0.1542106568813324
In grad_steps = 2867, loss = 0.12016107887029648
In grad_steps = 2868, loss = 0.12810063362121582
In grad_steps = 2869, loss = 0.4013473391532898
In grad_steps = 2870, loss = 0.3127439022064209
In grad_steps = 2871, loss = 0.2934393286705017
In grad_steps = 2872, loss = 0.2580322027206421
In grad_steps = 2873, loss = 0.09113342314958572
In grad_steps = 2874, loss = 0.2759750485420227
In grad_steps = 2875, loss = 0.0826473981142044
In grad_steps = 2876, loss = 0.039466071873903275
In grad_steps = 2877, loss = 0.026872489601373672
In grad_steps = 2878, loss = 0.12825675308704376
In grad_steps = 2879, loss = 0.14165012538433075
In grad_steps = 2880, loss = 0.08381843566894531
In grad_steps = 2881, loss = 0.30260926485061646
In grad_steps = 2882, loss = 0.009974321350455284
In grad_steps = 2883, loss = 0.02967703714966774
In grad_steps = 2884, loss = 0.38230305910110474
In grad_steps = 2885, loss = 0.06097971647977829
In grad_steps = 2886, loss = 0.02613215334713459
In grad_steps = 2887, loss = 1.139405608177185
In grad_steps = 2888, loss = 0.7325448393821716
In grad_steps = 2889, loss = 0.21408286690711975
In grad_steps = 2890, loss = 0.07260313630104065
In grad_steps = 2891, loss = 0.11146149039268494
In grad_steps = 2892, loss = 0.033781103789806366
In grad_steps = 2893, loss = 0.046030398458242416
In grad_steps = 2894, loss = 0.02034381963312626
In grad_steps = 2895, loss = 0.10002072155475616
In grad_steps = 2896, loss = 0.01082371361553669
In grad_steps = 2897, loss = 1.065024733543396
In grad_steps = 2898, loss = 0.7030068039894104
In grad_steps = 2899, loss = 0.03586631640791893
In grad_steps = 2900, loss = 0.6328118443489075
In grad_steps = 2901, loss = 0.05528315529227257
In grad_steps = 2902, loss = 0.18295349180698395
In grad_steps = 2903, loss = 0.9307058453559875
In grad_steps = 2904, loss = 0.07046248018741608
In grad_steps = 2905, loss = 0.47596222162246704
In grad_steps = 2906, loss = 0.12239854037761688
In grad_steps = 2907, loss = 0.21380701661109924
In grad_steps = 2908, loss = 0.25909698009490967
In grad_steps = 2909, loss = 0.8798342943191528
In grad_steps = 2910, loss = 0.24211707711219788
In grad_steps = 2911, loss = 0.5422889590263367
In grad_steps = 2912, loss = 0.4301098883152008
In grad_steps = 2913, loss = 0.6798101663589478
In grad_steps = 2914, loss = 0.23004424571990967
In grad_steps = 2915, loss = 0.1402289718389511
In grad_steps = 2916, loss = 0.24465733766555786
In grad_steps = 2917, loss = 0.8334662318229675
In grad_steps = 2918, loss = 0.33289834856987
In grad_steps = 2919, loss = 0.33075180649757385
In grad_steps = 2920, loss = 0.7952971458435059
In grad_steps = 2921, loss = 0.2844899594783783
In grad_steps = 2922, loss = 0.05794291943311691
In grad_steps = 2923, loss = 0.1415879726409912
In grad_steps = 2924, loss = 0.07552618533372879
In grad_steps = 2925, loss = 0.17825019359588623
In grad_steps = 2926, loss = 0.0640929639339447
In grad_steps = 2927, loss = 0.19870977103710175
In grad_steps = 2928, loss = 0.1132989376783371
In grad_steps = 2929, loss = 0.18516905605793
In grad_steps = 2930, loss = 0.09138220548629761
In grad_steps = 2931, loss = 0.071178138256073
In grad_steps = 2932, loss = 0.3114626407623291
In grad_steps = 2933, loss = 0.6026460528373718
In grad_steps = 2934, loss = 0.3453875482082367
In grad_steps = 2935, loss = 0.1565767228603363
In grad_steps = 2936, loss = 0.131938636302948
In grad_steps = 2937, loss = 0.045562997460365295
In grad_steps = 2938, loss = 0.04813143610954285
In grad_steps = 2939, loss = 0.085944764316082
In grad_steps = 2940, loss = 0.4130447506904602
In grad_steps = 2941, loss = 0.316038578748703
In grad_steps = 2942, loss = 0.9658530354499817
In grad_steps = 2943, loss = 0.09855902940034866
In grad_steps = 2944, loss = 0.17393112182617188
In grad_steps = 2945, loss = 0.9645769596099854
In grad_steps = 2946, loss = 0.050249241292476654
In grad_steps = 2947, loss = 0.0787133276462555
In grad_steps = 2948, loss = 0.15152327716350555
In grad_steps = 2949, loss = 0.3213612139225006
In grad_steps = 2950, loss = 0.4239364266395569
In grad_steps = 2951, loss = 0.1326858252286911
In grad_steps = 2952, loss = 0.29358622431755066
In grad_steps = 2953, loss = 0.3869398534297943
In grad_steps = 2954, loss = 0.018298029899597168
In grad_steps = 2955, loss = 0.04973958432674408
In grad_steps = 2956, loss = 0.05085643753409386
In grad_steps = 2957, loss = 0.06859052181243896
In grad_steps = 2958, loss = 0.02765679731965065
In grad_steps = 2959, loss = 0.08270987868309021
In grad_steps = 2960, loss = 0.13172678649425507
In grad_steps = 2961, loss = 0.03266428783535957
In grad_steps = 2962, loss = 0.08379265666007996
In grad_steps = 2963, loss = 0.32969069480895996
In grad_steps = 2964, loss = 0.543553352355957
In grad_steps = 2965, loss = 0.08618713915348053
In grad_steps = 2966, loss = 0.3872798979282379
In grad_steps = 2967, loss = 0.28707534074783325
In grad_steps = 2968, loss = 0.06546298414468765
In grad_steps = 2969, loss = 0.0663338154554367
In grad_steps = 2970, loss = 0.01107831671833992
In grad_steps = 2971, loss = 0.39917314052581787
In grad_steps = 2972, loss = 0.017108367756009102
In grad_steps = 2973, loss = 0.25442254543304443
In grad_steps = 2974, loss = 1.768714427947998
In grad_steps = 2975, loss = 0.07907245308160782
In grad_steps = 2976, loss = 0.05724914371967316
In grad_steps = 2977, loss = 0.03402320668101311
In grad_steps = 2978, loss = 0.2965414524078369
In grad_steps = 2979, loss = 0.4861028492450714
In grad_steps = 2980, loss = 0.02474208176136017
In grad_steps = 2981, loss = 0.1932479739189148
In grad_steps = 2982, loss = 0.10572279989719391
In grad_steps = 2983, loss = 0.13245318830013275
In grad_steps = 2984, loss = 0.0451267845928669
In grad_steps = 2985, loss = 0.06523219496011734
In grad_steps = 2986, loss = 0.11474663019180298
In grad_steps = 2987, loss = 0.03252299502491951
In grad_steps = 2988, loss = 0.5569184422492981
In grad_steps = 2989, loss = 0.0411083847284317
In grad_steps = 2990, loss = 0.10998894274234772
In grad_steps = 2991, loss = 0.012734849005937576
In grad_steps = 2992, loss = 0.886702299118042
In grad_steps = 2993, loss = 0.25098830461502075
In grad_steps = 2994, loss = 0.0238829143345356
In grad_steps = 2995, loss = 0.0674295499920845
In grad_steps = 2996, loss = 0.03538552299141884
In grad_steps = 2997, loss = 0.19865836203098297
In grad_steps = 2998, loss = 1.220262050628662
In grad_steps = 2999, loss = 0.021309545263648033
In grad_steps = 3000, loss = 0.10580608993768692
In grad_steps = 3001, loss = 0.08171339333057404
In grad_steps = 3002, loss = 0.10949018597602844
In grad_steps = 3003, loss = 0.45607221126556396
In grad_steps = 3004, loss = 0.058692239224910736
In grad_steps = 3005, loss = 1.7600659132003784
In grad_steps = 3006, loss = 0.7840704917907715
In grad_steps = 3007, loss = 0.7795220017433167
In grad_steps = 3008, loss = 0.7462827563285828
In grad_steps = 3009, loss = 0.034276220947504044
In grad_steps = 3010, loss = 0.04742716625332832
In grad_steps = 3011, loss = 0.27967414259910583
In grad_steps = 3012, loss = 0.029558591544628143
In grad_steps = 3013, loss = 0.08678268641233444
In grad_steps = 3014, loss = 0.06391973793506622
In grad_steps = 3015, loss = 0.746103823184967
In grad_steps = 3016, loss = 0.08679529279470444
In grad_steps = 3017, loss = 0.06072070077061653
In grad_steps = 3018, loss = 0.09806030243635178
In grad_steps = 3019, loss = 0.10769711434841156
In grad_steps = 3020, loss = 0.1963501274585724
In grad_steps = 3021, loss = 0.10187728703022003
In grad_steps = 3022, loss = 0.7386401295661926
In grad_steps = 3023, loss = 0.14544516801834106
In grad_steps = 3024, loss = 0.7445928454399109
In grad_steps = 3025, loss = 0.07175391167402267
In grad_steps = 3026, loss = 1.1217085123062134
In grad_steps = 3027, loss = 0.05870986729860306
In grad_steps = 3028, loss = 0.1425778865814209
In grad_steps = 3029, loss = 0.08110606670379639
In grad_steps = 3030, loss = 0.23427140712738037
In grad_steps = 3031, loss = 0.08896027505397797
In grad_steps = 3032, loss = 0.20531809329986572
In grad_steps = 3033, loss = 0.03170774504542351
In grad_steps = 3034, loss = 0.6442798376083374
In grad_steps = 3035, loss = 0.29911378026008606
In grad_steps = 3036, loss = 0.4175129532814026
In grad_steps = 3037, loss = 0.10841333866119385
In grad_steps = 3038, loss = 0.13640588521957397
In grad_steps = 3039, loss = 0.19737927615642548
In grad_steps = 3040, loss = 0.5437992215156555
In grad_steps = 3041, loss = 0.37216809391975403
In grad_steps = 3042, loss = 0.031065117567777634
In grad_steps = 3043, loss = 0.41628602147102356
In grad_steps = 3044, loss = 0.6151334643363953
In grad_steps = 3045, loss = 0.07994063198566437
In grad_steps = 3046, loss = 0.1631021797657013
In grad_steps = 3047, loss = 0.10097190737724304
In grad_steps = 3048, loss = 0.09968733042478561
In grad_steps = 3049, loss = 0.40199464559555054
In grad_steps = 3050, loss = 0.45083093643188477
In grad_steps = 3051, loss = 0.05715295672416687
In grad_steps = 3052, loss = 0.3322049081325531
In grad_steps = 3053, loss = 0.2822127044200897
In grad_steps = 3054, loss = 0.030484789982438087
In grad_steps = 3055, loss = 0.06744129955768585
In grad_steps = 3056, loss = 0.03418838232755661
In grad_steps = 3057, loss = 0.04893755912780762
In grad_steps = 3058, loss = 0.06421337276697159
In grad_steps = 3059, loss = 0.07604077458381653
In grad_steps = 3060, loss = 0.029123496264219284
In grad_steps = 3061, loss = 0.014311565086245537
In grad_steps = 3062, loss = 0.027559569105505943
In grad_steps = 3063, loss = 0.03986119106411934
In grad_steps = 3064, loss = 0.5186843872070312
In grad_steps = 3065, loss = 0.5871115922927856
In grad_steps = 3066, loss = 0.2761281132698059
In grad_steps = 3067, loss = 1.5131378173828125
In grad_steps = 3068, loss = 1.940453052520752
In grad_steps = 3069, loss = 0.06471331417560577
In grad_steps = 3070, loss = 0.04258904233574867
In grad_steps = 3071, loss = 0.037222135812044144
In grad_steps = 3072, loss = 0.037967879325151443
In grad_steps = 3073, loss = 0.019606757909059525
In grad_steps = 3074, loss = 1.572021484375
In grad_steps = 3075, loss = 0.7554284930229187
In grad_steps = 3076, loss = 0.7647259831428528
In grad_steps = 3077, loss = 0.05588739737868309
In grad_steps = 3078, loss = 0.07350480556488037
In grad_steps = 3079, loss = 0.30653661489486694
In grad_steps = 3080, loss = 0.08760468661785126
In grad_steps = 3081, loss = 0.049452733248472214
In grad_steps = 3082, loss = 0.06448960304260254
In grad_steps = 3083, loss = 0.06570039689540863
In grad_steps = 3084, loss = 0.1751251518726349
In grad_steps = 3085, loss = 0.34544962644577026
In grad_steps = 3086, loss = 1.1720194816589355
In grad_steps = 3087, loss = 0.2194744050502777
In grad_steps = 3088, loss = 0.500001072883606
In grad_steps = 3089, loss = 0.07162823528051376
In grad_steps = 3090, loss = 0.0689452737569809
In grad_steps = 3091, loss = 0.7883307933807373
In grad_steps = 3092, loss = 0.12351928651332855
In grad_steps = 3093, loss = 0.44342219829559326
In grad_steps = 3094, loss = 0.24683067202568054
In grad_steps = 3095, loss = 0.05578339099884033
In grad_steps = 3096, loss = 0.06847260892391205
In grad_steps = 3097, loss = 0.8567948341369629
In grad_steps = 3098, loss = 0.6244907975196838
In grad_steps = 3099, loss = 0.15188193321228027
In grad_steps = 3100, loss = 0.43575596809387207
In grad_steps = 3101, loss = 0.11393054574728012
In grad_steps = 3102, loss = 0.5017011165618896
In grad_steps = 3103, loss = 0.2965250015258789
In grad_steps = 3104, loss = 0.11294227093458176
In grad_steps = 3105, loss = 0.11938980221748352
In grad_steps = 3106, loss = 0.16332462430000305
In grad_steps = 3107, loss = 0.3644893765449524
In grad_steps = 3108, loss = 0.4043610394001007
In grad_steps = 3109, loss = 0.14059893786907196
In grad_steps = 3110, loss = 0.1364791989326477
In grad_steps = 3111, loss = 0.8246009349822998
In grad_steps = 3112, loss = 0.38501113653182983
In grad_steps = 3113, loss = 0.1819605529308319
In grad_steps = 3114, loss = 0.40001869201660156
In grad_steps = 3115, loss = 0.2733670473098755
In grad_steps = 3116, loss = 0.14155761897563934
In grad_steps = 3117, loss = 0.34208041429519653
In grad_steps = 3118, loss = 0.5446633696556091
In grad_steps = 3119, loss = 0.030964087694883347
In grad_steps = 3120, loss = 0.14467072486877441
In grad_steps = 3121, loss = 0.07814767211675644
In grad_steps = 3122, loss = 0.7207522392272949
In grad_steps = 3123, loss = 0.18047255277633667
In grad_steps = 3124, loss = 0.08036583662033081
In grad_steps = 3125, loss = 0.22419121861457825
In grad_steps = 3126, loss = 1.6097520589828491
In grad_steps = 3127, loss = 0.03246702998876572
In grad_steps = 3128, loss = 0.10216322541236877
In grad_steps = 3129, loss = 0.06410111486911774
In grad_steps = 3130, loss = 0.04420362040400505
In grad_steps = 3131, loss = 0.10682906955480576
In grad_steps = 3132, loss = 0.1958330124616623
In grad_steps = 3133, loss = 0.17733989655971527
In grad_steps = 3134, loss = 0.11862900853157043
In grad_steps = 3135, loss = 0.03707905858755112
In grad_steps = 3136, loss = 0.8700144290924072
In grad_steps = 3137, loss = 0.5954654812812805
In grad_steps = 3138, loss = 0.021195147186517715
In grad_steps = 3139, loss = 0.03540663421154022
In grad_steps = 3140, loss = 0.10021188855171204
In grad_steps = 3141, loss = 0.2103530764579773
In grad_steps = 3142, loss = 0.1632857471704483
In grad_steps = 3143, loss = 0.10585696250200272
In grad_steps = 3144, loss = 0.056293316185474396
In grad_steps = 3145, loss = 0.02589801326394081
In grad_steps = 3146, loss = 0.2879125475883484
In grad_steps = 3147, loss = 1.2522318363189697
In grad_steps = 3148, loss = 0.4586547613143921
In grad_steps = 3149, loss = 0.025175539776682854
In grad_steps = 3150, loss = 0.8208405375480652
In grad_steps = 3151, loss = 0.07031174749135971
In grad_steps = 3152, loss = 0.43519169092178345
In grad_steps = 3153, loss = 0.24093937873840332
In grad_steps = 3154, loss = 0.07868444919586182
In grad_steps = 3155, loss = 0.126329243183136
In grad_steps = 3156, loss = 0.0587196871638298
In grad_steps = 3157, loss = 0.030994856730103493
In grad_steps = 3158, loss = 0.47506648302078247
In grad_steps = 3159, loss = 0.03212755173444748
In grad_steps = 3160, loss = 0.14948409795761108
In grad_steps = 3161, loss = 0.17723481357097626
In grad_steps = 3162, loss = 0.06308220326900482
In grad_steps = 3163, loss = 0.7196508646011353
In grad_steps = 3164, loss = 0.030046768486499786
In grad_steps = 3165, loss = 0.1897541582584381
In grad_steps = 3166, loss = 0.2642737627029419
In grad_steps = 3167, loss = 0.07318040728569031
In grad_steps = 3168, loss = 0.06215690076351166
In grad_steps = 3169, loss = 0.609639048576355
In grad_steps = 3170, loss = 0.02027883753180504
In grad_steps = 3171, loss = 0.3180507719516754
In grad_steps = 3172, loss = 0.1364182084798813
In grad_steps = 3173, loss = 0.02546584978699684
In grad_steps = 3174, loss = 0.20466704666614532
In grad_steps = 3175, loss = 0.20876553654670715
In grad_steps = 3176, loss = 0.12163389474153519
In grad_steps = 3177, loss = 0.013470102101564407
In grad_steps = 3178, loss = 0.019753335043787956
In grad_steps = 3179, loss = 1.0741115808486938
In grad_steps = 3180, loss = 0.08498815447092056
In grad_steps = 3181, loss = 0.6059535145759583
In grad_steps = 3182, loss = 0.4613085389137268
In grad_steps = 3183, loss = 0.5410169363021851
In grad_steps = 3184, loss = 0.2367696315050125
In grad_steps = 3185, loss = 0.8563466668128967
In grad_steps = 3186, loss = 0.5151424407958984
In grad_steps = 3187, loss = 0.14960844814777374
In grad_steps = 3188, loss = 1.239180088043213
In grad_steps = 3189, loss = 0.2579953372478485
In grad_steps = 3190, loss = 0.100584976375103
In grad_steps = 3191, loss = 0.38512587547302246
In grad_steps = 3192, loss = 0.16562877595424652
In grad_steps = 3193, loss = 0.10392651706933975
In grad_steps = 3194, loss = 0.7095048427581787
In grad_steps = 3195, loss = 0.061089321970939636
In grad_steps = 3196, loss = 0.5403268337249756
In grad_steps = 3197, loss = 0.549968957901001
In grad_steps = 3198, loss = 0.08580950647592545
In grad_steps = 3199, loss = 0.31338921189308167
In grad_steps = 3200, loss = 0.1127704530954361
In grad_steps = 3201, loss = 1.2726398706436157
In grad_steps = 3202, loss = 0.504264235496521
In grad_steps = 3203, loss = 0.46402621269226074
In grad_steps = 3204, loss = 0.5122395753860474
In grad_steps = 3205, loss = 0.4164847731590271
In grad_steps = 3206, loss = 0.12276172637939453
In grad_steps = 3207, loss = 0.11197589337825775
In grad_steps = 3208, loss = 0.4337678551673889
In grad_steps = 3209, loss = 0.17003092169761658
In grad_steps = 3210, loss = 0.3061852753162384
In grad_steps = 3211, loss = 0.5112636685371399
In grad_steps = 3212, loss = 0.12590596079826355
In grad_steps = 3213, loss = 0.36109617352485657
In grad_steps = 3214, loss = 0.5035690069198608
In grad_steps = 3215, loss = 0.06663727015256882
In grad_steps = 3216, loss = 0.17510850727558136
In grad_steps = 3217, loss = 0.12501181662082672
In grad_steps = 3218, loss = 0.20310063660144806
In grad_steps = 3219, loss = 0.10566415637731552
In grad_steps = 3220, loss = 0.16657030582427979
In grad_steps = 3221, loss = 0.18818382918834686
In grad_steps = 3222, loss = 0.19682356715202332
In grad_steps = 3223, loss = 0.14846166968345642
In grad_steps = 3224, loss = 0.0729864090681076
In grad_steps = 3225, loss = 0.5655852556228638
In grad_steps = 3226, loss = 0.3718702793121338
In grad_steps = 3227, loss = 0.09049446880817413
In grad_steps = 3228, loss = 0.15592843294143677
In grad_steps = 3229, loss = 0.04293669015169144
In grad_steps = 3230, loss = 0.0135788070037961
In grad_steps = 3231, loss = 0.01946377009153366
In grad_steps = 3232, loss = 0.5264029502868652
In grad_steps = 3233, loss = 0.027977684512734413
In grad_steps = 3234, loss = 0.5693889856338501
In grad_steps = 3235, loss = 0.011310476809740067
In grad_steps = 3236, loss = 0.0064880335703492165
In grad_steps = 3237, loss = 0.006062908563762903
In grad_steps = 3238, loss = 0.009808779694139957
In grad_steps = 3239, loss = 0.09445883333683014
In grad_steps = 3240, loss = 0.007401809096336365
In grad_steps = 3241, loss = 0.06720342487096786
In grad_steps = 3242, loss = 1.1775799989700317
In grad_steps = 3243, loss = 0.7528247237205505
In grad_steps = 3244, loss = 0.1368059366941452
In grad_steps = 3245, loss = 1.656988263130188
In grad_steps = 3246, loss = 0.1268310397863388
In grad_steps = 3247, loss = 0.08416101336479187
In grad_steps = 3248, loss = 0.19914595782756805
In grad_steps = 3249, loss = 0.1630113124847412
In grad_steps = 3250, loss = 0.24554623663425446
In grad_steps = 3251, loss = 0.7648691534996033
In grad_steps = 3252, loss = 0.11819280683994293
In grad_steps = 3253, loss = 0.22299131751060486
In grad_steps = 3254, loss = 0.20795972645282745
In grad_steps = 3255, loss = 0.11816675215959549
In grad_steps = 3256, loss = 0.3371365964412689
In grad_steps = 3257, loss = 0.24804411828517914
In grad_steps = 3258, loss = 0.22818788886070251
In grad_steps = 3259, loss = 0.2820538282394409
In grad_steps = 3260, loss = 0.053690843284130096
In grad_steps = 3261, loss = 0.04708249866962433
In grad_steps = 3262, loss = 0.1307240128517151
In grad_steps = 3263, loss = 0.04176507145166397
In grad_steps = 3264, loss = 0.04650065302848816
In grad_steps = 3265, loss = 0.0767802819609642
In grad_steps = 3266, loss = 0.7172415852546692
In grad_steps = 3267, loss = 0.5594579577445984
In grad_steps = 3268, loss = 0.11237852275371552
In grad_steps = 3269, loss = 0.21935594081878662
In grad_steps = 3270, loss = 0.02731778845191002
In grad_steps = 3271, loss = 0.9809020757675171
In grad_steps = 3272, loss = 1.4538350105285645
In grad_steps = 3273, loss = 0.9850295782089233
In grad_steps = 3274, loss = 0.7207883596420288
In grad_steps = 3275, loss = 0.028200320899486542
In grad_steps = 3276, loss = 1.2257113456726074
In grad_steps = 3277, loss = 0.7987183332443237
In grad_steps = 3278, loss = 0.06450340151786804
In grad_steps = 3279, loss = 0.17836745083332062
In grad_steps = 3280, loss = 0.2942868769168854
In grad_steps = 3281, loss = 0.9753156304359436
In grad_steps = 3282, loss = 0.1269875168800354
In grad_steps = 3283, loss = 0.3586726784706116
In grad_steps = 3284, loss = 0.1630929410457611
In grad_steps = 3285, loss = 0.424513041973114
In grad_steps = 3286, loss = 0.3460148870944977
In grad_steps = 3287, loss = 0.07146104425191879
In grad_steps = 3288, loss = 0.1131683886051178
In grad_steps = 3289, loss = 0.17864741384983063
In grad_steps = 3290, loss = 0.58657306432724
In grad_steps = 3291, loss = 0.29110628366470337
In grad_steps = 3292, loss = 0.3991541862487793
In grad_steps = 3293, loss = 0.27998286485671997
In grad_steps = 3294, loss = 0.09541677683591843
In grad_steps = 3295, loss = 0.15167534351348877
In grad_steps = 3296, loss = 0.5555514097213745
In grad_steps = 3297, loss = 0.11786434799432755
In grad_steps = 3298, loss = 0.07365482300519943
In grad_steps = 3299, loss = 0.23697814345359802
In grad_steps = 3300, loss = 0.4716746509075165
In grad_steps = 3301, loss = 0.28617554903030396
In grad_steps = 3302, loss = 0.6146391034126282
In grad_steps = 3303, loss = 0.09929098188877106
In grad_steps = 3304, loss = 0.515177845954895
In grad_steps = 3305, loss = 0.19258897006511688
In grad_steps = 3306, loss = 0.40330642461776733
In grad_steps = 3307, loss = 0.2104935646057129
In grad_steps = 3308, loss = 0.31471481919288635
In grad_steps = 3309, loss = 0.46847569942474365
In grad_steps = 3310, loss = 0.22805441915988922
In grad_steps = 3311, loss = 0.26873141527175903
In grad_steps = 3312, loss = 0.15477517247200012
In grad_steps = 3313, loss = 0.05197267234325409
In grad_steps = 3314, loss = 0.08452879637479782
In grad_steps = 3315, loss = 0.04145238175988197
In grad_steps = 3316, loss = 0.5535838007926941
In grad_steps = 3317, loss = 0.03600602596998215
In grad_steps = 3318, loss = 0.6739194393157959
In grad_steps = 3319, loss = 0.09423885494470596
In grad_steps = 3320, loss = 0.6423110961914062
In grad_steps = 3321, loss = 0.06503091007471085
In grad_steps = 3322, loss = 0.08912859857082367
In grad_steps = 3323, loss = 0.32544073462486267
In grad_steps = 3324, loss = 0.034100793302059174
In grad_steps = 3325, loss = 0.03821519762277603
In grad_steps = 3326, loss = 0.2807895839214325
In grad_steps = 3327, loss = 0.022258101031184196
In grad_steps = 3328, loss = 0.07660950720310211
In grad_steps = 3329, loss = 0.06128811836242676
In grad_steps = 3330, loss = 0.02975984290242195
In grad_steps = 3331, loss = 0.03052574023604393
In grad_steps = 3332, loss = 0.1746029555797577
In grad_steps = 3333, loss = 0.06861632317304611
In grad_steps = 3334, loss = 0.10369077324867249
In grad_steps = 3335, loss = 0.2879805564880371
In grad_steps = 3336, loss = 0.02370722033083439
In grad_steps = 3337, loss = 0.011136802844703197
In grad_steps = 3338, loss = 0.043921057134866714
In grad_steps = 3339, loss = 0.040914684534072876
In grad_steps = 3340, loss = 0.01908307708799839
In grad_steps = 3341, loss = 0.026079086586833
In grad_steps = 3342, loss = 0.005645632743835449
In grad_steps = 3343, loss = 0.006030194461345673
In grad_steps = 3344, loss = 0.0998833179473877
In grad_steps = 3345, loss = 0.2814532518386841
In grad_steps = 3346, loss = 0.07981748133897781
In grad_steps = 3347, loss = 0.4876663088798523
In grad_steps = 3348, loss = 0.0023170930799096823
In grad_steps = 3349, loss = 0.2604379653930664
In grad_steps = 3350, loss = 0.04520953819155693
In grad_steps = 3351, loss = 0.016154002398252487
In grad_steps = 3352, loss = 0.013263227418065071
In grad_steps = 3353, loss = 0.2772064805030823
In grad_steps = 3354, loss = 0.08660252392292023
In grad_steps = 3355, loss = 0.07409795373678207
In grad_steps = 3356, loss = 0.7341706156730652
In grad_steps = 3357, loss = 0.017574695870280266
In grad_steps = 3358, loss = 0.37696021795272827
In grad_steps = 3359, loss = 0.22588610649108887
In grad_steps = 3360, loss = 0.0046930620446801186
In grad_steps = 3361, loss = 0.003397305030375719
In grad_steps = 3362, loss = 0.0057633668184280396
In grad_steps = 3363, loss = 0.0026422080118209124
In grad_steps = 3364, loss = 0.021509310230612755
In grad_steps = 3365, loss = 0.32577574253082275
In grad_steps = 3366, loss = 0.15107113122940063
In grad_steps = 3367, loss = 0.12562914192676544
In grad_steps = 3368, loss = 0.02129986323416233
In grad_steps = 3369, loss = 0.0008165647741407156
In grad_steps = 3370, loss = 0.20838330686092377
In grad_steps = 3371, loss = 0.23035334050655365
In grad_steps = 3372, loss = 1.1940566301345825
In grad_steps = 3373, loss = 0.13692981004714966
In grad_steps = 3374, loss = 0.5842718482017517
In grad_steps = 3375, loss = 1.4340527057647705
In grad_steps = 3376, loss = 0.2398606389760971
In grad_steps = 3377, loss = 0.14710012078285217
In grad_steps = 3378, loss = 0.6470820903778076
In grad_steps = 3379, loss = 0.3127148151397705
In grad_steps = 3380, loss = 0.09547609090805054
In grad_steps = 3381, loss = 0.04957238957285881
In grad_steps = 3382, loss = 0.2282499521970749
In grad_steps = 3383, loss = 0.39861464500427246
In grad_steps = 3384, loss = 0.6985728144645691
In grad_steps = 3385, loss = 0.042093947529792786
In grad_steps = 3386, loss = 0.27318161725997925
In grad_steps = 3387, loss = 0.05923985317349434
In grad_steps = 3388, loss = 0.32026407122612
In grad_steps = 3389, loss = 0.06634408980607986
In grad_steps = 3390, loss = 0.270835280418396
In grad_steps = 3391, loss = 0.23748917877674103
In grad_steps = 3392, loss = 0.0960792601108551
In grad_steps = 3393, loss = 0.3506114184856415
In grad_steps = 3394, loss = 0.010170826688408852
In grad_steps = 3395, loss = 0.22811108827590942
In grad_steps = 3396, loss = 0.04665062949061394
In grad_steps = 3397, loss = 0.49922341108322144
In grad_steps = 3398, loss = 0.03603418543934822
In grad_steps = 3399, loss = 0.2084570825099945
In grad_steps = 3400, loss = 0.06511856615543365
In grad_steps = 3401, loss = 0.02740675024688244
In grad_steps = 3402, loss = 0.059034593403339386
In grad_steps = 3403, loss = 1.110994577407837
In grad_steps = 3404, loss = 0.11702632158994675
In grad_steps = 3405, loss = 0.532111644744873
In grad_steps = 3406, loss = 0.015020252205431461
In grad_steps = 3407, loss = 0.20378762483596802
In grad_steps = 3408, loss = 0.7942136526107788
In grad_steps = 3409, loss = 0.06868012249469757
In grad_steps = 3410, loss = 0.07335952669382095
In grad_steps = 3411, loss = 0.006714518181979656
In grad_steps = 3412, loss = 0.03329281508922577
In grad_steps = 3413, loss = 0.08919647336006165
In grad_steps = 3414, loss = 0.026658473536372185
In grad_steps = 3415, loss = 0.017315054312348366
In grad_steps = 3416, loss = 0.9568077921867371
In grad_steps = 3417, loss = 0.018187079578638077
In grad_steps = 3418, loss = 0.014453415758907795
In grad_steps = 3419, loss = 0.3714435398578644
In grad_steps = 3420, loss = 0.1606983095407486
In grad_steps = 3421, loss = 0.034543465822935104
In grad_steps = 3422, loss = 0.22732634842395782
In grad_steps = 3423, loss = 0.5189598798751831
In grad_steps = 3424, loss = 0.13173949718475342
In grad_steps = 3425, loss = 0.1545206755399704
In grad_steps = 3426, loss = 1.1106210947036743
In grad_steps = 3427, loss = 0.02219466306269169
In grad_steps = 3428, loss = 0.02779463678598404
In grad_steps = 3429, loss = 1.2467278242111206
In grad_steps = 3430, loss = 0.3460124135017395
In grad_steps = 3431, loss = 0.10180217027664185
In grad_steps = 3432, loss = 0.8759472966194153
In grad_steps = 3433, loss = 0.08447402715682983
In grad_steps = 3434, loss = 0.10944828391075134
In grad_steps = 3435, loss = 0.30249831080436707
In grad_steps = 3436, loss = 0.05712514370679855
In grad_steps = 3437, loss = 0.10344640910625458
In grad_steps = 3438, loss = 0.17814740538597107
In grad_steps = 3439, loss = 0.06537576764822006
In grad_steps = 3440, loss = 0.05316203460097313
In grad_steps = 3441, loss = 1.2261850833892822
In grad_steps = 3442, loss = 0.5294187068939209
In grad_steps = 3443, loss = 0.09986844658851624
In grad_steps = 3444, loss = 0.16957105696201324
In grad_steps = 3445, loss = 0.2953086793422699
In grad_steps = 3446, loss = 0.06767552345991135
In grad_steps = 3447, loss = 0.12172548472881317
In grad_steps = 3448, loss = 0.10541978478431702
In grad_steps = 3449, loss = 0.0492931492626667
In grad_steps = 3450, loss = 0.15627355873584747
In grad_steps = 3451, loss = 0.12441681325435638
In grad_steps = 3452, loss = 0.07751407474279404
In grad_steps = 3453, loss = 0.1603851616382599
In grad_steps = 3454, loss = 0.047137223184108734
In grad_steps = 3455, loss = 0.11867951601743698
In grad_steps = 3456, loss = 0.7792437076568604
In grad_steps = 3457, loss = 0.042531147599220276
In grad_steps = 3458, loss = 0.03758285939693451
In grad_steps = 3459, loss = 0.5869399309158325
In grad_steps = 3460, loss = 0.6381021738052368
In grad_steps = 3461, loss = 0.10423959046602249
In grad_steps = 3462, loss = 0.5890901684761047
In grad_steps = 3463, loss = 0.07734451442956924
In grad_steps = 3464, loss = 0.11760936677455902
In grad_steps = 3465, loss = 0.5163201689720154
In grad_steps = 3466, loss = 0.07692675292491913
In grad_steps = 3467, loss = 0.5662190914154053
In grad_steps = 3468, loss = 0.0802970826625824
In grad_steps = 3469, loss = 0.09623438119888306
In grad_steps = 3470, loss = 0.43282097578048706
In grad_steps = 3471, loss = 0.535466194152832
In grad_steps = 3472, loss = 0.07144643366336823
In grad_steps = 3473, loss = 0.0344986654818058
In grad_steps = 3474, loss = 0.02395532839000225
In grad_steps = 3475, loss = 0.16858920454978943
In grad_steps = 3476, loss = 0.07698983699083328
In grad_steps = 3477, loss = 0.15325185656547546
In grad_steps = 3478, loss = 0.02424270287156105
In grad_steps = 3479, loss = 0.03591283783316612
In grad_steps = 3480, loss = 0.06244586035609245
In grad_steps = 3481, loss = 0.031142866238951683
In grad_steps = 3482, loss = 0.0484657846391201
In grad_steps = 3483, loss = 0.828717052936554
In grad_steps = 3484, loss = 0.14366553723812103
In grad_steps = 3485, loss = 0.1704641431570053
In grad_steps = 3486, loss = 0.02289009653031826
In grad_steps = 3487, loss = 0.8509660363197327
In grad_steps = 3488, loss = 0.7969405055046082
In grad_steps = 3489, loss = 0.024907507002353668
In grad_steps = 3490, loss = 0.9803171157836914
In grad_steps = 3491, loss = 0.4109920859336853
In grad_steps = 3492, loss = 0.027963856235146523
In grad_steps = 3493, loss = 0.4213254451751709
In grad_steps = 3494, loss = 0.06674778461456299
In grad_steps = 3495, loss = 0.9294251203536987
In grad_steps = 3496, loss = 0.13405442237854004
In grad_steps = 3497, loss = 0.07746542990207672
In grad_steps = 3498, loss = 0.1957072615623474
In grad_steps = 3499, loss = 0.15615803003311157
In grad_steps = 3500, loss = 0.08208394795656204
In grad_steps = 3501, loss = 0.3928203880786896
In grad_steps = 3502, loss = 0.16889649629592896
In grad_steps = 3503, loss = 0.07699977606534958
In grad_steps = 3504, loss = 0.5439426898956299
In grad_steps = 3505, loss = 0.37077295780181885
In grad_steps = 3506, loss = 0.4929666221141815
In grad_steps = 3507, loss = 0.07000327855348587
In grad_steps = 3508, loss = 0.24151493608951569
In grad_steps = 3509, loss = 0.15887124836444855
In grad_steps = 3510, loss = 0.04783663898706436
In grad_steps = 3511, loss = 0.15261799097061157
In grad_steps = 3512, loss = 0.04213416576385498
In grad_steps = 3513, loss = 0.5095551609992981
In grad_steps = 3514, loss = 0.17747461795806885
In grad_steps = 3515, loss = 0.05066032335162163
In grad_steps = 3516, loss = 0.10508744418621063
In grad_steps = 3517, loss = 0.03145507350564003
In grad_steps = 3518, loss = 0.16457688808441162
In grad_steps = 3519, loss = 0.29091134667396545
In grad_steps = 3520, loss = 0.08022172003984451
In grad_steps = 3521, loss = 0.22753453254699707
In grad_steps = 3522, loss = 1.1312470436096191
In grad_steps = 3523, loss = 0.1939706653356552
In grad_steps = 3524, loss = 0.07054820656776428
In grad_steps = 3525, loss = 0.018931550905108452
In grad_steps = 3526, loss = 0.19496023654937744
In grad_steps = 3527, loss = 0.3714202642440796
In grad_steps = 3528, loss = 0.1649971604347229
In grad_steps = 3529, loss = 0.6823114156723022
In grad_steps = 3530, loss = 0.08665188401937485
In grad_steps = 3531, loss = 0.030405249446630478
In grad_steps = 3532, loss = 0.041195884346961975
In grad_steps = 3533, loss = 0.18082071840763092
In grad_steps = 3534, loss = 0.7996604442596436
In grad_steps = 3535, loss = 0.7024508118629456
In grad_steps = 3536, loss = 0.10149755328893661
In grad_steps = 3537, loss = 0.0565129779279232
In grad_steps = 3538, loss = 0.7896196842193604
In grad_steps = 3539, loss = 0.07804165035486221
In grad_steps = 3540, loss = 0.2489154189825058
In grad_steps = 3541, loss = 0.048366472125053406
In grad_steps = 3542, loss = 0.08701913058757782
In grad_steps = 3543, loss = 0.2439972460269928
In grad_steps = 3544, loss = 0.09314832836389542
In grad_steps = 3545, loss = 0.08861541748046875
In grad_steps = 3546, loss = 0.06521224230527878
In grad_steps = 3547, loss = 0.06787052005529404
In grad_steps = 3548, loss = 0.6229439973831177
In grad_steps = 3549, loss = 0.30596452951431274
In grad_steps = 3550, loss = 0.05121425539255142
In grad_steps = 3551, loss = 0.01580846682190895
In grad_steps = 3552, loss = 0.2416650652885437
In grad_steps = 3553, loss = 0.24446548521518707
In grad_steps = 3554, loss = 0.4060901403427124
In grad_steps = 3555, loss = 0.1576497107744217
In grad_steps = 3556, loss = 0.4776120185852051
In grad_steps = 3557, loss = 0.053565725684165955
In grad_steps = 3558, loss = 0.011634111404418945
In grad_steps = 3559, loss = 0.7347853183746338
In grad_steps = 3560, loss = 0.10598708689212799
In grad_steps = 3561, loss = 0.7344155311584473
In grad_steps = 3562, loss = 0.33104556798934937
In grad_steps = 3563, loss = 0.0336836613714695
In grad_steps = 3564, loss = 0.016377272084355354
In grad_steps = 3565, loss = 0.027600642293691635
In grad_steps = 3566, loss = 0.4911560118198395
In grad_steps = 3567, loss = 0.44301891326904297
In grad_steps = 3568, loss = 0.2193780541419983
In grad_steps = 3569, loss = 0.10902506113052368
In grad_steps = 3570, loss = 0.053226567804813385
In grad_steps = 3571, loss = 0.07195264846086502
In grad_steps = 3572, loss = 0.5190441608428955
In grad_steps = 3573, loss = 0.09771834313869476
In grad_steps = 3574, loss = 0.308321088552475
In grad_steps = 3575, loss = 0.03816292807459831
In grad_steps = 3576, loss = 0.303056538105011
In grad_steps = 3577, loss = 0.010790934786200523
In grad_steps = 3578, loss = 0.26203057169914246
In grad_steps = 3579, loss = 0.17383675277233124
In grad_steps = 3580, loss = 0.03666532412171364
In grad_steps = 3581, loss = 0.058160483837127686
In grad_steps = 3582, loss = 0.09968335926532745
In grad_steps = 3583, loss = 0.042564451694488525
In grad_steps = 3584, loss = 0.25190335512161255
In grad_steps = 3585, loss = 0.6430408358573914
In grad_steps = 3586, loss = 0.046884700655937195
In grad_steps = 3587, loss = 0.22052434086799622
In grad_steps = 3588, loss = 0.1739853173494339
In grad_steps = 3589, loss = 0.107557013630867
In grad_steps = 3590, loss = 0.26867154240608215
In grad_steps = 3591, loss = 0.7780960202217102
In grad_steps = 3592, loss = 0.4986644983291626
In grad_steps = 3593, loss = 1.1812188625335693
In grad_steps = 3594, loss = 0.3329509496688843
In grad_steps = 3595, loss = 0.019157100468873978
In grad_steps = 3596, loss = 0.9159965515136719
In grad_steps = 3597, loss = 0.19912365078926086
In grad_steps = 3598, loss = 0.08918444067239761
In grad_steps = 3599, loss = 0.41117826104164124
In grad_steps = 3600, loss = 0.5310544371604919
In grad_steps = 3601, loss = 0.7709091901779175
In grad_steps = 3602, loss = 0.13225694000720978
In grad_steps = 3603, loss = 0.06638628244400024
In grad_steps = 3604, loss = 0.2080327868461609
In grad_steps = 3605, loss = 0.08512065559625626
In grad_steps = 3606, loss = 0.36817121505737305
In grad_steps = 3607, loss = 0.11409508436918259
In grad_steps = 3608, loss = 0.12700417637825012
In grad_steps = 3609, loss = 0.4990903437137604
In grad_steps = 3610, loss = 0.014829234220087528
In grad_steps = 3611, loss = 0.16397453844547272
In grad_steps = 3612, loss = 0.16497980058193207
In grad_steps = 3613, loss = 0.2319219410419464
In grad_steps = 3614, loss = 0.23271474242210388
In grad_steps = 3615, loss = 0.029509661719202995
In grad_steps = 3616, loss = 0.16308268904685974
In grad_steps = 3617, loss = 0.1493733674287796
In grad_steps = 3618, loss = 0.05843406915664673
In grad_steps = 3619, loss = 0.7203879952430725
In grad_steps = 3620, loss = 0.15388333797454834
In grad_steps = 3621, loss = 0.1263475865125656
In grad_steps = 3622, loss = 0.11568478494882584
In grad_steps = 3623, loss = 0.5035272240638733
In grad_steps = 3624, loss = 0.44741395115852356
In grad_steps = 3625, loss = 0.006333046592772007
In grad_steps = 3626, loss = 0.009085091762244701
In grad_steps = 3627, loss = 0.07434137165546417
In grad_steps = 3628, loss = 0.007406231015920639
In grad_steps = 3629, loss = 0.46212038397789
In grad_steps = 3630, loss = 0.32224559783935547
In grad_steps = 3631, loss = 0.012377025559544563
In grad_steps = 3632, loss = 0.005528551526367664
In grad_steps = 3633, loss = 0.008796310052275658
In grad_steps = 3634, loss = 0.012595384381711483
In grad_steps = 3635, loss = 0.021985555067658424
In grad_steps = 3636, loss = 0.04350589960813522
In grad_steps = 3637, loss = 0.009985261596739292
In grad_steps = 3638, loss = 1.0178390741348267
In grad_steps = 3639, loss = 0.37786051630973816
In grad_steps = 3640, loss = 0.39659392833709717
In grad_steps = 3641, loss = 0.2813798189163208
In grad_steps = 3642, loss = 0.019310906529426575
In grad_steps = 3643, loss = 0.046760667115449905
In grad_steps = 3644, loss = 0.030719108879566193
In grad_steps = 3645, loss = 0.19734400510787964
In grad_steps = 3646, loss = 0.031770024448633194
In grad_steps = 3647, loss = 0.712518572807312
In grad_steps = 3648, loss = 0.08513618260622025
In grad_steps = 3649, loss = 0.7233346700668335
In grad_steps = 3650, loss = 0.08206582814455032
In grad_steps = 3651, loss = 0.0594555027782917
In grad_steps = 3652, loss = 0.046040236949920654
In grad_steps = 3653, loss = 0.10495372116565704
In grad_steps = 3654, loss = 0.009219659492373466
In grad_steps = 3655, loss = 0.35626113414764404
In grad_steps = 3656, loss = 0.1044314056634903
In grad_steps = 3657, loss = 0.20179155468940735
In grad_steps = 3658, loss = 0.040138665586709976
In grad_steps = 3659, loss = 0.428184449672699
In grad_steps = 3660, loss = 0.25408607721328735
In grad_steps = 3661, loss = 0.04787324368953705
In grad_steps = 3662, loss = 1.030828595161438
In grad_steps = 3663, loss = 0.16244390606880188
In grad_steps = 3664, loss = 0.006575302686542273
In grad_steps = 3665, loss = 0.3653953969478607
In grad_steps = 3666, loss = 0.22446812689304352
In grad_steps = 3667, loss = 0.07978875935077667
In grad_steps = 3668, loss = 0.0220625102519989
In grad_steps = 3669, loss = 0.3993578553199768
In grad_steps = 3670, loss = 0.7113616466522217
In grad_steps = 3671, loss = 0.5713618993759155
In grad_steps = 3672, loss = 0.05303731933236122
In grad_steps = 3673, loss = 0.5324711799621582
In grad_steps = 3674, loss = 0.07808130234479904
In grad_steps = 3675, loss = 0.5041046142578125
In grad_steps = 3676, loss = 0.0961189717054367
In grad_steps = 3677, loss = 0.13520030677318573
In grad_steps = 3678, loss = 0.4732845425605774
In grad_steps = 3679, loss = 0.13613858819007874
In grad_steps = 3680, loss = 0.03458282724022865
In grad_steps = 3681, loss = 1.0037851333618164
In grad_steps = 3682, loss = 0.033457014709711075
In grad_steps = 3683, loss = 0.18421876430511475
In grad_steps = 3684, loss = 0.09978137165307999
In grad_steps = 3685, loss = 0.3970557749271393
In grad_steps = 3686, loss = 0.07039736211299896
In grad_steps = 3687, loss = 0.07452202588319778
In grad_steps = 3688, loss = 0.13339178264141083
In grad_steps = 3689, loss = 0.039167825132608414
In grad_steps = 3690, loss = 0.3299087882041931
In grad_steps = 3691, loss = 1.1665440797805786
In grad_steps = 3692, loss = 0.7749906778335571
In grad_steps = 3693, loss = 0.38117486238479614
In grad_steps = 3694, loss = 0.09245671331882477
In grad_steps = 3695, loss = 0.1238732635974884
In grad_steps = 3696, loss = 0.14405325055122375
In grad_steps = 3697, loss = 0.2500999867916107
In grad_steps = 3698, loss = 0.12187933921813965
In grad_steps = 3699, loss = 0.11595653742551804
In grad_steps = 3700, loss = 0.1769932508468628
In grad_steps = 3701, loss = 0.1726171374320984
In grad_steps = 3702, loss = 0.30687621235847473
In grad_steps = 3703, loss = 0.029330287128686905
In grad_steps = 3704, loss = 0.08645731210708618
In grad_steps = 3705, loss = 0.16694208979606628
In grad_steps = 3706, loss = 0.11019925773143768
In grad_steps = 3707, loss = 0.3386850357055664
In grad_steps = 3708, loss = 0.1039251908659935
In grad_steps = 3709, loss = 0.028299320489168167
In grad_steps = 3710, loss = 0.2353035807609558
In grad_steps = 3711, loss = 0.28196364641189575
In grad_steps = 3712, loss = 0.05345377326011658
In grad_steps = 3713, loss = 0.49809059500694275
In grad_steps = 3714, loss = 0.038138341158628464
In grad_steps = 3715, loss = 0.012821444310247898
In grad_steps = 3716, loss = 0.05779420956969261
In grad_steps = 3717, loss = 0.025731364265084267
In grad_steps = 3718, loss = 0.022398076951503754
In grad_steps = 3719, loss = 0.03427054360508919
In grad_steps = 3720, loss = 0.7690540552139282
In grad_steps = 3721, loss = 0.6216670274734497
In grad_steps = 3722, loss = 0.08839990198612213
In grad_steps = 3723, loss = 0.19336163997650146
In grad_steps = 3724, loss = 0.1564609557390213
In grad_steps = 3725, loss = 1.3757232427597046
In grad_steps = 3726, loss = 0.022020963951945305
In grad_steps = 3727, loss = 0.008174682036042213
In grad_steps = 3728, loss = 0.15302640199661255
In grad_steps = 3729, loss = 0.0072095962241292
In grad_steps = 3730, loss = 0.9304974675178528
In grad_steps = 3731, loss = 0.0835600420832634
In grad_steps = 3732, loss = 0.504320502281189
In grad_steps = 3733, loss = 0.035255350172519684
In grad_steps = 3734, loss = 0.019054297357797623
In grad_steps = 3735, loss = 0.5330299735069275
In grad_steps = 3736, loss = 0.06176973879337311
In grad_steps = 3737, loss = 0.013310158625245094
In grad_steps = 3738, loss = 0.06761389970779419
In grad_steps = 3739, loss = 0.04175816476345062
In grad_steps = 3740, loss = 0.05324354022741318
In grad_steps = 3741, loss = 0.4395245909690857
In grad_steps = 3742, loss = 0.014867273159325123
In grad_steps = 3743, loss = 0.01962675154209137
In grad_steps = 3744, loss = 0.059626296162605286
In grad_steps = 3745, loss = 0.009536110796034336
In grad_steps = 3746, loss = 0.08932255208492279
In grad_steps = 3747, loss = 0.037995412945747375
In grad_steps = 3748, loss = 0.0715663880109787
In grad_steps = 3749, loss = 0.43556514382362366
In grad_steps = 3750, loss = 1.4400620460510254
In grad_steps = 3751, loss = 0.10065922886133194
In grad_steps = 3752, loss = 0.6616397500038147
In grad_steps = 3753, loss = 0.03299878537654877
In grad_steps = 3754, loss = 0.2975255846977234
In grad_steps = 3755, loss = 0.030818037688732147
In grad_steps = 3756, loss = 0.15056112408638
In grad_steps = 3757, loss = 0.13266147673130035
In grad_steps = 3758, loss = 0.044164981693029404
In grad_steps = 3759, loss = 0.17699843645095825
In grad_steps = 3760, loss = 0.2785167396068573
In grad_steps = 3761, loss = 0.38785064220428467
In grad_steps = 3762, loss = 0.06322610378265381
In grad_steps = 3763, loss = 0.22446207702159882
In grad_steps = 3764, loss = 0.04181230813264847
In grad_steps = 3765, loss = 0.1524057686328888
In grad_steps = 3766, loss = 0.3325841426849365
In grad_steps = 3767, loss = 0.016023794189095497
In grad_steps = 3768, loss = 0.06348918378353119
In grad_steps = 3769, loss = 0.07307133078575134
In grad_steps = 3770, loss = 0.13283468782901764
In grad_steps = 3771, loss = 0.01678994670510292
In grad_steps = 3772, loss = 0.053949035704135895
In grad_steps = 3773, loss = 0.0475611612200737
In grad_steps = 3774, loss = 0.3198695480823517
In grad_steps = 3775, loss = 0.19914549589157104
In grad_steps = 3776, loss = 0.011689942330121994
In grad_steps = 3777, loss = 0.354122519493103
In grad_steps = 3778, loss = 0.02252371609210968
In grad_steps = 3779, loss = 0.02918998897075653
In grad_steps = 3780, loss = 0.04155236482620239
In grad_steps = 3781, loss = 0.0055326311849057674
In grad_steps = 3782, loss = 1.3200377225875854
In grad_steps = 3783, loss = 1.6279536485671997
In grad_steps = 3784, loss = 0.025968672707676888
In grad_steps = 3785, loss = 0.02612149342894554
In grad_steps = 3786, loss = 0.26543065905570984
In grad_steps = 3787, loss = 0.5147060751914978
In grad_steps = 3788, loss = 0.028445888310670853
In grad_steps = 3789, loss = 1.147958517074585
In grad_steps = 3790, loss = 0.5589607357978821
In grad_steps = 3791, loss = 0.03199644386768341
In grad_steps = 3792, loss = 0.044091373682022095
In grad_steps = 3793, loss = 0.6414165496826172
In grad_steps = 3794, loss = 0.08335650712251663
In grad_steps = 3795, loss = 0.028766624629497528
In grad_steps = 3796, loss = 0.37589210271835327
In grad_steps = 3797, loss = 0.31940019130706787
In grad_steps = 3798, loss = 0.04555784538388252
In grad_steps = 3799, loss = 0.05481932312250137
In grad_steps = 3800, loss = 0.160892054438591
In grad_steps = 3801, loss = 0.15718749165534973
In grad_steps = 3802, loss = 0.254060834646225
In grad_steps = 3803, loss = 0.15338942408561707
In grad_steps = 3804, loss = 0.22303441166877747
In grad_steps = 3805, loss = 0.18933233618736267
In grad_steps = 3806, loss = 0.009559852071106434
In grad_steps = 3807, loss = 0.21912053227424622
In grad_steps = 3808, loss = 0.6297691464424133
In grad_steps = 3809, loss = 0.2174711525440216
In grad_steps = 3810, loss = 1.1402775049209595
In grad_steps = 3811, loss = 0.09630662202835083
In grad_steps = 3812, loss = 0.06163337826728821
In grad_steps = 3813, loss = 0.023919785395264626
In grad_steps = 3814, loss = 0.11673831194639206
In grad_steps = 3815, loss = 0.11755774170160294
In grad_steps = 3816, loss = 0.6627130508422852
In grad_steps = 3817, loss = 0.041628580540418625
In grad_steps = 3818, loss = 1.1308717727661133
In grad_steps = 3819, loss = 0.43000200390815735
In grad_steps = 3820, loss = 0.04217300936579704
In grad_steps = 3821, loss = 0.06923181563615799
In grad_steps = 3822, loss = 0.7005609273910522
In grad_steps = 3823, loss = 0.03083766996860504
In grad_steps = 3824, loss = 0.09752847999334335
In grad_steps = 3825, loss = 0.4922681152820587
In grad_steps = 3826, loss = 0.07517405599355698
In grad_steps = 3827, loss = 0.4689715504646301
In grad_steps = 3828, loss = 0.7245620489120483
In grad_steps = 3829, loss = 0.3244091272354126
In grad_steps = 3830, loss = 0.07708743959665298
In grad_steps = 3831, loss = 0.920088529586792
In grad_steps = 3832, loss = 0.30180883407592773
In grad_steps = 3833, loss = 0.3610714077949524
In grad_steps = 3834, loss = 0.11553501337766647
In grad_steps = 3835, loss = 0.05001429095864296
In grad_steps = 3836, loss = 0.11640665680170059
In grad_steps = 3837, loss = 0.09902328997850418
In grad_steps = 3838, loss = 0.1616438925266266
In grad_steps = 3839, loss = 0.14295822381973267
In grad_steps = 3840, loss = 0.20485730469226837
In grad_steps = 3841, loss = 0.15006613731384277
In grad_steps = 3842, loss = 0.041473638266325
In grad_steps = 3843, loss = 0.5939903855323792
In grad_steps = 3844, loss = 0.132980614900589
In grad_steps = 3845, loss = 0.293192058801651
In grad_steps = 3846, loss = 0.3719290792942047
In grad_steps = 3847, loss = 0.2807435393333435
In grad_steps = 3848, loss = 0.06580942869186401
In grad_steps = 3849, loss = 0.5693790316581726
In grad_steps = 3850, loss = 0.2011045217514038
In grad_steps = 3851, loss = 0.12507788836956024
In grad_steps = 3852, loss = 0.03045024164021015
In grad_steps = 3853, loss = 0.13199806213378906
In grad_steps = 3854, loss = 0.43975958228111267
In grad_steps = 3855, loss = 0.015682576224207878
In grad_steps = 3856, loss = 0.4360741078853607
In grad_steps = 3857, loss = 0.13991479575634003
In grad_steps = 3858, loss = 0.029690325260162354
In grad_steps = 3859, loss = 0.01693826913833618
In grad_steps = 3860, loss = 0.07380079478025436
In grad_steps = 3861, loss = 0.5772817730903625
In grad_steps = 3862, loss = 0.44285792112350464
In grad_steps = 3863, loss = 0.12431991845369339
In grad_steps = 3864, loss = 0.09225866198539734
In grad_steps = 3865, loss = 0.023039400577545166
In grad_steps = 3866, loss = 1.3458192348480225
In grad_steps = 3867, loss = 0.135028675198555
In grad_steps = 3868, loss = 0.09988899528980255
In grad_steps = 3869, loss = 0.0368875153362751
In grad_steps = 3870, loss = 0.06408363580703735
In grad_steps = 3871, loss = 0.048629626631736755
In grad_steps = 3872, loss = 0.45136234164237976
In grad_steps = 3873, loss = 0.027751144021749496
In grad_steps = 3874, loss = 0.12945255637168884
In grad_steps = 3875, loss = 0.2075616419315338
In grad_steps = 3876, loss = 0.17997995018959045
In grad_steps = 3877, loss = 0.34553995728492737
In grad_steps = 3878, loss = 0.40848803520202637
In grad_steps = 3879, loss = 0.038294076919555664
In grad_steps = 3880, loss = 0.039997413754463196
In grad_steps = 3881, loss = 0.03773660212755203
In grad_steps = 3882, loss = 0.035827863961458206
In grad_steps = 3883, loss = 0.05520717799663544
In grad_steps = 3884, loss = 0.08623203635215759
In grad_steps = 3885, loss = 0.10177439451217651
In grad_steps = 3886, loss = 0.26418668031692505
In grad_steps = 3887, loss = 0.05088801309466362
In grad_steps = 3888, loss = 0.024335181340575218
In grad_steps = 3889, loss = 0.08798964321613312
In grad_steps = 3890, loss = 0.35234540700912476
In grad_steps = 3891, loss = 0.3705800175666809
In grad_steps = 3892, loss = 0.01264974195510149
In grad_steps = 3893, loss = 0.008820690214633942
In grad_steps = 3894, loss = 0.0168425515294075
In grad_steps = 3895, loss = 0.01044877152889967
In grad_steps = 3896, loss = 0.6189906597137451
In grad_steps = 3897, loss = 0.018140748143196106
In grad_steps = 3898, loss = 0.003977897576987743
In grad_steps = 3899, loss = 0.5948811173439026
In grad_steps = 3900, loss = 0.0061132344417274
In grad_steps = 3901, loss = 0.1275416612625122
In grad_steps = 3902, loss = 0.006761716213077307
In grad_steps = 3903, loss = 0.05436718463897705
In grad_steps = 3904, loss = 0.02787165716290474
In grad_steps = 3905, loss = 0.4934217035770416
In grad_steps = 3906, loss = 0.07530825585126877
In grad_steps = 3907, loss = 0.1900494396686554
In grad_steps = 3908, loss = 0.01358146034181118
In grad_steps = 3909, loss = 0.007508507929742336
In grad_steps = 3910, loss = 1.188961386680603
In grad_steps = 3911, loss = 0.15361082553863525
In grad_steps = 3912, loss = 0.5134592652320862
In grad_steps = 3913, loss = 0.2448134422302246
In grad_steps = 3914, loss = 0.0949559360742569
In grad_steps = 3915, loss = 0.037407249212265015
In grad_steps = 3916, loss = 0.3383820056915283
In grad_steps = 3917, loss = 0.04736108332872391
In grad_steps = 3918, loss = 0.06286189705133438
In grad_steps = 3919, loss = 1.0170923471450806
In grad_steps = 3920, loss = 0.06704205274581909
In grad_steps = 3921, loss = 0.251034677028656
In grad_steps = 3922, loss = 0.06545950472354889
In grad_steps = 3923, loss = 0.4610047936439514
In grad_steps = 3924, loss = 0.21321266889572144
In grad_steps = 3925, loss = 0.4107004404067993
In grad_steps = 3926, loss = 0.2911464273929596
In grad_steps = 3927, loss = 0.36803436279296875
In grad_steps = 3928, loss = 0.08829782158136368
In grad_steps = 3929, loss = 0.10189367830753326
In grad_steps = 3930, loss = 0.1261226236820221
In grad_steps = 3931, loss = 0.4477676749229431
In grad_steps = 3932, loss = 0.16114667057991028
In grad_steps = 3933, loss = 0.025755878537893295
In grad_steps = 3934, loss = 0.20783936977386475
In grad_steps = 3935, loss = 0.02126370184123516
In grad_steps = 3936, loss = 0.32816189527511597
In grad_steps = 3937, loss = 0.7854555249214172
In grad_steps = 3938, loss = 1.4123542308807373
In grad_steps = 3939, loss = 0.3267800807952881
In grad_steps = 3940, loss = 0.2768065929412842
In grad_steps = 3941, loss = 0.13268914818763733
In grad_steps = 3942, loss = 0.04060842841863632
In grad_steps = 3943, loss = 0.218995600938797
In grad_steps = 3944, loss = 0.013488917611539364
In grad_steps = 3945, loss = 0.2631867825984955
In grad_steps = 3946, loss = 0.03177565708756447
In grad_steps = 3947, loss = 0.042126234620809555
In grad_steps = 3948, loss = 0.5758811831474304
In grad_steps = 3949, loss = 0.25323835015296936
In grad_steps = 3950, loss = 0.05261705815792084
In grad_steps = 3951, loss = 0.03436703607439995
In grad_steps = 3952, loss = 0.20927868783473969
In grad_steps = 3953, loss = 0.2037348449230194
In grad_steps = 3954, loss = 0.04604891687631607
In grad_steps = 3955, loss = 0.781099259853363
In grad_steps = 3956, loss = 0.12420539557933807
In grad_steps = 3957, loss = 0.06081528589129448
In grad_steps = 3958, loss = 0.05213377997279167
In grad_steps = 3959, loss = 0.24287596344947815
In grad_steps = 3960, loss = 0.04114125296473503
In grad_steps = 3961, loss = 0.08540166914463043
In grad_steps = 3962, loss = 0.08029778301715851
In grad_steps = 3963, loss = 0.029453366994857788
In grad_steps = 3964, loss = 0.022341091185808182
In grad_steps = 3965, loss = 0.5036681294441223
In grad_steps = 3966, loss = 0.2331741601228714
In grad_steps = 3967, loss = 0.11052539199590683
In grad_steps = 3968, loss = 0.015080178156495094
In grad_steps = 3969, loss = 0.21265432238578796
In grad_steps = 3970, loss = 0.01236969605088234
In grad_steps = 3971, loss = 0.23675327003002167
In grad_steps = 3972, loss = 0.19292859733104706
In grad_steps = 3973, loss = 0.019319549202919006
In grad_steps = 3974, loss = 0.2623126804828644
In grad_steps = 3975, loss = 0.13639268279075623
In grad_steps = 3976, loss = 0.02333446405827999
In grad_steps = 3977, loss = 0.18397872149944305
In grad_steps = 3978, loss = 0.16139665246009827
In grad_steps = 3979, loss = 0.017199557274580002
In grad_steps = 3980, loss = 0.05149896442890167
In grad_steps = 3981, loss = 0.010825968347489834
In grad_steps = 3982, loss = 0.01174840983003378
In grad_steps = 3983, loss = 0.5478062033653259
In grad_steps = 3984, loss = 0.03725424036383629
In grad_steps = 3985, loss = 0.10514691472053528
In grad_steps = 3986, loss = 0.01439861673861742
In grad_steps = 3987, loss = 0.05912356451153755
In grad_steps = 3988, loss = 0.06913784891366959
In grad_steps = 3989, loss = 0.07255616784095764
In grad_steps = 3990, loss = 0.004891515709459782
In grad_steps = 3991, loss = 0.020302876830101013
In grad_steps = 3992, loss = 0.11994331330060959
In grad_steps = 3993, loss = 0.021328363567590714
In grad_steps = 3994, loss = 0.011644154787063599
In grad_steps = 3995, loss = 1.0803523063659668
In grad_steps = 3996, loss = 0.009038734249770641
In grad_steps = 3997, loss = 1.0217208862304688
In grad_steps = 3998, loss = 0.033515725284814835
In grad_steps = 3999, loss = 0.05087781697511673
In grad_steps = 4000, loss = 0.058182138949632645
In grad_steps = 4001, loss = 0.016862329095602036
In grad_steps = 4002, loss = 0.007526123430579901
In grad_steps = 4003, loss = 0.36135411262512207
In grad_steps = 4004, loss = 0.15263554453849792
In grad_steps = 4005, loss = 0.019613642245531082
In grad_steps = 4006, loss = 0.14729364216327667
In grad_steps = 4007, loss = 0.2728537619113922
In grad_steps = 4008, loss = 0.14319339394569397
In grad_steps = 4009, loss = 0.874300479888916
In grad_steps = 4010, loss = 0.02548697404563427
In grad_steps = 4011, loss = 0.4734291136264801
In grad_steps = 4012, loss = 2.246671199798584
In grad_steps = 4013, loss = 0.5593124032020569
In grad_steps = 4014, loss = 0.3081726133823395
In grad_steps = 4015, loss = 0.8240165114402771
In grad_steps = 4016, loss = 0.057688791304826736
In grad_steps = 4017, loss = 0.05219890922307968
In grad_steps = 4018, loss = 0.0842691957950592
In grad_steps = 4019, loss = 0.028729509562253952
In grad_steps = 4020, loss = 0.09071566164493561
In grad_steps = 4021, loss = 0.8176141381263733
In grad_steps = 4022, loss = 0.1836356222629547
In grad_steps = 4023, loss = 0.06615663319826126
In grad_steps = 4024, loss = 0.14569294452667236
In grad_steps = 4025, loss = 0.7219495177268982
In grad_steps = 4026, loss = 0.0721554234623909
In grad_steps = 4027, loss = 0.0804021880030632
In grad_steps = 4028, loss = 0.5722042322158813
In grad_steps = 4029, loss = 0.16422425210475922
In grad_steps = 4030, loss = 0.15646201372146606
In grad_steps = 4031, loss = 0.10560184717178345
In grad_steps = 4032, loss = 0.630302906036377
In grad_steps = 4033, loss = 0.10263054817914963
In grad_steps = 4034, loss = 0.30538684129714966
In grad_steps = 4035, loss = 0.07225093245506287
In grad_steps = 4036, loss = 0.11013028770685196
In grad_steps = 4037, loss = 0.4172995984554291
In grad_steps = 4038, loss = 0.10298238694667816
In grad_steps = 4039, loss = 0.18612763285636902
In grad_steps = 4040, loss = 0.2791176438331604
In grad_steps = 4041, loss = 0.15675464272499084
In grad_steps = 4042, loss = 0.10941274464130402
In grad_steps = 4043, loss = 0.09686439484357834
In grad_steps = 4044, loss = 0.1717967540025711
In grad_steps = 4045, loss = 0.046380288898944855
In grad_steps = 4046, loss = 0.11162247508764267
In grad_steps = 4047, loss = 0.21174964308738708
In grad_steps = 4048, loss = 0.18245741724967957
In grad_steps = 4049, loss = 0.24655546247959137
In grad_steps = 4050, loss = 0.033237308263778687
In grad_steps = 4051, loss = 0.01153247244656086
In grad_steps = 4052, loss = 0.04815518110990524
In grad_steps = 4053, loss = 0.023048534989356995
In grad_steps = 4054, loss = 1.0022393465042114
In grad_steps = 4055, loss = 0.007110954262316227
In grad_steps = 4056, loss = 0.010231846943497658
In grad_steps = 4057, loss = 0.02368970774114132
In grad_steps = 4058, loss = 0.019565947353839874
In grad_steps = 4059, loss = 0.37297675013542175
In grad_steps = 4060, loss = 0.6658682227134705
In grad_steps = 4061, loss = 0.025457510724663734
In grad_steps = 4062, loss = 0.01218540221452713
In grad_steps = 4063, loss = 0.038083307445049286
In grad_steps = 4064, loss = 0.01594237983226776
In grad_steps = 4065, loss = 0.007566992659121752
In grad_steps = 4066, loss = 1.0106488466262817
In grad_steps = 4067, loss = 0.01879734918475151
In grad_steps = 4068, loss = 0.34961676597595215
In grad_steps = 4069, loss = 0.31376075744628906
In grad_steps = 4070, loss = 0.9319039583206177
In grad_steps = 4071, loss = 0.013400978408753872
In grad_steps = 4072, loss = 0.03962121903896332
In grad_steps = 4073, loss = 0.06670062243938446
In grad_steps = 4074, loss = 0.026037532836198807
In grad_steps = 4075, loss = 0.3525886535644531
In grad_steps = 4076, loss = 0.04761071875691414
In grad_steps = 4077, loss = 0.3802856206893921
In grad_steps = 4078, loss = 0.060521259903907776
In grad_steps = 4079, loss = 0.465614914894104
In grad_steps = 4080, loss = 0.21530881524085999
In grad_steps = 4081, loss = 0.09765055775642395
In grad_steps = 4082, loss = 0.2559787631034851
In grad_steps = 4083, loss = 0.05734246224164963
In grad_steps = 4084, loss = 0.023717433214187622
In grad_steps = 4085, loss = 0.04559280723333359
In grad_steps = 4086, loss = 0.038151949644088745
In grad_steps = 4087, loss = 0.0790344625711441
In grad_steps = 4088, loss = 0.1319551169872284
In grad_steps = 4089, loss = 0.17450496554374695
In grad_steps = 4090, loss = 0.8117071986198425
In grad_steps = 4091, loss = 0.0944286584854126
In grad_steps = 4092, loss = 0.04109881818294525
In grad_steps = 4093, loss = 0.021964181214571
In grad_steps = 4094, loss = 0.029742849990725517
In grad_steps = 4095, loss = 0.33797982335090637
In grad_steps = 4096, loss = 0.0222727432847023
In grad_steps = 4097, loss = 0.6102248430252075
In grad_steps = 4098, loss = 0.1681499183177948
In grad_steps = 4099, loss = 0.10288161039352417
In grad_steps = 4100, loss = 0.03715139254927635
In grad_steps = 4101, loss = 0.7246658205986023
In grad_steps = 4102, loss = 0.011482996866106987
In grad_steps = 4103, loss = 0.9298413395881653
In grad_steps = 4104, loss = 0.042505327612161636
In grad_steps = 4105, loss = 0.08194156736135483
In grad_steps = 4106, loss = 0.4140773117542267
In grad_steps = 4107, loss = 0.06709766387939453
In grad_steps = 4108, loss = 0.01457425206899643
In grad_steps = 4109, loss = 0.06674676388502121
In grad_steps = 4110, loss = 0.9043561816215515
In grad_steps = 4111, loss = 0.0442587211728096
In grad_steps = 4112, loss = 0.48766982555389404
In grad_steps = 4113, loss = 0.3351249694824219
In grad_steps = 4114, loss = 0.03344178944826126
In grad_steps = 4115, loss = 0.480043888092041
In grad_steps = 4116, loss = 0.04181606322526932
In grad_steps = 4117, loss = 0.25010761618614197
In grad_steps = 4118, loss = 0.19974496960639954
In grad_steps = 4119, loss = 0.0033351313322782516
In grad_steps = 4120, loss = 0.0603540763258934
In grad_steps = 4121, loss = 0.057034678757190704
In grad_steps = 4122, loss = 0.42806124687194824
In grad_steps = 4123, loss = 0.40165573358535767
In grad_steps = 4124, loss = 0.33268123865127563
In grad_steps = 4125, loss = 0.20645402371883392
In grad_steps = 4126, loss = 0.05814572051167488
In grad_steps = 4127, loss = 0.06446517258882523
In grad_steps = 4128, loss = 0.34778890013694763
In grad_steps = 4129, loss = 0.14924469590187073
In grad_steps = 4130, loss = 0.041645780205726624
In grad_steps = 4131, loss = 0.14402201771736145
In grad_steps = 4132, loss = 0.06373900175094604
In grad_steps = 4133, loss = 0.025597546249628067
In grad_steps = 4134, loss = 0.058007486164569855
In grad_steps = 4135, loss = 0.32130593061447144
In grad_steps = 4136, loss = 0.28339555859565735
In grad_steps = 4137, loss = 1.164966106414795
In grad_steps = 4138, loss = 0.025858430191874504
In grad_steps = 4139, loss = 0.011172057129442692
In grad_steps = 4140, loss = 0.13535529375076294
In grad_steps = 4141, loss = 0.07075958698987961
In grad_steps = 4142, loss = 0.08856961876153946
In grad_steps = 4143, loss = 0.6736514568328857
In grad_steps = 4144, loss = 0.42255622148513794
In grad_steps = 4145, loss = 0.025669649243354797
In grad_steps = 4146, loss = 0.025605544447898865
In grad_steps = 4147, loss = 0.03803841024637222
In grad_steps = 4148, loss = 0.013266469351947308
In grad_steps = 4149, loss = 0.1115419864654541
In grad_steps = 4150, loss = 0.10881388932466507
In grad_steps = 4151, loss = 0.02340668998658657
In grad_steps = 4152, loss = 0.832934558391571
In grad_steps = 4153, loss = 0.5885581374168396
In grad_steps = 4154, loss = 0.5080690383911133
In grad_steps = 4155, loss = 0.9741184115409851
In grad_steps = 4156, loss = 0.019458772614598274
In grad_steps = 4157, loss = 0.41140103340148926
In grad_steps = 4158, loss = 0.11316326260566711
In grad_steps = 4159, loss = 0.15339088439941406
In grad_steps = 4160, loss = 0.06586602330207825
In grad_steps = 4161, loss = 0.37208718061447144
In grad_steps = 4162, loss = 0.13585542142391205
In grad_steps = 4163, loss = 0.01920919679105282
In grad_steps = 4164, loss = 0.016233250498771667
In grad_steps = 4165, loss = 0.3369705080986023
In grad_steps = 4166, loss = 0.18335048854351044
In grad_steps = 4167, loss = 0.04222550615668297
In grad_steps = 4168, loss = 0.23560068011283875
In grad_steps = 4169, loss = 0.05991706624627113
In grad_steps = 4170, loss = 0.32945528626441956
In grad_steps = 4171, loss = 0.47855353355407715
In grad_steps = 4172, loss = 0.2839173376560211
In grad_steps = 4173, loss = 1.137129545211792
In grad_steps = 4174, loss = 0.05406441539525986
In grad_steps = 4175, loss = 0.015050090849399567
In grad_steps = 4176, loss = 1.712717056274414
In grad_steps = 4177, loss = 0.10824950039386749
In grad_steps = 4178, loss = 0.021472111344337463
In grad_steps = 4179, loss = 1.1887823343276978
In grad_steps = 4180, loss = 0.8684283494949341
In grad_steps = 4181, loss = 1.0501761436462402
In grad_steps = 4182, loss = 0.33074554800987244
In grad_steps = 4183, loss = 0.09694317728281021
In grad_steps = 4184, loss = 0.06747128814458847
In grad_steps = 4185, loss = 0.7471253275871277
In grad_steps = 4186, loss = 1.0461381673812866
In grad_steps = 4187, loss = 0.08871887624263763
In grad_steps = 4188, loss = 0.26876625418663025
In grad_steps = 4189, loss = 0.5856371521949768
In grad_steps = 4190, loss = 0.46546703577041626
In grad_steps = 4191, loss = 0.4990725815296173
In grad_steps = 4192, loss = 0.414082407951355
In grad_steps = 4193, loss = 0.0890195369720459
In grad_steps = 4194, loss = 0.2314235270023346
In grad_steps = 4195, loss = 0.23164527118206024
In grad_steps = 4196, loss = 0.27456170320510864
In grad_steps = 4197, loss = 0.7595220804214478
In grad_steps = 4198, loss = 0.22780512273311615
In grad_steps = 4199, loss = 0.4373210668563843
In grad_steps = 4200, loss = 0.28780248761177063
In grad_steps = 4201, loss = 0.12143583595752716
In grad_steps = 4202, loss = 0.09193789213895798
In grad_steps = 4203, loss = 0.1294660121202469
In grad_steps = 4204, loss = 0.13415977358818054
In grad_steps = 4205, loss = 0.09046593308448792
In grad_steps = 4206, loss = 0.3167649507522583
In grad_steps = 4207, loss = 0.07916422188282013
In grad_steps = 4208, loss = 0.024036042392253876
In grad_steps = 4209, loss = 0.13423584401607513
In grad_steps = 4210, loss = 0.03705982863903046
In grad_steps = 4211, loss = 0.15365926921367645
In grad_steps = 4212, loss = 0.024184755980968475
In grad_steps = 4213, loss = 1.1690876483917236
In grad_steps = 4214, loss = 0.033919092267751694
In grad_steps = 4215, loss = 0.12436575442552567
In grad_steps = 4216, loss = 0.04381696134805679
In grad_steps = 4217, loss = 0.12290061265230179
In grad_steps = 4218, loss = 0.11476141959428787
In grad_steps = 4219, loss = 0.012290149927139282
In grad_steps = 4220, loss = 0.9362747669219971
In grad_steps = 4221, loss = 0.028963914141058922
In grad_steps = 4222, loss = 0.045914553105831146
In grad_steps = 4223, loss = 0.029568463563919067
In grad_steps = 4224, loss = 0.02235090732574463
In grad_steps = 4225, loss = 0.019338451325893402
In grad_steps = 4226, loss = 0.06789561361074448
In grad_steps = 4227, loss = 0.0141574926674366
In grad_steps = 4228, loss = 0.15020343661308289
In grad_steps = 4229, loss = 0.022616790607571602
In grad_steps = 4230, loss = 0.8039897680282593
In grad_steps = 4231, loss = 0.04129234328866005
In grad_steps = 4232, loss = 0.2594400942325592
In grad_steps = 4233, loss = 0.021680103614926338
In grad_steps = 4234, loss = 0.35540780425071716
In grad_steps = 4235, loss = 0.020508816465735435
In grad_steps = 4236, loss = 0.4779186248779297
In grad_steps = 4237, loss = 0.03334923833608627
In grad_steps = 4238, loss = 0.07615000754594803
In grad_steps = 4239, loss = 0.2339058667421341
In grad_steps = 4240, loss = 0.04372808337211609
In grad_steps = 4241, loss = 0.1539134532213211
In grad_steps = 4242, loss = 0.02055508643388748
In grad_steps = 4243, loss = 0.012410000897943974
In grad_steps = 4244, loss = 0.00719101307913661
In grad_steps = 4245, loss = 0.3015579283237457
In grad_steps = 4246, loss = 0.011884992010891438
In grad_steps = 4247, loss = 0.006137662101536989
In grad_steps = 4248, loss = 0.026576731353998184
In grad_steps = 4249, loss = 0.045208416879177094
In grad_steps = 4250, loss = 0.011629165150225163
In grad_steps = 4251, loss = 0.017646750435233116
In grad_steps = 4252, loss = 0.06920573115348816
In grad_steps = 4253, loss = 0.011631229892373085
In grad_steps = 4254, loss = 0.014682593755424023
In grad_steps = 4255, loss = 0.05404515564441681
In grad_steps = 4256, loss = 0.01412804052233696
In grad_steps = 4257, loss = 0.0581522099673748
In grad_steps = 4258, loss = 0.10754656791687012
In grad_steps = 4259, loss = 0.06045493483543396
In grad_steps = 4260, loss = 2.2098519802093506
In grad_steps = 4261, loss = 0.41405630111694336
In grad_steps = 4262, loss = 0.017874587327241898
In grad_steps = 4263, loss = 0.06192575767636299
In grad_steps = 4264, loss = 0.06701186299324036
In grad_steps = 4265, loss = 0.02384255640208721
In grad_steps = 4266, loss = 0.011647624894976616
In grad_steps = 4267, loss = 0.03711499646306038
In grad_steps = 4268, loss = 0.895873486995697
In grad_steps = 4269, loss = 0.658256471157074
In grad_steps = 4270, loss = 0.2610783278942108
In grad_steps = 4271, loss = 0.041149623692035675
In grad_steps = 4272, loss = 0.5729525685310364
In grad_steps = 4273, loss = 0.11757256090641022
In grad_steps = 4274, loss = 0.10325805842876434
In grad_steps = 4275, loss = 0.09019846469163895
In grad_steps = 4276, loss = 0.04226650297641754
In grad_steps = 4277, loss = 0.44564127922058105
In grad_steps = 4278, loss = 0.36498138308525085
In grad_steps = 4279, loss = 0.4723721742630005
In grad_steps = 4280, loss = 0.06215011328458786
In grad_steps = 4281, loss = 0.7344915866851807
In grad_steps = 4282, loss = 0.23800405859947205
In grad_steps = 4283, loss = 0.33997228741645813
In grad_steps = 4284, loss = 1.0210260152816772
In grad_steps = 4285, loss = 0.08586196601390839
In grad_steps = 4286, loss = 0.2844099998474121
In grad_steps = 4287, loss = 0.1392609179019928
In grad_steps = 4288, loss = 0.10913851857185364
In grad_steps = 4289, loss = 0.09735921770334244
In grad_steps = 4290, loss = 0.03498682379722595
In grad_steps = 4291, loss = 0.21210870146751404
In grad_steps = 4292, loss = 0.5881384015083313
In grad_steps = 4293, loss = 0.27480486035346985
In grad_steps = 4294, loss = 0.10863599926233292
In grad_steps = 4295, loss = 0.16081370413303375
In grad_steps = 4296, loss = 0.9843422174453735
In grad_steps = 4297, loss = 0.05676855146884918
In grad_steps = 4298, loss = 0.10593514144420624
In grad_steps = 4299, loss = 0.22383713722229004
In grad_steps = 4300, loss = 0.29085057973861694
In grad_steps = 4301, loss = 0.08940047025680542
In grad_steps = 4302, loss = 0.150425985455513
In grad_steps = 4303, loss = 0.07556769996881485
In grad_steps = 4304, loss = 0.62117999792099
In grad_steps = 4305, loss = 0.10741797089576721
In grad_steps = 4306, loss = 0.08294930309057236
In grad_steps = 4307, loss = 0.3741813600063324
In grad_steps = 4308, loss = 0.028553932905197144
In grad_steps = 4309, loss = 0.1739189177751541
In grad_steps = 4310, loss = 0.0771295577287674
In grad_steps = 4311, loss = 0.2657647132873535
In grad_steps = 4312, loss = 0.054241158068180084
In grad_steps = 4313, loss = 0.03624377399682999
In grad_steps = 4314, loss = 0.36653557419776917
In grad_steps = 4315, loss = 0.061813898384571075
In grad_steps = 4316, loss = 0.9973005652427673
In grad_steps = 4317, loss = 0.4768638610839844
In grad_steps = 4318, loss = 0.11536690592765808
In grad_steps = 4319, loss = 1.9845991134643555
In grad_steps = 4320, loss = 0.244696244597435
In grad_steps = 4321, loss = 0.1730419397354126
In grad_steps = 4322, loss = 0.1256178468465805
In grad_steps = 4323, loss = 0.23842480778694153
In grad_steps = 4324, loss = 0.421843945980072
In grad_steps = 4325, loss = 0.5728804469108582
In grad_steps = 4326, loss = 0.14126478135585785
In grad_steps = 4327, loss = 0.14373338222503662
In grad_steps = 4328, loss = 0.23744697868824005
In grad_steps = 4329, loss = 0.0506623312830925
In grad_steps = 4330, loss = 0.7250102162361145
In grad_steps = 4331, loss = 0.20943498611450195
In grad_steps = 4332, loss = 0.04013364389538765
In grad_steps = 4333, loss = 0.19301308691501617
In grad_steps = 4334, loss = 0.31603553891181946
In grad_steps = 4335, loss = 0.26057907938957214
In grad_steps = 4336, loss = 0.043936315923929214
In grad_steps = 4337, loss = 0.09403480589389801
In grad_steps = 4338, loss = 0.13835644721984863
In grad_steps = 4339, loss = 0.1877298355102539
In grad_steps = 4340, loss = 0.41776642203330994
In grad_steps = 4341, loss = 0.24495509266853333
In grad_steps = 4342, loss = 0.2070213258266449
In grad_steps = 4343, loss = 0.20641973614692688
In grad_steps = 4344, loss = 0.061736397445201874
In grad_steps = 4345, loss = 0.36205291748046875
In grad_steps = 4346, loss = 0.04220746085047722
In grad_steps = 4347, loss = 0.2645800709724426
In grad_steps = 4348, loss = 0.3506193459033966
In grad_steps = 4349, loss = 0.04870385304093361
In grad_steps = 4350, loss = 0.05795956403017044
In grad_steps = 4351, loss = 0.11430146545171738
In grad_steps = 4352, loss = 0.07186510413885117
In grad_steps = 4353, loss = 0.025884442031383514
In grad_steps = 4354, loss = 0.4880153238773346
In grad_steps = 4355, loss = 0.7765934467315674
In grad_steps = 4356, loss = 0.11249880492687225
In grad_steps = 4357, loss = 0.028265492990612984
In grad_steps = 4358, loss = 0.014853200875222683
In grad_steps = 4359, loss = 0.05218849703669548
In grad_steps = 4360, loss = 0.27434101700782776
In grad_steps = 4361, loss = 0.08400082588195801
In grad_steps = 4362, loss = 0.5576546788215637
In grad_steps = 4363, loss = 0.07693376392126083
In grad_steps = 4364, loss = 0.1975649893283844
In grad_steps = 4365, loss = 0.0906090959906578
In grad_steps = 4366, loss = 0.03285325691103935
In grad_steps = 4367, loss = 0.07216455042362213
In grad_steps = 4368, loss = 0.008971278555691242
In grad_steps = 4369, loss = 0.014771601185202599
In grad_steps = 4370, loss = 0.12040968239307404
In grad_steps = 4371, loss = 0.029148023575544357
In grad_steps = 4372, loss = 0.8398690819740295
In grad_steps = 4373, loss = 0.06760822236537933
In grad_steps = 4374, loss = 0.030523870140314102
In grad_steps = 4375, loss = 0.9434663653373718
In grad_steps = 4376, loss = 0.1032571867108345
In grad_steps = 4377, loss = 0.028006672859191895
In grad_steps = 4378, loss = 0.04887649416923523
In grad_steps = 4379, loss = 0.024282952770590782
In grad_steps = 4380, loss = 0.3150921165943146
In grad_steps = 4381, loss = 0.024214964359998703
In grad_steps = 4382, loss = 0.06194893270730972
In grad_steps = 4383, loss = 0.041188329458236694
In grad_steps = 4384, loss = 0.020205382257699966
In grad_steps = 4385, loss = 0.0070883664302527905
In grad_steps = 4386, loss = 0.036851637065410614
In grad_steps = 4387, loss = 0.022008836269378662
In grad_steps = 4388, loss = 0.10172567516565323
In grad_steps = 4389, loss = 0.013818515464663506
In grad_steps = 4390, loss = 0.031148700043559074
In grad_steps = 4391, loss = 0.4157694876194
In grad_steps = 4392, loss = 1.3012349605560303
In grad_steps = 4393, loss = 0.38732171058654785
In grad_steps = 4394, loss = 0.02691248059272766
In grad_steps = 4395, loss = 0.38734132051467896
In grad_steps = 4396, loss = 0.03231191262602806
In grad_steps = 4397, loss = 0.03083011880517006
In grad_steps = 4398, loss = 0.2863328158855438
In grad_steps = 4399, loss = 0.10479039698839188
In grad_steps = 4400, loss = 0.0234164297580719
In grad_steps = 4401, loss = 0.075999915599823
In grad_steps = 4402, loss = 2.5169103145599365
In grad_steps = 4403, loss = 0.128005713224411
In grad_steps = 4404, loss = 0.2340036928653717
In grad_steps = 4405, loss = 0.05216897279024124
In grad_steps = 4406, loss = 0.9404831528663635
In grad_steps = 4407, loss = 0.09135220944881439
In grad_steps = 4408, loss = 0.14279964566230774
In grad_steps = 4409, loss = 0.040928494185209274
In grad_steps = 4410, loss = 0.08735530078411102
In grad_steps = 4411, loss = 0.09214384108781815
In grad_steps = 4412, loss = 0.052388980984687805
In grad_steps = 4413, loss = 0.12749919295310974
In grad_steps = 4414, loss = 0.04805149510502815
In grad_steps = 4415, loss = 0.02519814670085907
In grad_steps = 4416, loss = 0.020317193120718002
In grad_steps = 4417, loss = 0.45651620626449585
In grad_steps = 4418, loss = 0.05296069383621216
In grad_steps = 4419, loss = 0.034287940710783005
In grad_steps = 4420, loss = 0.029427243396639824
In grad_steps = 4421, loss = 0.039975106716156006
In grad_steps = 4422, loss = 0.08415484428405762
In grad_steps = 4423, loss = 1.1031742095947266
In grad_steps = 4424, loss = 0.08717720210552216
In grad_steps = 4425, loss = 0.02850906178355217
In grad_steps = 4426, loss = 0.1653592586517334
In grad_steps = 4427, loss = 0.017766622826457024
In grad_steps = 4428, loss = 0.03318183496594429
In grad_steps = 4429, loss = 0.023974556475877762
In grad_steps = 4430, loss = 0.03532027080655098
In grad_steps = 4431, loss = 0.9875211715698242
In grad_steps = 4432, loss = 0.22583170235157013
In grad_steps = 4433, loss = 0.2972065210342407
In grad_steps = 4434, loss = 0.020030030980706215
In grad_steps = 4435, loss = 0.02497030422091484
In grad_steps = 4436, loss = 0.019085818901658058
In grad_steps = 4437, loss = 0.025908563286066055
In grad_steps = 4438, loss = 0.5756364464759827
In grad_steps = 4439, loss = 0.030840598046779633
In grad_steps = 4440, loss = 0.16822287440299988
In grad_steps = 4441, loss = 0.018984302878379822
In grad_steps = 4442, loss = 0.027379225939512253
In grad_steps = 4443, loss = 0.03273307532072067
In grad_steps = 4444, loss = 0.024597246199846268
In grad_steps = 4445, loss = 0.7679303884506226
In grad_steps = 4446, loss = 0.03775998204946518
In grad_steps = 4447, loss = 0.5326258540153503
In grad_steps = 4448, loss = 0.0369904562830925
In grad_steps = 4449, loss = 0.06051632761955261
In grad_steps = 4450, loss = 0.47004449367523193
In grad_steps = 4451, loss = 0.03104827180504799
In grad_steps = 4452, loss = 0.03416679427027702
In grad_steps = 4453, loss = 0.06503620743751526
In grad_steps = 4454, loss = 0.587114691734314
In grad_steps = 4455, loss = 0.026324091479182243
In grad_steps = 4456, loss = 0.24430912733078003
In grad_steps = 4457, loss = 0.5729038715362549
In grad_steps = 4458, loss = 0.02630130760371685
In grad_steps = 4459, loss = 0.10054489970207214
In grad_steps = 4460, loss = 1.562149167060852
In grad_steps = 4461, loss = 0.32397663593292236
In grad_steps = 4462, loss = 0.05602762848138809
In grad_steps = 4463, loss = 0.22892668843269348
In grad_steps = 4464, loss = 0.07520036399364471
In grad_steps = 4465, loss = 0.5940495133399963
In grad_steps = 4466, loss = 0.08748310059309006
In grad_steps = 4467, loss = 0.37932878732681274
In grad_steps = 4468, loss = 0.02589363045990467
In grad_steps = 4469, loss = 0.030161000788211823
In grad_steps = 4470, loss = 0.3136941194534302
In grad_steps = 4471, loss = 0.06359577924013138
In grad_steps = 4472, loss = 0.02104106731712818
In grad_steps = 4473, loss = 0.01592148281633854
In grad_steps = 4474, loss = 0.11882490664720535
In grad_steps = 4475, loss = 0.08469259738922119
In grad_steps = 4476, loss = 0.38213875889778137
In grad_steps = 4477, loss = 0.046123944222927094
In grad_steps = 4478, loss = 0.01975557580590248
In grad_steps = 4479, loss = 1.8330177068710327
In grad_steps = 4480, loss = 0.070738285779953
In grad_steps = 4481, loss = 0.05022639408707619
In grad_steps = 4482, loss = 0.071299709379673
In grad_steps = 4483, loss = 0.47763484716415405
In grad_steps = 4484, loss = 0.14471346139907837
In grad_steps = 4485, loss = 0.0640796422958374
In grad_steps = 4486, loss = 0.0358908474445343
In grad_steps = 4487, loss = 0.28831109404563904
In grad_steps = 4488, loss = 0.18901926279067993
In grad_steps = 4489, loss = 0.4008617699146271
In grad_steps = 4490, loss = 0.08231320977210999
In grad_steps = 4491, loss = 0.6187648177146912
In grad_steps = 4492, loss = 0.12221431732177734
In grad_steps = 4493, loss = 0.03901900723576546
In grad_steps = 4494, loss = 0.13390129804611206
In grad_steps = 4495, loss = 0.03718246519565582
In grad_steps = 4496, loss = 0.03194289654493332
In grad_steps = 4497, loss = 0.9785118103027344
In grad_steps = 4498, loss = 0.10645735263824463
In grad_steps = 4499, loss = 0.062024448066949844
In grad_steps = 4500, loss = 0.16377897560596466
In grad_steps = 4501, loss = 0.3095751404762268
In grad_steps = 4502, loss = 0.026502154767513275
In grad_steps = 4503, loss = 0.1111917570233345
In grad_steps = 4504, loss = 0.10086044669151306
In grad_steps = 4505, loss = 0.03874925151467323
In grad_steps = 4506, loss = 0.24387997388839722
In grad_steps = 4507, loss = 0.04638011381030083
In grad_steps = 4508, loss = 0.3939044177532196
In grad_steps = 4509, loss = 0.05399470776319504
In grad_steps = 4510, loss = 0.020557740703225136
In grad_steps = 4511, loss = 0.056610241532325745
In grad_steps = 4512, loss = 1.0293772220611572
In grad_steps = 4513, loss = 0.03805935010313988
In grad_steps = 4514, loss = 0.023436857387423515
In grad_steps = 4515, loss = 0.9732344746589661
In grad_steps = 4516, loss = 0.021755218505859375
In grad_steps = 4517, loss = 0.05412116274237633
In grad_steps = 4518, loss = 0.12928064167499542
In grad_steps = 4519, loss = 0.2936408519744873
In grad_steps = 4520, loss = 0.021585360169410706
In grad_steps = 4521, loss = 0.0767381563782692
In grad_steps = 4522, loss = 0.2704278230667114
In grad_steps = 4523, loss = 0.014521723613142967
In grad_steps = 4524, loss = 0.7911354303359985
In grad_steps = 4525, loss = 0.8308213949203491
In grad_steps = 4526, loss = 0.01174007449299097
In grad_steps = 4527, loss = 0.021250901743769646
In grad_steps = 4528, loss = 0.1733710914850235
In grad_steps = 4529, loss = 0.020954646170139313
In grad_steps = 4530, loss = 0.0465155728161335
In grad_steps = 4531, loss = 0.04379101097583771
In grad_steps = 4532, loss = 0.9105864763259888
In grad_steps = 4533, loss = 0.03957386314868927
In grad_steps = 4534, loss = 0.10732425004243851
In grad_steps = 4535, loss = 0.051711585372686386
In grad_steps = 4536, loss = 0.20520953834056854
In grad_steps = 4537, loss = 0.07691987603902817
In grad_steps = 4538, loss = 0.19555366039276123
In grad_steps = 4539, loss = 0.09570203721523285
In grad_steps = 4540, loss = 0.12282849103212357
In grad_steps = 4541, loss = 0.029264014214277267
In grad_steps = 4542, loss = 0.7889525890350342
In grad_steps = 4543, loss = 0.04142307490110397
In grad_steps = 4544, loss = 0.03298410773277283
In grad_steps = 4545, loss = 0.16174671053886414
In grad_steps = 4546, loss = 0.5252705216407776
In grad_steps = 4547, loss = 0.04065569117665291
In grad_steps = 4548, loss = 0.021574873477220535
In grad_steps = 4549, loss = 0.046119507402181625
In grad_steps = 4550, loss = 0.02729674242436886
In grad_steps = 4551, loss = 1.019600749015808
In grad_steps = 4552, loss = 0.050103507936000824
In grad_steps = 4553, loss = 0.0549154207110405
In grad_steps = 4554, loss = 0.043302759528160095
In grad_steps = 4555, loss = 0.07522793859243393
In grad_steps = 4556, loss = 0.04481468349695206
In grad_steps = 4557, loss = 0.06700727343559265
In grad_steps = 4558, loss = 0.02103971503674984
In grad_steps = 4559, loss = 0.602916419506073
In grad_steps = 4560, loss = 0.027003545314073563
In grad_steps = 4561, loss = 0.3107377588748932
In grad_steps = 4562, loss = 0.15196916460990906
In grad_steps = 4563, loss = 0.030907010659575462
In grad_steps = 4564, loss = 0.0701347067952156
In grad_steps = 4565, loss = 0.9812403321266174
In grad_steps = 4566, loss = 0.8125635385513306
In grad_steps = 4567, loss = 0.037695132195949554
In grad_steps = 4568, loss = 0.14361457526683807
In grad_steps = 4569, loss = 0.05608389899134636
In grad_steps = 4570, loss = 1.0984909534454346
In grad_steps = 4571, loss = 0.7438061833381653
In grad_steps = 4572, loss = 0.29391276836395264
In grad_steps = 4573, loss = 0.03917292505502701
In grad_steps = 4574, loss = 0.11669433861970901
In grad_steps = 4575, loss = 0.16188199818134308
In grad_steps = 4576, loss = 0.12013638764619827
In grad_steps = 4577, loss = 0.027534805238246918
In grad_steps = 4578, loss = 0.11227862536907196
In grad_steps = 4579, loss = 0.3837922513484955
In grad_steps = 4580, loss = 0.36367538571357727
In grad_steps = 4581, loss = 0.16942469775676727
In grad_steps = 4582, loss = 0.033694375306367874
In grad_steps = 4583, loss = 0.06907963007688522
In grad_steps = 4584, loss = 0.03067605197429657
In grad_steps = 4585, loss = 0.06781698763370514
In grad_steps = 4586, loss = 0.11129121482372284
In grad_steps = 4587, loss = 0.06327634304761887
In grad_steps = 4588, loss = 0.02723521739244461
In grad_steps = 4589, loss = 0.2835257649421692
In grad_steps = 4590, loss = 0.1117657944560051
In grad_steps = 4591, loss = 0.16974718868732452
In grad_steps = 4592, loss = 0.06910310685634613
In grad_steps = 4593, loss = 0.023827077820897102
In grad_steps = 4594, loss = 0.028011128306388855
In grad_steps = 4595, loss = 0.18590421974658966
In grad_steps = 4596, loss = 0.026631904765963554
In grad_steps = 4597, loss = 0.016589121893048286
In grad_steps = 4598, loss = 1.1943937540054321
In grad_steps = 4599, loss = 0.11861611157655716
In grad_steps = 4600, loss = 0.02079867757856846
In grad_steps = 4601, loss = 1.483483076095581
In grad_steps = 4602, loss = 0.16620047390460968
In grad_steps = 4603, loss = 0.06570450961589813
In grad_steps = 4604, loss = 0.03541870042681694
In grad_steps = 4605, loss = 0.7923629283905029
In grad_steps = 4606, loss = 0.050716694444417953
In grad_steps = 4607, loss = 0.18843546509742737
In grad_steps = 4608, loss = 0.13006161153316498
In grad_steps = 4609, loss = 0.6246927976608276
In grad_steps = 4610, loss = 0.03673557937145233
In grad_steps = 4611, loss = 0.05969756096601486
In grad_steps = 4612, loss = 0.2487596869468689
In grad_steps = 4613, loss = 0.07262100279331207
In grad_steps = 4614, loss = 0.046809256076812744
In grad_steps = 4615, loss = 0.11289801448583603
In grad_steps = 4616, loss = 0.12035298347473145
In grad_steps = 4617, loss = 0.6584856510162354
In grad_steps = 4618, loss = 0.09912210702896118
In grad_steps = 4619, loss = 0.6495171189308167
In grad_steps = 4620, loss = 0.13393786549568176
In grad_steps = 4621, loss = 0.02347555197775364
In grad_steps = 4622, loss = 0.6233397126197815
In grad_steps = 4623, loss = 0.05078456550836563
In grad_steps = 4624, loss = 0.09844543784856796
In grad_steps = 4625, loss = 0.02522045001387596
In grad_steps = 4626, loss = 0.07235581427812576
In grad_steps = 4627, loss = 0.16951628029346466
In grad_steps = 4628, loss = 0.03770649433135986
In grad_steps = 4629, loss = 0.03975134715437889
In grad_steps = 4630, loss = 0.04427553713321686
In grad_steps = 4631, loss = 0.04932340234518051
In grad_steps = 4632, loss = 0.021890148520469666
In grad_steps = 4633, loss = 0.02704565040767193
In grad_steps = 4634, loss = 0.13469719886779785
In grad_steps = 4635, loss = 0.053881868720054626
In grad_steps = 4636, loss = 0.6911859512329102
In grad_steps = 4637, loss = 0.05123665928840637
In grad_steps = 4638, loss = 0.027135513722896576
In grad_steps = 4639, loss = 0.03221830725669861
In grad_steps = 4640, loss = 0.6296254396438599
In grad_steps = 4641, loss = 0.047103144228458405
In grad_steps = 4642, loss = 0.021411167457699776
In grad_steps = 4643, loss = 0.09054718166589737
In grad_steps = 4644, loss = 0.08428570628166199
In grad_steps = 4645, loss = 0.37871256470680237
In grad_steps = 4646, loss = 0.06105850636959076
In grad_steps = 4647, loss = 0.02661195397377014
In grad_steps = 4648, loss = 0.06019961088895798
In grad_steps = 4649, loss = 0.04760739579796791
In grad_steps = 4650, loss = 0.1341908723115921
In grad_steps = 4651, loss = 0.039975542575120926
In grad_steps = 4652, loss = 0.07409444451332092
In grad_steps = 4653, loss = 0.03762757033109665
In grad_steps = 4654, loss = 0.017620781436562538
In grad_steps = 4655, loss = 0.10305319726467133
In grad_steps = 4656, loss = 0.008914552628993988
In grad_steps = 4657, loss = 0.016575755551457405
In grad_steps = 4658, loss = 0.010041781701147556
In grad_steps = 4659, loss = 0.028247453272342682
In grad_steps = 4660, loss = 0.06461811065673828
In grad_steps = 4661, loss = 0.3731326460838318
In grad_steps = 4662, loss = 0.00625809608027339
In grad_steps = 4663, loss = 0.9016727209091187
In grad_steps = 4664, loss = 0.008405886590480804
In grad_steps = 4665, loss = 0.4874817430973053
In grad_steps = 4666, loss = 0.1992637664079666
In grad_steps = 4667, loss = 0.020441440865397453
In grad_steps = 4668, loss = 0.039470743387937546
In grad_steps = 4669, loss = 0.04819202423095703
In grad_steps = 4670, loss = 0.03000139445066452
In grad_steps = 4671, loss = 0.08559032529592514
In grad_steps = 4672, loss = 1.5208731889724731
In grad_steps = 4673, loss = 0.020036403089761734
In grad_steps = 4674, loss = 0.017812542617321014
In grad_steps = 4675, loss = 0.03286490589380264
In grad_steps = 4676, loss = 0.016675284132361412
In grad_steps = 4677, loss = 0.7171488404273987
In grad_steps = 4678, loss = 0.057293422520160675
In grad_steps = 4679, loss = 0.01845437102019787
In grad_steps = 4680, loss = 0.496786892414093
In grad_steps = 4681, loss = 0.16045406460762024
In grad_steps = 4682, loss = 0.21870368719100952
In grad_steps = 4683, loss = 0.02417551539838314
In grad_steps = 4684, loss = 1.560373067855835
In grad_steps = 4685, loss = 0.06837479025125504
In grad_steps = 4686, loss = 0.7137270569801331
In grad_steps = 4687, loss = 0.05609048902988434
In grad_steps = 4688, loss = 0.20262618362903595
In grad_steps = 4689, loss = 0.12401379644870758
In grad_steps = 4690, loss = 0.07317522913217545
In grad_steps = 4691, loss = 0.06742211431264877
In grad_steps = 4692, loss = 0.09475996345281601
In grad_steps = 4693, loss = 0.05400485545396805
In grad_steps = 4694, loss = 0.06196632608771324
In grad_steps = 4695, loss = 0.5294393301010132
In grad_steps = 4696, loss = 0.035915788263082504
In grad_steps = 4697, loss = 0.43983814120292664
In grad_steps = 4698, loss = 0.055477872490882874
In grad_steps = 4699, loss = 0.04004427790641785
In grad_steps = 4700, loss = 0.06874997913837433
In grad_steps = 4701, loss = 0.04326816648244858
In grad_steps = 4702, loss = 0.08386804163455963
In grad_steps = 4703, loss = 0.08535051345825195
In grad_steps = 4704, loss = 0.3282855153083801
In grad_steps = 4705, loss = 0.00821106880903244
Elapsed time: 2688.800834417343 seconds for ensemble 0 with 2 epochs
LoRA instance 0 evaluation complete. Data saved to /pscratch/sd/t/tianle/lucid/other_source/SURP_2024/results/tmp_lora_ensemble/Llama3-8B-set-4/test_data_instance_0_seed_1.npz.
lora instance i = 0 Successfully finished.
Training lora instance 1
DEBUG: Total parameters: 8037076992
DEBUG: Trainable parameters: 6815744
Beginning epoch 1
In grad_steps = 0, loss = 0.5826846361160278
In grad_steps = 1, loss = 0.4778849184513092
In grad_steps = 2, loss = 1.736154317855835
In grad_steps = 3, loss = 0.743016242980957
In grad_steps = 4, loss = 0.9330778121948242
In grad_steps = 5, loss = 0.7297873497009277
In grad_steps = 6, loss = 1.0950602293014526
In grad_steps = 7, loss = 0.8173299431800842
In grad_steps = 8, loss = 0.45599040389060974
In grad_steps = 9, loss = 0.7038832902908325
In grad_steps = 10, loss = 0.773681104183197
In grad_steps = 11, loss = 1.0054594278335571
In grad_steps = 12, loss = 1.0314877033233643
In grad_steps = 13, loss = 0.68821781873703
In grad_steps = 14, loss = 0.6429781317710876
In grad_steps = 15, loss = 0.7569127678871155
In grad_steps = 16, loss = 0.7401519417762756
In grad_steps = 17, loss = 0.5783193111419678
In grad_steps = 18, loss = 1.1178983449935913
In grad_steps = 19, loss = 1.0203795433044434
In grad_steps = 20, loss = 0.9896478056907654
In grad_steps = 21, loss = 0.5971971154212952
In grad_steps = 22, loss = 0.6971873044967651
In grad_steps = 23, loss = 0.5844528675079346
In grad_steps = 24, loss = 0.7495173215866089
In grad_steps = 25, loss = 0.6665260791778564
In grad_steps = 26, loss = 0.662092924118042
In grad_steps = 27, loss = 0.663720428943634
In grad_steps = 28, loss = 0.7079296112060547
In grad_steps = 29, loss = 0.7177296876907349
In grad_steps = 30, loss = 0.7103749513626099
In grad_steps = 31, loss = 0.6770374178886414
In grad_steps = 32, loss = 0.6757166385650635
In grad_steps = 33, loss = 0.7468744516372681
In grad_steps = 34, loss = 0.7393470406532288
In grad_steps = 35, loss = 0.5776046514511108
In grad_steps = 36, loss = 0.7130985260009766
In grad_steps = 37, loss = 0.6829831004142761
In grad_steps = 38, loss = 0.7480261325836182
In grad_steps = 39, loss = 0.6405621767044067
In grad_steps = 40, loss = 0.6642208695411682
In grad_steps = 41, loss = 0.5715323686599731
In grad_steps = 42, loss = 0.530390739440918
In grad_steps = 43, loss = 0.5473213791847229
In grad_steps = 44, loss = 0.746417760848999
In grad_steps = 45, loss = 0.7343951463699341
In grad_steps = 46, loss = 0.9037057161331177
In grad_steps = 47, loss = 0.6202465295791626
In grad_steps = 48, loss = 0.9159439206123352
In grad_steps = 49, loss = 0.5254287719726562
In grad_steps = 50, loss = 0.9990927577018738
In grad_steps = 51, loss = 0.9943022727966309
In grad_steps = 52, loss = 0.7108711004257202
In grad_steps = 53, loss = 0.6745525598526001
In grad_steps = 54, loss = 0.7047330141067505
In grad_steps = 55, loss = 0.6116673946380615
In grad_steps = 56, loss = 0.6337848901748657
In grad_steps = 57, loss = 2.9136815071105957
In grad_steps = 58, loss = 0.6630827188491821
In grad_steps = 59, loss = 0.7035949230194092
In grad_steps = 60, loss = 0.6813426613807678
In grad_steps = 61, loss = 0.7396263480186462
In grad_steps = 62, loss = 0.8082391023635864
In grad_steps = 63, loss = 0.6370649337768555
In grad_steps = 64, loss = 0.6983005404472351
In grad_steps = 65, loss = 0.7359061241149902
In grad_steps = 66, loss = 0.6922180652618408
In grad_steps = 67, loss = 0.7121825218200684
In grad_steps = 68, loss = 0.6962539553642273
In grad_steps = 69, loss = 0.7689876556396484
In grad_steps = 70, loss = 0.6876352429389954
In grad_steps = 71, loss = 0.6825499534606934
In grad_steps = 72, loss = 0.6235231161117554
In grad_steps = 73, loss = 0.7372295260429382
In grad_steps = 74, loss = 1.758115530014038
In grad_steps = 75, loss = 0.5809506773948669
In grad_steps = 76, loss = 0.7177448272705078
In grad_steps = 77, loss = 0.7839261889457703
In grad_steps = 78, loss = 0.6472695469856262
In grad_steps = 79, loss = 0.6069712042808533
In grad_steps = 80, loss = 0.7039759159088135
In grad_steps = 81, loss = 0.6068509817123413
In grad_steps = 82, loss = 0.6855554580688477
In grad_steps = 83, loss = 0.7091729640960693
In grad_steps = 84, loss = 0.6747973561286926
In grad_steps = 85, loss = 0.7362208366394043
In grad_steps = 86, loss = 0.6110764145851135
In grad_steps = 87, loss = 0.6086107492446899
In grad_steps = 88, loss = 0.6633493304252625
In grad_steps = 89, loss = 0.7227407097816467
In grad_steps = 90, loss = 0.667741596698761
In grad_steps = 91, loss = 0.7114512920379639
In grad_steps = 92, loss = 0.7161716222763062
In grad_steps = 93, loss = 0.5941879749298096
In grad_steps = 94, loss = 0.6202773451805115
In grad_steps = 95, loss = 0.5818918943405151
In grad_steps = 96, loss = 0.782095730304718
In grad_steps = 97, loss = 0.6698547601699829
In grad_steps = 98, loss = 0.5623977184295654
In grad_steps = 99, loss = 0.6347514390945435
In grad_steps = 100, loss = 0.5481719970703125
In grad_steps = 101, loss = 0.5387874245643616
In grad_steps = 102, loss = 0.8256494402885437
In grad_steps = 103, loss = 0.6594378352165222
In grad_steps = 104, loss = 0.7070559859275818
In grad_steps = 105, loss = 0.6759138107299805
In grad_steps = 106, loss = 0.5720239877700806
In grad_steps = 107, loss = 0.8392564654350281
In grad_steps = 108, loss = 0.4589250087738037
In grad_steps = 109, loss = 0.5877993702888489
In grad_steps = 110, loss = 0.5331295132637024
In grad_steps = 111, loss = 0.513789176940918
In grad_steps = 112, loss = 0.7956119179725647
In grad_steps = 113, loss = 0.8197529911994934
In grad_steps = 114, loss = 0.5239333510398865
In grad_steps = 115, loss = 0.419479638338089
In grad_steps = 116, loss = 0.40874287486076355
In grad_steps = 117, loss = 0.5865975618362427
In grad_steps = 118, loss = 1.3319439888000488
In grad_steps = 119, loss = 1.3958762884140015
In grad_steps = 120, loss = 0.8768104314804077
In grad_steps = 121, loss = 0.8557974100112915
In grad_steps = 122, loss = 0.7572895288467407
In grad_steps = 123, loss = 0.6713921427726746
In grad_steps = 124, loss = 0.637328565120697
In grad_steps = 125, loss = 0.7545874714851379
In grad_steps = 126, loss = 0.4241591989994049
In grad_steps = 127, loss = 0.767524242401123
In grad_steps = 128, loss = 0.700243353843689
In grad_steps = 129, loss = 0.8103102445602417
In grad_steps = 130, loss = 0.5803735256195068
In grad_steps = 131, loss = 0.8320927619934082
In grad_steps = 132, loss = 0.6333670020103455
In grad_steps = 133, loss = 0.711856484413147
In grad_steps = 134, loss = 0.7100852727890015
In grad_steps = 135, loss = 0.6588224172592163
In grad_steps = 136, loss = 0.6864955425262451
In grad_steps = 137, loss = 0.7021558284759521
In grad_steps = 138, loss = 0.650166928768158
In grad_steps = 139, loss = 0.5820977687835693
In grad_steps = 140, loss = 0.623075544834137
In grad_steps = 141, loss = 0.6645445823669434
In grad_steps = 142, loss = 0.6945209503173828
In grad_steps = 143, loss = 0.6230605244636536
In grad_steps = 144, loss = 0.6268140077590942
In grad_steps = 145, loss = 0.7990412712097168
In grad_steps = 146, loss = 0.5946078896522522
In grad_steps = 147, loss = 0.6373592019081116
In grad_steps = 148, loss = 0.6534741520881653
In grad_steps = 149, loss = 0.468679279088974
In grad_steps = 150, loss = 0.4511326551437378
In grad_steps = 151, loss = 0.7923914194107056
In grad_steps = 152, loss = 0.20418523252010345
In grad_steps = 153, loss = 1.0421807765960693
In grad_steps = 154, loss = 1.2452218532562256
In grad_steps = 155, loss = 0.5464769601821899
In grad_steps = 156, loss = 0.6128134727478027
In grad_steps = 157, loss = 0.745654284954071
In grad_steps = 158, loss = 0.5597899556159973
In grad_steps = 159, loss = 0.6629490852355957
In grad_steps = 160, loss = 0.5814652442932129
In grad_steps = 161, loss = 0.4184589087963104
In grad_steps = 162, loss = 0.8694095611572266
In grad_steps = 163, loss = 0.48675671219825745
In grad_steps = 164, loss = 1.0059062242507935
In grad_steps = 165, loss = 0.5581548810005188
In grad_steps = 166, loss = 0.6915080547332764
In grad_steps = 167, loss = 0.3802403211593628
In grad_steps = 168, loss = 0.4493405818939209
In grad_steps = 169, loss = 0.7638360261917114
In grad_steps = 170, loss = 0.611026406288147
In grad_steps = 171, loss = 0.7110982537269592
In grad_steps = 172, loss = 0.6469139456748962
In grad_steps = 173, loss = 0.47041237354278564
In grad_steps = 174, loss = 0.8692494034767151
In grad_steps = 175, loss = 0.5462753176689148
In grad_steps = 176, loss = 0.4135802984237671
In grad_steps = 177, loss = 0.5188261270523071
In grad_steps = 178, loss = 0.5569823384284973
In grad_steps = 179, loss = 0.5798991918563843
In grad_steps = 180, loss = 0.49777400493621826
In grad_steps = 181, loss = 0.5144635438919067
In grad_steps = 182, loss = 0.5808573961257935
In grad_steps = 183, loss = 0.49648886919021606
In grad_steps = 184, loss = 0.41067609190940857
In grad_steps = 185, loss = 0.49163711071014404
In grad_steps = 186, loss = 1.240957260131836
In grad_steps = 187, loss = 0.22179822623729706
In grad_steps = 188, loss = 0.7898972630500793
In grad_steps = 189, loss = 0.19580702483654022
In grad_steps = 190, loss = 0.5630111694335938
In grad_steps = 191, loss = 0.606090784072876
In grad_steps = 192, loss = 0.3308454155921936
In grad_steps = 193, loss = 0.3620832860469818
In grad_steps = 194, loss = 0.6694784760475159
In grad_steps = 195, loss = 0.25057223439216614
In grad_steps = 196, loss = 0.49094682931900024
In grad_steps = 197, loss = 0.17768917977809906
In grad_steps = 198, loss = 0.7646584510803223
In grad_steps = 199, loss = 0.6572734713554382
In grad_steps = 200, loss = 0.35378769040107727
In grad_steps = 201, loss = 0.9438856840133667
In grad_steps = 202, loss = 0.24244055151939392
In grad_steps = 203, loss = 0.2296994924545288
In grad_steps = 204, loss = 0.39584285020828247
In grad_steps = 205, loss = 0.7227687835693359
In grad_steps = 206, loss = 1.1546995639801025
In grad_steps = 207, loss = 0.35591331124305725
In grad_steps = 208, loss = 0.2514711618423462
In grad_steps = 209, loss = 0.7410168647766113
In grad_steps = 210, loss = 0.3931671679019928
In grad_steps = 211, loss = 0.37612515687942505
In grad_steps = 212, loss = 0.32767772674560547
In grad_steps = 213, loss = 0.5581135749816895
In grad_steps = 214, loss = 0.5141726732254028
In grad_steps = 215, loss = 0.4911661446094513
In grad_steps = 216, loss = 0.533466100692749
In grad_steps = 217, loss = 0.5052530765533447
In grad_steps = 218, loss = 0.47606533765792847
In grad_steps = 219, loss = 0.4191523790359497
In grad_steps = 220, loss = 0.29221251606941223
In grad_steps = 221, loss = 0.24418947100639343
In grad_steps = 222, loss = 0.18756331503391266
In grad_steps = 223, loss = 0.519812822341919
In grad_steps = 224, loss = 0.1400749683380127
In grad_steps = 225, loss = 1.068858027458191
In grad_steps = 226, loss = 0.6691397428512573
In grad_steps = 227, loss = 1.2219983339309692
In grad_steps = 228, loss = 0.3037453889846802
In grad_steps = 229, loss = 0.4144570231437683
In grad_steps = 230, loss = 0.6558914184570312
In grad_steps = 231, loss = 0.30848121643066406
In grad_steps = 232, loss = 0.18760554492473602
In grad_steps = 233, loss = 0.5861803293228149
In grad_steps = 234, loss = 0.5871763229370117
In grad_steps = 235, loss = 0.3634871542453766
In grad_steps = 236, loss = 0.5950208306312561
In grad_steps = 237, loss = 0.5240377187728882
In grad_steps = 238, loss = 0.29516324400901794
In grad_steps = 239, loss = 0.35549336671829224
In grad_steps = 240, loss = 0.7742916941642761
In grad_steps = 241, loss = 0.33329474925994873
In grad_steps = 242, loss = 0.6280702352523804
In grad_steps = 243, loss = 0.5366175770759583
In grad_steps = 244, loss = 0.2768017053604126
In grad_steps = 245, loss = 0.22393479943275452
In grad_steps = 246, loss = 0.24346278607845306
In grad_steps = 247, loss = 0.5991944074630737
In grad_steps = 248, loss = 0.11681082099676132
In grad_steps = 249, loss = 0.3246021568775177
In grad_steps = 250, loss = 0.3750600218772888
In grad_steps = 251, loss = 0.7058355212211609
In grad_steps = 252, loss = 1.4303230047225952
In grad_steps = 253, loss = 0.1835046410560608
In grad_steps = 254, loss = 0.7351608276367188
In grad_steps = 255, loss = 0.6421576738357544
In grad_steps = 256, loss = 0.4651053547859192
In grad_steps = 257, loss = 0.49153804779052734
In grad_steps = 258, loss = 0.20623506605625153
In grad_steps = 259, loss = 0.7313600778579712
In grad_steps = 260, loss = 0.7890129089355469
In grad_steps = 261, loss = 0.6089538931846619
In grad_steps = 262, loss = 0.35567688941955566
In grad_steps = 263, loss = 0.7493090629577637
In grad_steps = 264, loss = 0.31456103920936584
In grad_steps = 265, loss = 0.4178028404712677
In grad_steps = 266, loss = 0.5275000929832458
In grad_steps = 267, loss = 0.3932807743549347
In grad_steps = 268, loss = 0.4610605239868164
In grad_steps = 269, loss = 0.3795621693134308
In grad_steps = 270, loss = 0.32127076387405396
In grad_steps = 271, loss = 0.3514825999736786
In grad_steps = 272, loss = 0.23395825922489166
In grad_steps = 273, loss = 0.5576614737510681
In grad_steps = 274, loss = 0.13617752492427826
In grad_steps = 275, loss = 0.26499199867248535
In grad_steps = 276, loss = 0.49053335189819336
In grad_steps = 277, loss = 0.6599253416061401
In grad_steps = 278, loss = 0.11748848855495453
In grad_steps = 279, loss = 0.9705486297607422
In grad_steps = 280, loss = 0.23284612596035004
In grad_steps = 281, loss = 0.47007185220718384
In grad_steps = 282, loss = 0.567618191242218
In grad_steps = 283, loss = 1.0068840980529785
In grad_steps = 284, loss = 0.277015745639801
In grad_steps = 285, loss = 0.669045090675354
In grad_steps = 286, loss = 0.6883308291435242
In grad_steps = 287, loss = 0.0834827646613121
In grad_steps = 288, loss = 0.07881330698728561
In grad_steps = 289, loss = 0.6212689876556396
In grad_steps = 290, loss = 0.10420197248458862
In grad_steps = 291, loss = 0.30276936292648315
In grad_steps = 292, loss = 0.5800340175628662
In grad_steps = 293, loss = 0.4202510118484497
In grad_steps = 294, loss = 0.31623825430870056
In grad_steps = 295, loss = 0.257598340511322
In grad_steps = 296, loss = 0.2078295350074768
In grad_steps = 297, loss = 0.3142979145050049
In grad_steps = 298, loss = 0.3506377041339874
In grad_steps = 299, loss = 0.527661144733429
In grad_steps = 300, loss = 0.5916403532028198
In grad_steps = 301, loss = 0.3356768488883972
In grad_steps = 302, loss = 0.34493476152420044
In grad_steps = 303, loss = 0.2764989733695984
In grad_steps = 304, loss = 0.4422522783279419
In grad_steps = 305, loss = 1.681220293045044
In grad_steps = 306, loss = 0.09867692738771439
In grad_steps = 307, loss = 0.3912310004234314
In grad_steps = 308, loss = 0.4632716476917267
In grad_steps = 309, loss = 0.87107253074646
In grad_steps = 310, loss = 0.4265565276145935
In grad_steps = 311, loss = 0.7918059229850769
In grad_steps = 312, loss = 0.5462157726287842
In grad_steps = 313, loss = 0.6577877402305603
In grad_steps = 314, loss = 0.6850149631500244
In grad_steps = 315, loss = 0.3662823736667633
In grad_steps = 316, loss = 0.35489368438720703
In grad_steps = 317, loss = 0.40101590752601624
In grad_steps = 318, loss = 0.28759104013442993
In grad_steps = 319, loss = 0.44255295395851135
In grad_steps = 320, loss = 0.23130623996257782
In grad_steps = 321, loss = 0.3998781740665436
In grad_steps = 322, loss = 0.6473363041877747
In grad_steps = 323, loss = 0.5642613768577576
In grad_steps = 324, loss = 0.25716838240623474
In grad_steps = 325, loss = 0.21365372836589813
In grad_steps = 326, loss = 0.15762656927108765
In grad_steps = 327, loss = 0.24570776522159576
In grad_steps = 328, loss = 0.19639340043067932
In grad_steps = 329, loss = 0.14828307926654816
In grad_steps = 330, loss = 0.6205356121063232
In grad_steps = 331, loss = 0.239187553524971
In grad_steps = 332, loss = 0.13621246814727783
In grad_steps = 333, loss = 0.38565030694007874
In grad_steps = 334, loss = 0.04534539952874184
In grad_steps = 335, loss = 0.23422369360923767
In grad_steps = 336, loss = 0.46795371174812317
In grad_steps = 337, loss = 0.0654141902923584
In grad_steps = 338, loss = 0.2955978214740753
In grad_steps = 339, loss = 0.725193977355957
In grad_steps = 340, loss = 0.5601953864097595
In grad_steps = 341, loss = 0.03724166750907898
In grad_steps = 342, loss = 1.5091792345046997
In grad_steps = 343, loss = 0.78587806224823
In grad_steps = 344, loss = 0.06559322774410248
In grad_steps = 345, loss = 0.0779372975230217
In grad_steps = 346, loss = 0.4333471655845642
In grad_steps = 347, loss = 0.650046169757843
In grad_steps = 348, loss = 0.5130597352981567
In grad_steps = 349, loss = 0.924261748790741
In grad_steps = 350, loss = 0.7434315085411072
In grad_steps = 351, loss = 0.49772584438323975
In grad_steps = 352, loss = 0.7586303949356079
In grad_steps = 353, loss = 0.22151115536689758
In grad_steps = 354, loss = 0.3956203758716583
In grad_steps = 355, loss = 1.220676064491272
In grad_steps = 356, loss = 0.1438707858324051
In grad_steps = 357, loss = 0.608954906463623
In grad_steps = 358, loss = 0.12967577576637268
In grad_steps = 359, loss = 0.5813767910003662
In grad_steps = 360, loss = 0.6709076762199402
In grad_steps = 361, loss = 0.48469048738479614
In grad_steps = 362, loss = 0.23887589573860168
In grad_steps = 363, loss = 0.27098262310028076
In grad_steps = 364, loss = 0.42812982201576233
In grad_steps = 365, loss = 0.3165580928325653
In grad_steps = 366, loss = 0.2627374827861786
In grad_steps = 367, loss = 0.34991782903671265
In grad_steps = 368, loss = 0.2550746202468872
In grad_steps = 369, loss = 0.27623629570007324
In grad_steps = 370, loss = 0.6778154969215393
In grad_steps = 371, loss = 0.29012590646743774
In grad_steps = 372, loss = 0.1477450430393219
In grad_steps = 373, loss = 0.47010716795921326
In grad_steps = 374, loss = 0.1652175784111023
In grad_steps = 375, loss = 0.2400279939174652
In grad_steps = 376, loss = 0.054972462356090546
In grad_steps = 377, loss = 0.5349107384681702
In grad_steps = 378, loss = 0.44878020882606506
In grad_steps = 379, loss = 0.11841832101345062
In grad_steps = 380, loss = 0.3667607605457306
In grad_steps = 381, loss = 0.8477411866188049
In grad_steps = 382, loss = 0.27069658041000366
In grad_steps = 383, loss = 0.234991654753685
In grad_steps = 384, loss = 0.2951333522796631
In grad_steps = 385, loss = 0.19339364767074585
In grad_steps = 386, loss = 1.6486164331436157
In grad_steps = 387, loss = 0.2894841730594635
In grad_steps = 388, loss = 0.7368270754814148
In grad_steps = 389, loss = 0.7555861473083496
In grad_steps = 390, loss = 0.2739710509777069
In grad_steps = 391, loss = 0.19065921008586884
In grad_steps = 392, loss = 0.12102697044610977
In grad_steps = 393, loss = 0.5926949977874756
In grad_steps = 394, loss = 0.17268109321594238
In grad_steps = 395, loss = 0.4733724594116211
In grad_steps = 396, loss = 0.30375948548316956
In grad_steps = 397, loss = 0.5254841446876526
In grad_steps = 398, loss = 0.15742209553718567
In grad_steps = 399, loss = 0.3177849054336548
In grad_steps = 400, loss = 0.35883384943008423
In grad_steps = 401, loss = 0.1570100039243698
In grad_steps = 402, loss = 0.4670639932155609
In grad_steps = 403, loss = 0.8557962775230408
In grad_steps = 404, loss = 0.16657894849777222
In grad_steps = 405, loss = 0.13396580517292023
In grad_steps = 406, loss = 0.9999410510063171
In grad_steps = 407, loss = 0.47067660093307495
In grad_steps = 408, loss = 0.11762046813964844
In grad_steps = 409, loss = 0.6199942827224731
In grad_steps = 410, loss = 0.19431716203689575
In grad_steps = 411, loss = 0.6867643594741821
In grad_steps = 412, loss = 0.25724372267723083
In grad_steps = 413, loss = 0.28487658500671387
In grad_steps = 414, loss = 0.6272342801094055
In grad_steps = 415, loss = 0.453646719455719
In grad_steps = 416, loss = 0.18412335216999054
In grad_steps = 417, loss = 0.3143014907836914
In grad_steps = 418, loss = 0.40051794052124023
In grad_steps = 419, loss = 0.21309231221675873
In grad_steps = 420, loss = 0.07284781336784363
In grad_steps = 421, loss = 0.8469531536102295
In grad_steps = 422, loss = 0.7293797731399536
In grad_steps = 423, loss = 0.19844621419906616
In grad_steps = 424, loss = 0.6044723391532898
In grad_steps = 425, loss = 0.06918352097272873
In grad_steps = 426, loss = 0.5913933515548706
In grad_steps = 427, loss = 0.22140195965766907
In grad_steps = 428, loss = 0.09166847169399261
In grad_steps = 429, loss = 0.5660349726676941
In grad_steps = 430, loss = 0.4083516299724579
In grad_steps = 431, loss = 0.312955379486084
In grad_steps = 432, loss = 0.1346648633480072
In grad_steps = 433, loss = 0.45455402135849
In grad_steps = 434, loss = 0.9443941116333008
In grad_steps = 435, loss = 0.06066175550222397
In grad_steps = 436, loss = 0.26353883743286133
In grad_steps = 437, loss = 0.2641414999961853
In grad_steps = 438, loss = 1.1892623901367188
In grad_steps = 439, loss = 0.13524606823921204
In grad_steps = 440, loss = 0.10008707642555237
In grad_steps = 441, loss = 0.7366003394126892
In grad_steps = 442, loss = 0.5179167985916138
In grad_steps = 443, loss = 0.13613663613796234
In grad_steps = 444, loss = 0.5835809707641602
In grad_steps = 445, loss = 0.1540820449590683
In grad_steps = 446, loss = 1.4165245294570923
In grad_steps = 447, loss = 0.8832886815071106
In grad_steps = 448, loss = 0.7375121712684631
In grad_steps = 449, loss = 0.17713192105293274
In grad_steps = 450, loss = 0.4691789746284485
In grad_steps = 451, loss = 0.3573305010795593
In grad_steps = 452, loss = 0.35274386405944824
In grad_steps = 453, loss = 0.35003775358200073
In grad_steps = 454, loss = 0.38919657468795776
In grad_steps = 455, loss = 0.4884806275367737
In grad_steps = 456, loss = 0.7330095767974854
In grad_steps = 457, loss = 0.33049774169921875
In grad_steps = 458, loss = 0.7743579745292664
In grad_steps = 459, loss = 0.3488917350769043
In grad_steps = 460, loss = 0.46521565318107605
In grad_steps = 461, loss = 0.11440806090831757
In grad_steps = 462, loss = 0.6401655673980713
In grad_steps = 463, loss = 0.30247294902801514
In grad_steps = 464, loss = 0.5688053369522095
In grad_steps = 465, loss = 0.5981200933456421
In grad_steps = 466, loss = 0.549170732498169
In grad_steps = 467, loss = 0.42018797993659973
In grad_steps = 468, loss = 0.9391143321990967
In grad_steps = 469, loss = 0.3611365854740143
In grad_steps = 470, loss = 0.45304691791534424
In grad_steps = 471, loss = 0.1322779357433319
In grad_steps = 472, loss = 0.18935814499855042
In grad_steps = 473, loss = 0.15590745210647583
In grad_steps = 474, loss = 0.925299882888794
In grad_steps = 475, loss = 0.7720748782157898
In grad_steps = 476, loss = 0.16392797231674194
In grad_steps = 477, loss = 0.30514198541641235
In grad_steps = 478, loss = 0.22741390764713287
In grad_steps = 479, loss = 0.6269822716712952
In grad_steps = 480, loss = 0.17756865918636322
In grad_steps = 481, loss = 0.6308385133743286
In grad_steps = 482, loss = 0.12719625234603882
In grad_steps = 483, loss = 0.19963552057743073
In grad_steps = 484, loss = 0.47988012433052063
In grad_steps = 485, loss = 0.4731903374195099
In grad_steps = 486, loss = 0.2257116287946701
In grad_steps = 487, loss = 0.3169565498828888
In grad_steps = 488, loss = 1.0133401155471802
In grad_steps = 489, loss = 1.038899540901184
In grad_steps = 490, loss = 0.15074419975280762
In grad_steps = 491, loss = 0.4803474545478821
In grad_steps = 492, loss = 0.20925959944725037
In grad_steps = 493, loss = 0.07085376977920532
In grad_steps = 494, loss = 1.0505942106246948
In grad_steps = 495, loss = 0.14615139365196228
In grad_steps = 496, loss = 0.6907958388328552
In grad_steps = 497, loss = 0.1837378740310669
In grad_steps = 498, loss = 0.5594629645347595
In grad_steps = 499, loss = 0.2636531591415405
In grad_steps = 500, loss = 0.13757681846618652
In grad_steps = 501, loss = 0.42093950510025024
In grad_steps = 502, loss = 0.6672410368919373
In grad_steps = 503, loss = 0.5874162316322327
In grad_steps = 504, loss = 0.4642375707626343
In grad_steps = 505, loss = 0.6995816826820374
In grad_steps = 506, loss = 0.32898613810539246
In grad_steps = 507, loss = 0.7025332450866699
In grad_steps = 508, loss = 0.733618438243866
In grad_steps = 509, loss = 0.3893309235572815
In grad_steps = 510, loss = 0.12103703618049622
In grad_steps = 511, loss = 0.35737910866737366
In grad_steps = 512, loss = 0.23891526460647583
In grad_steps = 513, loss = 0.48603877425193787
In grad_steps = 514, loss = 0.4147299528121948
In grad_steps = 515, loss = 0.4592415392398834
In grad_steps = 516, loss = 0.5308225154876709
In grad_steps = 517, loss = 0.46130284667015076
In grad_steps = 518, loss = 0.6155085563659668
In grad_steps = 519, loss = 0.314554363489151
In grad_steps = 520, loss = 0.16553324460983276
In grad_steps = 521, loss = 0.46484825015068054
In grad_steps = 522, loss = 0.18493889272212982
In grad_steps = 523, loss = 0.2903297543525696
In grad_steps = 524, loss = 0.3085552752017975
In grad_steps = 525, loss = 0.3409658670425415
In grad_steps = 526, loss = 0.16219255328178406
In grad_steps = 527, loss = 0.5068050622940063
In grad_steps = 528, loss = 0.21240234375
In grad_steps = 529, loss = 0.08912260085344315
In grad_steps = 530, loss = 0.0834711343050003
In grad_steps = 531, loss = 0.5806336402893066
In grad_steps = 532, loss = 0.18034055829048157
In grad_steps = 533, loss = 0.2828214466571808
In grad_steps = 534, loss = 0.9786898493766785
In grad_steps = 535, loss = 0.5431724786758423
In grad_steps = 536, loss = 0.15667018294334412
In grad_steps = 537, loss = 0.5574239492416382
In grad_steps = 538, loss = 1.4344795942306519
In grad_steps = 539, loss = 0.2972964942455292
In grad_steps = 540, loss = 0.10357698053121567
In grad_steps = 541, loss = 0.5240316390991211
In grad_steps = 542, loss = 0.8057349920272827
In grad_steps = 543, loss = 0.07860256731510162
In grad_steps = 544, loss = 0.7863749861717224
In grad_steps = 545, loss = 0.9363433122634888
In grad_steps = 546, loss = 0.22370511293411255
In grad_steps = 547, loss = 0.3585583567619324
In grad_steps = 548, loss = 0.25286492705345154
In grad_steps = 549, loss = 0.4256942868232727
In grad_steps = 550, loss = 0.718921422958374
In grad_steps = 551, loss = 0.17947907745838165
In grad_steps = 552, loss = 0.24973735213279724
In grad_steps = 553, loss = 0.38823768496513367
In grad_steps = 554, loss = 0.5739803910255432
In grad_steps = 555, loss = 0.5958050489425659
In grad_steps = 556, loss = 0.5925868153572083
In grad_steps = 557, loss = 0.3539462089538574
In grad_steps = 558, loss = 0.43564674258232117
In grad_steps = 559, loss = 0.3060358166694641
In grad_steps = 560, loss = 1.011440634727478
In grad_steps = 561, loss = 0.346590131521225
In grad_steps = 562, loss = 0.4522915482521057
In grad_steps = 563, loss = 0.33430516719818115
In grad_steps = 564, loss = 0.9158703088760376
In grad_steps = 565, loss = 0.42978477478027344
In grad_steps = 566, loss = 0.23763662576675415
In grad_steps = 567, loss = 0.787837028503418
In grad_steps = 568, loss = 0.7486891746520996
In grad_steps = 569, loss = 0.17619651556015015
In grad_steps = 570, loss = 0.5379390120506287
In grad_steps = 571, loss = 0.16376997530460358
In grad_steps = 572, loss = 0.3280884027481079
In grad_steps = 573, loss = 0.23648959398269653
In grad_steps = 574, loss = 0.17869368195533752
In grad_steps = 575, loss = 0.2981935143470764
In grad_steps = 576, loss = 0.18864496052265167
In grad_steps = 577, loss = 0.5824426412582397
In grad_steps = 578, loss = 0.512749433517456
In grad_steps = 579, loss = 0.3744073510169983
In grad_steps = 580, loss = 0.8288396000862122
In grad_steps = 581, loss = 0.4253447949886322
In grad_steps = 582, loss = 0.6406265497207642
In grad_steps = 583, loss = 0.10999154299497604
In grad_steps = 584, loss = 0.0646609365940094
In grad_steps = 585, loss = 0.07378903776407242
In grad_steps = 586, loss = 0.6244465112686157
In grad_steps = 587, loss = 0.24153780937194824
In grad_steps = 588, loss = 0.3819303512573242
In grad_steps = 589, loss = 0.9131621718406677
In grad_steps = 590, loss = 0.3143072724342346
In grad_steps = 591, loss = 0.44636932015419006
In grad_steps = 592, loss = 1.3801714181900024
In grad_steps = 593, loss = 0.13221889734268188
In grad_steps = 594, loss = 0.11112505197525024
In grad_steps = 595, loss = 0.41972410678863525
In grad_steps = 596, loss = 0.8071483373641968
In grad_steps = 597, loss = 0.5107966661453247
In grad_steps = 598, loss = 0.36774805188179016
In grad_steps = 599, loss = 0.32365190982818604
In grad_steps = 600, loss = 0.6250699758529663
In grad_steps = 601, loss = 0.1729867160320282
In grad_steps = 602, loss = 0.2980469763278961
In grad_steps = 603, loss = 0.3860936164855957
In grad_steps = 604, loss = 0.17130334675312042
In grad_steps = 605, loss = 0.39297986030578613
In grad_steps = 606, loss = 0.191768079996109
In grad_steps = 607, loss = 0.2091885805130005
In grad_steps = 608, loss = 0.3941989541053772
In grad_steps = 609, loss = 0.4220743179321289
In grad_steps = 610, loss = 0.4601490795612335
In grad_steps = 611, loss = 0.44047611951828003
In grad_steps = 612, loss = 0.38748401403427124
In grad_steps = 613, loss = 0.3978255093097687
In grad_steps = 614, loss = 0.2727715075016022
In grad_steps = 615, loss = 0.6192488670349121
In grad_steps = 616, loss = 0.15834444761276245
In grad_steps = 617, loss = 0.07528466731309891
In grad_steps = 618, loss = 0.151735320687294
In grad_steps = 619, loss = 0.06914083659648895
In grad_steps = 620, loss = 0.11946243047714233
In grad_steps = 621, loss = 1.4060423374176025
In grad_steps = 622, loss = 0.19073303043842316
In grad_steps = 623, loss = 0.09367614984512329
In grad_steps = 624, loss = 0.059165969491004944
In grad_steps = 625, loss = 0.4968538284301758
In grad_steps = 626, loss = 0.27457162737846375
In grad_steps = 627, loss = 0.02363322675228119
In grad_steps = 628, loss = 0.16896867752075195
In grad_steps = 629, loss = 0.06902921944856644
In grad_steps = 630, loss = 0.11712455749511719
In grad_steps = 631, loss = 0.10030268877744675
In grad_steps = 632, loss = 0.057421792298555374
In grad_steps = 633, loss = 0.8872402310371399
In grad_steps = 634, loss = 0.20480595529079437
In grad_steps = 635, loss = 0.40978342294692993
In grad_steps = 636, loss = 0.12338917702436447
In grad_steps = 637, loss = 0.3090077042579651
In grad_steps = 638, loss = 0.021919576451182365
In grad_steps = 639, loss = 0.33898547291755676
In grad_steps = 640, loss = 0.13650883734226227
In grad_steps = 641, loss = 0.2122959941625595
In grad_steps = 642, loss = 0.5962667465209961
In grad_steps = 643, loss = 0.6256824731826782
In grad_steps = 644, loss = 1.1206086874008179
In grad_steps = 645, loss = 1.2425217628479004
In grad_steps = 646, loss = 0.15310540795326233
In grad_steps = 647, loss = 0.3103320002555847
In grad_steps = 648, loss = 0.10247214138507843
In grad_steps = 649, loss = 0.5342972874641418
In grad_steps = 650, loss = 0.7759417295455933
In grad_steps = 651, loss = 0.2713043689727783
In grad_steps = 652, loss = 0.9067841172218323
In grad_steps = 653, loss = 0.4665822684764862
In grad_steps = 654, loss = 0.6709667444229126
In grad_steps = 655, loss = 0.4696698486804962
In grad_steps = 656, loss = 0.2654552757740021
In grad_steps = 657, loss = 0.26131078600883484
In grad_steps = 658, loss = 0.41025370359420776
In grad_steps = 659, loss = 0.33144158124923706
In grad_steps = 660, loss = 0.38565483689308167
In grad_steps = 661, loss = 0.24815402925014496
In grad_steps = 662, loss = 0.387898325920105
In grad_steps = 663, loss = 0.23646202683448792
In grad_steps = 664, loss = 0.12652896344661713
In grad_steps = 665, loss = 0.254315584897995
In grad_steps = 666, loss = 0.18975719809532166
In grad_steps = 667, loss = 0.5568833351135254
In grad_steps = 668, loss = 0.18718892335891724
In grad_steps = 669, loss = 0.2827228307723999
In grad_steps = 670, loss = 0.5756192207336426
In grad_steps = 671, loss = 0.6269957423210144
In grad_steps = 672, loss = 0.08309230208396912
In grad_steps = 673, loss = 0.749055027961731
In grad_steps = 674, loss = 0.09696496278047562
In grad_steps = 675, loss = 1.0110822916030884
In grad_steps = 676, loss = 0.12823127210140228
In grad_steps = 677, loss = 0.5213741660118103
In grad_steps = 678, loss = 0.029173515737056732
In grad_steps = 679, loss = 0.1505138874053955
In grad_steps = 680, loss = 0.039541635662317276
In grad_steps = 681, loss = 1.1594117879867554
In grad_steps = 682, loss = 0.6922014951705933
In grad_steps = 683, loss = 0.1592322289943695
In grad_steps = 684, loss = 0.1705971360206604
In grad_steps = 685, loss = 0.1826705038547516
In grad_steps = 686, loss = 0.31588152050971985
In grad_steps = 687, loss = 0.5291635990142822
In grad_steps = 688, loss = 0.7239402532577515
In grad_steps = 689, loss = 0.1261546015739441
In grad_steps = 690, loss = 0.2736435830593109
In grad_steps = 691, loss = 0.22779786586761475
In grad_steps = 692, loss = 0.26863500475883484
In grad_steps = 693, loss = 0.3859814703464508
In grad_steps = 694, loss = 0.28379708528518677
In grad_steps = 695, loss = 0.4504619836807251
In grad_steps = 696, loss = 0.7067381143569946
In grad_steps = 697, loss = 0.32524073123931885
In grad_steps = 698, loss = 0.2296752780675888
In grad_steps = 699, loss = 0.6708365678787231
In grad_steps = 700, loss = 0.5478029251098633
In grad_steps = 701, loss = 0.09460754692554474
In grad_steps = 702, loss = 0.14540119469165802
In grad_steps = 703, loss = 0.1530509740114212
In grad_steps = 704, loss = 0.1894962340593338
In grad_steps = 705, loss = 0.06906181573867798
In grad_steps = 706, loss = 0.5879570245742798
In grad_steps = 707, loss = 0.08192196488380432
In grad_steps = 708, loss = 0.035862721502780914
In grad_steps = 709, loss = 0.3401431143283844
In grad_steps = 710, loss = 0.3704872131347656
In grad_steps = 711, loss = 1.1051759719848633
In grad_steps = 712, loss = 0.6809407472610474
In grad_steps = 713, loss = 1.479176640510559
In grad_steps = 714, loss = 0.47463202476501465
In grad_steps = 715, loss = 1.0398145914077759
In grad_steps = 716, loss = 0.3722492754459381
In grad_steps = 717, loss = 0.1622307449579239
In grad_steps = 718, loss = 0.37664490938186646
In grad_steps = 719, loss = 0.07083183526992798
In grad_steps = 720, loss = 0.07544064521789551
In grad_steps = 721, loss = 1.1683803796768188
In grad_steps = 722, loss = 0.30632925033569336
In grad_steps = 723, loss = 0.44560492038726807
In grad_steps = 724, loss = 0.09730251133441925
In grad_steps = 725, loss = 0.5353284478187561
In grad_steps = 726, loss = 0.2236080765724182
In grad_steps = 727, loss = 0.49294888973236084
In grad_steps = 728, loss = 0.09762747585773468
In grad_steps = 729, loss = 0.11942752450704575
In grad_steps = 730, loss = 0.22740428149700165
In grad_steps = 731, loss = 0.5762603878974915
In grad_steps = 732, loss = 0.37706825137138367
In grad_steps = 733, loss = 1.0995020866394043
In grad_steps = 734, loss = 0.33327189087867737
In grad_steps = 735, loss = 0.36968541145324707
In grad_steps = 736, loss = 0.23871968686580658
In grad_steps = 737, loss = 0.13477198779582977
In grad_steps = 738, loss = 0.9282535314559937
In grad_steps = 739, loss = 0.10359753668308258
In grad_steps = 740, loss = 0.1788487583398819
In grad_steps = 741, loss = 0.3413078784942627
In grad_steps = 742, loss = 0.14954805374145508
In grad_steps = 743, loss = 0.11129790544509888
In grad_steps = 744, loss = 0.9183570146560669
In grad_steps = 745, loss = 0.710586428642273
In grad_steps = 746, loss = 0.3458670675754547
In grad_steps = 747, loss = 0.5052499771118164
In grad_steps = 748, loss = 0.09294098615646362
In grad_steps = 749, loss = 0.7719255685806274
In grad_steps = 750, loss = 0.7769399285316467
In grad_steps = 751, loss = 0.12750056385993958
In grad_steps = 752, loss = 0.07125751674175262
In grad_steps = 753, loss = 0.14313587546348572
In grad_steps = 754, loss = 0.702899694442749
In grad_steps = 755, loss = 0.713640034198761
In grad_steps = 756, loss = 0.11917568743228912
In grad_steps = 757, loss = 0.1224924772977829
In grad_steps = 758, loss = 0.792017936706543
In grad_steps = 759, loss = 0.5965238809585571
In grad_steps = 760, loss = 0.2797301709651947
In grad_steps = 761, loss = 0.3926607370376587
In grad_steps = 762, loss = 0.2860424816608429
In grad_steps = 763, loss = 0.5281272530555725
In grad_steps = 764, loss = 0.6829870939254761
In grad_steps = 765, loss = 0.5659019947052002
In grad_steps = 766, loss = 0.13437840342521667
In grad_steps = 767, loss = 0.4259878695011139
In grad_steps = 768, loss = 0.38420286774635315
In grad_steps = 769, loss = 1.0744074583053589
In grad_steps = 770, loss = 0.18431824445724487
In grad_steps = 771, loss = 0.1699436753988266
In grad_steps = 772, loss = 0.32448774576187134
In grad_steps = 773, loss = 1.0231027603149414
In grad_steps = 774, loss = 0.39165669679641724
In grad_steps = 775, loss = 0.13854044675827026
In grad_steps = 776, loss = 0.24632368981838226
In grad_steps = 777, loss = 0.6663910150527954
In grad_steps = 778, loss = 0.24682262539863586
In grad_steps = 779, loss = 0.27824676036834717
In grad_steps = 780, loss = 0.5157676935195923
In grad_steps = 781, loss = 0.5109785199165344
In grad_steps = 782, loss = 0.26090767979621887
In grad_steps = 783, loss = 0.9735146164894104
In grad_steps = 784, loss = 0.4659891724586487
In grad_steps = 785, loss = 0.11684375256299973
In grad_steps = 786, loss = 0.2687959671020508
In grad_steps = 787, loss = 0.4275999665260315
In grad_steps = 788, loss = 0.5068252086639404
In grad_steps = 789, loss = 0.2944243550300598
In grad_steps = 790, loss = 0.4708544611930847
In grad_steps = 791, loss = 0.15071716904640198
In grad_steps = 792, loss = 0.12021669000387192
In grad_steps = 793, loss = 0.49662595987319946
In grad_steps = 794, loss = 1.4869744777679443
In grad_steps = 795, loss = 0.6924269199371338
In grad_steps = 796, loss = 0.17157119512557983
In grad_steps = 797, loss = 0.40021970868110657
In grad_steps = 798, loss = 0.18802420794963837
In grad_steps = 799, loss = 0.49043625593185425
In grad_steps = 800, loss = 0.4750599265098572
In grad_steps = 801, loss = 0.34912264347076416
In grad_steps = 802, loss = 0.27504199743270874
In grad_steps = 803, loss = 0.2020845264196396
In grad_steps = 804, loss = 0.09297792613506317
In grad_steps = 805, loss = 0.5155527591705322
In grad_steps = 806, loss = 0.15237627923488617
In grad_steps = 807, loss = 0.13781039416790009
In grad_steps = 808, loss = 0.2154586762189865
In grad_steps = 809, loss = 0.08560369163751602
In grad_steps = 810, loss = 0.6710901260375977
In grad_steps = 811, loss = 0.19581809639930725
In grad_steps = 812, loss = 0.5051558613777161
In grad_steps = 813, loss = 0.36820635199546814
In grad_steps = 814, loss = 0.12875398993492126
In grad_steps = 815, loss = 0.1300184577703476
In grad_steps = 816, loss = 1.260990858078003
In grad_steps = 817, loss = 0.0607636421918869
In grad_steps = 818, loss = 0.5592939853668213
In grad_steps = 819, loss = 0.0703057125210762
In grad_steps = 820, loss = 0.15299926698207855
In grad_steps = 821, loss = 0.1074673980474472
In grad_steps = 822, loss = 0.6101954579353333
In grad_steps = 823, loss = 0.2614985704421997
In grad_steps = 824, loss = 0.048719558864831924
In grad_steps = 825, loss = 0.05831295996904373
In grad_steps = 826, loss = 0.7916989922523499
In grad_steps = 827, loss = 0.1285085380077362
In grad_steps = 828, loss = 0.9356394410133362
In grad_steps = 829, loss = 0.2635864019393921
In grad_steps = 830, loss = 0.1973663866519928
In grad_steps = 831, loss = 0.046456314623355865
In grad_steps = 832, loss = 0.8878962397575378
In grad_steps = 833, loss = 0.6271756887435913
In grad_steps = 834, loss = 0.5152736306190491
In grad_steps = 835, loss = 1.5026986598968506
In grad_steps = 836, loss = 0.5690420269966125
In grad_steps = 837, loss = 0.7165830731391907
In grad_steps = 838, loss = 1.370760202407837
In grad_steps = 839, loss = 0.6601331233978271
In grad_steps = 840, loss = 0.17749032378196716
In grad_steps = 841, loss = 0.7676783800125122
In grad_steps = 842, loss = 0.17267119884490967
In grad_steps = 843, loss = 0.7348119020462036
In grad_steps = 844, loss = 0.5682101249694824
In grad_steps = 845, loss = 0.26738253235816956
In grad_steps = 846, loss = 0.800134539604187
In grad_steps = 847, loss = 0.6171636581420898
In grad_steps = 848, loss = 0.6563954949378967
In grad_steps = 849, loss = 0.9318283796310425
In grad_steps = 850, loss = 0.6852822303771973
In grad_steps = 851, loss = 0.5615681409835815
In grad_steps = 852, loss = 0.41341447830200195
In grad_steps = 853, loss = 0.3329384922981262
In grad_steps = 854, loss = 0.30193376541137695
In grad_steps = 855, loss = 0.7306352257728577
In grad_steps = 856, loss = 0.4552777409553528
In grad_steps = 857, loss = 0.3416799008846283
In grad_steps = 858, loss = 0.6380442976951599
In grad_steps = 859, loss = 0.36414283514022827
In grad_steps = 860, loss = 0.35197019577026367
In grad_steps = 861, loss = 0.39439141750335693
In grad_steps = 862, loss = 0.22940221428871155
In grad_steps = 863, loss = 0.37632590532302856
In grad_steps = 864, loss = 0.3014644980430603
In grad_steps = 865, loss = 0.5180599093437195
In grad_steps = 866, loss = 0.3281078040599823
In grad_steps = 867, loss = 0.23040355741977692
In grad_steps = 868, loss = 0.15213192999362946
In grad_steps = 869, loss = 0.31040865182876587
In grad_steps = 870, loss = 0.11217454075813293
In grad_steps = 871, loss = 0.22041814029216766
In grad_steps = 872, loss = 0.17556622624397278
In grad_steps = 873, loss = 0.4092026650905609
In grad_steps = 874, loss = 0.04121829569339752
In grad_steps = 875, loss = 0.028761647641658783
In grad_steps = 876, loss = 0.2179262936115265
In grad_steps = 877, loss = 0.04469798505306244
In grad_steps = 878, loss = 0.013552158139646053
In grad_steps = 879, loss = 0.013661017641425133
In grad_steps = 880, loss = 1.9552323818206787
In grad_steps = 881, loss = 0.7459739446640015
In grad_steps = 882, loss = 0.04674641042947769
In grad_steps = 883, loss = 0.5039085745811462
In grad_steps = 884, loss = 0.045557569712400436
In grad_steps = 885, loss = 0.36688515543937683
In grad_steps = 886, loss = 0.8301321864128113
In grad_steps = 887, loss = 0.2374434471130371
In grad_steps = 888, loss = 0.2571184039115906
In grad_steps = 889, loss = 0.9185230135917664
In grad_steps = 890, loss = 0.2582762539386749
In grad_steps = 891, loss = 0.5862089395523071
In grad_steps = 892, loss = 1.3458248376846313
In grad_steps = 893, loss = 0.36967727541923523
In grad_steps = 894, loss = 0.2113737165927887
In grad_steps = 895, loss = 0.32748711109161377
In grad_steps = 896, loss = 0.3941790759563446
In grad_steps = 897, loss = 0.5270360708236694
In grad_steps = 898, loss = 0.762698233127594
In grad_steps = 899, loss = 0.3377722501754761
In grad_steps = 900, loss = 0.44174087047576904
In grad_steps = 901, loss = 0.5455492734909058
In grad_steps = 902, loss = 0.29301902651786804
In grad_steps = 903, loss = 0.14751005172729492
In grad_steps = 904, loss = 0.24388036131858826
In grad_steps = 905, loss = 0.45174288749694824
In grad_steps = 906, loss = 0.23438677191734314
In grad_steps = 907, loss = 0.2964571714401245
In grad_steps = 908, loss = 0.1294599175453186
In grad_steps = 909, loss = 0.6891962885856628
In grad_steps = 910, loss = 0.07790251821279526
In grad_steps = 911, loss = 0.19555336236953735
In grad_steps = 912, loss = 0.08095564693212509
In grad_steps = 913, loss = 0.7197004556655884
In grad_steps = 914, loss = 0.38823017477989197
In grad_steps = 915, loss = 0.11025966703891754
In grad_steps = 916, loss = 0.0978819727897644
In grad_steps = 917, loss = 0.05420856550335884
In grad_steps = 918, loss = 1.4357126951217651
In grad_steps = 919, loss = 1.7595256567001343
In grad_steps = 920, loss = 0.9201037883758545
In grad_steps = 921, loss = 0.46871837973594666
In grad_steps = 922, loss = 0.06035125255584717
In grad_steps = 923, loss = 1.0272817611694336
In grad_steps = 924, loss = 0.9716081023216248
In grad_steps = 925, loss = 0.20267722010612488
In grad_steps = 926, loss = 0.257077157497406
In grad_steps = 927, loss = 0.5276626944541931
In grad_steps = 928, loss = 0.8382099270820618
In grad_steps = 929, loss = 0.19045841693878174
In grad_steps = 930, loss = 0.3141273260116577
In grad_steps = 931, loss = 0.4300828278064728
In grad_steps = 932, loss = 0.49307191371917725
In grad_steps = 933, loss = 0.42809557914733887
In grad_steps = 934, loss = 0.16454939544200897
In grad_steps = 935, loss = 0.27975648641586304
In grad_steps = 936, loss = 0.1882016956806183
In grad_steps = 937, loss = 0.6314716935157776
In grad_steps = 938, loss = 0.22233910858631134
In grad_steps = 939, loss = 0.5992259979248047
In grad_steps = 940, loss = 0.70586758852005
In grad_steps = 941, loss = 0.28107932209968567
In grad_steps = 942, loss = 0.14694707095623016
In grad_steps = 943, loss = 0.7726383209228516
In grad_steps = 944, loss = 0.26638466119766235
In grad_steps = 945, loss = 0.11993744224309921
In grad_steps = 946, loss = 0.2245638370513916
In grad_steps = 947, loss = 0.8108596205711365
In grad_steps = 948, loss = 0.5577936172485352
In grad_steps = 949, loss = 0.6908227801322937
In grad_steps = 950, loss = 0.2827325761318207
In grad_steps = 951, loss = 0.7049573659896851
In grad_steps = 952, loss = 0.17740389704704285
In grad_steps = 953, loss = 0.8725034594535828
In grad_steps = 954, loss = 0.6994499564170837
In grad_steps = 955, loss = 0.4060881733894348
In grad_steps = 956, loss = 0.8739734888076782
In grad_steps = 957, loss = 0.6087860465049744
In grad_steps = 958, loss = 0.3535948693752289
In grad_steps = 959, loss = 0.2750113010406494
In grad_steps = 960, loss = 0.3056856393814087
In grad_steps = 961, loss = 0.4319775700569153
In grad_steps = 962, loss = 0.5983678698539734
In grad_steps = 963, loss = 0.9019328355789185
In grad_steps = 964, loss = 0.37545329332351685
In grad_steps = 965, loss = 0.5776506662368774
In grad_steps = 966, loss = 0.487156480550766
In grad_steps = 967, loss = 0.6001694202423096
In grad_steps = 968, loss = 0.3480989336967468
In grad_steps = 969, loss = 0.31601518392562866
In grad_steps = 970, loss = 0.276760995388031
In grad_steps = 971, loss = 0.2885162830352783
In grad_steps = 972, loss = 0.3028024137020111
In grad_steps = 973, loss = 0.3226941227912903
In grad_steps = 974, loss = 0.1334001123905182
In grad_steps = 975, loss = 0.2789040505886078
In grad_steps = 976, loss = 0.19812263548374176
In grad_steps = 977, loss = 0.1807153820991516
In grad_steps = 978, loss = 0.23023074865341187
In grad_steps = 979, loss = 0.46186724305152893
In grad_steps = 980, loss = 0.16953784227371216
In grad_steps = 981, loss = 0.4682469069957733
In grad_steps = 982, loss = 0.19757474958896637
In grad_steps = 983, loss = 0.5242827534675598
In grad_steps = 984, loss = 0.04154520481824875
In grad_steps = 985, loss = 0.2939329743385315
In grad_steps = 986, loss = 0.9866675734519958
In grad_steps = 987, loss = 0.35708242654800415
In grad_steps = 988, loss = 0.06172413378953934
In grad_steps = 989, loss = 0.03182573616504669
In grad_steps = 990, loss = 0.6016021370887756
In grad_steps = 991, loss = 0.5386377573013306
In grad_steps = 992, loss = 0.7188990116119385
In grad_steps = 993, loss = 0.5696088671684265
In grad_steps = 994, loss = 0.3303946256637573
In grad_steps = 995, loss = 0.03953050076961517
In grad_steps = 996, loss = 0.23103825747966766
In grad_steps = 997, loss = 0.1313214749097824
In grad_steps = 998, loss = 0.054364461451768875
In grad_steps = 999, loss = 0.4618501663208008
In grad_steps = 1000, loss = 0.40550529956817627
In grad_steps = 1001, loss = 0.25218555331230164
In grad_steps = 1002, loss = 0.08316900581121445
In grad_steps = 1003, loss = 0.4869574308395386
In grad_steps = 1004, loss = 0.2828277051448822
In grad_steps = 1005, loss = 0.2365296185016632
In grad_steps = 1006, loss = 0.2082412987947464
In grad_steps = 1007, loss = 0.2452310174703598
In grad_steps = 1008, loss = 0.06946353614330292
In grad_steps = 1009, loss = 0.02401120401918888
In grad_steps = 1010, loss = 0.029784688726067543
In grad_steps = 1011, loss = 0.24474918842315674
In grad_steps = 1012, loss = 0.7343063950538635
In grad_steps = 1013, loss = 0.12571677565574646
In grad_steps = 1014, loss = 0.6336689591407776
In grad_steps = 1015, loss = 0.6232438087463379
In grad_steps = 1016, loss = 0.1369962841272354
In grad_steps = 1017, loss = 1.8198803663253784
In grad_steps = 1018, loss = 0.10928386449813843
In grad_steps = 1019, loss = 0.9821087718009949
In grad_steps = 1020, loss = 0.2671416401863098
In grad_steps = 1021, loss = 0.23514148592948914
In grad_steps = 1022, loss = 0.5183039903640747
In grad_steps = 1023, loss = 0.31337881088256836
In grad_steps = 1024, loss = 0.4992668628692627
In grad_steps = 1025, loss = 1.1987419128417969
In grad_steps = 1026, loss = 0.8262561559677124
In grad_steps = 1027, loss = 0.32555460929870605
In grad_steps = 1028, loss = 0.3216474950313568
In grad_steps = 1029, loss = 0.5030841827392578
In grad_steps = 1030, loss = 0.501006007194519
In grad_steps = 1031, loss = 0.6011933088302612
In grad_steps = 1032, loss = 0.18440932035446167
In grad_steps = 1033, loss = 0.6888842582702637
In grad_steps = 1034, loss = 0.283747136592865
In grad_steps = 1035, loss = 0.36583060026168823
In grad_steps = 1036, loss = 0.23210710287094116
In grad_steps = 1037, loss = 0.30819442868232727
In grad_steps = 1038, loss = 0.26905950903892517
In grad_steps = 1039, loss = 0.18476781249046326
In grad_steps = 1040, loss = 0.5144044160842896
In grad_steps = 1041, loss = 0.0690217837691307
In grad_steps = 1042, loss = 0.17692168056964874
In grad_steps = 1043, loss = 0.11483161896467209
In grad_steps = 1044, loss = 0.7923237085342407
In grad_steps = 1045, loss = 0.05250905454158783
In grad_steps = 1046, loss = 0.6325390338897705
In grad_steps = 1047, loss = 0.033482689410448074
In grad_steps = 1048, loss = 0.20849476754665375
In grad_steps = 1049, loss = 0.15860967338085175
In grad_steps = 1050, loss = 0.7784725427627563
In grad_steps = 1051, loss = 0.5932291746139526
In grad_steps = 1052, loss = 1.0059854984283447
In grad_steps = 1053, loss = 0.04369200021028519
In grad_steps = 1054, loss = 0.53240966796875
In grad_steps = 1055, loss = 0.7458860278129578
In grad_steps = 1056, loss = 0.18576453626155853
In grad_steps = 1057, loss = 0.21791833639144897
In grad_steps = 1058, loss = 0.49866244196891785
In grad_steps = 1059, loss = 0.7706605792045593
In grad_steps = 1060, loss = 0.10496076941490173
In grad_steps = 1061, loss = 0.23162850737571716
In grad_steps = 1062, loss = 0.0732901394367218
In grad_steps = 1063, loss = 0.6398320198059082
In grad_steps = 1064, loss = 0.18870441615581512
In grad_steps = 1065, loss = 0.20500779151916504
In grad_steps = 1066, loss = 0.7086225152015686
In grad_steps = 1067, loss = 0.3918688893318176
In grad_steps = 1068, loss = 0.04447220638394356
In grad_steps = 1069, loss = 0.589583694934845
In grad_steps = 1070, loss = 0.4850085377693176
In grad_steps = 1071, loss = 0.139412522315979
In grad_steps = 1072, loss = 0.32166990637779236
In grad_steps = 1073, loss = 0.7524234652519226
In grad_steps = 1074, loss = 0.10201464593410492
In grad_steps = 1075, loss = 0.07398942857980728
In grad_steps = 1076, loss = 0.5341247320175171
In grad_steps = 1077, loss = 0.8019796013832092
In grad_steps = 1078, loss = 0.22305381298065186
In grad_steps = 1079, loss = 0.6474615335464478
In grad_steps = 1080, loss = 0.11500030755996704
In grad_steps = 1081, loss = 0.17485295236110687
In grad_steps = 1082, loss = 0.6422715187072754
In grad_steps = 1083, loss = 0.16150224208831787
In grad_steps = 1084, loss = 0.2036920040845871
In grad_steps = 1085, loss = 0.20321865379810333
In grad_steps = 1086, loss = 0.18030917644500732
In grad_steps = 1087, loss = 0.1791435033082962
In grad_steps = 1088, loss = 0.6671514511108398
In grad_steps = 1089, loss = 0.18824037909507751
In grad_steps = 1090, loss = 0.26627522706985474
In grad_steps = 1091, loss = 0.4959803819656372
In grad_steps = 1092, loss = 0.463034987449646
In grad_steps = 1093, loss = 0.19493156671524048
In grad_steps = 1094, loss = 0.247249573469162
In grad_steps = 1095, loss = 0.15412963926792145
In grad_steps = 1096, loss = 0.057042788714170456
In grad_steps = 1097, loss = 0.04577111825346947
In grad_steps = 1098, loss = 0.11304581165313721
In grad_steps = 1099, loss = 1.1809399127960205
In grad_steps = 1100, loss = 0.5331614017486572
In grad_steps = 1101, loss = 0.1640574038028717
In grad_steps = 1102, loss = 0.3303396701812744
In grad_steps = 1103, loss = 0.6360018849372864
In grad_steps = 1104, loss = 0.11392951756715775
In grad_steps = 1105, loss = 0.05254651978611946
In grad_steps = 1106, loss = 0.3025597035884857
In grad_steps = 1107, loss = 0.507889449596405
In grad_steps = 1108, loss = 0.38953179121017456
In grad_steps = 1109, loss = 0.12512773275375366
In grad_steps = 1110, loss = 0.311640202999115
In grad_steps = 1111, loss = 0.12783457338809967
In grad_steps = 1112, loss = 1.2941313982009888
In grad_steps = 1113, loss = 0.8186694979667664
In grad_steps = 1114, loss = 1.2489655017852783
In grad_steps = 1115, loss = 0.062484677881002426
In grad_steps = 1116, loss = 0.14969930052757263
In grad_steps = 1117, loss = 0.37188205122947693
In grad_steps = 1118, loss = 0.26754605770111084
In grad_steps = 1119, loss = 0.3132755160331726
In grad_steps = 1120, loss = 0.08763406425714493
In grad_steps = 1121, loss = 0.1303136944770813
In grad_steps = 1122, loss = 0.20904061198234558
In grad_steps = 1123, loss = 0.492825448513031
In grad_steps = 1124, loss = 0.6939442753791809
In grad_steps = 1125, loss = 0.1779562532901764
In grad_steps = 1126, loss = 0.194929301738739
In grad_steps = 1127, loss = 0.2796365022659302
In grad_steps = 1128, loss = 0.06100117415189743
In grad_steps = 1129, loss = 0.30719950795173645
In grad_steps = 1130, loss = 0.747208297252655
In grad_steps = 1131, loss = 0.7753070592880249
In grad_steps = 1132, loss = 0.15835246443748474
In grad_steps = 1133, loss = 0.06788668781518936
In grad_steps = 1134, loss = 0.5791237354278564
In grad_steps = 1135, loss = 0.8508142828941345
In grad_steps = 1136, loss = 0.14080530405044556
In grad_steps = 1137, loss = 0.38583454489707947
In grad_steps = 1138, loss = 0.4771442413330078
In grad_steps = 1139, loss = 0.3311673700809479
In grad_steps = 1140, loss = 0.09291239827871323
In grad_steps = 1141, loss = 0.48080670833587646
In grad_steps = 1142, loss = 0.39129915833473206
In grad_steps = 1143, loss = 0.3198446035385132
In grad_steps = 1144, loss = 0.15675537288188934
In grad_steps = 1145, loss = 0.12164086103439331
In grad_steps = 1146, loss = 0.2839406132698059
In grad_steps = 1147, loss = 0.21790021657943726
In grad_steps = 1148, loss = 0.3143278658390045
In grad_steps = 1149, loss = 0.15452724695205688
In grad_steps = 1150, loss = 0.09690836817026138
In grad_steps = 1151, loss = 0.9596470594406128
In grad_steps = 1152, loss = 0.02572357840836048
In grad_steps = 1153, loss = 0.2602244019508362
In grad_steps = 1154, loss = 0.037237245589494705
In grad_steps = 1155, loss = 0.7583953738212585
In grad_steps = 1156, loss = 0.8245952129364014
In grad_steps = 1157, loss = 0.0385880246758461
In grad_steps = 1158, loss = 0.05419536307454109
In grad_steps = 1159, loss = 0.03350029140710831
In grad_steps = 1160, loss = 0.23456241190433502
In grad_steps = 1161, loss = 0.48162972927093506
In grad_steps = 1162, loss = 0.05073169246315956
In grad_steps = 1163, loss = 0.1018035039305687
In grad_steps = 1164, loss = 0.05164368823170662
In grad_steps = 1165, loss = 0.5922478437423706
In grad_steps = 1166, loss = 0.36000651121139526
In grad_steps = 1167, loss = 0.06739852577447891
In grad_steps = 1168, loss = 0.6333501935005188
In grad_steps = 1169, loss = 0.7207868099212646
In grad_steps = 1170, loss = 0.39127808809280396
In grad_steps = 1171, loss = 0.12397532165050507
In grad_steps = 1172, loss = 0.089196115732193
In grad_steps = 1173, loss = 0.43874236941337585
In grad_steps = 1174, loss = 1.6412160396575928
In grad_steps = 1175, loss = 0.5424304604530334
In grad_steps = 1176, loss = 0.961936891078949
In grad_steps = 1177, loss = 0.22799906134605408
In grad_steps = 1178, loss = 0.07409952580928802
In grad_steps = 1179, loss = 0.40358078479766846
In grad_steps = 1180, loss = 0.3741579055786133
In grad_steps = 1181, loss = 0.6929368376731873
In grad_steps = 1182, loss = 0.5811817049980164
In grad_steps = 1183, loss = 0.45374131202697754
In grad_steps = 1184, loss = 0.15908436477184296
In grad_steps = 1185, loss = 0.8415815234184265
In grad_steps = 1186, loss = 0.6565244197845459
In grad_steps = 1187, loss = 0.8750102519989014
In grad_steps = 1188, loss = 0.14038953185081482
In grad_steps = 1189, loss = 0.6867239475250244
In grad_steps = 1190, loss = 0.35855209827423096
In grad_steps = 1191, loss = 0.3015943169593811
In grad_steps = 1192, loss = 0.302151083946228
In grad_steps = 1193, loss = 0.16134071350097656
In grad_steps = 1194, loss = 0.1960950791835785
In grad_steps = 1195, loss = 0.6585569381713867
In grad_steps = 1196, loss = 0.7242816090583801
In grad_steps = 1197, loss = 0.2794390320777893
In grad_steps = 1198, loss = 0.21678708493709564
In grad_steps = 1199, loss = 0.627485990524292
In grad_steps = 1200, loss = 0.464026540517807
In grad_steps = 1201, loss = 0.2913539409637451
In grad_steps = 1202, loss = 0.12422706931829453
In grad_steps = 1203, loss = 0.31558600068092346
In grad_steps = 1204, loss = 0.14310789108276367
In grad_steps = 1205, loss = 0.19847586750984192
In grad_steps = 1206, loss = 0.6354352235794067
In grad_steps = 1207, loss = 0.22766843438148499
In grad_steps = 1208, loss = 0.10786654055118561
In grad_steps = 1209, loss = 0.6426479816436768
In grad_steps = 1210, loss = 0.06395303457975388
In grad_steps = 1211, loss = 0.16093729436397552
In grad_steps = 1212, loss = 0.03742377460002899
In grad_steps = 1213, loss = 0.49719691276550293
In grad_steps = 1214, loss = 0.40080729126930237
In grad_steps = 1215, loss = 0.26511088013648987
In grad_steps = 1216, loss = 0.045822322368621826
In grad_steps = 1217, loss = 0.04224556311964989
In grad_steps = 1218, loss = 0.03300705924630165
In grad_steps = 1219, loss = 0.047562114894390106
In grad_steps = 1220, loss = 0.920829713344574
In grad_steps = 1221, loss = 0.9475406408309937
In grad_steps = 1222, loss = 0.10593527555465698
In grad_steps = 1223, loss = 0.8492870926856995
In grad_steps = 1224, loss = 0.021036699414253235
In grad_steps = 1225, loss = 0.4807353615760803
In grad_steps = 1226, loss = 0.14282888174057007
In grad_steps = 1227, loss = 0.045554738491773605
In grad_steps = 1228, loss = 0.41294416785240173
In grad_steps = 1229, loss = 0.0697483941912651
In grad_steps = 1230, loss = 0.05217242240905762
In grad_steps = 1231, loss = 0.9754084944725037
In grad_steps = 1232, loss = 0.8902686834335327
In grad_steps = 1233, loss = 0.06455609947443008
In grad_steps = 1234, loss = 0.4478476345539093
In grad_steps = 1235, loss = 1.237436294555664
In grad_steps = 1236, loss = 0.3407656252384186
In grad_steps = 1237, loss = 0.5618011355400085
In grad_steps = 1238, loss = 0.4193381071090698
In grad_steps = 1239, loss = 0.5770995616912842
In grad_steps = 1240, loss = 0.8897043466567993
In grad_steps = 1241, loss = 0.4249618649482727
In grad_steps = 1242, loss = 0.23117831349372864
In grad_steps = 1243, loss = 1.06276273727417
In grad_steps = 1244, loss = 0.5079477429389954
In grad_steps = 1245, loss = 0.29086431860923767
In grad_steps = 1246, loss = 0.3993694484233856
In grad_steps = 1247, loss = 0.6937914490699768
In grad_steps = 1248, loss = 0.7343762516975403
In grad_steps = 1249, loss = 0.26197144389152527
In grad_steps = 1250, loss = 0.3264733552932739
In grad_steps = 1251, loss = 0.38654547929763794
In grad_steps = 1252, loss = 0.4591861069202423
In grad_steps = 1253, loss = 0.5544895529747009
In grad_steps = 1254, loss = 0.2690161466598511
In grad_steps = 1255, loss = 0.26076680421829224
In grad_steps = 1256, loss = 0.6613507270812988
In grad_steps = 1257, loss = 0.07917429506778717
In grad_steps = 1258, loss = 0.6827870011329651
In grad_steps = 1259, loss = 0.28298816084861755
In grad_steps = 1260, loss = 0.4075154662132263
In grad_steps = 1261, loss = 0.20092672109603882
In grad_steps = 1262, loss = 0.08287456631660461
In grad_steps = 1263, loss = 0.43330031633377075
In grad_steps = 1264, loss = 0.699918270111084
In grad_steps = 1265, loss = 0.22315412759780884
In grad_steps = 1266, loss = 0.8608953356742859
In grad_steps = 1267, loss = 0.21875537931919098
In grad_steps = 1268, loss = 0.28664708137512207
In grad_steps = 1269, loss = 0.2649514973163605
In grad_steps = 1270, loss = 0.37923985719680786
In grad_steps = 1271, loss = 0.5697997808456421
In grad_steps = 1272, loss = 0.044286664575338364
In grad_steps = 1273, loss = 0.133368581533432
In grad_steps = 1274, loss = 0.22654792666435242
In grad_steps = 1275, loss = 0.02663915418088436
In grad_steps = 1276, loss = 0.34191763401031494
In grad_steps = 1277, loss = 0.0859358012676239
In grad_steps = 1278, loss = 0.05828447639942169
In grad_steps = 1279, loss = 0.018968138843774796
In grad_steps = 1280, loss = 0.03290454298257828
In grad_steps = 1281, loss = 0.031211301684379578
In grad_steps = 1282, loss = 0.491408109664917
In grad_steps = 1283, loss = 0.0501011423766613
In grad_steps = 1284, loss = 0.029591817408800125
In grad_steps = 1285, loss = 0.7124037742614746
In grad_steps = 1286, loss = 0.644730806350708
In grad_steps = 1287, loss = 0.4263729453086853
In grad_steps = 1288, loss = 0.22244225442409515
In grad_steps = 1289, loss = 0.09590914100408554
In grad_steps = 1290, loss = 0.6276260614395142
In grad_steps = 1291, loss = 0.13935710489749908
In grad_steps = 1292, loss = 0.5433624386787415
In grad_steps = 1293, loss = 0.16943439841270447
In grad_steps = 1294, loss = 0.614802896976471
In grad_steps = 1295, loss = 0.10990510880947113
In grad_steps = 1296, loss = 0.34653621912002563
In grad_steps = 1297, loss = 0.2768875062465668
In grad_steps = 1298, loss = 0.1521504670381546
In grad_steps = 1299, loss = 0.5095970034599304
In grad_steps = 1300, loss = 0.5534112453460693
In grad_steps = 1301, loss = 0.02148027904331684
In grad_steps = 1302, loss = 0.6300121545791626
In grad_steps = 1303, loss = 0.15693458914756775
In grad_steps = 1304, loss = 0.8692125678062439
In grad_steps = 1305, loss = 0.04738713800907135
In grad_steps = 1306, loss = 0.3104831874370575
In grad_steps = 1307, loss = 0.07501261681318283
In grad_steps = 1308, loss = 0.4638589024543762
In grad_steps = 1309, loss = 0.16852974891662598
In grad_steps = 1310, loss = 0.3068787455558777
In grad_steps = 1311, loss = 0.034344855695962906
In grad_steps = 1312, loss = 0.8000256419181824
In grad_steps = 1313, loss = 0.03071582317352295
In grad_steps = 1314, loss = 0.37348854541778564
In grad_steps = 1315, loss = 0.07330441474914551
In grad_steps = 1316, loss = 1.0614244937896729
In grad_steps = 1317, loss = 0.3861447274684906
In grad_steps = 1318, loss = 0.38516631722450256
In grad_steps = 1319, loss = 0.6175256371498108
In grad_steps = 1320, loss = 0.25788095593452454
In grad_steps = 1321, loss = 0.2027358114719391
In grad_steps = 1322, loss = 0.6358967423439026
In grad_steps = 1323, loss = 0.07445196807384491
In grad_steps = 1324, loss = 0.47238993644714355
In grad_steps = 1325, loss = 0.45356082916259766
In grad_steps = 1326, loss = 0.5485455989837646
In grad_steps = 1327, loss = 0.19258393347263336
In grad_steps = 1328, loss = 1.090159296989441
In grad_steps = 1329, loss = 0.09408143162727356
In grad_steps = 1330, loss = 0.33440449833869934
In grad_steps = 1331, loss = 0.1517416387796402
In grad_steps = 1332, loss = 0.7881775498390198
In grad_steps = 1333, loss = 0.08380509912967682
In grad_steps = 1334, loss = 0.2807082533836365
In grad_steps = 1335, loss = 0.13268646597862244
In grad_steps = 1336, loss = 0.08444579690694809
In grad_steps = 1337, loss = 0.3936153054237366
In grad_steps = 1338, loss = 0.8656123876571655
In grad_steps = 1339, loss = 0.6674218773841858
In grad_steps = 1340, loss = 0.595867395401001
In grad_steps = 1341, loss = 0.35611146688461304
In grad_steps = 1342, loss = 0.11750371009111404
In grad_steps = 1343, loss = 0.15188756585121155
In grad_steps = 1344, loss = 0.2466777265071869
In grad_steps = 1345, loss = 0.5929067134857178
In grad_steps = 1346, loss = 0.16567380726337433
In grad_steps = 1347, loss = 0.1151127889752388
In grad_steps = 1348, loss = 0.42221301794052124
In grad_steps = 1349, loss = 0.05065944418311119
In grad_steps = 1350, loss = 0.05891694873571396
In grad_steps = 1351, loss = 0.13202685117721558
In grad_steps = 1352, loss = 0.1336074024438858
In grad_steps = 1353, loss = 0.13377957046031952
In grad_steps = 1354, loss = 0.4174787998199463
In grad_steps = 1355, loss = 0.33400481939315796
In grad_steps = 1356, loss = 0.02771741710603237
In grad_steps = 1357, loss = 0.10065672546625137
In grad_steps = 1358, loss = 0.9227663278579712
In grad_steps = 1359, loss = 0.03764910623431206
In grad_steps = 1360, loss = 0.45282500982284546
In grad_steps = 1361, loss = 0.03217138722538948
In grad_steps = 1362, loss = 0.021887676790356636
In grad_steps = 1363, loss = 0.05481445789337158
In grad_steps = 1364, loss = 0.19877296686172485
In grad_steps = 1365, loss = 0.13963693380355835
In grad_steps = 1366, loss = 0.6098043322563171
In grad_steps = 1367, loss = 1.2509175539016724
In grad_steps = 1368, loss = 0.5958168506622314
In grad_steps = 1369, loss = 0.25159719586372375
In grad_steps = 1370, loss = 0.6480058431625366
In grad_steps = 1371, loss = 0.2541663348674774
In grad_steps = 1372, loss = 1.1947287321090698
In grad_steps = 1373, loss = 0.7466821670532227
In grad_steps = 1374, loss = 0.024973910301923752
In grad_steps = 1375, loss = 0.24353361129760742
In grad_steps = 1376, loss = 0.11560407280921936
In grad_steps = 1377, loss = 0.6575126647949219
In grad_steps = 1378, loss = 0.12531380355358124
In grad_steps = 1379, loss = 0.5296938419342041
In grad_steps = 1380, loss = 0.2184481918811798
In grad_steps = 1381, loss = 0.27221331000328064
In grad_steps = 1382, loss = 0.922848105430603
In grad_steps = 1383, loss = 0.31600189208984375
In grad_steps = 1384, loss = 0.046345580369234085
In grad_steps = 1385, loss = 0.3273146450519562
In grad_steps = 1386, loss = 0.19574669003486633
In grad_steps = 1387, loss = 0.4286205768585205
In grad_steps = 1388, loss = 0.6044849753379822
In grad_steps = 1389, loss = 0.14904844760894775
In grad_steps = 1390, loss = 0.1127837523818016
In grad_steps = 1391, loss = 0.13094721734523773
In grad_steps = 1392, loss = 0.03852582722902298
In grad_steps = 1393, loss = 0.7460376620292664
In grad_steps = 1394, loss = 0.2643716633319855
In grad_steps = 1395, loss = 0.3514983057975769
In grad_steps = 1396, loss = 0.17759642004966736
In grad_steps = 1397, loss = 0.859892725944519
In grad_steps = 1398, loss = 0.1376272737979889
In grad_steps = 1399, loss = 0.613312840461731
In grad_steps = 1400, loss = 0.4007311761379242
In grad_steps = 1401, loss = 0.7760555148124695
In grad_steps = 1402, loss = 0.058298509567976
In grad_steps = 1403, loss = 0.4364088177680969
In grad_steps = 1404, loss = 0.1379118263721466
In grad_steps = 1405, loss = 0.07561684399843216
In grad_steps = 1406, loss = 0.3504924476146698
In grad_steps = 1407, loss = 0.2902153730392456
In grad_steps = 1408, loss = 0.7541438341140747
In grad_steps = 1409, loss = 0.24662908911705017
In grad_steps = 1410, loss = 0.4604259431362152
In grad_steps = 1411, loss = 0.07922381907701492
In grad_steps = 1412, loss = 0.27333346009254456
In grad_steps = 1413, loss = 0.23427101969718933
In grad_steps = 1414, loss = 0.0888703316450119
In grad_steps = 1415, loss = 0.16355063021183014
In grad_steps = 1416, loss = 0.2192523181438446
In grad_steps = 1417, loss = 0.6027525067329407
In grad_steps = 1418, loss = 0.05769548565149307
In grad_steps = 1419, loss = 0.255229651927948
In grad_steps = 1420, loss = 0.17276901006698608
In grad_steps = 1421, loss = 0.6647438406944275
In grad_steps = 1422, loss = 0.15790364146232605
In grad_steps = 1423, loss = 0.12299783527851105
In grad_steps = 1424, loss = 0.4601171910762787
In grad_steps = 1425, loss = 0.22339043021202087
In grad_steps = 1426, loss = 0.06682135164737701
In grad_steps = 1427, loss = 0.07936572283506393
In grad_steps = 1428, loss = 0.009201429784297943
In grad_steps = 1429, loss = 0.9066431522369385
In grad_steps = 1430, loss = 1.3540058135986328
In grad_steps = 1431, loss = 0.23624351620674133
In grad_steps = 1432, loss = 0.7115910053253174
In grad_steps = 1433, loss = 0.5606735348701477
In grad_steps = 1434, loss = 0.5952245593070984
In grad_steps = 1435, loss = 0.1160348504781723
In grad_steps = 1436, loss = 1.2114992141723633
In grad_steps = 1437, loss = 0.7988471388816833
In grad_steps = 1438, loss = 0.08406858146190643
In grad_steps = 1439, loss = 0.12782226502895355
In grad_steps = 1440, loss = 0.747377336025238
In grad_steps = 1441, loss = 0.16428828239440918
In grad_steps = 1442, loss = 0.07655897736549377
In grad_steps = 1443, loss = 0.2436605542898178
In grad_steps = 1444, loss = 0.7204593420028687
In grad_steps = 1445, loss = 0.5943214297294617
In grad_steps = 1446, loss = 0.26304835081100464
In grad_steps = 1447, loss = 0.20452828705310822
In grad_steps = 1448, loss = 0.5355392694473267
In grad_steps = 1449, loss = 0.3503682017326355
In grad_steps = 1450, loss = 0.14612087607383728
In grad_steps = 1451, loss = 0.29952284693717957
In grad_steps = 1452, loss = 0.45269307494163513
In grad_steps = 1453, loss = 0.13512277603149414
In grad_steps = 1454, loss = 0.45345303416252136
In grad_steps = 1455, loss = 0.5604417324066162
In grad_steps = 1456, loss = 0.5437799692153931
In grad_steps = 1457, loss = 0.422899454832077
In grad_steps = 1458, loss = 0.32513993978500366
In grad_steps = 1459, loss = 0.44927144050598145
In grad_steps = 1460, loss = 0.06615205854177475
In grad_steps = 1461, loss = 0.5772191286087036
In grad_steps = 1462, loss = 0.24268077313899994
In grad_steps = 1463, loss = 0.5278784036636353
In grad_steps = 1464, loss = 0.18654969334602356
In grad_steps = 1465, loss = 0.8850929737091064
In grad_steps = 1466, loss = 0.41866445541381836
In grad_steps = 1467, loss = 0.05576600134372711
In grad_steps = 1468, loss = 0.16022738814353943
In grad_steps = 1469, loss = 0.6575820446014404
In grad_steps = 1470, loss = 0.059328626841306686
In grad_steps = 1471, loss = 0.22705122828483582
In grad_steps = 1472, loss = 0.7350186705589294
In grad_steps = 1473, loss = 0.10692751407623291
In grad_steps = 1474, loss = 0.3890007436275482
In grad_steps = 1475, loss = 1.3280647993087769
In grad_steps = 1476, loss = 0.3577491343021393
In grad_steps = 1477, loss = 0.19190993905067444
In grad_steps = 1478, loss = 0.7386983633041382
In grad_steps = 1479, loss = 0.365358829498291
In grad_steps = 1480, loss = 0.37519919872283936
In grad_steps = 1481, loss = 0.19341999292373657
In grad_steps = 1482, loss = 0.0810210332274437
In grad_steps = 1483, loss = 0.4759221076965332
In grad_steps = 1484, loss = 0.09233583509922028
In grad_steps = 1485, loss = 0.10444190353155136
In grad_steps = 1486, loss = 0.531284511089325
In grad_steps = 1487, loss = 0.34508123993873596
In grad_steps = 1488, loss = 0.14686623215675354
In grad_steps = 1489, loss = 0.315757691860199
In grad_steps = 1490, loss = 0.2017589509487152
In grad_steps = 1491, loss = 0.47062575817108154
In grad_steps = 1492, loss = 0.10682480037212372
In grad_steps = 1493, loss = 0.2773313522338867
In grad_steps = 1494, loss = 0.6038167476654053
In grad_steps = 1495, loss = 0.19631324708461761
In grad_steps = 1496, loss = 0.4638442397117615
In grad_steps = 1497, loss = 0.3637944459915161
In grad_steps = 1498, loss = 0.3921343982219696
In grad_steps = 1499, loss = 0.12078365683555603
In grad_steps = 1500, loss = 0.42273446917533875
In grad_steps = 1501, loss = 0.4798714518547058
In grad_steps = 1502, loss = 0.05916747450828552
In grad_steps = 1503, loss = 0.5786490440368652
In grad_steps = 1504, loss = 0.2029237300157547
In grad_steps = 1505, loss = 0.10738123953342438
In grad_steps = 1506, loss = 0.06032650172710419
In grad_steps = 1507, loss = 0.16267892718315125
In grad_steps = 1508, loss = 0.7180481553077698
In grad_steps = 1509, loss = 0.7742717862129211
In grad_steps = 1510, loss = 0.36946478486061096
In grad_steps = 1511, loss = 0.21880266070365906
In grad_steps = 1512, loss = 0.035450421273708344
In grad_steps = 1513, loss = 1.0198652744293213
In grad_steps = 1514, loss = 0.5623814463615417
In grad_steps = 1515, loss = 0.11027079075574875
In grad_steps = 1516, loss = 0.09385946393013
In grad_steps = 1517, loss = 0.5225794911384583
In grad_steps = 1518, loss = 0.4442957639694214
In grad_steps = 1519, loss = 0.668032169342041
In grad_steps = 1520, loss = 0.12997692823410034
In grad_steps = 1521, loss = 0.3597266674041748
In grad_steps = 1522, loss = 0.2741752862930298
In grad_steps = 1523, loss = 0.29525575041770935
In grad_steps = 1524, loss = 0.10457155853509903
In grad_steps = 1525, loss = 0.561886727809906
In grad_steps = 1526, loss = 0.09646746516227722
In grad_steps = 1527, loss = 0.06433802098035812
In grad_steps = 1528, loss = 0.2850564420223236
In grad_steps = 1529, loss = 0.12829020619392395
In grad_steps = 1530, loss = 0.09733951091766357
In grad_steps = 1531, loss = 0.08934454619884491
In grad_steps = 1532, loss = 0.3540041744709015
In grad_steps = 1533, loss = 0.7455614805221558
In grad_steps = 1534, loss = 0.19736486673355103
In grad_steps = 1535, loss = 0.034873802214860916
In grad_steps = 1536, loss = 0.17853951454162598
In grad_steps = 1537, loss = 0.14160117506980896
In grad_steps = 1538, loss = 0.250274658203125
In grad_steps = 1539, loss = 0.025450080633163452
In grad_steps = 1540, loss = 0.031877897679805756
In grad_steps = 1541, loss = 0.25881248712539673
In grad_steps = 1542, loss = 0.12157174944877625
In grad_steps = 1543, loss = 0.3720296323299408
In grad_steps = 1544, loss = 0.4147379398345947
In grad_steps = 1545, loss = 0.012750549241900444
In grad_steps = 1546, loss = 0.892633855342865
In grad_steps = 1547, loss = 0.018682848662137985
In grad_steps = 1548, loss = 0.06588674336671829
In grad_steps = 1549, loss = 0.039148665964603424
In grad_steps = 1550, loss = 0.0387871116399765
In grad_steps = 1551, loss = 0.19371260702610016
In grad_steps = 1552, loss = 0.39613890647888184
In grad_steps = 1553, loss = 0.04098570719361305
In grad_steps = 1554, loss = 0.711652934551239
In grad_steps = 1555, loss = 0.017380978912115097
In grad_steps = 1556, loss = 0.04028748720884323
In grad_steps = 1557, loss = 0.9074404835700989
In grad_steps = 1558, loss = 0.4804674983024597
In grad_steps = 1559, loss = 0.22746744751930237
In grad_steps = 1560, loss = 0.6466130614280701
In grad_steps = 1561, loss = 0.3334975838661194
In grad_steps = 1562, loss = 0.03236234188079834
In grad_steps = 1563, loss = 0.6453185081481934
In grad_steps = 1564, loss = 0.0830683559179306
In grad_steps = 1565, loss = 0.27147653698921204
In grad_steps = 1566, loss = 0.9342138171195984
In grad_steps = 1567, loss = 0.13032837212085724
In grad_steps = 1568, loss = 0.47407132387161255
In grad_steps = 1569, loss = 0.3952944576740265
In grad_steps = 1570, loss = 0.3878716230392456
In grad_steps = 1571, loss = 0.24247393012046814
In grad_steps = 1572, loss = 0.6405274868011475
In grad_steps = 1573, loss = 0.25454309582710266
In grad_steps = 1574, loss = 0.46627020835876465
In grad_steps = 1575, loss = 0.11556749045848846
In grad_steps = 1576, loss = 0.1521204710006714
In grad_steps = 1577, loss = 0.36324939131736755
In grad_steps = 1578, loss = 0.6177592277526855
In grad_steps = 1579, loss = 0.11327505856752396
In grad_steps = 1580, loss = 0.06434575468301773
In grad_steps = 1581, loss = 0.20046581327915192
In grad_steps = 1582, loss = 0.11753863096237183
In grad_steps = 1583, loss = 0.7494087815284729
In grad_steps = 1584, loss = 0.32506871223449707
In grad_steps = 1585, loss = 1.0817654132843018
In grad_steps = 1586, loss = 0.7345455884933472
In grad_steps = 1587, loss = 0.6034857630729675
In grad_steps = 1588, loss = 0.1954779326915741
In grad_steps = 1589, loss = 0.09831401705741882
In grad_steps = 1590, loss = 0.20320004224777222
In grad_steps = 1591, loss = 0.03690740093588829
In grad_steps = 1592, loss = 0.42739203572273254
In grad_steps = 1593, loss = 0.1576988697052002
In grad_steps = 1594, loss = 0.13906902074813843
In grad_steps = 1595, loss = 0.3170199394226074
In grad_steps = 1596, loss = 0.35750919580459595
In grad_steps = 1597, loss = 0.49024954438209534
In grad_steps = 1598, loss = 0.1624523401260376
In grad_steps = 1599, loss = 0.4558619260787964
In grad_steps = 1600, loss = 0.5733225345611572
In grad_steps = 1601, loss = 0.3294525444507599
In grad_steps = 1602, loss = 0.514843761920929
In grad_steps = 1603, loss = 0.10555388033390045
In grad_steps = 1604, loss = 0.22779598832130432
In grad_steps = 1605, loss = 0.11154310405254364
In grad_steps = 1606, loss = 0.7156417369842529
In grad_steps = 1607, loss = 0.17656368017196655
In grad_steps = 1608, loss = 0.09768526256084442
In grad_steps = 1609, loss = 0.4876231551170349
In grad_steps = 1610, loss = 0.25158441066741943
In grad_steps = 1611, loss = 0.14265336096286774
In grad_steps = 1612, loss = 1.0064697265625
In grad_steps = 1613, loss = 0.2507571578025818
In grad_steps = 1614, loss = 0.1711910516023636
In grad_steps = 1615, loss = 0.12064068019390106
In grad_steps = 1616, loss = 0.9290937185287476
In grad_steps = 1617, loss = 0.01286887377500534
In grad_steps = 1618, loss = 0.4548982083797455
In grad_steps = 1619, loss = 0.312512069940567
In grad_steps = 1620, loss = 0.029322827234864235
In grad_steps = 1621, loss = 0.259658545255661
In grad_steps = 1622, loss = 0.40834563970565796
In grad_steps = 1623, loss = 0.19885648787021637
In grad_steps = 1624, loss = 0.16063441336154938
In grad_steps = 1625, loss = 0.2106284499168396
In grad_steps = 1626, loss = 0.022759586572647095
In grad_steps = 1627, loss = 0.10890480875968933
In grad_steps = 1628, loss = 0.23535871505737305
In grad_steps = 1629, loss = 0.0888405591249466
In grad_steps = 1630, loss = 0.6569488644599915
In grad_steps = 1631, loss = 0.23496972024440765
In grad_steps = 1632, loss = 0.5106418132781982
In grad_steps = 1633, loss = 0.05927490442991257
In grad_steps = 1634, loss = 0.06343504786491394
In grad_steps = 1635, loss = 0.366424560546875
In grad_steps = 1636, loss = 0.708865761756897
In grad_steps = 1637, loss = 0.014867933467030525
In grad_steps = 1638, loss = 0.090022012591362
In grad_steps = 1639, loss = 1.8456895351409912
In grad_steps = 1640, loss = 0.38976147770881653
In grad_steps = 1641, loss = 0.03990975022315979
In grad_steps = 1642, loss = 1.0550931692123413
In grad_steps = 1643, loss = 0.08644390851259232
In grad_steps = 1644, loss = 1.0766727924346924
In grad_steps = 1645, loss = 0.15938833355903625
In grad_steps = 1646, loss = 0.3720875382423401
In grad_steps = 1647, loss = 0.7449798583984375
In grad_steps = 1648, loss = 0.0740128681063652
In grad_steps = 1649, loss = 0.08274144679307938
In grad_steps = 1650, loss = 0.6174649000167847
In grad_steps = 1651, loss = 0.275259792804718
In grad_steps = 1652, loss = 0.1914179027080536
In grad_steps = 1653, loss = 0.39432916045188904
In grad_steps = 1654, loss = 0.4949171245098114
In grad_steps = 1655, loss = 0.43523892760276794
In grad_steps = 1656, loss = 0.6112669706344604
In grad_steps = 1657, loss = 0.09146248549222946
In grad_steps = 1658, loss = 0.1440332978963852
In grad_steps = 1659, loss = 0.9136038422584534
In grad_steps = 1660, loss = 0.5038750767707825
In grad_steps = 1661, loss = 0.39960166811943054
In grad_steps = 1662, loss = 0.8930124640464783
In grad_steps = 1663, loss = 0.23761054873466492
In grad_steps = 1664, loss = 0.19411389529705048
In grad_steps = 1665, loss = 0.29563483595848083
In grad_steps = 1666, loss = 0.0791616216301918
In grad_steps = 1667, loss = 0.1193804070353508
In grad_steps = 1668, loss = 0.8485750555992126
In grad_steps = 1669, loss = 0.38083547353744507
In grad_steps = 1670, loss = 0.14071756601333618
In grad_steps = 1671, loss = 0.23124444484710693
In grad_steps = 1672, loss = 0.6806665062904358
In grad_steps = 1673, loss = 0.3615231513977051
In grad_steps = 1674, loss = 0.1456417292356491
In grad_steps = 1675, loss = 0.6370878219604492
In grad_steps = 1676, loss = 0.4041033387184143
In grad_steps = 1677, loss = 0.2097274214029312
In grad_steps = 1678, loss = 0.2814434766769409
In grad_steps = 1679, loss = 0.6404494643211365
In grad_steps = 1680, loss = 0.25356927514076233
In grad_steps = 1681, loss = 0.3597724735736847
In grad_steps = 1682, loss = 0.1835087090730667
In grad_steps = 1683, loss = 0.2492493987083435
In grad_steps = 1684, loss = 0.5566602349281311
In grad_steps = 1685, loss = 0.18524512648582458
In grad_steps = 1686, loss = 0.514862060546875
In grad_steps = 1687, loss = 0.373355507850647
In grad_steps = 1688, loss = 0.29630666971206665
In grad_steps = 1689, loss = 0.226666659116745
In grad_steps = 1690, loss = 0.09299719333648682
In grad_steps = 1691, loss = 0.2923544645309448
In grad_steps = 1692, loss = 0.17482063174247742
In grad_steps = 1693, loss = 0.2664470374584198
In grad_steps = 1694, loss = 0.09210624545812607
In grad_steps = 1695, loss = 0.03712866082787514
In grad_steps = 1696, loss = 0.789322018623352
In grad_steps = 1697, loss = 0.11792703717947006
In grad_steps = 1698, loss = 0.019009865820407867
In grad_steps = 1699, loss = 0.0898565798997879
In grad_steps = 1700, loss = 0.18038588762283325
In grad_steps = 1701, loss = 1.0876951217651367
In grad_steps = 1702, loss = 0.18264439702033997
In grad_steps = 1703, loss = 0.3498260974884033
In grad_steps = 1704, loss = 0.1165669783949852
In grad_steps = 1705, loss = 0.40101122856140137
In grad_steps = 1706, loss = 0.1782069206237793
In grad_steps = 1707, loss = 0.871452271938324
In grad_steps = 1708, loss = 0.963850736618042
In grad_steps = 1709, loss = 0.09748713672161102
In grad_steps = 1710, loss = 0.13630057871341705
In grad_steps = 1711, loss = 0.06320278346538544
In grad_steps = 1712, loss = 0.26580679416656494
In grad_steps = 1713, loss = 0.6992735862731934
In grad_steps = 1714, loss = 0.15358217060565948
In grad_steps = 1715, loss = 0.6438159942626953
In grad_steps = 1716, loss = 0.6963949203491211
In grad_steps = 1717, loss = 0.6661797165870667
In grad_steps = 1718, loss = 0.1165161058306694
In grad_steps = 1719, loss = 0.3171362280845642
In grad_steps = 1720, loss = 0.29625391960144043
In grad_steps = 1721, loss = 0.06432903558015823
In grad_steps = 1722, loss = 0.576244056224823
In grad_steps = 1723, loss = 0.16018784046173096
In grad_steps = 1724, loss = 1.0120360851287842
In grad_steps = 1725, loss = 0.29374220967292786
In grad_steps = 1726, loss = 0.468903124332428
In grad_steps = 1727, loss = 0.41971713304519653
In grad_steps = 1728, loss = 0.22681352496147156
In grad_steps = 1729, loss = 0.5044230818748474
In grad_steps = 1730, loss = 0.1524209976196289
In grad_steps = 1731, loss = 0.4314308166503906
In grad_steps = 1732, loss = 0.057873744517564774
In grad_steps = 1733, loss = 0.0450594425201416
In grad_steps = 1734, loss = 0.0718904435634613
In grad_steps = 1735, loss = 0.2855570614337921
In grad_steps = 1736, loss = 0.3642417788505554
In grad_steps = 1737, loss = 0.39412620663642883
In grad_steps = 1738, loss = 0.1344415843486786
In grad_steps = 1739, loss = 0.03312092646956444
In grad_steps = 1740, loss = 0.04276077076792717
In grad_steps = 1741, loss = 0.24917122721672058
In grad_steps = 1742, loss = 0.41000381112098694
In grad_steps = 1743, loss = 0.023255977779626846
In grad_steps = 1744, loss = 0.815629243850708
In grad_steps = 1745, loss = 0.19580839574337006
In grad_steps = 1746, loss = 0.2982891798019409
In grad_steps = 1747, loss = 0.02761150524020195
In grad_steps = 1748, loss = 0.3354017734527588
In grad_steps = 1749, loss = 0.02452223375439644
In grad_steps = 1750, loss = 0.8167601823806763
In grad_steps = 1751, loss = 0.07998980581760406
In grad_steps = 1752, loss = 0.11659202724695206
In grad_steps = 1753, loss = 0.07918272912502289
In grad_steps = 1754, loss = 0.07141633331775665
In grad_steps = 1755, loss = 0.03844795748591423
In grad_steps = 1756, loss = 0.15305519104003906
In grad_steps = 1757, loss = 0.953118622303009
In grad_steps = 1758, loss = 0.038745760917663574
In grad_steps = 1759, loss = 0.03558443486690521
In grad_steps = 1760, loss = 0.4211483299732208
In grad_steps = 1761, loss = 0.08537338674068451
In grad_steps = 1762, loss = 0.44682687520980835
In grad_steps = 1763, loss = 0.12438112497329712
In grad_steps = 1764, loss = 0.27065804600715637
In grad_steps = 1765, loss = 0.10024774074554443
In grad_steps = 1766, loss = 0.05234236270189285
In grad_steps = 1767, loss = 0.09990258514881134
In grad_steps = 1768, loss = 0.6409047842025757
In grad_steps = 1769, loss = 0.09390042722225189
In grad_steps = 1770, loss = 1.1082996129989624
In grad_steps = 1771, loss = 0.7123445272445679
In grad_steps = 1772, loss = 0.051921043545007706
In grad_steps = 1773, loss = 0.03200392797589302
In grad_steps = 1774, loss = 0.031802643090486526
In grad_steps = 1775, loss = 0.4397144317626953
In grad_steps = 1776, loss = 0.5885910987854004
In grad_steps = 1777, loss = 0.18795327842235565
In grad_steps = 1778, loss = 0.4531441032886505
In grad_steps = 1779, loss = 0.5623131990432739
In grad_steps = 1780, loss = 0.028438597917556763
In grad_steps = 1781, loss = 0.06827796250581741
In grad_steps = 1782, loss = 0.4429314136505127
In grad_steps = 1783, loss = 0.24185071885585785
In grad_steps = 1784, loss = 1.2194936275482178
In grad_steps = 1785, loss = 0.07135506719350815
In grad_steps = 1786, loss = 0.1186089888215065
In grad_steps = 1787, loss = 0.18762773275375366
In grad_steps = 1788, loss = 0.21832042932510376
In grad_steps = 1789, loss = 0.2527293264865875
In grad_steps = 1790, loss = 0.4128226637840271
In grad_steps = 1791, loss = 0.46733734011650085
In grad_steps = 1792, loss = 0.10013125836849213
In grad_steps = 1793, loss = 0.11663606017827988
In grad_steps = 1794, loss = 0.16944731771945953
In grad_steps = 1795, loss = 0.23478657007217407
In grad_steps = 1796, loss = 0.10087069869041443
In grad_steps = 1797, loss = 0.32146155834198
In grad_steps = 1798, loss = 0.07416233420372009
In grad_steps = 1799, loss = 0.893517017364502
In grad_steps = 1800, loss = 0.6114962697029114
In grad_steps = 1801, loss = 0.2370227575302124
In grad_steps = 1802, loss = 0.9700945615768433
In grad_steps = 1803, loss = 0.059264861047267914
In grad_steps = 1804, loss = 0.6738091707229614
In grad_steps = 1805, loss = 0.24883732199668884
In grad_steps = 1806, loss = 0.15636229515075684
In grad_steps = 1807, loss = 0.25788411498069763
In grad_steps = 1808, loss = 0.7776421904563904
In grad_steps = 1809, loss = 0.2530773878097534
In grad_steps = 1810, loss = 0.023111578077077866
In grad_steps = 1811, loss = 0.042787134647369385
In grad_steps = 1812, loss = 0.3309178650379181
In grad_steps = 1813, loss = 0.2672792375087738
In grad_steps = 1814, loss = 0.07161512970924377
In grad_steps = 1815, loss = 0.36991363763809204
In grad_steps = 1816, loss = 0.09521138668060303
In grad_steps = 1817, loss = 0.5208549499511719
In grad_steps = 1818, loss = 0.32991525530815125
In grad_steps = 1819, loss = 0.08899769932031631
In grad_steps = 1820, loss = 1.1674724817276
In grad_steps = 1821, loss = 0.18409334123134613
In grad_steps = 1822, loss = 0.05416341498494148
In grad_steps = 1823, loss = 1.4694582223892212
In grad_steps = 1824, loss = 0.09758920967578888
In grad_steps = 1825, loss = 0.14142611622810364
In grad_steps = 1826, loss = 0.801003098487854
In grad_steps = 1827, loss = 1.198277235031128
In grad_steps = 1828, loss = 0.8297778964042664
In grad_steps = 1829, loss = 0.7329450845718384
In grad_steps = 1830, loss = 0.34301936626434326
In grad_steps = 1831, loss = 0.2805318236351013
In grad_steps = 1832, loss = 0.673208475112915
In grad_steps = 1833, loss = 1.212119698524475
In grad_steps = 1834, loss = 0.1242842897772789
In grad_steps = 1835, loss = 0.3048587143421173
In grad_steps = 1836, loss = 0.43764787912368774
In grad_steps = 1837, loss = 0.6256932020187378
In grad_steps = 1838, loss = 0.5995612144470215
In grad_steps = 1839, loss = 0.25774088501930237
In grad_steps = 1840, loss = 0.15660221874713898
In grad_steps = 1841, loss = 0.3469410240650177
In grad_steps = 1842, loss = 0.380383163690567
In grad_steps = 1843, loss = 0.25112125277519226
In grad_steps = 1844, loss = 0.7625689506530762
In grad_steps = 1845, loss = 0.2303047478199005
In grad_steps = 1846, loss = 0.3401736617088318
In grad_steps = 1847, loss = 0.514595091342926
In grad_steps = 1848, loss = 0.25010982155799866
In grad_steps = 1849, loss = 0.1527308076620102
In grad_steps = 1850, loss = 0.1702325940132141
In grad_steps = 1851, loss = 0.21802805364131927
In grad_steps = 1852, loss = 0.17778140306472778
In grad_steps = 1853, loss = 0.4477878510951996
In grad_steps = 1854, loss = 0.3660603165626526
In grad_steps = 1855, loss = 0.07414942979812622
In grad_steps = 1856, loss = 0.09919476509094238
In grad_steps = 1857, loss = 0.10637842118740082
In grad_steps = 1858, loss = 0.07596561312675476
In grad_steps = 1859, loss = 0.35299408435821533
In grad_steps = 1860, loss = 0.817663311958313
In grad_steps = 1861, loss = 0.0910656675696373
In grad_steps = 1862, loss = 0.3870198726654053
In grad_steps = 1863, loss = 0.0754835307598114
In grad_steps = 1864, loss = 0.08959499001502991
In grad_steps = 1865, loss = 0.19311374425888062
In grad_steps = 1866, loss = 0.02554788440465927
In grad_steps = 1867, loss = 1.0318354368209839
In grad_steps = 1868, loss = 0.01856849528849125
In grad_steps = 1869, loss = 0.05680336058139801
In grad_steps = 1870, loss = 0.03993784263730049
In grad_steps = 1871, loss = 0.033249832689762115
In grad_steps = 1872, loss = 0.05333539843559265
In grad_steps = 1873, loss = 0.017767151817679405
In grad_steps = 1874, loss = 0.10497792810201645
In grad_steps = 1875, loss = 0.22035779058933258
In grad_steps = 1876, loss = 0.022761868312954903
In grad_steps = 1877, loss = 0.7642227411270142
In grad_steps = 1878, loss = 0.03968967869877815
In grad_steps = 1879, loss = 0.7772204875946045
In grad_steps = 1880, loss = 0.033632759004831314
In grad_steps = 1881, loss = 0.9984438419342041
In grad_steps = 1882, loss = 0.28663668036460876
In grad_steps = 1883, loss = 0.7925432324409485
In grad_steps = 1884, loss = 0.057632140815258026
In grad_steps = 1885, loss = 0.14441686868667603
In grad_steps = 1886, loss = 0.40847599506378174
In grad_steps = 1887, loss = 0.06253623962402344
In grad_steps = 1888, loss = 0.6804655194282532
In grad_steps = 1889, loss = 0.06778644770383835
In grad_steps = 1890, loss = 0.054188624024391174
In grad_steps = 1891, loss = 0.3286914527416229
In grad_steps = 1892, loss = 0.4459070563316345
In grad_steps = 1893, loss = 0.06736139953136444
In grad_steps = 1894, loss = 0.05741056054830551
In grad_steps = 1895, loss = 0.30642467737197876
In grad_steps = 1896, loss = 0.3904721438884735
In grad_steps = 1897, loss = 0.055839404463768005
In grad_steps = 1898, loss = 0.05861930176615715
In grad_steps = 1899, loss = 0.09743955731391907
In grad_steps = 1900, loss = 0.20997291803359985
In grad_steps = 1901, loss = 0.15981146693229675
In grad_steps = 1902, loss = 0.20554088056087494
In grad_steps = 1903, loss = 0.5684099197387695
In grad_steps = 1904, loss = 0.3412581980228424
In grad_steps = 1905, loss = 0.09714213758707047
In grad_steps = 1906, loss = 0.18763960897922516
In grad_steps = 1907, loss = 1.562256097793579
In grad_steps = 1908, loss = 0.462056964635849
In grad_steps = 1909, loss = 0.09961353987455368
In grad_steps = 1910, loss = 0.839029848575592
In grad_steps = 1911, loss = 0.3034891188144684
In grad_steps = 1912, loss = 0.2879491150379181
In grad_steps = 1913, loss = 0.16450229287147522
In grad_steps = 1914, loss = 0.0509905144572258
In grad_steps = 1915, loss = 0.42925068736076355
In grad_steps = 1916, loss = 0.6204819083213806
In grad_steps = 1917, loss = 0.1702193021774292
In grad_steps = 1918, loss = 0.3164331614971161
In grad_steps = 1919, loss = 0.3590061366558075
In grad_steps = 1920, loss = 0.3116609454154968
In grad_steps = 1921, loss = 0.39421936869621277
In grad_steps = 1922, loss = 0.14106369018554688
In grad_steps = 1923, loss = 0.01580502651631832
In grad_steps = 1924, loss = 0.38289520144462585
In grad_steps = 1925, loss = 0.6974281668663025
In grad_steps = 1926, loss = 0.46195101737976074
In grad_steps = 1927, loss = 0.13061924278736115
In grad_steps = 1928, loss = 1.1825815439224243
In grad_steps = 1929, loss = 0.3672141432762146
In grad_steps = 1930, loss = 0.29087698459625244
In grad_steps = 1931, loss = 1.3519341945648193
In grad_steps = 1932, loss = 0.2651662528514862
In grad_steps = 1933, loss = 0.7132448554039001
In grad_steps = 1934, loss = 0.2933371365070343
In grad_steps = 1935, loss = 0.2171701192855835
In grad_steps = 1936, loss = 0.08667546510696411
In grad_steps = 1937, loss = 0.0863962322473526
In grad_steps = 1938, loss = 0.43275654315948486
In grad_steps = 1939, loss = 0.4370633661746979
In grad_steps = 1940, loss = 0.17877709865570068
In grad_steps = 1941, loss = 0.19356217980384827
In grad_steps = 1942, loss = 0.30500948429107666
In grad_steps = 1943, loss = 1.2281506061553955
In grad_steps = 1944, loss = 0.22880522906780243
In grad_steps = 1945, loss = 0.1996162384748459
In grad_steps = 1946, loss = 0.22850830852985382
In grad_steps = 1947, loss = 0.23221246898174286
In grad_steps = 1948, loss = 0.18891336023807526
In grad_steps = 1949, loss = 0.21388131380081177
In grad_steps = 1950, loss = 0.2808397710323334
In grad_steps = 1951, loss = 0.5229340195655823
In grad_steps = 1952, loss = 0.1376125067472458
In grad_steps = 1953, loss = 0.16479438543319702
In grad_steps = 1954, loss = 0.49306878447532654
In grad_steps = 1955, loss = 0.024432379752397537
In grad_steps = 1956, loss = 0.09681679308414459
In grad_steps = 1957, loss = 0.04333359748125076
In grad_steps = 1958, loss = 0.09307781606912613
In grad_steps = 1959, loss = 0.07859140634536743
In grad_steps = 1960, loss = 0.05023784190416336
In grad_steps = 1961, loss = 0.49118661880493164
In grad_steps = 1962, loss = 0.06305761635303497
In grad_steps = 1963, loss = 0.3457379937171936
In grad_steps = 1964, loss = 0.23890961706638336
In grad_steps = 1965, loss = 0.14087927341461182
In grad_steps = 1966, loss = 1.5908464193344116
In grad_steps = 1967, loss = 0.12002880871295929
In grad_steps = 1968, loss = 0.4170440435409546
In grad_steps = 1969, loss = 0.09172027558088303
In grad_steps = 1970, loss = 0.25667884945869446
In grad_steps = 1971, loss = 0.48176494240760803
In grad_steps = 1972, loss = 0.4085821807384491
In grad_steps = 1973, loss = 0.22753620147705078
In grad_steps = 1974, loss = 0.6671550273895264
In grad_steps = 1975, loss = 0.44421738386154175
In grad_steps = 1976, loss = 0.20091094076633453
In grad_steps = 1977, loss = 0.5717460513114929
In grad_steps = 1978, loss = 0.4396894574165344
In grad_steps = 1979, loss = 0.030525555834174156
In grad_steps = 1980, loss = 0.15829062461853027
In grad_steps = 1981, loss = 0.6687664985656738
In grad_steps = 1982, loss = 0.050617970526218414
In grad_steps = 1983, loss = 0.12748803198337555
In grad_steps = 1984, loss = 0.28775277733802795
In grad_steps = 1985, loss = 0.27541813254356384
In grad_steps = 1986, loss = 0.53254234790802
In grad_steps = 1987, loss = 0.15730391442775726
In grad_steps = 1988, loss = 0.5996227264404297
In grad_steps = 1989, loss = 0.36401745676994324
In grad_steps = 1990, loss = 0.33736732602119446
In grad_steps = 1991, loss = 0.11656763404607773
In grad_steps = 1992, loss = 0.3785738945007324
In grad_steps = 1993, loss = 0.14003700017929077
In grad_steps = 1994, loss = 0.20311062037944794
In grad_steps = 1995, loss = 0.7660441994667053
In grad_steps = 1996, loss = 0.031579967588186264
In grad_steps = 1997, loss = 0.06548945605754852
In grad_steps = 1998, loss = 0.42735055088996887
In grad_steps = 1999, loss = 0.09323197603225708
In grad_steps = 2000, loss = 0.06781448423862457
In grad_steps = 2001, loss = 0.19797283411026
In grad_steps = 2002, loss = 0.8297095894813538
In grad_steps = 2003, loss = 0.08925147354602814
In grad_steps = 2004, loss = 0.137024387717247
In grad_steps = 2005, loss = 0.025844119489192963
In grad_steps = 2006, loss = 0.17468991875648499
In grad_steps = 2007, loss = 0.13549885153770447
In grad_steps = 2008, loss = 0.06647228449583054
In grad_steps = 2009, loss = 0.4055532217025757
In grad_steps = 2010, loss = 0.09645387530326843
In grad_steps = 2011, loss = 0.30424243211746216
In grad_steps = 2012, loss = 0.3996589779853821
In grad_steps = 2013, loss = 0.053703390061855316
In grad_steps = 2014, loss = 0.14800114929676056
In grad_steps = 2015, loss = 0.007748772855848074
In grad_steps = 2016, loss = 0.017628416419029236
In grad_steps = 2017, loss = 0.10307755321264267
In grad_steps = 2018, loss = 0.04859423264861107
In grad_steps = 2019, loss = 0.7197672128677368
In grad_steps = 2020, loss = 0.035533711314201355
In grad_steps = 2021, loss = 0.11485044658184052
In grad_steps = 2022, loss = 1.240251064300537
In grad_steps = 2023, loss = 0.7822872400283813
In grad_steps = 2024, loss = 0.034225352108478546
In grad_steps = 2025, loss = 0.15760454535484314
In grad_steps = 2026, loss = 0.1830911934375763
In grad_steps = 2027, loss = 0.4049082100391388
In grad_steps = 2028, loss = 0.28405022621154785
In grad_steps = 2029, loss = 0.45470625162124634
In grad_steps = 2030, loss = 0.18983186781406403
In grad_steps = 2031, loss = 0.035192571580410004
In grad_steps = 2032, loss = 0.020990002900362015
In grad_steps = 2033, loss = 0.05302422121167183
In grad_steps = 2034, loss = 0.059421103447675705
In grad_steps = 2035, loss = 0.06508108228445053
In grad_steps = 2036, loss = 0.17451006174087524
In grad_steps = 2037, loss = 0.4269314408302307
In grad_steps = 2038, loss = 0.20051085948944092
In grad_steps = 2039, loss = 0.4040635824203491
In grad_steps = 2040, loss = 0.23623549938201904
In grad_steps = 2041, loss = 0.11354728043079376
In grad_steps = 2042, loss = 0.418134868144989
In grad_steps = 2043, loss = 0.17734278738498688
In grad_steps = 2044, loss = 0.061644941568374634
In grad_steps = 2045, loss = 0.6030788421630859
In grad_steps = 2046, loss = 0.46696531772613525
In grad_steps = 2047, loss = 0.020841004326939583
In grad_steps = 2048, loss = 0.18334132432937622
In grad_steps = 2049, loss = 2.1630752086639404
In grad_steps = 2050, loss = 0.4585047960281372
In grad_steps = 2051, loss = 0.4309375584125519
In grad_steps = 2052, loss = 0.05069773644208908
In grad_steps = 2053, loss = 1.0275458097457886
In grad_steps = 2054, loss = 0.345500111579895
In grad_steps = 2055, loss = 0.12271372228860855
In grad_steps = 2056, loss = 0.08465762436389923
In grad_steps = 2057, loss = 0.07133526355028152
In grad_steps = 2058, loss = 0.2995821237564087
In grad_steps = 2059, loss = 0.14876043796539307
In grad_steps = 2060, loss = 0.4297786056995392
In grad_steps = 2061, loss = 0.2670266926288605
In grad_steps = 2062, loss = 0.13379496335983276
In grad_steps = 2063, loss = 0.11051509529352188
In grad_steps = 2064, loss = 0.38216984272003174
In grad_steps = 2065, loss = 0.06934380531311035
In grad_steps = 2066, loss = 0.11302248388528824
In grad_steps = 2067, loss = 0.1548389047384262
In grad_steps = 2068, loss = 0.10268478840589523
In grad_steps = 2069, loss = 0.20138971507549286
In grad_steps = 2070, loss = 0.6071232557296753
In grad_steps = 2071, loss = 0.33315420150756836
In grad_steps = 2072, loss = 0.04736533388495445
In grad_steps = 2073, loss = 0.7824347615242004
In grad_steps = 2074, loss = 0.11938725411891937
In grad_steps = 2075, loss = 0.06181437522172928
In grad_steps = 2076, loss = 0.04933623597025871
In grad_steps = 2077, loss = 0.10573974996805191
In grad_steps = 2078, loss = 0.9817124009132385
In grad_steps = 2079, loss = 0.2145005613565445
In grad_steps = 2080, loss = 0.4083907902240753
In grad_steps = 2081, loss = 0.01232166774570942
In grad_steps = 2082, loss = 0.037900410592556
In grad_steps = 2083, loss = 0.04401993751525879
In grad_steps = 2084, loss = 0.085911326110363
In grad_steps = 2085, loss = 0.12860651314258575
In grad_steps = 2086, loss = 0.16845913231372833
In grad_steps = 2087, loss = 0.3891514539718628
In grad_steps = 2088, loss = 0.015563778579235077
In grad_steps = 2089, loss = 0.025312500074505806
In grad_steps = 2090, loss = 0.08940445631742477
In grad_steps = 2091, loss = 0.03485150635242462
In grad_steps = 2092, loss = 0.1955028623342514
In grad_steps = 2093, loss = 0.017778532579541206
In grad_steps = 2094, loss = 0.12412568926811218
In grad_steps = 2095, loss = 0.07425379008054733
In grad_steps = 2096, loss = 0.03144674003124237
In grad_steps = 2097, loss = 1.9340788125991821
In grad_steps = 2098, loss = 0.017005404457449913
In grad_steps = 2099, loss = 0.01505373790860176
In grad_steps = 2100, loss = 0.036851752549409866
In grad_steps = 2101, loss = 0.23995696008205414
In grad_steps = 2102, loss = 0.03377454727888107
In grad_steps = 2103, loss = 0.06776098906993866
In grad_steps = 2104, loss = 0.7585421204566956
In grad_steps = 2105, loss = 0.985646665096283
In grad_steps = 2106, loss = 0.06123579666018486
In grad_steps = 2107, loss = 0.7498199343681335
In grad_steps = 2108, loss = 0.14853535592556
In grad_steps = 2109, loss = 0.05323480814695358
In grad_steps = 2110, loss = 0.6390653252601624
In grad_steps = 2111, loss = 0.08382365107536316
In grad_steps = 2112, loss = 0.6577430367469788
In grad_steps = 2113, loss = 0.22101235389709473
In grad_steps = 2114, loss = 0.11109267175197601
In grad_steps = 2115, loss = 0.057427778840065
In grad_steps = 2116, loss = 0.08586328476667404
In grad_steps = 2117, loss = 0.4079868495464325
In grad_steps = 2118, loss = 0.3752613365650177
In grad_steps = 2119, loss = 0.039708640426397324
In grad_steps = 2120, loss = 0.050683509558439255
In grad_steps = 2121, loss = 0.19864152371883392
In grad_steps = 2122, loss = 0.14347350597381592
In grad_steps = 2123, loss = 0.38483357429504395
In grad_steps = 2124, loss = 0.1274123340845108
In grad_steps = 2125, loss = 0.06589699536561966
In grad_steps = 2126, loss = 1.1578119993209839
In grad_steps = 2127, loss = 0.13898664712905884
In grad_steps = 2128, loss = 0.16508150100708008
In grad_steps = 2129, loss = 0.2021235078573227
In grad_steps = 2130, loss = 0.25620943307876587
In grad_steps = 2131, loss = 0.5318033695220947
In grad_steps = 2132, loss = 0.4545469284057617
In grad_steps = 2133, loss = 0.043479740619659424
In grad_steps = 2134, loss = 0.20162466168403625
In grad_steps = 2135, loss = 0.5290404558181763
In grad_steps = 2136, loss = 0.6419297456741333
In grad_steps = 2137, loss = 0.45553088188171387
In grad_steps = 2138, loss = 0.35466498136520386
In grad_steps = 2139, loss = 0.0955926775932312
In grad_steps = 2140, loss = 0.17290836572647095
In grad_steps = 2141, loss = 0.13371950387954712
In grad_steps = 2142, loss = 0.037657480686903
In grad_steps = 2143, loss = 0.32629862427711487
In grad_steps = 2144, loss = 0.9347655177116394
In grad_steps = 2145, loss = 0.3429458439350128
In grad_steps = 2146, loss = 0.31965768337249756
In grad_steps = 2147, loss = 0.64178067445755
In grad_steps = 2148, loss = 0.9981272220611572
In grad_steps = 2149, loss = 0.04300510510802269
In grad_steps = 2150, loss = 0.218772292137146
In grad_steps = 2151, loss = 0.4708648920059204
In grad_steps = 2152, loss = 0.41943806409835815
In grad_steps = 2153, loss = 0.11986428499221802
In grad_steps = 2154, loss = 0.0706353634595871
In grad_steps = 2155, loss = 0.24351666867733002
In grad_steps = 2156, loss = 0.3937068581581116
In grad_steps = 2157, loss = 0.06870698183774948
In grad_steps = 2158, loss = 0.2419000267982483
In grad_steps = 2159, loss = 0.6775580048561096
In grad_steps = 2160, loss = 0.18447935581207275
In grad_steps = 2161, loss = 0.03508564084768295
In grad_steps = 2162, loss = 1.0076367855072021
In grad_steps = 2163, loss = 0.11558205634355545
In grad_steps = 2164, loss = 0.09748391062021255
In grad_steps = 2165, loss = 0.5222969055175781
In grad_steps = 2166, loss = 0.8142122626304626
In grad_steps = 2167, loss = 0.09297622740268707
In grad_steps = 2168, loss = 0.1440977305173874
In grad_steps = 2169, loss = 0.36294278502464294
In grad_steps = 2170, loss = 0.02586246468126774
In grad_steps = 2171, loss = 0.6845875382423401
In grad_steps = 2172, loss = 0.8180988430976868
In grad_steps = 2173, loss = 0.02234213799238205
In grad_steps = 2174, loss = 0.10374222695827484
In grad_steps = 2175, loss = 0.2897724211215973
In grad_steps = 2176, loss = 0.4170016646385193
In grad_steps = 2177, loss = 0.09237174689769745
In grad_steps = 2178, loss = 0.0873710960149765
In grad_steps = 2179, loss = 0.22937381267547607
In grad_steps = 2180, loss = 0.07414598762989044
In grad_steps = 2181, loss = 0.09202879667282104
In grad_steps = 2182, loss = 0.2711378037929535
In grad_steps = 2183, loss = 0.39877986907958984
In grad_steps = 2184, loss = 0.12315651029348373
In grad_steps = 2185, loss = 0.3463194966316223
In grad_steps = 2186, loss = 0.22331777215003967
In grad_steps = 2187, loss = 0.13556301593780518
In grad_steps = 2188, loss = 0.04043026268482208
In grad_steps = 2189, loss = 0.18758384883403778
In grad_steps = 2190, loss = 0.05594905465841293
In grad_steps = 2191, loss = 0.17193369567394257
In grad_steps = 2192, loss = 0.49477046728134155
In grad_steps = 2193, loss = 0.15754440426826477
In grad_steps = 2194, loss = 0.05518663674592972
In grad_steps = 2195, loss = 0.017341848462820053
In grad_steps = 2196, loss = 0.09257661551237106
In grad_steps = 2197, loss = 0.02952064387500286
In grad_steps = 2198, loss = 1.379543662071228
In grad_steps = 2199, loss = 0.08102340251207352
In grad_steps = 2200, loss = 0.15565618872642517
In grad_steps = 2201, loss = 0.29432255029678345
In grad_steps = 2202, loss = 0.32859116792678833
In grad_steps = 2203, loss = 0.13529376685619354
In grad_steps = 2204, loss = 0.19547533988952637
In grad_steps = 2205, loss = 0.01418017502874136
In grad_steps = 2206, loss = 0.21986541152000427
In grad_steps = 2207, loss = 0.047299567610025406
In grad_steps = 2208, loss = 0.5556326508522034
In grad_steps = 2209, loss = 0.03125588595867157
In grad_steps = 2210, loss = 0.11110600084066391
In grad_steps = 2211, loss = 0.07485581934452057
In grad_steps = 2212, loss = 1.4736751317977905
In grad_steps = 2213, loss = 0.9263730645179749
In grad_steps = 2214, loss = 0.2826981246471405
In grad_steps = 2215, loss = 0.26286599040031433
In grad_steps = 2216, loss = 0.29657480120658875
In grad_steps = 2217, loss = 1.4216971397399902
In grad_steps = 2218, loss = 0.5569630861282349
In grad_steps = 2219, loss = 0.35241490602493286
In grad_steps = 2220, loss = 0.05734371393918991
In grad_steps = 2221, loss = 0.24295611679553986
In grad_steps = 2222, loss = 0.44150805473327637
In grad_steps = 2223, loss = 0.40820086002349854
In grad_steps = 2224, loss = 0.06127655506134033
In grad_steps = 2225, loss = 0.5143665671348572
In grad_steps = 2226, loss = 0.3756282925605774
In grad_steps = 2227, loss = 0.4607137143611908
In grad_steps = 2228, loss = 0.5049572587013245
In grad_steps = 2229, loss = 0.19855573773384094
In grad_steps = 2230, loss = 0.24564875662326813
In grad_steps = 2231, loss = 0.08867699652910233
In grad_steps = 2232, loss = 0.11867114156484604
In grad_steps = 2233, loss = 0.13698185980319977
In grad_steps = 2234, loss = 0.24080026149749756
In grad_steps = 2235, loss = 0.029473572969436646
In grad_steps = 2236, loss = 0.3396987318992615
In grad_steps = 2237, loss = 0.8053188920021057
In grad_steps = 2238, loss = 0.2810268998146057
In grad_steps = 2239, loss = 0.28727269172668457
In grad_steps = 2240, loss = 0.2288796454668045
In grad_steps = 2241, loss = 0.19754068553447723
In grad_steps = 2242, loss = 0.40113675594329834
In grad_steps = 2243, loss = 0.08231542259454727
In grad_steps = 2244, loss = 0.17673993110656738
In grad_steps = 2245, loss = 1.1227658987045288
In grad_steps = 2246, loss = 0.4187588393688202
In grad_steps = 2247, loss = 0.10684628784656525
In grad_steps = 2248, loss = 0.5936653017997742
In grad_steps = 2249, loss = 0.5724398493766785
In grad_steps = 2250, loss = 0.055614762008190155
In grad_steps = 2251, loss = 0.1963358223438263
In grad_steps = 2252, loss = 0.8470895290374756
In grad_steps = 2253, loss = 0.5996531844139099
In grad_steps = 2254, loss = 0.23871932923793793
In grad_steps = 2255, loss = 0.27658843994140625
In grad_steps = 2256, loss = 0.5990242958068848
In grad_steps = 2257, loss = 0.16482247412204742
In grad_steps = 2258, loss = 0.1487847864627838
In grad_steps = 2259, loss = 0.40015119314193726
In grad_steps = 2260, loss = 0.10204015672206879
In grad_steps = 2261, loss = 0.2819516360759735
In grad_steps = 2262, loss = 0.13280081748962402
In grad_steps = 2263, loss = 0.32780319452285767
In grad_steps = 2264, loss = 0.49497541785240173
In grad_steps = 2265, loss = 0.07164699584245682
In grad_steps = 2266, loss = 0.3894391655921936
In grad_steps = 2267, loss = 0.23018494248390198
In grad_steps = 2268, loss = 0.03496991842985153
In grad_steps = 2269, loss = 0.567034125328064
In grad_steps = 2270, loss = 0.16147613525390625
In grad_steps = 2271, loss = 0.12550422549247742
In grad_steps = 2272, loss = 0.05868823826313019
In grad_steps = 2273, loss = 0.19202737510204315
In grad_steps = 2274, loss = 0.027780737727880478
In grad_steps = 2275, loss = 0.06570973992347717
In grad_steps = 2276, loss = 0.16081157326698303
In grad_steps = 2277, loss = 0.15297475457191467
In grad_steps = 2278, loss = 0.22889798879623413
In grad_steps = 2279, loss = 0.011117563582956791
In grad_steps = 2280, loss = 0.01623588614165783
In grad_steps = 2281, loss = 0.967710018157959
In grad_steps = 2282, loss = 0.02881033904850483
In grad_steps = 2283, loss = 0.8536713719367981
In grad_steps = 2284, loss = 0.07989372313022614
In grad_steps = 2285, loss = 0.03205103054642677
In grad_steps = 2286, loss = 0.13894513249397278
In grad_steps = 2287, loss = 0.4229605793952942
In grad_steps = 2288, loss = 0.06094731017947197
In grad_steps = 2289, loss = 0.028940508142113686
In grad_steps = 2290, loss = 0.36195963621139526
In grad_steps = 2291, loss = 0.0443115271627903
In grad_steps = 2292, loss = 0.5626049637794495
In grad_steps = 2293, loss = 0.11013442277908325
In grad_steps = 2294, loss = 0.05350188910961151
In grad_steps = 2295, loss = 0.1904790997505188
In grad_steps = 2296, loss = 0.08768893033266068
In grad_steps = 2297, loss = 0.9803358316421509
In grad_steps = 2298, loss = 0.1525290459394455
In grad_steps = 2299, loss = 0.12966160476207733
In grad_steps = 2300, loss = 0.07349030673503876
In grad_steps = 2301, loss = 0.08278796821832657
In grad_steps = 2302, loss = 0.13574431836605072
In grad_steps = 2303, loss = 0.018959131091833115
In grad_steps = 2304, loss = 0.03062477335333824
In grad_steps = 2305, loss = 0.22265002131462097
In grad_steps = 2306, loss = 0.03097614273428917
In grad_steps = 2307, loss = 0.2171962857246399
In grad_steps = 2308, loss = 0.6287680268287659
In grad_steps = 2309, loss = 0.008713282644748688
In grad_steps = 2310, loss = 0.808760404586792
In grad_steps = 2311, loss = 0.045347910374403
In grad_steps = 2312, loss = 0.9193511009216309
In grad_steps = 2313, loss = 1.3447608947753906
In grad_steps = 2314, loss = 0.042979903519153595
In grad_steps = 2315, loss = 0.33760273456573486
In grad_steps = 2316, loss = 0.11326965689659119
In grad_steps = 2317, loss = 0.12956570088863373
In grad_steps = 2318, loss = 0.3330303728580475
In grad_steps = 2319, loss = 1.459944486618042
In grad_steps = 2320, loss = 0.1154555231332779
In grad_steps = 2321, loss = 0.09288572520017624
In grad_steps = 2322, loss = 0.44059351086616516
In grad_steps = 2323, loss = 0.07051227986812592
In grad_steps = 2324, loss = 0.2876807153224945
In grad_steps = 2325, loss = 0.12683019042015076
In grad_steps = 2326, loss = 0.03513062000274658
In grad_steps = 2327, loss = 0.9678890705108643
In grad_steps = 2328, loss = 0.5019069314002991
In grad_steps = 2329, loss = 0.1424826830625534
In grad_steps = 2330, loss = 0.19791068136692047
In grad_steps = 2331, loss = 0.8082808256149292
In grad_steps = 2332, loss = 0.12546947598457336
In grad_steps = 2333, loss = 0.6464086771011353
In grad_steps = 2334, loss = 0.09580976516008377
In grad_steps = 2335, loss = 0.5511763095855713
In grad_steps = 2336, loss = 0.2820751965045929
In grad_steps = 2337, loss = 0.14549455046653748
In grad_steps = 2338, loss = 0.17294226586818695
In grad_steps = 2339, loss = 0.19724027812480927
In grad_steps = 2340, loss = 0.4087391793727875
In grad_steps = 2341, loss = 0.22155854105949402
In grad_steps = 2342, loss = 1.1186789274215698
In grad_steps = 2343, loss = 0.10374442487955093
In grad_steps = 2344, loss = 0.7431153059005737
In grad_steps = 2345, loss = 0.31655314564704895
In grad_steps = 2346, loss = 0.16324502229690552
In grad_steps = 2347, loss = 0.1943894475698471
In grad_steps = 2348, loss = 0.0864233523607254
In grad_steps = 2349, loss = 0.18598990142345428
In grad_steps = 2350, loss = 0.2453189194202423
In grad_steps = 2351, loss = 0.39756348729133606
In grad_steps = 2352, loss = 0.01031213253736496
Beginning epoch 2
In grad_steps = 2353, loss = 0.15739212930202484
In grad_steps = 2354, loss = 0.9301198124885559
In grad_steps = 2355, loss = 0.3664431869983673
In grad_steps = 2356, loss = 0.28751587867736816
In grad_steps = 2357, loss = 0.11925505846738815
In grad_steps = 2358, loss = 0.39116987586021423
In grad_steps = 2359, loss = 0.21486078202724457
In grad_steps = 2360, loss = 0.14709840714931488
In grad_steps = 2361, loss = 0.05455343797802925
In grad_steps = 2362, loss = 0.7875360250473022
In grad_steps = 2363, loss = 0.1806938350200653
In grad_steps = 2364, loss = 0.7058076858520508
In grad_steps = 2365, loss = 0.2965870499610901
In grad_steps = 2366, loss = 0.15207497775554657
In grad_steps = 2367, loss = 0.23865695297718048
In grad_steps = 2368, loss = 0.07245989888906479
In grad_steps = 2369, loss = 0.09644494950771332
In grad_steps = 2370, loss = 0.5733400583267212
In grad_steps = 2371, loss = 0.47117701172828674
In grad_steps = 2372, loss = 0.25164878368377686
In grad_steps = 2373, loss = 0.171352818608284
In grad_steps = 2374, loss = 0.14393121004104614
In grad_steps = 2375, loss = 0.27980148792266846
In grad_steps = 2376, loss = 0.7495423555374146
In grad_steps = 2377, loss = 0.4550504982471466
In grad_steps = 2378, loss = 0.12207521498203278
In grad_steps = 2379, loss = 0.06539779156446457
In grad_steps = 2380, loss = 0.009291927330195904
In grad_steps = 2381, loss = 0.11005142331123352
In grad_steps = 2382, loss = 0.03786568343639374
In grad_steps = 2383, loss = 0.09651399403810501
In grad_steps = 2384, loss = 0.24047817289829254
In grad_steps = 2385, loss = 0.2836725413799286
In grad_steps = 2386, loss = 0.03897332772612572
In grad_steps = 2387, loss = 0.16762104630470276
In grad_steps = 2388, loss = 0.27667564153671265
In grad_steps = 2389, loss = 0.019456302747130394
In grad_steps = 2390, loss = 0.016092266887426376
In grad_steps = 2391, loss = 0.21586942672729492
In grad_steps = 2392, loss = 0.37651050090789795
In grad_steps = 2393, loss = 0.017198912799358368
In grad_steps = 2394, loss = 0.10050126165151596
In grad_steps = 2395, loss = 0.10288630425930023
In grad_steps = 2396, loss = 0.011689474806189537
In grad_steps = 2397, loss = 0.27982673048973083
In grad_steps = 2398, loss = 0.08499525487422943
In grad_steps = 2399, loss = 0.24257516860961914
In grad_steps = 2400, loss = 1.1889152526855469
In grad_steps = 2401, loss = 0.15414784848690033
In grad_steps = 2402, loss = 0.600623369216919
In grad_steps = 2403, loss = 0.2846676707267761
In grad_steps = 2404, loss = 1.3734488487243652
In grad_steps = 2405, loss = 0.03574109077453613
In grad_steps = 2406, loss = 0.3719671666622162
In grad_steps = 2407, loss = 0.013898116536438465
In grad_steps = 2408, loss = 0.022913647815585136
In grad_steps = 2409, loss = 0.44168639183044434
In grad_steps = 2410, loss = 0.02799362689256668
In grad_steps = 2411, loss = 0.08171765506267548
In grad_steps = 2412, loss = 0.0375976637005806
In grad_steps = 2413, loss = 0.31863120198249817
In grad_steps = 2414, loss = 0.06618901342153549
In grad_steps = 2415, loss = 1.1185905933380127
In grad_steps = 2416, loss = 0.4572802186012268
In grad_steps = 2417, loss = 0.2838156521320343
In grad_steps = 2418, loss = 0.0406247079372406
In grad_steps = 2419, loss = 0.029749728739261627
In grad_steps = 2420, loss = 0.041655272245407104
In grad_steps = 2421, loss = 0.06057234853506088
In grad_steps = 2422, loss = 0.9914040565490723
In grad_steps = 2423, loss = 0.5254782438278198
In grad_steps = 2424, loss = 0.08549518138170242
In grad_steps = 2425, loss = 1.269303321838379
In grad_steps = 2426, loss = 0.11676955968141556
In grad_steps = 2427, loss = 0.07098007947206497
In grad_steps = 2428, loss = 0.2608996331691742
In grad_steps = 2429, loss = 0.3266843855381012
In grad_steps = 2430, loss = 0.4406048655509949
In grad_steps = 2431, loss = 0.2571500539779663
In grad_steps = 2432, loss = 0.5056289434432983
In grad_steps = 2433, loss = 0.12080824375152588
In grad_steps = 2434, loss = 0.06822353601455688
In grad_steps = 2435, loss = 0.1257946491241455
In grad_steps = 2436, loss = 0.2111719846725464
In grad_steps = 2437, loss = 0.08944539725780487
In grad_steps = 2438, loss = 0.4527449309825897
In grad_steps = 2439, loss = 1.3468499183654785
In grad_steps = 2440, loss = 0.11324137449264526
In grad_steps = 2441, loss = 0.390947163105011
In grad_steps = 2442, loss = 0.10180439054965973
In grad_steps = 2443, loss = 1.3222887516021729
In grad_steps = 2444, loss = 0.4382694363594055
In grad_steps = 2445, loss = 0.42974424362182617
In grad_steps = 2446, loss = 0.34600895643234253
In grad_steps = 2447, loss = 0.07183600962162018
In grad_steps = 2448, loss = 0.7936550974845886
In grad_steps = 2449, loss = 0.2088032364845276
In grad_steps = 2450, loss = 0.09602843225002289
In grad_steps = 2451, loss = 0.2812492549419403
In grad_steps = 2452, loss = 0.31694933772087097
In grad_steps = 2453, loss = 0.5253741145133972
In grad_steps = 2454, loss = 0.19003534317016602
In grad_steps = 2455, loss = 0.47398337721824646
In grad_steps = 2456, loss = 0.7180492281913757
In grad_steps = 2457, loss = 0.49072694778442383
In grad_steps = 2458, loss = 0.38459914922714233
In grad_steps = 2459, loss = 0.07786417752504349
In grad_steps = 2460, loss = 0.08738837391138077
In grad_steps = 2461, loss = 0.16365551948547363
In grad_steps = 2462, loss = 0.563515841960907
In grad_steps = 2463, loss = 0.20916080474853516
In grad_steps = 2464, loss = 0.07113486528396606
In grad_steps = 2465, loss = 0.5629376173019409
In grad_steps = 2466, loss = 0.2311343103647232
In grad_steps = 2467, loss = 0.5263620615005493
In grad_steps = 2468, loss = 0.285359263420105
In grad_steps = 2469, loss = 0.7461104393005371
In grad_steps = 2470, loss = 0.35095086693763733
In grad_steps = 2471, loss = 0.17081239819526672
In grad_steps = 2472, loss = 0.448053240776062
In grad_steps = 2473, loss = 0.7617717981338501
In grad_steps = 2474, loss = 0.1400647908449173
In grad_steps = 2475, loss = 0.3507007360458374
In grad_steps = 2476, loss = 0.08331628888845444
In grad_steps = 2477, loss = 0.33556050062179565
In grad_steps = 2478, loss = 1.0446596145629883
In grad_steps = 2479, loss = 0.4882895052433014
In grad_steps = 2480, loss = 0.12192535400390625
In grad_steps = 2481, loss = 0.4522087872028351
In grad_steps = 2482, loss = 0.13796542584896088
In grad_steps = 2483, loss = 0.3607942461967468
In grad_steps = 2484, loss = 0.30290383100509644
In grad_steps = 2485, loss = 0.22203050553798676
In grad_steps = 2486, loss = 0.05167713388800621
In grad_steps = 2487, loss = 0.553581178188324
In grad_steps = 2488, loss = 0.1102905347943306
In grad_steps = 2489, loss = 0.19879886507987976
In grad_steps = 2490, loss = 0.22642457485198975
In grad_steps = 2491, loss = 0.4908911883831024
In grad_steps = 2492, loss = 0.11987629532814026
In grad_steps = 2493, loss = 0.06078287214040756
In grad_steps = 2494, loss = 0.22050994634628296
In grad_steps = 2495, loss = 0.6818699240684509
In grad_steps = 2496, loss = 0.269912987947464
In grad_steps = 2497, loss = 0.11561368405818939
In grad_steps = 2498, loss = 0.33059564232826233
In grad_steps = 2499, loss = 0.3376536965370178
In grad_steps = 2500, loss = 0.7241082191467285
In grad_steps = 2501, loss = 0.8299486041069031
In grad_steps = 2502, loss = 0.9676937460899353
In grad_steps = 2503, loss = 0.15264356136322021
In grad_steps = 2504, loss = 0.2594073712825775
In grad_steps = 2505, loss = 0.34923720359802246
In grad_steps = 2506, loss = 0.3394550085067749
In grad_steps = 2507, loss = 0.9897018671035767
In grad_steps = 2508, loss = 0.18498975038528442
In grad_steps = 2509, loss = 0.15038955211639404
In grad_steps = 2510, loss = 0.40380972623825073
In grad_steps = 2511, loss = 0.09811648726463318
In grad_steps = 2512, loss = 0.2979304790496826
In grad_steps = 2513, loss = 0.5480391979217529
In grad_steps = 2514, loss = 0.08652043342590332
In grad_steps = 2515, loss = 0.36166924238204956
In grad_steps = 2516, loss = 0.12974105775356293
In grad_steps = 2517, loss = 0.15033146739006042
In grad_steps = 2518, loss = 0.2127574384212494
In grad_steps = 2519, loss = 0.3287160396575928
In grad_steps = 2520, loss = 0.054535187780857086
In grad_steps = 2521, loss = 0.06481234729290009
In grad_steps = 2522, loss = 0.638327419757843
In grad_steps = 2523, loss = 0.058250024914741516
In grad_steps = 2524, loss = 0.2778812050819397
In grad_steps = 2525, loss = 0.07992550730705261
In grad_steps = 2526, loss = 0.10638748109340668
In grad_steps = 2527, loss = 0.6389189958572388
In grad_steps = 2528, loss = 0.14296400547027588
In grad_steps = 2529, loss = 0.09504702687263489
In grad_steps = 2530, loss = 0.02507859468460083
In grad_steps = 2531, loss = 0.551621675491333
In grad_steps = 2532, loss = 0.040433332324028015
In grad_steps = 2533, loss = 0.22416992485523224
In grad_steps = 2534, loss = 0.020880723372101784
In grad_steps = 2535, loss = 0.05696999654173851
In grad_steps = 2536, loss = 0.034509431570768356
In grad_steps = 2537, loss = 0.06750710308551788
In grad_steps = 2538, loss = 0.7546703815460205
In grad_steps = 2539, loss = 0.22779837250709534
In grad_steps = 2540, loss = 0.01741741970181465
In grad_steps = 2541, loss = 0.27113407850265503
In grad_steps = 2542, loss = 0.007977385073900223
In grad_steps = 2543, loss = 0.06832210719585419
In grad_steps = 2544, loss = 0.7389357686042786
In grad_steps = 2545, loss = 0.6176515221595764
In grad_steps = 2546, loss = 0.017260633409023285
In grad_steps = 2547, loss = 0.5846765637397766
In grad_steps = 2548, loss = 0.01793254353106022
In grad_steps = 2549, loss = 0.8702355027198792
In grad_steps = 2550, loss = 0.019006654620170593
In grad_steps = 2551, loss = 0.024476608261466026
In grad_steps = 2552, loss = 0.08938033878803253
In grad_steps = 2553, loss = 0.04886829853057861
In grad_steps = 2554, loss = 0.642633318901062
In grad_steps = 2555, loss = 0.5424554347991943
In grad_steps = 2556, loss = 0.02786373719573021
In grad_steps = 2557, loss = 0.07476475089788437
In grad_steps = 2558, loss = 0.2645747661590576
In grad_steps = 2559, loss = 0.7635774612426758
In grad_steps = 2560, loss = 0.16571328043937683
In grad_steps = 2561, loss = 0.11881774663925171
In grad_steps = 2562, loss = 0.5066609382629395
In grad_steps = 2563, loss = 0.10560314357280731
In grad_steps = 2564, loss = 0.1393371969461441
In grad_steps = 2565, loss = 0.09509406983852386
In grad_steps = 2566, loss = 0.10693585872650146
In grad_steps = 2567, loss = 0.27594152092933655
In grad_steps = 2568, loss = 0.19613128900527954
In grad_steps = 2569, loss = 0.09427279233932495
In grad_steps = 2570, loss = 0.47794994711875916
In grad_steps = 2571, loss = 0.3514844477176666
In grad_steps = 2572, loss = 0.10398717224597931
In grad_steps = 2573, loss = 0.05435776710510254
In grad_steps = 2574, loss = 0.3197062611579895
In grad_steps = 2575, loss = 0.006425058469176292
In grad_steps = 2576, loss = 0.09359517693519592
In grad_steps = 2577, loss = 0.1631811410188675
In grad_steps = 2578, loss = 0.9909184575080872
In grad_steps = 2579, loss = 0.19339051842689514
In grad_steps = 2580, loss = 0.5257528424263
In grad_steps = 2581, loss = 0.3803744614124298
In grad_steps = 2582, loss = 0.4891497492790222
In grad_steps = 2583, loss = 0.17219823598861694
In grad_steps = 2584, loss = 0.06567178666591644
In grad_steps = 2585, loss = 0.040996234863996506
In grad_steps = 2586, loss = 0.8077432513237
In grad_steps = 2587, loss = 0.2441682666540146
In grad_steps = 2588, loss = 0.1170857697725296
In grad_steps = 2589, loss = 0.28790006041526794
In grad_steps = 2590, loss = 0.40910235047340393
In grad_steps = 2591, loss = 0.09717163443565369
In grad_steps = 2592, loss = 0.3452335298061371
In grad_steps = 2593, loss = 0.16314095258712769
In grad_steps = 2594, loss = 0.137460857629776
In grad_steps = 2595, loss = 0.0232427679002285
In grad_steps = 2596, loss = 0.0526006855070591
In grad_steps = 2597, loss = 0.0671844556927681
In grad_steps = 2598, loss = 0.02717442810535431
In grad_steps = 2599, loss = 0.013791561126708984
In grad_steps = 2600, loss = 0.6391494274139404
In grad_steps = 2601, loss = 0.012683154083788395
In grad_steps = 2602, loss = 0.03650713339447975
In grad_steps = 2603, loss = 0.18519048392772675
In grad_steps = 2604, loss = 0.16283755004405975
In grad_steps = 2605, loss = 1.5160709619522095
In grad_steps = 2606, loss = 0.04196374863386154
In grad_steps = 2607, loss = 0.16345714032649994
In grad_steps = 2608, loss = 0.927697479724884
In grad_steps = 2609, loss = 0.15970130264759064
In grad_steps = 2610, loss = 0.290540874004364
In grad_steps = 2611, loss = 0.044724397361278534
In grad_steps = 2612, loss = 0.0353083610534668
In grad_steps = 2613, loss = 0.5452002286911011
In grad_steps = 2614, loss = 0.1488158106803894
In grad_steps = 2615, loss = 0.07090164721012115
In grad_steps = 2616, loss = 0.5756712555885315
In grad_steps = 2617, loss = 0.06629953533411026
In grad_steps = 2618, loss = 0.07967336475849152
In grad_steps = 2619, loss = 0.2032184898853302
In grad_steps = 2620, loss = 0.05466942489147186
In grad_steps = 2621, loss = 0.3704344630241394
In grad_steps = 2622, loss = 0.1045626550912857
In grad_steps = 2623, loss = 0.17218917608261108
In grad_steps = 2624, loss = 0.11851601302623749
In grad_steps = 2625, loss = 0.06088311970233917
In grad_steps = 2626, loss = 0.5522738695144653
In grad_steps = 2627, loss = 0.03693766891956329
In grad_steps = 2628, loss = 0.04712814465165138
In grad_steps = 2629, loss = 0.18119733035564423
In grad_steps = 2630, loss = 0.39844298362731934
In grad_steps = 2631, loss = 0.018413081765174866
In grad_steps = 2632, loss = 0.99991774559021
In grad_steps = 2633, loss = 0.6673890948295593
In grad_steps = 2634, loss = 0.9483535289764404
In grad_steps = 2635, loss = 0.18345947563648224
In grad_steps = 2636, loss = 0.4535515010356903
In grad_steps = 2637, loss = 0.09258623421192169
In grad_steps = 2638, loss = 0.7324008345603943
In grad_steps = 2639, loss = 0.6191433668136597
In grad_steps = 2640, loss = 0.017545590177178383
In grad_steps = 2641, loss = 0.01899675652384758
In grad_steps = 2642, loss = 0.45597755908966064
In grad_steps = 2643, loss = 0.10608858615159988
In grad_steps = 2644, loss = 0.17202447354793549
In grad_steps = 2645, loss = 0.10494422167539597
In grad_steps = 2646, loss = 0.20369204878807068
In grad_steps = 2647, loss = 0.13441386818885803
In grad_steps = 2648, loss = 0.19165951013565063
In grad_steps = 2649, loss = 0.043807253241539
In grad_steps = 2650, loss = 0.14461559057235718
In grad_steps = 2651, loss = 0.16835471987724304
In grad_steps = 2652, loss = 0.11308527737855911
In grad_steps = 2653, loss = 0.11133931577205658
In grad_steps = 2654, loss = 0.2376086413860321
In grad_steps = 2655, loss = 0.012708723545074463
In grad_steps = 2656, loss = 0.3068374693393707
In grad_steps = 2657, loss = 0.6692144274711609
In grad_steps = 2658, loss = 0.8004077672958374
In grad_steps = 2659, loss = 0.03949865698814392
In grad_steps = 2660, loss = 0.3916322588920593
In grad_steps = 2661, loss = 0.020091675221920013
In grad_steps = 2662, loss = 0.5571193099021912
In grad_steps = 2663, loss = 0.18328478932380676
In grad_steps = 2664, loss = 0.22542256116867065
In grad_steps = 2665, loss = 0.23317407071590424
In grad_steps = 2666, loss = 0.28847822546958923
In grad_steps = 2667, loss = 1.2765636444091797
In grad_steps = 2668, loss = 0.5460984110832214
In grad_steps = 2669, loss = 0.0547439344227314
In grad_steps = 2670, loss = 0.09363599121570587
In grad_steps = 2671, loss = 0.4449194669723511
In grad_steps = 2672, loss = 0.06693635880947113
In grad_steps = 2673, loss = 0.0650978609919548
In grad_steps = 2674, loss = 0.11006129533052444
In grad_steps = 2675, loss = 0.11096721887588501
In grad_steps = 2676, loss = 0.5543321967124939
In grad_steps = 2677, loss = 0.08925795555114746
In grad_steps = 2678, loss = 0.11401356756687164
In grad_steps = 2679, loss = 0.050775445997714996
In grad_steps = 2680, loss = 0.038267169147729874
In grad_steps = 2681, loss = 0.06491037458181381
In grad_steps = 2682, loss = 0.014873914420604706
In grad_steps = 2683, loss = 0.08075573295354843
In grad_steps = 2684, loss = 0.145737886428833
In grad_steps = 2685, loss = 0.14142389595508575
In grad_steps = 2686, loss = 1.1788997650146484
In grad_steps = 2687, loss = 0.014982693828642368
In grad_steps = 2688, loss = 0.06925345957279205
In grad_steps = 2689, loss = 0.09836922585964203
In grad_steps = 2690, loss = 0.08443546295166016
In grad_steps = 2691, loss = 0.05785117298364639
In grad_steps = 2692, loss = 0.13478174805641174
In grad_steps = 2693, loss = 0.2501086890697479
In grad_steps = 2694, loss = 0.14886999130249023
In grad_steps = 2695, loss = 0.6255279183387756
In grad_steps = 2696, loss = 0.20648379623889923
In grad_steps = 2697, loss = 0.25092172622680664
In grad_steps = 2698, loss = 0.04772648960351944
In grad_steps = 2699, loss = 0.28334009647369385
In grad_steps = 2700, loss = 0.048970840871334076
In grad_steps = 2701, loss = 0.782565712928772
In grad_steps = 2702, loss = 0.6529541611671448
In grad_steps = 2703, loss = 0.409872829914093
In grad_steps = 2704, loss = 0.17609867453575134
In grad_steps = 2705, loss = 0.2851894199848175
In grad_steps = 2706, loss = 0.05239493027329445
In grad_steps = 2707, loss = 0.24530893564224243
In grad_steps = 2708, loss = 0.7335494756698608
In grad_steps = 2709, loss = 0.2638966143131256
In grad_steps = 2710, loss = 0.4416334331035614
In grad_steps = 2711, loss = 0.09378853440284729
In grad_steps = 2712, loss = 0.3560299575328827
In grad_steps = 2713, loss = 0.30080798268318176
In grad_steps = 2714, loss = 0.07528515160083771
In grad_steps = 2715, loss = 0.04725562408566475
In grad_steps = 2716, loss = 0.030518509447574615
In grad_steps = 2717, loss = 0.06920340657234192
In grad_steps = 2718, loss = 0.06401336193084717
In grad_steps = 2719, loss = 0.03105662204325199
In grad_steps = 2720, loss = 0.14068248867988586
In grad_steps = 2721, loss = 0.031125349923968315
In grad_steps = 2722, loss = 0.055957943201065063
In grad_steps = 2723, loss = 1.4074583053588867
In grad_steps = 2724, loss = 0.014528406783938408
In grad_steps = 2725, loss = 0.03162891045212746
In grad_steps = 2726, loss = 0.22581855952739716
In grad_steps = 2727, loss = 0.04284411668777466
In grad_steps = 2728, loss = 0.027644440531730652
In grad_steps = 2729, loss = 0.015149448066949844
In grad_steps = 2730, loss = 0.07884813845157623
In grad_steps = 2731, loss = 0.35233280062675476
In grad_steps = 2732, loss = 0.10587067157030106
In grad_steps = 2733, loss = 0.07930819690227509
In grad_steps = 2734, loss = 0.48479828238487244
In grad_steps = 2735, loss = 0.2590460777282715
In grad_steps = 2736, loss = 0.029943278059363365
In grad_steps = 2737, loss = 0.1716228574514389
In grad_steps = 2738, loss = 0.1968957483768463
In grad_steps = 2739, loss = 0.7814978361129761
In grad_steps = 2740, loss = 0.028714865446090698
In grad_steps = 2741, loss = 0.8501035571098328
In grad_steps = 2742, loss = 0.547072172164917
In grad_steps = 2743, loss = 0.10458803921937943
In grad_steps = 2744, loss = 0.08837484568357468
In grad_steps = 2745, loss = 0.01756703481078148
In grad_steps = 2746, loss = 0.39542651176452637
In grad_steps = 2747, loss = 0.030610471963882446
In grad_steps = 2748, loss = 0.16032128036022186
In grad_steps = 2749, loss = 0.029727913439273834
In grad_steps = 2750, loss = 0.15383082628250122
In grad_steps = 2751, loss = 0.07890138030052185
In grad_steps = 2752, loss = 0.3211206793785095
In grad_steps = 2753, loss = 0.19611141085624695
In grad_steps = 2754, loss = 0.0545479953289032
In grad_steps = 2755, loss = 0.11531344056129456
In grad_steps = 2756, loss = 0.6165539622306824
In grad_steps = 2757, loss = 0.13911449909210205
In grad_steps = 2758, loss = 0.06707543134689331
In grad_steps = 2759, loss = 0.8287197351455688
In grad_steps = 2760, loss = 0.16056516766548157
In grad_steps = 2761, loss = 0.20521652698516846
In grad_steps = 2762, loss = 0.49437975883483887
In grad_steps = 2763, loss = 0.02037791535258293
In grad_steps = 2764, loss = 0.5712320804595947
In grad_steps = 2765, loss = 0.10484347492456436
In grad_steps = 2766, loss = 0.235389843583107
In grad_steps = 2767, loss = 0.24988678097724915
In grad_steps = 2768, loss = 0.17481809854507446
In grad_steps = 2769, loss = 0.06951842457056046
In grad_steps = 2770, loss = 0.13070763647556305
In grad_steps = 2771, loss = 0.1414399892091751
In grad_steps = 2772, loss = 0.10251408815383911
In grad_steps = 2773, loss = 0.0029417858459055424
In grad_steps = 2774, loss = 0.30933669209480286
In grad_steps = 2775, loss = 0.23780111968517303
In grad_steps = 2776, loss = 0.08912022411823273
In grad_steps = 2777, loss = 0.1558767706155777
In grad_steps = 2778, loss = 0.0024806801229715347
In grad_steps = 2779, loss = 0.049426302313804626
In grad_steps = 2780, loss = 0.008431527763605118
In grad_steps = 2781, loss = 0.002492014318704605
In grad_steps = 2782, loss = 1.0050008296966553
In grad_steps = 2783, loss = 0.11438705027103424
In grad_steps = 2784, loss = 0.2691977322101593
In grad_steps = 2785, loss = 0.006830662488937378
In grad_steps = 2786, loss = 0.5064062476158142
In grad_steps = 2787, loss = 0.9179377555847168
In grad_steps = 2788, loss = 0.02605154924094677
In grad_steps = 2789, loss = 0.19043517112731934
In grad_steps = 2790, loss = 0.2518481910228729
In grad_steps = 2791, loss = 0.4707305431365967
In grad_steps = 2792, loss = 0.12197589129209518
In grad_steps = 2793, loss = 0.026418622583150864
In grad_steps = 2794, loss = 0.0401313416659832
In grad_steps = 2795, loss = 0.2511294484138489
In grad_steps = 2796, loss = 0.045947711914777756
In grad_steps = 2797, loss = 0.13366271555423737
In grad_steps = 2798, loss = 0.008660765364766121
In grad_steps = 2799, loss = 0.48180025815963745
In grad_steps = 2800, loss = 0.046202901750802994
In grad_steps = 2801, loss = 0.16329218447208405
In grad_steps = 2802, loss = 0.13236463069915771
In grad_steps = 2803, loss = 0.03134699910879135
In grad_steps = 2804, loss = 0.12714967131614685
In grad_steps = 2805, loss = 0.33872804045677185
In grad_steps = 2806, loss = 0.14466850459575653
In grad_steps = 2807, loss = 0.5676483511924744
In grad_steps = 2808, loss = 0.008851020596921444
In grad_steps = 2809, loss = 0.485727459192276
In grad_steps = 2810, loss = 0.040074680000543594
In grad_steps = 2811, loss = 0.5123600363731384
In grad_steps = 2812, loss = 0.028383517637848854
In grad_steps = 2813, loss = 0.02816498465836048
In grad_steps = 2814, loss = 0.005883830599486828
In grad_steps = 2815, loss = 0.9329391121864319
In grad_steps = 2816, loss = 0.04347014054656029
In grad_steps = 2817, loss = 0.09687899053096771
In grad_steps = 2818, loss = 0.8646013140678406
In grad_steps = 2819, loss = 0.17169559001922607
In grad_steps = 2820, loss = 0.41880807280540466
In grad_steps = 2821, loss = 0.1004234254360199
In grad_steps = 2822, loss = 0.1674877405166626
In grad_steps = 2823, loss = 0.24183985590934753
In grad_steps = 2824, loss = 0.013651793822646141
In grad_steps = 2825, loss = 0.10283060371875763
In grad_steps = 2826, loss = 0.030163493007421494
In grad_steps = 2827, loss = 0.31084713339805603
In grad_steps = 2828, loss = 0.08313298970460892
In grad_steps = 2829, loss = 0.011112513951957226
In grad_steps = 2830, loss = 0.06349437683820724
In grad_steps = 2831, loss = 0.06302409619092941
In grad_steps = 2832, loss = 0.984785795211792
In grad_steps = 2833, loss = 0.026415545493364334
In grad_steps = 2834, loss = 0.3306519389152527
In grad_steps = 2835, loss = 0.022560860961675644
In grad_steps = 2836, loss = 0.21425645053386688
In grad_steps = 2837, loss = 0.7535184025764465
In grad_steps = 2838, loss = 0.45909595489501953
In grad_steps = 2839, loss = 0.023825623095035553
In grad_steps = 2840, loss = 0.4930017292499542
In grad_steps = 2841, loss = 1.3460862636566162
In grad_steps = 2842, loss = 0.6496190428733826
In grad_steps = 2843, loss = 0.5016252994537354
In grad_steps = 2844, loss = 0.30434083938598633
In grad_steps = 2845, loss = 0.07759594917297363
In grad_steps = 2846, loss = 0.06498856097459793
In grad_steps = 2847, loss = 0.5943586230278015
In grad_steps = 2848, loss = 0.04123314097523689
In grad_steps = 2849, loss = 0.18507468700408936
In grad_steps = 2850, loss = 0.08047449588775635
In grad_steps = 2851, loss = 0.4889759421348572
In grad_steps = 2852, loss = 0.20385554432868958
In grad_steps = 2853, loss = 0.07129579037427902
In grad_steps = 2854, loss = 0.3516213595867157
In grad_steps = 2855, loss = 1.0816032886505127
In grad_steps = 2856, loss = 0.4734173119068146
In grad_steps = 2857, loss = 0.3382223844528198
In grad_steps = 2858, loss = 0.43357139825820923
In grad_steps = 2859, loss = 0.2510705590248108
In grad_steps = 2860, loss = 0.2848283350467682
In grad_steps = 2861, loss = 0.7505625486373901
In grad_steps = 2862, loss = 0.3261262774467468
In grad_steps = 2863, loss = 0.0466589480638504
In grad_steps = 2864, loss = 0.40215086936950684
In grad_steps = 2865, loss = 0.14510896801948547
In grad_steps = 2866, loss = 0.16042691469192505
In grad_steps = 2867, loss = 0.24076873064041138
In grad_steps = 2868, loss = 0.1882576048374176
In grad_steps = 2869, loss = 0.39015817642211914
In grad_steps = 2870, loss = 0.3637796938419342
In grad_steps = 2871, loss = 0.12289397418498993
In grad_steps = 2872, loss = 0.4720112383365631
In grad_steps = 2873, loss = 0.09337814897298813
In grad_steps = 2874, loss = 0.6670433282852173
In grad_steps = 2875, loss = 0.06628943979740143
In grad_steps = 2876, loss = 0.058422937989234924
In grad_steps = 2877, loss = 0.036501724272966385
In grad_steps = 2878, loss = 0.2055053412914276
In grad_steps = 2879, loss = 0.11200081557035446
In grad_steps = 2880, loss = 0.1117945984005928
In grad_steps = 2881, loss = 0.08543042838573456
In grad_steps = 2882, loss = 0.08101531118154526
In grad_steps = 2883, loss = 0.037139974534511566
In grad_steps = 2884, loss = 0.7009820938110352
In grad_steps = 2885, loss = 0.0978817492723465
In grad_steps = 2886, loss = 0.03762591630220413
In grad_steps = 2887, loss = 0.7891440391540527
In grad_steps = 2888, loss = 0.816596269607544
In grad_steps = 2889, loss = 0.39077287912368774
In grad_steps = 2890, loss = 0.09306906163692474
In grad_steps = 2891, loss = 0.26298144459724426
In grad_steps = 2892, loss = 0.056703999638557434
In grad_steps = 2893, loss = 0.04577513039112091
In grad_steps = 2894, loss = 0.04063566029071808
In grad_steps = 2895, loss = 0.14995668828487396
In grad_steps = 2896, loss = 0.014989899471402168
In grad_steps = 2897, loss = 1.2581132650375366
In grad_steps = 2898, loss = 0.5000858902931213
In grad_steps = 2899, loss = 0.04267868399620056
In grad_steps = 2900, loss = 0.9376858472824097
In grad_steps = 2901, loss = 0.08200877904891968
In grad_steps = 2902, loss = 0.2166333645582199
In grad_steps = 2903, loss = 0.6756306886672974
In grad_steps = 2904, loss = 0.31744053959846497
In grad_steps = 2905, loss = 0.3581258952617645
In grad_steps = 2906, loss = 0.05100001022219658
In grad_steps = 2907, loss = 0.3117676079273224
In grad_steps = 2908, loss = 0.32774293422698975
In grad_steps = 2909, loss = 0.500874400138855
In grad_steps = 2910, loss = 0.23507951200008392
In grad_steps = 2911, loss = 0.40049660205841064
In grad_steps = 2912, loss = 0.33248916268348694
In grad_steps = 2913, loss = 0.8761024475097656
In grad_steps = 2914, loss = 0.08388889580965042
In grad_steps = 2915, loss = 0.25768136978149414
In grad_steps = 2916, loss = 0.1370650827884674
In grad_steps = 2917, loss = 1.0703636407852173
In grad_steps = 2918, loss = 0.2645184099674225
In grad_steps = 2919, loss = 0.3787681758403778
In grad_steps = 2920, loss = 0.8147642016410828
In grad_steps = 2921, loss = 0.3308162987232208
In grad_steps = 2922, loss = 0.13412639498710632
In grad_steps = 2923, loss = 0.12830746173858643
In grad_steps = 2924, loss = 0.18824082612991333
In grad_steps = 2925, loss = 0.21224641799926758
In grad_steps = 2926, loss = 0.03885013237595558
In grad_steps = 2927, loss = 0.10000453889369965
In grad_steps = 2928, loss = 0.09343837201595306
In grad_steps = 2929, loss = 0.17901422083377838
In grad_steps = 2930, loss = 0.057572077959775925
In grad_steps = 2931, loss = 0.1660819947719574
In grad_steps = 2932, loss = 0.23017579317092896
In grad_steps = 2933, loss = 0.43684104084968567
In grad_steps = 2934, loss = 0.3002323508262634
In grad_steps = 2935, loss = 0.18873657286167145
In grad_steps = 2936, loss = 0.273090124130249
In grad_steps = 2937, loss = 0.02552296780049801
In grad_steps = 2938, loss = 0.03311290964484215
In grad_steps = 2939, loss = 0.06760235130786896
In grad_steps = 2940, loss = 0.12368785589933395
In grad_steps = 2941, loss = 0.05480009317398071
In grad_steps = 2942, loss = 1.0207486152648926
In grad_steps = 2943, loss = 0.07874137908220291
In grad_steps = 2944, loss = 0.10364818572998047
In grad_steps = 2945, loss = 0.7718634605407715
In grad_steps = 2946, loss = 0.03508269786834717
In grad_steps = 2947, loss = 0.03164590150117874
In grad_steps = 2948, loss = 0.3437662124633789
In grad_steps = 2949, loss = 0.3585641086101532
In grad_steps = 2950, loss = 0.2847795784473419
In grad_steps = 2951, loss = 0.14839406311511993
In grad_steps = 2952, loss = 0.4251333177089691
In grad_steps = 2953, loss = 0.3723708689212799
In grad_steps = 2954, loss = 0.013362378813326359
In grad_steps = 2955, loss = 0.03984682261943817
In grad_steps = 2956, loss = 0.05714227259159088
In grad_steps = 2957, loss = 0.014012153260409832
In grad_steps = 2958, loss = 0.08136440068483353
In grad_steps = 2959, loss = 0.027583859860897064
In grad_steps = 2960, loss = 0.20079383254051208
In grad_steps = 2961, loss = 0.07471749931573868
In grad_steps = 2962, loss = 0.3218590319156647
In grad_steps = 2963, loss = 0.2890236973762512
In grad_steps = 2964, loss = 0.46025606989860535
In grad_steps = 2965, loss = 0.04072566330432892
In grad_steps = 2966, loss = 0.7219159603118896
In grad_steps = 2967, loss = 0.2016313523054123
In grad_steps = 2968, loss = 0.33812686800956726
In grad_steps = 2969, loss = 0.10340283066034317
In grad_steps = 2970, loss = 0.006732907146215439
In grad_steps = 2971, loss = 0.2151009440422058
In grad_steps = 2972, loss = 0.027105750516057014
In grad_steps = 2973, loss = 0.029878253117203712
In grad_steps = 2974, loss = 1.5415713787078857
In grad_steps = 2975, loss = 0.14575427770614624
In grad_steps = 2976, loss = 0.08439493924379349
In grad_steps = 2977, loss = 0.019459862262010574
In grad_steps = 2978, loss = 0.362998366355896
In grad_steps = 2979, loss = 0.6019920706748962
In grad_steps = 2980, loss = 0.01299118623137474
In grad_steps = 2981, loss = 0.22640755772590637
In grad_steps = 2982, loss = 0.044481586664915085
In grad_steps = 2983, loss = 0.03604932874441147
In grad_steps = 2984, loss = 0.131860613822937
In grad_steps = 2985, loss = 0.09230002015829086
In grad_steps = 2986, loss = 0.20203529298305511
In grad_steps = 2987, loss = 0.022959094494581223
In grad_steps = 2988, loss = 0.07832842320203781
In grad_steps = 2989, loss = 0.10926373302936554
In grad_steps = 2990, loss = 0.0635734349489212
In grad_steps = 2991, loss = 0.009966839104890823
In grad_steps = 2992, loss = 0.831425130367279
In grad_steps = 2993, loss = 0.14905759692192078
In grad_steps = 2994, loss = 0.06402365863323212
In grad_steps = 2995, loss = 0.09306973218917847
In grad_steps = 2996, loss = 0.08145804703235626
In grad_steps = 2997, loss = 0.2810652256011963
In grad_steps = 2998, loss = 1.189246654510498
In grad_steps = 2999, loss = 0.012706222012639046
In grad_steps = 3000, loss = 0.0631599873304367
In grad_steps = 3001, loss = 0.21237581968307495
In grad_steps = 3002, loss = 0.05339061841368675
In grad_steps = 3003, loss = 0.3528939187526703
In grad_steps = 3004, loss = 0.15005052089691162
In grad_steps = 3005, loss = 1.3545317649841309
In grad_steps = 3006, loss = 0.3539620339870453
In grad_steps = 3007, loss = 0.924001157283783
In grad_steps = 3008, loss = 0.9961163401603699
In grad_steps = 3009, loss = 0.0321507565677166
In grad_steps = 3010, loss = 0.04024432972073555
In grad_steps = 3011, loss = 0.10929335653781891
In grad_steps = 3012, loss = 0.026119496673345566
In grad_steps = 3013, loss = 0.06756202131509781
In grad_steps = 3014, loss = 0.07663067430257797
In grad_steps = 3015, loss = 0.35949641466140747
In grad_steps = 3016, loss = 0.054559484124183655
In grad_steps = 3017, loss = 0.03580093756318092
In grad_steps = 3018, loss = 0.19134849309921265
In grad_steps = 3019, loss = 0.07138977944850922
In grad_steps = 3020, loss = 0.16982673108577728
In grad_steps = 3021, loss = 0.024908363819122314
In grad_steps = 3022, loss = 0.6748878359794617
In grad_steps = 3023, loss = 0.25217774510383606
In grad_steps = 3024, loss = 1.0043833255767822
In grad_steps = 3025, loss = 0.055584635585546494
In grad_steps = 3026, loss = 1.1006501913070679
In grad_steps = 3027, loss = 0.040697984397411346
In grad_steps = 3028, loss = 0.23862528800964355
In grad_steps = 3029, loss = 0.08047051727771759
In grad_steps = 3030, loss = 0.26608964800834656
In grad_steps = 3031, loss = 0.06104787439107895
In grad_steps = 3032, loss = 0.09268029034137726
In grad_steps = 3033, loss = 0.040875934064388275
In grad_steps = 3034, loss = 0.9129638075828552
In grad_steps = 3035, loss = 0.31711941957473755
In grad_steps = 3036, loss = 0.27230969071388245
In grad_steps = 3037, loss = 0.18975356221199036
In grad_steps = 3038, loss = 0.10815994441509247
In grad_steps = 3039, loss = 0.14402610063552856
In grad_steps = 3040, loss = 0.2469431757926941
In grad_steps = 3041, loss = 0.3249826431274414
In grad_steps = 3042, loss = 0.03499862924218178
In grad_steps = 3043, loss = 0.24304963648319244
In grad_steps = 3044, loss = 0.6713399887084961
In grad_steps = 3045, loss = 0.15940739214420319
In grad_steps = 3046, loss = 0.07597589492797852
In grad_steps = 3047, loss = 0.06514795124530792
In grad_steps = 3048, loss = 0.25215116143226624
In grad_steps = 3049, loss = 0.14076459407806396
In grad_steps = 3050, loss = 0.2891083359718323
In grad_steps = 3051, loss = 0.048386529088020325
In grad_steps = 3052, loss = 0.20720171928405762
In grad_steps = 3053, loss = 0.4410583972930908
In grad_steps = 3054, loss = 0.027928583323955536
In grad_steps = 3055, loss = 0.042686253786087036
In grad_steps = 3056, loss = 0.013152433559298515
In grad_steps = 3057, loss = 0.03576357290148735
In grad_steps = 3058, loss = 0.024977274239063263
In grad_steps = 3059, loss = 0.08656390011310577
In grad_steps = 3060, loss = 0.021656274795532227
In grad_steps = 3061, loss = 0.017770392820239067
In grad_steps = 3062, loss = 0.060198888182640076
In grad_steps = 3063, loss = 0.022952258586883545
In grad_steps = 3064, loss = 0.9942905902862549
In grad_steps = 3065, loss = 1.1832228899002075
In grad_steps = 3066, loss = 0.7178113460540771
In grad_steps = 3067, loss = 0.8579409718513489
In grad_steps = 3068, loss = 1.9453668594360352
In grad_steps = 3069, loss = 0.1841534674167633
In grad_steps = 3070, loss = 0.025998417288064957
In grad_steps = 3071, loss = 0.029807819053530693
In grad_steps = 3072, loss = 0.02799364924430847
In grad_steps = 3073, loss = 0.02248985692858696
In grad_steps = 3074, loss = 1.0192394256591797
In grad_steps = 3075, loss = 0.45654940605163574
In grad_steps = 3076, loss = 0.17263153195381165
In grad_steps = 3077, loss = 0.057285189628601074
In grad_steps = 3078, loss = 0.09673596173524857
In grad_steps = 3079, loss = 0.11792057752609253
In grad_steps = 3080, loss = 0.07072564214468002
In grad_steps = 3081, loss = 0.044445328414440155
In grad_steps = 3082, loss = 0.03967626765370369
In grad_steps = 3083, loss = 0.2144746482372284
In grad_steps = 3084, loss = 0.30305707454681396
In grad_steps = 3085, loss = 0.5267615914344788
In grad_steps = 3086, loss = 0.9820054769515991
In grad_steps = 3087, loss = 0.1843983232975006
In grad_steps = 3088, loss = 0.6772489547729492
In grad_steps = 3089, loss = 0.05300149321556091
In grad_steps = 3090, loss = 0.05433639883995056
In grad_steps = 3091, loss = 0.7597746849060059
In grad_steps = 3092, loss = 0.06096755713224411
In grad_steps = 3093, loss = 0.3338475227355957
In grad_steps = 3094, loss = 0.16573213040828705
In grad_steps = 3095, loss = 0.0693444088101387
In grad_steps = 3096, loss = 0.06141578033566475
In grad_steps = 3097, loss = 0.6070519089698792
In grad_steps = 3098, loss = 0.6454489827156067
In grad_steps = 3099, loss = 0.14902405440807343
In grad_steps = 3100, loss = 0.2145102322101593
In grad_steps = 3101, loss = 0.06345593929290771
In grad_steps = 3102, loss = 0.803776204586029
In grad_steps = 3103, loss = 0.6084529757499695
In grad_steps = 3104, loss = 0.09665623307228088
In grad_steps = 3105, loss = 0.12177836894989014
In grad_steps = 3106, loss = 0.07775876671075821
In grad_steps = 3107, loss = 0.3355322480201721
In grad_steps = 3108, loss = 0.41708314418792725
In grad_steps = 3109, loss = 0.3172527253627777
In grad_steps = 3110, loss = 0.18223857879638672
In grad_steps = 3111, loss = 0.5737391114234924
In grad_steps = 3112, loss = 0.1798160970211029
In grad_steps = 3113, loss = 0.1310395896434784
In grad_steps = 3114, loss = 0.26420071721076965
In grad_steps = 3115, loss = 0.5242668390274048
In grad_steps = 3116, loss = 0.18197184801101685
In grad_steps = 3117, loss = 0.6112548112869263
In grad_steps = 3118, loss = 0.6999673843383789
In grad_steps = 3119, loss = 0.047941599041223526
In grad_steps = 3120, loss = 0.06859118491411209
In grad_steps = 3121, loss = 0.048490408807992935
In grad_steps = 3122, loss = 0.5087414979934692
In grad_steps = 3123, loss = 0.1014561727643013
In grad_steps = 3124, loss = 0.09960814565420151
In grad_steps = 3125, loss = 0.2409532070159912
In grad_steps = 3126, loss = 1.3820685148239136
In grad_steps = 3127, loss = 0.050099581480026245
In grad_steps = 3128, loss = 0.11109302937984467
In grad_steps = 3129, loss = 0.1445867419242859
In grad_steps = 3130, loss = 0.05430431291460991
In grad_steps = 3131, loss = 0.13548025488853455
In grad_steps = 3132, loss = 0.03362586721777916
In grad_steps = 3133, loss = 0.13060371577739716
In grad_steps = 3134, loss = 0.03978905454277992
In grad_steps = 3135, loss = 0.05147910863161087
In grad_steps = 3136, loss = 0.5493782162666321
In grad_steps = 3137, loss = 0.9444139003753662
In grad_steps = 3138, loss = 0.021665271371603012
In grad_steps = 3139, loss = 0.03934321179986
In grad_steps = 3140, loss = 0.05742369964718819
In grad_steps = 3141, loss = 0.27460581064224243
In grad_steps = 3142, loss = 0.06847210973501205
In grad_steps = 3143, loss = 0.2840535044670105
In grad_steps = 3144, loss = 0.03421657532453537
In grad_steps = 3145, loss = 0.026135683059692383
In grad_steps = 3146, loss = 0.16714245080947876
In grad_steps = 3147, loss = 1.5728821754455566
In grad_steps = 3148, loss = 0.7199618816375732
In grad_steps = 3149, loss = 0.06259877979755402
In grad_steps = 3150, loss = 0.8415436148643494
In grad_steps = 3151, loss = 0.0614716112613678
In grad_steps = 3152, loss = 0.25213849544525146
In grad_steps = 3153, loss = 0.15464138984680176
In grad_steps = 3154, loss = 0.19118046760559082
In grad_steps = 3155, loss = 0.11690452694892883
In grad_steps = 3156, loss = 0.05589931830763817
In grad_steps = 3157, loss = 0.03979506716132164
In grad_steps = 3158, loss = 0.13013595342636108
In grad_steps = 3159, loss = 0.04840213060379028
In grad_steps = 3160, loss = 0.0717640146613121
In grad_steps = 3161, loss = 0.17527107894420624
In grad_steps = 3162, loss = 0.04085806384682655
In grad_steps = 3163, loss = 0.6353660225868225
In grad_steps = 3164, loss = 0.042166948318481445
In grad_steps = 3165, loss = 0.3640049695968628
In grad_steps = 3166, loss = 0.33108776807785034
In grad_steps = 3167, loss = 0.042133793234825134
In grad_steps = 3168, loss = 0.04146639257669449
In grad_steps = 3169, loss = 0.8709425330162048
In grad_steps = 3170, loss = 0.034199535846710205
In grad_steps = 3171, loss = 0.2423916757106781
In grad_steps = 3172, loss = 0.05367545783519745
In grad_steps = 3173, loss = 0.05797658488154411
In grad_steps = 3174, loss = 0.19004103541374207
In grad_steps = 3175, loss = 0.11715540289878845
In grad_steps = 3176, loss = 0.15171001851558685
In grad_steps = 3177, loss = 0.031239304691553116
In grad_steps = 3178, loss = 0.03111066110432148
In grad_steps = 3179, loss = 0.23301352560520172
In grad_steps = 3180, loss = 0.06962736696004868
In grad_steps = 3181, loss = 0.5048813819885254
In grad_steps = 3182, loss = 0.6886206865310669
In grad_steps = 3183, loss = 0.4200681447982788
In grad_steps = 3184, loss = 0.04617365449666977
In grad_steps = 3185, loss = 0.9638396501541138
In grad_steps = 3186, loss = 0.7202546000480652
In grad_steps = 3187, loss = 0.2762501835823059
In grad_steps = 3188, loss = 1.4462484121322632
In grad_steps = 3189, loss = 0.09259478747844696
In grad_steps = 3190, loss = 0.03990495949983597
In grad_steps = 3191, loss = 0.19010888040065765
In grad_steps = 3192, loss = 0.132987380027771
In grad_steps = 3193, loss = 0.056883521378040314
In grad_steps = 3194, loss = 0.5362972617149353
In grad_steps = 3195, loss = 0.05648379400372505
In grad_steps = 3196, loss = 0.6078578233718872
In grad_steps = 3197, loss = 0.514075517654419
In grad_steps = 3198, loss = 0.05683927237987518
In grad_steps = 3199, loss = 0.39997339248657227
In grad_steps = 3200, loss = 0.04930032044649124
In grad_steps = 3201, loss = 0.4853658080101013
In grad_steps = 3202, loss = 0.4932243824005127
In grad_steps = 3203, loss = 0.5858999490737915
In grad_steps = 3204, loss = 0.5326577425003052
In grad_steps = 3205, loss = 0.44015800952911377
In grad_steps = 3206, loss = 0.22187361121177673
In grad_steps = 3207, loss = 0.11279526352882385
In grad_steps = 3208, loss = 0.3512345254421234
In grad_steps = 3209, loss = 0.07945030927658081
In grad_steps = 3210, loss = 0.18118813633918762
In grad_steps = 3211, loss = 0.5328481197357178
In grad_steps = 3212, loss = 0.142695352435112
In grad_steps = 3213, loss = 0.28876054286956787
In grad_steps = 3214, loss = 0.4725441038608551
In grad_steps = 3215, loss = 0.06354957073926926
In grad_steps = 3216, loss = 0.4353726804256439
In grad_steps = 3217, loss = 0.1333843320608139
In grad_steps = 3218, loss = 0.21658045053482056
In grad_steps = 3219, loss = 0.1303248256444931
In grad_steps = 3220, loss = 0.2744467258453369
In grad_steps = 3221, loss = 0.24308031797409058
In grad_steps = 3222, loss = 0.4421464800834656
In grad_steps = 3223, loss = 0.1256842017173767
In grad_steps = 3224, loss = 0.08921900391578674
In grad_steps = 3225, loss = 0.05403469502925873
In grad_steps = 3226, loss = 0.21063028275966644
In grad_steps = 3227, loss = 0.046920619904994965
In grad_steps = 3228, loss = 0.030929021537303925
In grad_steps = 3229, loss = 0.02357259765267372
In grad_steps = 3230, loss = 0.02500537410378456
In grad_steps = 3231, loss = 0.10961489379405975
In grad_steps = 3232, loss = 0.45339834690093994
In grad_steps = 3233, loss = 0.025942042469978333
In grad_steps = 3234, loss = 0.3254411518573761
In grad_steps = 3235, loss = 0.11562765389680862
In grad_steps = 3236, loss = 0.003930015955120325
In grad_steps = 3237, loss = 0.008321933448314667
In grad_steps = 3238, loss = 0.023815114051103592
In grad_steps = 3239, loss = 0.3220769166946411
In grad_steps = 3240, loss = 0.007410268299281597
In grad_steps = 3241, loss = 0.019029997289180756
In grad_steps = 3242, loss = 1.287755012512207
In grad_steps = 3243, loss = 0.35135820508003235
In grad_steps = 3244, loss = 0.0018637592438608408
In grad_steps = 3245, loss = 1.6392396688461304
In grad_steps = 3246, loss = 0.05158248171210289
In grad_steps = 3247, loss = 0.10373514145612717
In grad_steps = 3248, loss = 0.18838448822498322
In grad_steps = 3249, loss = 0.12840288877487183
In grad_steps = 3250, loss = 0.30577924847602844
In grad_steps = 3251, loss = 0.7685272693634033
In grad_steps = 3252, loss = 0.12199763208627701
In grad_steps = 3253, loss = 0.33209919929504395
In grad_steps = 3254, loss = 0.23106975853443146
In grad_steps = 3255, loss = 0.14720897376537323
In grad_steps = 3256, loss = 0.144571915268898
In grad_steps = 3257, loss = 0.22315624356269836
In grad_steps = 3258, loss = 0.31721949577331543
In grad_steps = 3259, loss = 0.16024276614189148
In grad_steps = 3260, loss = 0.07560700178146362
In grad_steps = 3261, loss = 0.10079481452703476
In grad_steps = 3262, loss = 0.27573275566101074
In grad_steps = 3263, loss = 0.08021984249353409
In grad_steps = 3264, loss = 0.08683240413665771
In grad_steps = 3265, loss = 0.11041965335607529
In grad_steps = 3266, loss = 0.40257325768470764
In grad_steps = 3267, loss = 0.39703768491744995
In grad_steps = 3268, loss = 0.2786163091659546
In grad_steps = 3269, loss = 0.19491180777549744
In grad_steps = 3270, loss = 0.04216865450143814
In grad_steps = 3271, loss = 0.9358260631561279
In grad_steps = 3272, loss = 1.6565797328948975
In grad_steps = 3273, loss = 0.5831376314163208
In grad_steps = 3274, loss = 0.43748486042022705
In grad_steps = 3275, loss = 0.027828991413116455
In grad_steps = 3276, loss = 1.3514710664749146
In grad_steps = 3277, loss = 0.7607372999191284
In grad_steps = 3278, loss = 0.07607045024633408
In grad_steps = 3279, loss = 0.08564703911542892
In grad_steps = 3280, loss = 0.156342014670372
In grad_steps = 3281, loss = 0.6480593681335449
In grad_steps = 3282, loss = 0.16366541385650635
In grad_steps = 3283, loss = 0.579155683517456
In grad_steps = 3284, loss = 0.2711799740791321
In grad_steps = 3285, loss = 0.1646938920021057
In grad_steps = 3286, loss = 0.5348821878433228
In grad_steps = 3287, loss = 0.25742360949516296
In grad_steps = 3288, loss = 0.17890851199626923
In grad_steps = 3289, loss = 0.27170851826667786
In grad_steps = 3290, loss = 0.4499150216579437
In grad_steps = 3291, loss = 0.28974735736846924
In grad_steps = 3292, loss = 0.3183516561985016
In grad_steps = 3293, loss = 0.6132113337516785
In grad_steps = 3294, loss = 0.280587762594223
In grad_steps = 3295, loss = 0.2857859432697296
In grad_steps = 3296, loss = 0.7743517160415649
In grad_steps = 3297, loss = 0.14036229252815247
In grad_steps = 3298, loss = 0.10510766506195068
In grad_steps = 3299, loss = 0.07535621523857117
In grad_steps = 3300, loss = 0.39325302839279175
In grad_steps = 3301, loss = 0.2867414355278015
In grad_steps = 3302, loss = 0.6768448948860168
In grad_steps = 3303, loss = 0.09829002618789673
In grad_steps = 3304, loss = 0.7165605425834656
In grad_steps = 3305, loss = 0.27372846007347107
In grad_steps = 3306, loss = 0.3246000111103058
In grad_steps = 3307, loss = 0.2652769684791565
In grad_steps = 3308, loss = 0.4180227816104889
In grad_steps = 3309, loss = 0.5402563810348511
In grad_steps = 3310, loss = 0.6026006937026978
In grad_steps = 3311, loss = 0.23443195223808289
In grad_steps = 3312, loss = 0.10212661325931549
In grad_steps = 3313, loss = 0.09075655043125153
In grad_steps = 3314, loss = 0.04733223468065262
In grad_steps = 3315, loss = 0.16168509423732758
In grad_steps = 3316, loss = 0.5613451600074768
In grad_steps = 3317, loss = 0.05344823747873306
In grad_steps = 3318, loss = 0.5880544185638428
In grad_steps = 3319, loss = 0.10990841686725616
In grad_steps = 3320, loss = 0.19519716501235962
In grad_steps = 3321, loss = 0.20037971436977386
In grad_steps = 3322, loss = 0.12649953365325928
In grad_steps = 3323, loss = 0.2359512597322464
In grad_steps = 3324, loss = 0.03145604953169823
In grad_steps = 3325, loss = 0.1163412481546402
In grad_steps = 3326, loss = 0.38693445920944214
In grad_steps = 3327, loss = 0.02729569375514984
In grad_steps = 3328, loss = 0.06607029587030411
In grad_steps = 3329, loss = 0.09400337189435959
In grad_steps = 3330, loss = 0.07014612853527069
In grad_steps = 3331, loss = 0.029951658099889755
In grad_steps = 3332, loss = 0.0349508672952652
In grad_steps = 3333, loss = 0.12642894685268402
In grad_steps = 3334, loss = 0.026983486488461494
In grad_steps = 3335, loss = 0.725016176700592
In grad_steps = 3336, loss = 0.02687319740653038
In grad_steps = 3337, loss = 0.00830178800970316
In grad_steps = 3338, loss = 0.0938812866806984
In grad_steps = 3339, loss = 0.20500890910625458
In grad_steps = 3340, loss = 0.008037394843995571
In grad_steps = 3341, loss = 0.011598965153098106
In grad_steps = 3342, loss = 0.004835463594645262
In grad_steps = 3343, loss = 0.009277836419641972
In grad_steps = 3344, loss = 0.01889442279934883
In grad_steps = 3345, loss = 0.05870866775512695
In grad_steps = 3346, loss = 0.0320378877222538
In grad_steps = 3347, loss = 0.7732740044593811
In grad_steps = 3348, loss = 0.01864641346037388
In grad_steps = 3349, loss = 0.0844673216342926
In grad_steps = 3350, loss = 0.3101605474948883
In grad_steps = 3351, loss = 0.055549122393131256
In grad_steps = 3352, loss = 0.09336443245410919
In grad_steps = 3353, loss = 0.029006272554397583
In grad_steps = 3354, loss = 0.10696037113666534
In grad_steps = 3355, loss = 0.07883214205503464
In grad_steps = 3356, loss = 0.037994831800460815
In grad_steps = 3357, loss = 0.04027922451496124
In grad_steps = 3358, loss = 0.16235285997390747
In grad_steps = 3359, loss = 0.28766217827796936
In grad_steps = 3360, loss = 0.008745071478188038
In grad_steps = 3361, loss = 0.0025602083187550306
In grad_steps = 3362, loss = 0.013619530946016312
In grad_steps = 3363, loss = 0.002030732110142708
In grad_steps = 3364, loss = 0.015275206416845322
In grad_steps = 3365, loss = 0.9462852478027344
In grad_steps = 3366, loss = 0.00965946912765503
In grad_steps = 3367, loss = 0.11409656703472137
In grad_steps = 3368, loss = 0.03419792279601097
In grad_steps = 3369, loss = 0.0033316670451313257
In grad_steps = 3370, loss = 0.6338806748390198
In grad_steps = 3371, loss = 0.5495884418487549
In grad_steps = 3372, loss = 0.1557091772556305
In grad_steps = 3373, loss = 0.7958284616470337
In grad_steps = 3374, loss = 0.6924517750740051
In grad_steps = 3375, loss = 0.8285523056983948
In grad_steps = 3376, loss = 0.05022207647562027
In grad_steps = 3377, loss = 0.25772014260292053
In grad_steps = 3378, loss = 0.3481712341308594
In grad_steps = 3379, loss = 0.47864577174186707
In grad_steps = 3380, loss = 0.1749008595943451
In grad_steps = 3381, loss = 0.07152792066335678
In grad_steps = 3382, loss = 0.2873117923736572
In grad_steps = 3383, loss = 1.1439192295074463
In grad_steps = 3384, loss = 0.9820382595062256
In grad_steps = 3385, loss = 0.020984504371881485
In grad_steps = 3386, loss = 0.26550430059432983
In grad_steps = 3387, loss = 0.08383753150701523
In grad_steps = 3388, loss = 0.3828866183757782
In grad_steps = 3389, loss = 0.09003119170665741
In grad_steps = 3390, loss = 0.40118125081062317
In grad_steps = 3391, loss = 0.17379045486450195
In grad_steps = 3392, loss = 0.2879401743412018
In grad_steps = 3393, loss = 0.8049554228782654
In grad_steps = 3394, loss = 0.031107332557439804
In grad_steps = 3395, loss = 0.2237635850906372
In grad_steps = 3396, loss = 0.06829748302698135
In grad_steps = 3397, loss = 0.9657008051872253
In grad_steps = 3398, loss = 0.08592633903026581
In grad_steps = 3399, loss = 0.13671858608722687
In grad_steps = 3400, loss = 0.2598039507865906
In grad_steps = 3401, loss = 0.1759108603000641
In grad_steps = 3402, loss = 0.15252070128917694
In grad_steps = 3403, loss = 1.3787001371383667
In grad_steps = 3404, loss = 0.06867814064025879
In grad_steps = 3405, loss = 0.751507580280304
In grad_steps = 3406, loss = 0.04986304044723511
In grad_steps = 3407, loss = 0.7078930735588074
In grad_steps = 3408, loss = 0.5159403085708618
In grad_steps = 3409, loss = 0.11146895587444305
In grad_steps = 3410, loss = 0.04000626876950264
In grad_steps = 3411, loss = 0.16589023172855377
In grad_steps = 3412, loss = 0.2918051481246948
In grad_steps = 3413, loss = 0.08634085208177567
In grad_steps = 3414, loss = 0.09059102088212967
In grad_steps = 3415, loss = 0.03625478222966194
In grad_steps = 3416, loss = 0.8665161728858948
In grad_steps = 3417, loss = 0.051098182797431946
In grad_steps = 3418, loss = 0.22682243585586548
In grad_steps = 3419, loss = 0.34472155570983887
In grad_steps = 3420, loss = 0.2266983538866043
In grad_steps = 3421, loss = 0.03532242029905319
In grad_steps = 3422, loss = 0.15458737313747406
In grad_steps = 3423, loss = 0.4262041449546814
In grad_steps = 3424, loss = 0.0867428183555603
In grad_steps = 3425, loss = 0.10851763188838959
In grad_steps = 3426, loss = 0.6723787188529968
In grad_steps = 3427, loss = 0.12046189606189728
In grad_steps = 3428, loss = 0.12648163735866547
In grad_steps = 3429, loss = 0.6427151560783386
In grad_steps = 3430, loss = 0.3572695851325989
In grad_steps = 3431, loss = 0.20540936291217804
In grad_steps = 3432, loss = 0.23310168087482452
In grad_steps = 3433, loss = 0.043627530336380005
In grad_steps = 3434, loss = 0.06334120035171509
In grad_steps = 3435, loss = 0.3744414150714874
In grad_steps = 3436, loss = 0.05543351173400879
In grad_steps = 3437, loss = 0.06437984853982925
In grad_steps = 3438, loss = 0.059792645275592804
In grad_steps = 3439, loss = 0.03035898320376873
In grad_steps = 3440, loss = 0.04873988777399063
In grad_steps = 3441, loss = 0.9785835146903992
In grad_steps = 3442, loss = 0.25544503331184387
In grad_steps = 3443, loss = 0.23398007452487946
In grad_steps = 3444, loss = 0.2279578447341919
In grad_steps = 3445, loss = 0.42212170362472534
In grad_steps = 3446, loss = 0.10268602520227432
In grad_steps = 3447, loss = 0.030771613121032715
In grad_steps = 3448, loss = 0.07591520994901657
In grad_steps = 3449, loss = 0.06941350549459457
In grad_steps = 3450, loss = 0.024905672296881676
In grad_steps = 3451, loss = 0.06432845443487167
In grad_steps = 3452, loss = 0.3541402816772461
In grad_steps = 3453, loss = 0.06128305569291115
In grad_steps = 3454, loss = 0.08798965811729431
In grad_steps = 3455, loss = 0.0850730687379837
In grad_steps = 3456, loss = 0.8805890679359436
In grad_steps = 3457, loss = 0.022127747535705566
In grad_steps = 3458, loss = 0.016038993373513222
In grad_steps = 3459, loss = 0.38475286960601807
In grad_steps = 3460, loss = 0.4992733895778656
In grad_steps = 3461, loss = 0.10975770652294159
In grad_steps = 3462, loss = 0.32774850726127625
In grad_steps = 3463, loss = 0.13512387871742249
In grad_steps = 3464, loss = 0.02291014790534973
In grad_steps = 3465, loss = 0.8618799448013306
In grad_steps = 3466, loss = 0.605666995048523
In grad_steps = 3467, loss = 1.2011024951934814
In grad_steps = 3468, loss = 0.024599015712738037
In grad_steps = 3469, loss = 0.08020003139972687
In grad_steps = 3470, loss = 0.15389636158943176
In grad_steps = 3471, loss = 0.2233167439699173
In grad_steps = 3472, loss = 0.08796931058168411
In grad_steps = 3473, loss = 0.34774887561798096
In grad_steps = 3474, loss = 0.1050698533654213
In grad_steps = 3475, loss = 0.08157671988010406
In grad_steps = 3476, loss = 0.11365076899528503
In grad_steps = 3477, loss = 0.21117672324180603
In grad_steps = 3478, loss = 0.027320053428411484
In grad_steps = 3479, loss = 0.02881649322807789
In grad_steps = 3480, loss = 0.12238812446594238
In grad_steps = 3481, loss = 0.08822258561849594
In grad_steps = 3482, loss = 0.07667499035596848
In grad_steps = 3483, loss = 0.7499187588691711
In grad_steps = 3484, loss = 0.09356330335140228
In grad_steps = 3485, loss = 0.13845020532608032
In grad_steps = 3486, loss = 0.1465059220790863
In grad_steps = 3487, loss = 0.9651680588722229
In grad_steps = 3488, loss = 0.16348578035831451
In grad_steps = 3489, loss = 0.04651683196425438
In grad_steps = 3490, loss = 0.7822769284248352
In grad_steps = 3491, loss = 0.8450581431388855
In grad_steps = 3492, loss = 0.02115599438548088
In grad_steps = 3493, loss = 0.07012457400560379
In grad_steps = 3494, loss = 0.1902148276567459
In grad_steps = 3495, loss = 0.5949386954307556
In grad_steps = 3496, loss = 0.07859808206558228
In grad_steps = 3497, loss = 0.026779573410749435
In grad_steps = 3498, loss = 0.008030220866203308
In grad_steps = 3499, loss = 0.0644097551703453
In grad_steps = 3500, loss = 0.05886094644665718
In grad_steps = 3501, loss = 0.331674724817276
In grad_steps = 3502, loss = 0.10880257189273834
In grad_steps = 3503, loss = 0.03393276408314705
In grad_steps = 3504, loss = 0.6621711254119873
In grad_steps = 3505, loss = 0.4954163134098053
In grad_steps = 3506, loss = 0.20468874275684357
In grad_steps = 3507, loss = 0.03683091700077057
In grad_steps = 3508, loss = 0.15848849713802338
In grad_steps = 3509, loss = 0.441830575466156
In grad_steps = 3510, loss = 0.03295978531241417
In grad_steps = 3511, loss = 0.11560395359992981
In grad_steps = 3512, loss = 0.030759654939174652
In grad_steps = 3513, loss = 0.7102434635162354
In grad_steps = 3514, loss = 0.2633506655693054
In grad_steps = 3515, loss = 0.053129591047763824
In grad_steps = 3516, loss = 0.05476968735456467
In grad_steps = 3517, loss = 0.025806544348597527
In grad_steps = 3518, loss = 0.5751714706420898
In grad_steps = 3519, loss = 0.2705978751182556
In grad_steps = 3520, loss = 0.048284318298101425
In grad_steps = 3521, loss = 0.8399155735969543
In grad_steps = 3522, loss = 1.1586413383483887
In grad_steps = 3523, loss = 1.2133722305297852
In grad_steps = 3524, loss = 0.07291439175605774
In grad_steps = 3525, loss = 0.037513617426157
In grad_steps = 3526, loss = 0.10525336116552353
In grad_steps = 3527, loss = 0.33406350016593933
In grad_steps = 3528, loss = 0.21280385553836823
In grad_steps = 3529, loss = 0.22698648273944855
In grad_steps = 3530, loss = 0.22186748683452606
In grad_steps = 3531, loss = 0.10859763622283936
In grad_steps = 3532, loss = 0.12281307578086853
In grad_steps = 3533, loss = 0.0602489598095417
In grad_steps = 3534, loss = 0.621455192565918
In grad_steps = 3535, loss = 0.30319154262542725
In grad_steps = 3536, loss = 0.10424260795116425
In grad_steps = 3537, loss = 0.08078055828809738
In grad_steps = 3538, loss = 0.8653521537780762
In grad_steps = 3539, loss = 0.37385886907577515
In grad_steps = 3540, loss = 0.21956515312194824
In grad_steps = 3541, loss = 0.07133305072784424
In grad_steps = 3542, loss = 0.15752798318862915
In grad_steps = 3543, loss = 0.18314094841480255
In grad_steps = 3544, loss = 0.06965135782957077
In grad_steps = 3545, loss = 0.06905830651521683
In grad_steps = 3546, loss = 0.02578822337090969
In grad_steps = 3547, loss = 0.0443352609872818
In grad_steps = 3548, loss = 0.7997316122055054
In grad_steps = 3549, loss = 0.13815046846866608
In grad_steps = 3550, loss = 0.03533432260155678
In grad_steps = 3551, loss = 0.026933742687106133
In grad_steps = 3552, loss = 0.11337985098361969
In grad_steps = 3553, loss = 0.2221587896347046
In grad_steps = 3554, loss = 0.3791228234767914
In grad_steps = 3555, loss = 0.7949238419532776
In grad_steps = 3556, loss = 0.170230895280838
In grad_steps = 3557, loss = 0.040373556315898895
In grad_steps = 3558, loss = 0.07074303179979324
In grad_steps = 3559, loss = 0.7453075051307678
In grad_steps = 3560, loss = 0.04131380841135979
In grad_steps = 3561, loss = 0.04273080825805664
In grad_steps = 3562, loss = 0.4507492184638977
In grad_steps = 3563, loss = 0.027147425338625908
In grad_steps = 3564, loss = 0.022238409146666527
In grad_steps = 3565, loss = 0.022361338138580322
In grad_steps = 3566, loss = 0.7795423269271851
In grad_steps = 3567, loss = 0.09373994171619415
In grad_steps = 3568, loss = 0.1590237021446228
In grad_steps = 3569, loss = 0.073113813996315
In grad_steps = 3570, loss = 0.08517059683799744
In grad_steps = 3571, loss = 0.03151512145996094
In grad_steps = 3572, loss = 0.3745814561843872
In grad_steps = 3573, loss = 0.18101680278778076
In grad_steps = 3574, loss = 0.09614473581314087
In grad_steps = 3575, loss = 0.058891840279102325
In grad_steps = 3576, loss = 0.11189131438732147
In grad_steps = 3577, loss = 0.015689950436353683
In grad_steps = 3578, loss = 0.09606495499610901
In grad_steps = 3579, loss = 0.056293413043022156
In grad_steps = 3580, loss = 0.054729897528886795
In grad_steps = 3581, loss = 0.03653578832745552
In grad_steps = 3582, loss = 0.07148592174053192
In grad_steps = 3583, loss = 0.04213725030422211
In grad_steps = 3584, loss = 0.6004896759986877
In grad_steps = 3585, loss = 0.3540302813053131
In grad_steps = 3586, loss = 0.03451348468661308
In grad_steps = 3587, loss = 0.5290129780769348
In grad_steps = 3588, loss = 0.4371531307697296
In grad_steps = 3589, loss = 0.08323165774345398
In grad_steps = 3590, loss = 0.2065122276544571
In grad_steps = 3591, loss = 0.5342745780944824
In grad_steps = 3592, loss = 0.8453595042228699
In grad_steps = 3593, loss = 1.755344271659851
In grad_steps = 3594, loss = 0.417648583650589
In grad_steps = 3595, loss = 0.013369099237024784
In grad_steps = 3596, loss = 0.6913623213768005
In grad_steps = 3597, loss = 0.13491946458816528
In grad_steps = 3598, loss = 0.059864163398742676
In grad_steps = 3599, loss = 0.5411047339439392
In grad_steps = 3600, loss = 0.16952602565288544
In grad_steps = 3601, loss = 0.33323290944099426
In grad_steps = 3602, loss = 0.049602191895246506
In grad_steps = 3603, loss = 0.3516709506511688
In grad_steps = 3604, loss = 0.14532321691513062
In grad_steps = 3605, loss = 0.09704510122537613
In grad_steps = 3606, loss = 0.25038447976112366
In grad_steps = 3607, loss = 0.15814277529716492
In grad_steps = 3608, loss = 0.10354980826377869
In grad_steps = 3609, loss = 0.567886233329773
In grad_steps = 3610, loss = 0.024826563894748688
In grad_steps = 3611, loss = 0.23809926211833954
In grad_steps = 3612, loss = 0.32045722007751465
In grad_steps = 3613, loss = 0.12434306740760803
In grad_steps = 3614, loss = 0.11653406918048859
In grad_steps = 3615, loss = 0.04358191415667534
In grad_steps = 3616, loss = 0.11011304706335068
In grad_steps = 3617, loss = 0.281347393989563
In grad_steps = 3618, loss = 0.09873101115226746
In grad_steps = 3619, loss = 0.6010528206825256
In grad_steps = 3620, loss = 0.07560195028781891
In grad_steps = 3621, loss = 0.1325058490037918
In grad_steps = 3622, loss = 0.14044837653636932
In grad_steps = 3623, loss = 0.3531832695007324
In grad_steps = 3624, loss = 0.08529408276081085
In grad_steps = 3625, loss = 0.00551613699644804
In grad_steps = 3626, loss = 0.010863544419407845
In grad_steps = 3627, loss = 0.013950096443295479
In grad_steps = 3628, loss = 0.006232289597392082
In grad_steps = 3629, loss = 0.21868407726287842
In grad_steps = 3630, loss = 0.10548259317874908
In grad_steps = 3631, loss = 0.029542822390794754
In grad_steps = 3632, loss = 0.00412765983492136
In grad_steps = 3633, loss = 0.00618189899250865
In grad_steps = 3634, loss = 0.008465454913675785
In grad_steps = 3635, loss = 0.07571600377559662
In grad_steps = 3636, loss = 0.012093823403120041
In grad_steps = 3637, loss = 0.004283883608877659
In grad_steps = 3638, loss = 0.6519376039505005
In grad_steps = 3639, loss = 0.14547473192214966
In grad_steps = 3640, loss = 0.1523352414369583
In grad_steps = 3641, loss = 0.07721062004566193
In grad_steps = 3642, loss = 0.008590301498770714
In grad_steps = 3643, loss = 0.010800771415233612
In grad_steps = 3644, loss = 0.2710101306438446
In grad_steps = 3645, loss = 0.029353808611631393
In grad_steps = 3646, loss = 0.014292947947978973
In grad_steps = 3647, loss = 0.7528998851776123
In grad_steps = 3648, loss = 0.02632260136306286
In grad_steps = 3649, loss = 0.37567636370658875
In grad_steps = 3650, loss = 0.15876108407974243
In grad_steps = 3651, loss = 0.00912244338542223
In grad_steps = 3652, loss = 0.06414531171321869
In grad_steps = 3653, loss = 0.15990254282951355
In grad_steps = 3654, loss = 0.0042265309020876884
In grad_steps = 3655, loss = 0.19024008512496948
In grad_steps = 3656, loss = 0.023295771330595016
In grad_steps = 3657, loss = 0.32248935103416443
In grad_steps = 3658, loss = 0.01348956860601902
In grad_steps = 3659, loss = 0.14362215995788574
In grad_steps = 3660, loss = 0.019488949328660965
In grad_steps = 3661, loss = 0.13275989890098572
In grad_steps = 3662, loss = 0.45889171957969666
In grad_steps = 3663, loss = 0.04855150729417801
In grad_steps = 3664, loss = 0.006750473752617836
In grad_steps = 3665, loss = 0.13380078971385956
In grad_steps = 3666, loss = 0.045536793768405914
In grad_steps = 3667, loss = 0.06087541580200195
In grad_steps = 3668, loss = 0.09350505471229553
In grad_steps = 3669, loss = 0.1877577006816864
In grad_steps = 3670, loss = 0.8229240775108337
In grad_steps = 3671, loss = 0.8657541275024414
In grad_steps = 3672, loss = 0.030558183789253235
In grad_steps = 3673, loss = 0.12798890471458435
In grad_steps = 3674, loss = 0.13446590304374695
In grad_steps = 3675, loss = 0.32739758491516113
In grad_steps = 3676, loss = 0.012224439531564713
In grad_steps = 3677, loss = 0.12986436486244202
In grad_steps = 3678, loss = 0.09217299520969391
In grad_steps = 3679, loss = 0.1070995107293129
In grad_steps = 3680, loss = 0.04821540787816048
In grad_steps = 3681, loss = 1.2333194017410278
In grad_steps = 3682, loss = 0.0056807915680110455
In grad_steps = 3683, loss = 0.007935351692140102
In grad_steps = 3684, loss = 0.012178704142570496
In grad_steps = 3685, loss = 0.1196567714214325
In grad_steps = 3686, loss = 0.020989200100302696
In grad_steps = 3687, loss = 0.027766603976488113
In grad_steps = 3688, loss = 0.029878096655011177
In grad_steps = 3689, loss = 0.005640607792884111
In grad_steps = 3690, loss = 0.6319851875305176
In grad_steps = 3691, loss = 1.6021778583526611
In grad_steps = 3692, loss = 0.9237433671951294
In grad_steps = 3693, loss = 0.4877942204475403
In grad_steps = 3694, loss = 0.07345613092184067
In grad_steps = 3695, loss = 0.05477077513933182
In grad_steps = 3696, loss = 0.0703117847442627
In grad_steps = 3697, loss = 0.6528750061988831
In grad_steps = 3698, loss = 0.04807810112833977
In grad_steps = 3699, loss = 0.09785924851894379
In grad_steps = 3700, loss = 0.166794553399086
In grad_steps = 3701, loss = 0.15315961837768555
In grad_steps = 3702, loss = 0.10910429060459137
In grad_steps = 3703, loss = 0.06472232937812805
In grad_steps = 3704, loss = 0.09236419200897217
In grad_steps = 3705, loss = 0.1921662986278534
In grad_steps = 3706, loss = 0.07409682869911194
In grad_steps = 3707, loss = 0.3333313465118408
In grad_steps = 3708, loss = 0.08973108977079391
In grad_steps = 3709, loss = 0.05331778526306152
In grad_steps = 3710, loss = 0.18183369934558868
In grad_steps = 3711, loss = 0.41644763946533203
In grad_steps = 3712, loss = 0.030245184898376465
In grad_steps = 3713, loss = 0.354958176612854
In grad_steps = 3714, loss = 0.06302925199270248
In grad_steps = 3715, loss = 0.01775095984339714
In grad_steps = 3716, loss = 0.09597063064575195
In grad_steps = 3717, loss = 0.06411051750183105
In grad_steps = 3718, loss = 0.01281146239489317
In grad_steps = 3719, loss = 0.3530215919017792
In grad_steps = 3720, loss = 0.38498303294181824
In grad_steps = 3721, loss = 0.16690780222415924
In grad_steps = 3722, loss = 0.050756778568029404
In grad_steps = 3723, loss = 0.10908887535333633
In grad_steps = 3724, loss = 0.16498643159866333
In grad_steps = 3725, loss = 1.3236174583435059
In grad_steps = 3726, loss = 0.019945476204156876
In grad_steps = 3727, loss = 0.005045338533818722
In grad_steps = 3728, loss = 0.03010917268693447
In grad_steps = 3729, loss = 0.007637771777808666
In grad_steps = 3730, loss = 1.0124293565750122
In grad_steps = 3731, loss = 0.01482518669217825
In grad_steps = 3732, loss = 1.296412467956543
In grad_steps = 3733, loss = 0.05195091292262077
In grad_steps = 3734, loss = 0.08894817531108856
In grad_steps = 3735, loss = 0.26515334844589233
In grad_steps = 3736, loss = 0.8513181209564209
In grad_steps = 3737, loss = 0.01162826456129551
In grad_steps = 3738, loss = 0.07856537401676178
In grad_steps = 3739, loss = 0.07304735481739044
In grad_steps = 3740, loss = 0.10761944949626923
In grad_steps = 3741, loss = 0.4723684787750244
In grad_steps = 3742, loss = 0.05226835981011391
In grad_steps = 3743, loss = 0.049211807548999786
In grad_steps = 3744, loss = 0.06989038735628128
In grad_steps = 3745, loss = 0.016441097483038902
In grad_steps = 3746, loss = 0.27729111909866333
In grad_steps = 3747, loss = 0.05776088684797287
In grad_steps = 3748, loss = 0.3109522759914398
In grad_steps = 3749, loss = 0.2584068477153778
In grad_steps = 3750, loss = 0.8446186780929565
In grad_steps = 3751, loss = 0.0859530046582222
In grad_steps = 3752, loss = 0.6256261467933655
In grad_steps = 3753, loss = 0.3385685384273529
In grad_steps = 3754, loss = 0.15911352634429932
In grad_steps = 3755, loss = 0.04924273490905762
In grad_steps = 3756, loss = 0.06305961310863495
In grad_steps = 3757, loss = 0.10010810941457748
In grad_steps = 3758, loss = 0.05526578798890114
In grad_steps = 3759, loss = 0.11052766442298889
In grad_steps = 3760, loss = 0.26351872086524963
In grad_steps = 3761, loss = 1.0101814270019531
In grad_steps = 3762, loss = 0.07752569019794464
In grad_steps = 3763, loss = 0.06276342272758484
In grad_steps = 3764, loss = 0.03617125749588013
In grad_steps = 3765, loss = 0.17938853800296783
In grad_steps = 3766, loss = 0.23817948997020721
In grad_steps = 3767, loss = 0.03662162646651268
In grad_steps = 3768, loss = 0.039545938372612
In grad_steps = 3769, loss = 0.1445225179195404
In grad_steps = 3770, loss = 0.11274420469999313
In grad_steps = 3771, loss = 0.0340581089258194
In grad_steps = 3772, loss = 0.04663540795445442
In grad_steps = 3773, loss = 0.02819996140897274
In grad_steps = 3774, loss = 0.10890627652406693
In grad_steps = 3775, loss = 0.2574117183685303
In grad_steps = 3776, loss = 0.03411617875099182
In grad_steps = 3777, loss = 0.19309833645820618
In grad_steps = 3778, loss = 0.11187082529067993
In grad_steps = 3779, loss = 0.031864337623119354
In grad_steps = 3780, loss = 0.02980911359190941
In grad_steps = 3781, loss = 0.020318392664194107
In grad_steps = 3782, loss = 0.3799484968185425
In grad_steps = 3783, loss = 1.2442818880081177
In grad_steps = 3784, loss = 0.02627955749630928
In grad_steps = 3785, loss = 0.023868348449468613
In grad_steps = 3786, loss = 0.8352721929550171
In grad_steps = 3787, loss = 0.07846686244010925
In grad_steps = 3788, loss = 0.022979803383350372
In grad_steps = 3789, loss = 1.6548216342926025
In grad_steps = 3790, loss = 0.4035879075527191
In grad_steps = 3791, loss = 0.02047090232372284
In grad_steps = 3792, loss = 0.041035719215869904
In grad_steps = 3793, loss = 0.5454214811325073
In grad_steps = 3794, loss = 0.035643626004457474
In grad_steps = 3795, loss = 0.02434539422392845
In grad_steps = 3796, loss = 0.22711408138275146
In grad_steps = 3797, loss = 0.5520702004432678
In grad_steps = 3798, loss = 0.21081052720546722
In grad_steps = 3799, loss = 0.01956603117287159
In grad_steps = 3800, loss = 0.5963448882102966
In grad_steps = 3801, loss = 0.5097039937973022
In grad_steps = 3802, loss = 0.36246874928474426
In grad_steps = 3803, loss = 0.08682417124509811
In grad_steps = 3804, loss = 0.14279165863990784
In grad_steps = 3805, loss = 0.08331121504306793
In grad_steps = 3806, loss = 0.024829860776662827
In grad_steps = 3807, loss = 0.37093082070350647
In grad_steps = 3808, loss = 0.1346672773361206
In grad_steps = 3809, loss = 0.2447727769613266
In grad_steps = 3810, loss = 0.7587634921073914
In grad_steps = 3811, loss = 0.08177577704191208
In grad_steps = 3812, loss = 0.2821912169456482
In grad_steps = 3813, loss = 0.017425615340471268
In grad_steps = 3814, loss = 0.2171381711959839
In grad_steps = 3815, loss = 0.7213188409805298
In grad_steps = 3816, loss = 0.3551867604255676
In grad_steps = 3817, loss = 0.1110663041472435
In grad_steps = 3818, loss = 0.8720951080322266
In grad_steps = 3819, loss = 0.3137863576412201
In grad_steps = 3820, loss = 0.11706531792879105
In grad_steps = 3821, loss = 0.055647995322942734
In grad_steps = 3822, loss = 0.5845474004745483
In grad_steps = 3823, loss = 0.026688609272241592
In grad_steps = 3824, loss = 0.10592371225357056
In grad_steps = 3825, loss = 0.32489633560180664
In grad_steps = 3826, loss = 0.05056636780500412
In grad_steps = 3827, loss = 0.4680509865283966
In grad_steps = 3828, loss = 0.7928351759910583
In grad_steps = 3829, loss = 0.25882095098495483
In grad_steps = 3830, loss = 0.12304067611694336
In grad_steps = 3831, loss = 1.1400364637374878
In grad_steps = 3832, loss = 0.23389938473701477
In grad_steps = 3833, loss = 0.3334193527698517
In grad_steps = 3834, loss = 0.15207751095294952
In grad_steps = 3835, loss = 0.03404713794589043
In grad_steps = 3836, loss = 0.09786088764667511
In grad_steps = 3837, loss = 0.14610719680786133
In grad_steps = 3838, loss = 0.03236439451575279
In grad_steps = 3839, loss = 0.17431530356407166
In grad_steps = 3840, loss = 0.7065356969833374
In grad_steps = 3841, loss = 0.08741331100463867
In grad_steps = 3842, loss = 0.047437217086553574
In grad_steps = 3843, loss = 0.41930651664733887
In grad_steps = 3844, loss = 0.07683233916759491
In grad_steps = 3845, loss = 0.22734883427619934
In grad_steps = 3846, loss = 0.1731487214565277
In grad_steps = 3847, loss = 0.2507835030555725
In grad_steps = 3848, loss = 0.059953585267066956
In grad_steps = 3849, loss = 0.5206214785575867
In grad_steps = 3850, loss = 0.07430413365364075
In grad_steps = 3851, loss = 0.1661301553249359
In grad_steps = 3852, loss = 0.06825457513332367
In grad_steps = 3853, loss = 0.04658576473593712
In grad_steps = 3854, loss = 0.4284198582172394
In grad_steps = 3855, loss = 0.019240252673625946
In grad_steps = 3856, loss = 0.7757670283317566
In grad_steps = 3857, loss = 0.4241255223751068
In grad_steps = 3858, loss = 0.044105783104896545
In grad_steps = 3859, loss = 0.015551204793155193
In grad_steps = 3860, loss = 0.22386950254440308
In grad_steps = 3861, loss = 0.4447386860847473
In grad_steps = 3862, loss = 0.0641130581498146
In grad_steps = 3863, loss = 0.2100805640220642
In grad_steps = 3864, loss = 0.42870110273361206
In grad_steps = 3865, loss = 0.07452806830406189
In grad_steps = 3866, loss = 1.1506035327911377
In grad_steps = 3867, loss = 0.12981727719306946
In grad_steps = 3868, loss = 0.11064904183149338
In grad_steps = 3869, loss = 0.10423492640256882
In grad_steps = 3870, loss = 0.118526890873909
In grad_steps = 3871, loss = 0.12218338251113892
In grad_steps = 3872, loss = 0.7517210245132446
In grad_steps = 3873, loss = 0.024240130558609962
In grad_steps = 3874, loss = 0.4518042206764221
In grad_steps = 3875, loss = 0.06900240480899811
In grad_steps = 3876, loss = 0.09987170249223709
In grad_steps = 3877, loss = 0.09600536525249481
In grad_steps = 3878, loss = 0.47455263137817383
In grad_steps = 3879, loss = 0.028493955731391907
In grad_steps = 3880, loss = 0.02367246337234974
In grad_steps = 3881, loss = 0.04658423736691475
In grad_steps = 3882, loss = 0.0863509476184845
In grad_steps = 3883, loss = 0.037315089255571365
In grad_steps = 3884, loss = 0.08508659899234772
In grad_steps = 3885, loss = 0.1123003214597702
In grad_steps = 3886, loss = 0.15941022336483002
In grad_steps = 3887, loss = 0.016637306660413742
In grad_steps = 3888, loss = 0.013218281790614128
In grad_steps = 3889, loss = 0.4433317184448242
In grad_steps = 3890, loss = 0.5288713574409485
In grad_steps = 3891, loss = 0.20741820335388184
In grad_steps = 3892, loss = 0.01521841436624527
In grad_steps = 3893, loss = 0.011804942972958088
In grad_steps = 3894, loss = 0.054204847663640976
In grad_steps = 3895, loss = 0.014007141813635826
In grad_steps = 3896, loss = 0.22975392639636993
In grad_steps = 3897, loss = 0.06789529323577881
In grad_steps = 3898, loss = 0.0026644833851605654
In grad_steps = 3899, loss = 0.6382741332054138
In grad_steps = 3900, loss = 0.006837966851890087
In grad_steps = 3901, loss = 0.04618677496910095
In grad_steps = 3902, loss = 0.031591206789016724
In grad_steps = 3903, loss = 0.1416478008031845
In grad_steps = 3904, loss = 0.042258892208337784
In grad_steps = 3905, loss = 0.11983971297740936
In grad_steps = 3906, loss = 0.3766067326068878
In grad_steps = 3907, loss = 0.023163825273513794
In grad_steps = 3908, loss = 0.013310997746884823
In grad_steps = 3909, loss = 0.007996786385774612
In grad_steps = 3910, loss = 1.3692028522491455
In grad_steps = 3911, loss = 0.7006621956825256
In grad_steps = 3912, loss = 0.7228317260742188
In grad_steps = 3913, loss = 0.41276660561561584
In grad_steps = 3914, loss = 0.38285359740257263
In grad_steps = 3915, loss = 0.030794335529208183
In grad_steps = 3916, loss = 0.05196227878332138
In grad_steps = 3917, loss = 0.0399886853992939
In grad_steps = 3918, loss = 0.7444329857826233
In grad_steps = 3919, loss = 0.8620092868804932
In grad_steps = 3920, loss = 0.052235718816518784
In grad_steps = 3921, loss = 0.08740495145320892
In grad_steps = 3922, loss = 0.08770203590393066
In grad_steps = 3923, loss = 0.44886860251426697
In grad_steps = 3924, loss = 0.07095865905284882
In grad_steps = 3925, loss = 0.43286123871803284
In grad_steps = 3926, loss = 0.1549597680568695
In grad_steps = 3927, loss = 0.2602939307689667
In grad_steps = 3928, loss = 0.10590296238660812
In grad_steps = 3929, loss = 0.11331089586019516
In grad_steps = 3930, loss = 0.43372786045074463
In grad_steps = 3931, loss = 0.8564047813415527
In grad_steps = 3932, loss = 0.07206372916698456
In grad_steps = 3933, loss = 0.0501224622130394
In grad_steps = 3934, loss = 0.7036850452423096
In grad_steps = 3935, loss = 0.08701147139072418
In grad_steps = 3936, loss = 0.2838597893714905
In grad_steps = 3937, loss = 0.1631602793931961
In grad_steps = 3938, loss = 0.8840838670730591
In grad_steps = 3939, loss = 0.20599302649497986
In grad_steps = 3940, loss = 0.15395304560661316
In grad_steps = 3941, loss = 0.27693432569503784
In grad_steps = 3942, loss = 0.024459119886159897
In grad_steps = 3943, loss = 0.22110088169574738
In grad_steps = 3944, loss = 0.05866019427776337
In grad_steps = 3945, loss = 0.5705545544624329
In grad_steps = 3946, loss = 0.047582440078258514
In grad_steps = 3947, loss = 0.039130449295043945
In grad_steps = 3948, loss = 0.310759961605072
In grad_steps = 3949, loss = 0.3353065848350525
In grad_steps = 3950, loss = 0.08217784762382507
In grad_steps = 3951, loss = 0.23332273960113525
In grad_steps = 3952, loss = 0.30484065413475037
In grad_steps = 3953, loss = 0.45697182416915894
In grad_steps = 3954, loss = 0.2514725923538208
In grad_steps = 3955, loss = 0.5958976149559021
In grad_steps = 3956, loss = 0.08434977382421494
In grad_steps = 3957, loss = 0.10051332414150238
In grad_steps = 3958, loss = 0.09775568544864655
In grad_steps = 3959, loss = 0.49165910482406616
In grad_steps = 3960, loss = 0.05057081952691078
In grad_steps = 3961, loss = 0.057355307042598724
In grad_steps = 3962, loss = 0.21215111017227173
In grad_steps = 3963, loss = 0.07039567083120346
In grad_steps = 3964, loss = 0.1036209836602211
In grad_steps = 3965, loss = 0.35692092776298523
In grad_steps = 3966, loss = 0.06356493383646011
In grad_steps = 3967, loss = 0.2654973566532135
In grad_steps = 3968, loss = 0.011162126436829567
In grad_steps = 3969, loss = 0.6297445297241211
In grad_steps = 3970, loss = 0.007542054634541273
In grad_steps = 3971, loss = 0.9693743586540222
In grad_steps = 3972, loss = 0.7135967016220093
In grad_steps = 3973, loss = 0.01626766473054886
In grad_steps = 3974, loss = 0.3515520691871643
In grad_steps = 3975, loss = 0.28163570165634155
In grad_steps = 3976, loss = 0.03586602210998535
In grad_steps = 3977, loss = 0.08999485522508621
In grad_steps = 3978, loss = 0.09475047886371613
In grad_steps = 3979, loss = 0.07574135065078735
In grad_steps = 3980, loss = 0.1156817376613617
In grad_steps = 3981, loss = 0.041467368602752686
In grad_steps = 3982, loss = 0.04941825196146965
In grad_steps = 3983, loss = 0.750626266002655
In grad_steps = 3984, loss = 0.07155236601829529
In grad_steps = 3985, loss = 0.37596797943115234
In grad_steps = 3986, loss = 0.02220248617231846
In grad_steps = 3987, loss = 0.03867952153086662
In grad_steps = 3988, loss = 0.04633259028196335
In grad_steps = 3989, loss = 0.6034761667251587
In grad_steps = 3990, loss = 0.009397177025675774
In grad_steps = 3991, loss = 0.13293398916721344
In grad_steps = 3992, loss = 0.5099189877510071
In grad_steps = 3993, loss = 0.031536512076854706
In grad_steps = 3994, loss = 0.009885581210255623
In grad_steps = 3995, loss = 0.9729647040367126
In grad_steps = 3996, loss = 0.038561053574085236
In grad_steps = 3997, loss = 0.8134580850601196
In grad_steps = 3998, loss = 0.020632237195968628
In grad_steps = 3999, loss = 0.17820414900779724
In grad_steps = 4000, loss = 0.08542642742395401
In grad_steps = 4001, loss = 0.02522432804107666
In grad_steps = 4002, loss = 0.032243866473436356
In grad_steps = 4003, loss = 0.4954506754875183
In grad_steps = 4004, loss = 0.10714834928512573
In grad_steps = 4005, loss = 0.08105514198541641
In grad_steps = 4006, loss = 0.18930868804454803
In grad_steps = 4007, loss = 0.749751091003418
In grad_steps = 4008, loss = 0.2930246889591217
In grad_steps = 4009, loss = 0.6221646070480347
In grad_steps = 4010, loss = 0.16287609934806824
In grad_steps = 4011, loss = 0.17384004592895508
In grad_steps = 4012, loss = 0.39549699425697327
In grad_steps = 4013, loss = 0.40692242980003357
In grad_steps = 4014, loss = 0.7984884977340698
In grad_steps = 4015, loss = 0.46295595169067383
In grad_steps = 4016, loss = 0.24661312997341156
In grad_steps = 4017, loss = 0.10137297213077545
In grad_steps = 4018, loss = 0.5788850784301758
In grad_steps = 4019, loss = 0.01628248021006584
In grad_steps = 4020, loss = 0.0901569202542305
In grad_steps = 4021, loss = 0.7549190521240234
In grad_steps = 4022, loss = 0.17585882544517517
In grad_steps = 4023, loss = 0.07366427034139633
In grad_steps = 4024, loss = 0.1795816272497177
In grad_steps = 4025, loss = 0.6667802929878235
In grad_steps = 4026, loss = 0.10044265538454056
In grad_steps = 4027, loss = 0.05779578536748886
In grad_steps = 4028, loss = 0.7861347198486328
In grad_steps = 4029, loss = 0.39745959639549255
In grad_steps = 4030, loss = 0.3135995864868164
In grad_steps = 4031, loss = 0.12653067708015442
In grad_steps = 4032, loss = 0.7506754398345947
In grad_steps = 4033, loss = 0.2736346125602722
In grad_steps = 4034, loss = 0.32737264037132263
In grad_steps = 4035, loss = 0.1640419065952301
In grad_steps = 4036, loss = 0.18786808848381042
In grad_steps = 4037, loss = 0.4518143832683563
In grad_steps = 4038, loss = 0.08015953749418259
In grad_steps = 4039, loss = 0.3937756419181824
In grad_steps = 4040, loss = 0.1347792148590088
In grad_steps = 4041, loss = 0.16203677654266357
In grad_steps = 4042, loss = 0.13147559762001038
In grad_steps = 4043, loss = 0.14340130984783173
In grad_steps = 4044, loss = 0.06017208844423294
In grad_steps = 4045, loss = 0.14391784369945526
In grad_steps = 4046, loss = 0.17310962080955505
In grad_steps = 4047, loss = 0.11693655699491501
In grad_steps = 4048, loss = 0.12583473324775696
In grad_steps = 4049, loss = 0.19785286486148834
In grad_steps = 4050, loss = 0.0940183624625206
In grad_steps = 4051, loss = 0.018357697874307632
In grad_steps = 4052, loss = 0.10703578591346741
In grad_steps = 4053, loss = 0.061445921659469604
In grad_steps = 4054, loss = 0.5990734696388245
In grad_steps = 4055, loss = 0.007709368132054806
In grad_steps = 4056, loss = 0.4403858780860901
In grad_steps = 4057, loss = 0.29005399346351624
In grad_steps = 4058, loss = 0.02601611614227295
In grad_steps = 4059, loss = 0.6079200506210327
In grad_steps = 4060, loss = 0.09411685168743134
In grad_steps = 4061, loss = 0.14175423979759216
In grad_steps = 4062, loss = 0.048451751470565796
In grad_steps = 4063, loss = 0.026756128296256065
In grad_steps = 4064, loss = 0.014869890175759792
In grad_steps = 4065, loss = 0.05557782202959061
In grad_steps = 4066, loss = 0.873843252658844
In grad_steps = 4067, loss = 0.008879134431481361
In grad_steps = 4068, loss = 0.16220596432685852
In grad_steps = 4069, loss = 0.22907952964305878
In grad_steps = 4070, loss = 0.5460602641105652
In grad_steps = 4071, loss = 0.008322272449731827
In grad_steps = 4072, loss = 0.04364851117134094
In grad_steps = 4073, loss = 0.08915391564369202
In grad_steps = 4074, loss = 0.04714726656675339
In grad_steps = 4075, loss = 0.30675122141838074
In grad_steps = 4076, loss = 0.047330670058727264
In grad_steps = 4077, loss = 0.30432069301605225
In grad_steps = 4078, loss = 0.039216022938489914
In grad_steps = 4079, loss = 0.26607218384742737
In grad_steps = 4080, loss = 0.22701261937618256
In grad_steps = 4081, loss = 0.06866329908370972
In grad_steps = 4082, loss = 0.6711999177932739
In grad_steps = 4083, loss = 0.053244899958372116
In grad_steps = 4084, loss = 0.009976271539926529
In grad_steps = 4085, loss = 0.01423814706504345
In grad_steps = 4086, loss = 0.007750860881060362
In grad_steps = 4087, loss = 0.0856541097164154
In grad_steps = 4088, loss = 0.02854028157889843
In grad_steps = 4089, loss = 0.27366307377815247
In grad_steps = 4090, loss = 1.0298118591308594
In grad_steps = 4091, loss = 0.06412900984287262
In grad_steps = 4092, loss = 0.021703729405999184
In grad_steps = 4093, loss = 0.016322022303938866
In grad_steps = 4094, loss = 0.09564239531755447
In grad_steps = 4095, loss = 0.09780897945165634
In grad_steps = 4096, loss = 0.010033905506134033
In grad_steps = 4097, loss = 0.12072630971670151
In grad_steps = 4098, loss = 0.3562397062778473
In grad_steps = 4099, loss = 0.22514177858829498
In grad_steps = 4100, loss = 0.018684934824705124
In grad_steps = 4101, loss = 0.40342098474502563
In grad_steps = 4102, loss = 0.005905511323362589
In grad_steps = 4103, loss = 0.7240855097770691
In grad_steps = 4104, loss = 0.11720045655965805
In grad_steps = 4105, loss = 0.03364922106266022
In grad_steps = 4106, loss = 0.2340477555990219
In grad_steps = 4107, loss = 0.05014406889677048
In grad_steps = 4108, loss = 0.017190128564834595
In grad_steps = 4109, loss = 0.009281385689973831
In grad_steps = 4110, loss = 1.0117862224578857
In grad_steps = 4111, loss = 0.05107564106583595
In grad_steps = 4112, loss = 0.06858254969120026
In grad_steps = 4113, loss = 0.0785733312368393
In grad_steps = 4114, loss = 0.07041923701763153
In grad_steps = 4115, loss = 0.12539708614349365
In grad_steps = 4116, loss = 0.030969232320785522
In grad_steps = 4117, loss = 0.024339528754353523
In grad_steps = 4118, loss = 0.23120497167110443
In grad_steps = 4119, loss = 0.0018949068617075682
In grad_steps = 4120, loss = 0.025104278698563576
In grad_steps = 4121, loss = 0.023075086995959282
In grad_steps = 4122, loss = 0.6540567874908447
In grad_steps = 4123, loss = 0.8632919192314148
In grad_steps = 4124, loss = 0.16353093087673187
In grad_steps = 4125, loss = 0.028122475370764732
In grad_steps = 4126, loss = 0.016351640224456787
In grad_steps = 4127, loss = 0.03798980638384819
In grad_steps = 4128, loss = 0.07111474871635437
In grad_steps = 4129, loss = 0.42886340618133545
In grad_steps = 4130, loss = 0.022717004641890526
In grad_steps = 4131, loss = 0.016966115683317184
In grad_steps = 4132, loss = 0.0359952338039875
In grad_steps = 4133, loss = 0.006596132181584835
In grad_steps = 4134, loss = 0.032130587846040726
In grad_steps = 4135, loss = 0.01668565347790718
In grad_steps = 4136, loss = 0.7436890006065369
In grad_steps = 4137, loss = 0.6821632385253906
In grad_steps = 4138, loss = 0.021528074517846107
In grad_steps = 4139, loss = 0.01209240686148405
In grad_steps = 4140, loss = 0.09588900953531265
In grad_steps = 4141, loss = 0.08629514276981354
In grad_steps = 4142, loss = 0.16559424996376038
In grad_steps = 4143, loss = 0.3106931447982788
In grad_steps = 4144, loss = 0.32971814274787903
In grad_steps = 4145, loss = 0.005359177477657795
In grad_steps = 4146, loss = 0.03158961236476898
In grad_steps = 4147, loss = 0.062220048159360886
In grad_steps = 4148, loss = 0.020503699779510498
In grad_steps = 4149, loss = 0.026479700580239296
In grad_steps = 4150, loss = 0.33858606219291687
In grad_steps = 4151, loss = 0.012683807872235775
In grad_steps = 4152, loss = 1.3593475818634033
In grad_steps = 4153, loss = 0.6195991635322571
In grad_steps = 4154, loss = 0.0601276196539402
In grad_steps = 4155, loss = 0.8580656051635742
In grad_steps = 4156, loss = 0.030548440292477608
In grad_steps = 4157, loss = 0.09435078501701355
In grad_steps = 4158, loss = 0.2554377615451813
In grad_steps = 4159, loss = 0.07959729433059692
In grad_steps = 4160, loss = 0.02620132640004158
In grad_steps = 4161, loss = 0.5070998072624207
In grad_steps = 4162, loss = 0.06271253526210785
In grad_steps = 4163, loss = 0.016840294003486633
In grad_steps = 4164, loss = 0.014166970737278461
In grad_steps = 4165, loss = 0.06507296860218048
In grad_steps = 4166, loss = 0.15272828936576843
In grad_steps = 4167, loss = 0.02544046752154827
In grad_steps = 4168, loss = 0.36818253993988037
In grad_steps = 4169, loss = 0.01995687186717987
In grad_steps = 4170, loss = 0.291765958070755
In grad_steps = 4171, loss = 0.06908653676509857
In grad_steps = 4172, loss = 0.5245422124862671
In grad_steps = 4173, loss = 1.1380078792572021
In grad_steps = 4174, loss = 0.3334057331085205
In grad_steps = 4175, loss = 0.021352536976337433
In grad_steps = 4176, loss = 1.2441866397857666
In grad_steps = 4177, loss = 0.10566283762454987
In grad_steps = 4178, loss = 0.019558802247047424
In grad_steps = 4179, loss = 0.8712385892868042
In grad_steps = 4180, loss = 0.44640859961509705
In grad_steps = 4181, loss = 0.7369379997253418
In grad_steps = 4182, loss = 0.14856573939323425
In grad_steps = 4183, loss = 0.04246623441576958
In grad_steps = 4184, loss = 0.16462048888206482
In grad_steps = 4185, loss = 0.8289628028869629
In grad_steps = 4186, loss = 0.8629042506217957
In grad_steps = 4187, loss = 0.064796581864357
In grad_steps = 4188, loss = 0.10444816946983337
In grad_steps = 4189, loss = 0.11791731417179108
In grad_steps = 4190, loss = 0.532363772392273
In grad_steps = 4191, loss = 0.4252949059009552
In grad_steps = 4192, loss = 0.41880014538764954
In grad_steps = 4193, loss = 0.08911186456680298
In grad_steps = 4194, loss = 0.20130778849124908
In grad_steps = 4195, loss = 0.10975825786590576
In grad_steps = 4196, loss = 0.17946571111679077
In grad_steps = 4197, loss = 0.3392542898654938
In grad_steps = 4198, loss = 0.12448577582836151
In grad_steps = 4199, loss = 0.11085422337055206
In grad_steps = 4200, loss = 0.378953754901886
In grad_steps = 4201, loss = 0.06657111644744873
In grad_steps = 4202, loss = 0.10546544194221497
In grad_steps = 4203, loss = 0.16120441257953644
In grad_steps = 4204, loss = 0.08366312831640244
In grad_steps = 4205, loss = 0.07625240832567215
In grad_steps = 4206, loss = 0.1866694986820221
In grad_steps = 4207, loss = 0.1720573455095291
In grad_steps = 4208, loss = 0.022237660363316536
In grad_steps = 4209, loss = 0.06755447387695312
In grad_steps = 4210, loss = 0.33057740330696106
In grad_steps = 4211, loss = 0.03522448241710663
In grad_steps = 4212, loss = 0.237953782081604
In grad_steps = 4213, loss = 0.0356089249253273
In grad_steps = 4214, loss = 0.023824961856007576
In grad_steps = 4215, loss = 0.03393134847283363
In grad_steps = 4216, loss = 0.13242986798286438
In grad_steps = 4217, loss = 0.05341485142707825
In grad_steps = 4218, loss = 0.12492845207452774
In grad_steps = 4219, loss = 0.04205670952796936
In grad_steps = 4220, loss = 0.7095922827720642
In grad_steps = 4221, loss = 0.01102972961962223
In grad_steps = 4222, loss = 0.011934086680412292
In grad_steps = 4223, loss = 0.009666324593126774
In grad_steps = 4224, loss = 0.0228964164853096
In grad_steps = 4225, loss = 0.26095694303512573
In grad_steps = 4226, loss = 0.010195563547313213
In grad_steps = 4227, loss = 0.024022463709115982
In grad_steps = 4228, loss = 0.027582241222262383
In grad_steps = 4229, loss = 0.007004308048635721
In grad_steps = 4230, loss = 0.7482653856277466
In grad_steps = 4231, loss = 0.014490670524537563
In grad_steps = 4232, loss = 0.727255642414093
In grad_steps = 4233, loss = 0.010637590661644936
In grad_steps = 4234, loss = 0.0326116606593132
In grad_steps = 4235, loss = 0.029801111668348312
In grad_steps = 4236, loss = 0.06699560582637787
In grad_steps = 4237, loss = 0.016205187886953354
In grad_steps = 4238, loss = 0.2786791920661926
In grad_steps = 4239, loss = 0.2477351874113083
In grad_steps = 4240, loss = 0.06307631731033325
In grad_steps = 4241, loss = 0.2639274001121521
In grad_steps = 4242, loss = 0.0348099023103714
In grad_steps = 4243, loss = 0.11765746027231216
In grad_steps = 4244, loss = 0.0031707691960036755
In grad_steps = 4245, loss = 0.03632846474647522
In grad_steps = 4246, loss = 0.01685662753880024
In grad_steps = 4247, loss = 0.03251679614186287
In grad_steps = 4248, loss = 0.30463722348213196
In grad_steps = 4249, loss = 0.1357835829257965
In grad_steps = 4250, loss = 0.007901025004684925
In grad_steps = 4251, loss = 0.009193573147058487
In grad_steps = 4252, loss = 0.09853994101285934
In grad_steps = 4253, loss = 0.04396430775523186
In grad_steps = 4254, loss = 0.0440417043864727
In grad_steps = 4255, loss = 0.029259340837597847
In grad_steps = 4256, loss = 0.41114163398742676
In grad_steps = 4257, loss = 0.9584639668464661
In grad_steps = 4258, loss = 0.08778741210699081
In grad_steps = 4259, loss = 0.31771335005760193
In grad_steps = 4260, loss = 0.6375959515571594
In grad_steps = 4261, loss = 0.2329549938440323
In grad_steps = 4262, loss = 0.39278411865234375
In grad_steps = 4263, loss = 0.599897027015686
In grad_steps = 4264, loss = 0.14788459241390228
In grad_steps = 4265, loss = 0.02796364761888981
In grad_steps = 4266, loss = 0.01701732724905014
In grad_steps = 4267, loss = 0.010318191722035408
In grad_steps = 4268, loss = 0.7044851779937744
In grad_steps = 4269, loss = 0.7405251264572144
In grad_steps = 4270, loss = 0.24364736676216125
In grad_steps = 4271, loss = 0.1038895696401596
In grad_steps = 4272, loss = 0.44586077332496643
In grad_steps = 4273, loss = 0.2571577727794647
In grad_steps = 4274, loss = 0.16867029666900635
In grad_steps = 4275, loss = 0.21129977703094482
In grad_steps = 4276, loss = 0.03256870433688164
In grad_steps = 4277, loss = 0.3763673007488251
In grad_steps = 4278, loss = 0.4657014310359955
In grad_steps = 4279, loss = 0.3577243387699127
In grad_steps = 4280, loss = 0.1111023873090744
In grad_steps = 4281, loss = 1.3787214756011963
In grad_steps = 4282, loss = 0.6964232325553894
In grad_steps = 4283, loss = 0.4301660656929016
In grad_steps = 4284, loss = 0.8411339521408081
In grad_steps = 4285, loss = 0.17718446254730225
In grad_steps = 4286, loss = 0.18238964676856995
In grad_steps = 4287, loss = 0.021202893927693367
In grad_steps = 4288, loss = 0.10944648087024689
In grad_steps = 4289, loss = 0.1230858713388443
In grad_steps = 4290, loss = 0.1424688994884491
In grad_steps = 4291, loss = 0.11920253932476044
In grad_steps = 4292, loss = 0.47626733779907227
In grad_steps = 4293, loss = 0.06378936767578125
In grad_steps = 4294, loss = 0.04954642057418823
In grad_steps = 4295, loss = 0.09098443388938904
In grad_steps = 4296, loss = 1.035239577293396
In grad_steps = 4297, loss = 0.06720384955406189
In grad_steps = 4298, loss = 0.22697396576404572
In grad_steps = 4299, loss = 0.11317325383424759
In grad_steps = 4300, loss = 0.1686406135559082
In grad_steps = 4301, loss = 0.08109766989946365
In grad_steps = 4302, loss = 0.10357065498828888
In grad_steps = 4303, loss = 0.3628864884376526
In grad_steps = 4304, loss = 0.6490322947502136
In grad_steps = 4305, loss = 0.03712926805019379
In grad_steps = 4306, loss = 0.1349196881055832
In grad_steps = 4307, loss = 0.3117380142211914
In grad_steps = 4308, loss = 0.055846262723207474
In grad_steps = 4309, loss = 0.019732780754566193
In grad_steps = 4310, loss = 0.11838100105524063
In grad_steps = 4311, loss = 0.20682808756828308
In grad_steps = 4312, loss = 0.05344904586672783
In grad_steps = 4313, loss = 0.04139716178178787
In grad_steps = 4314, loss = 0.29423201084136963
In grad_steps = 4315, loss = 0.21485932171344757
In grad_steps = 4316, loss = 1.3073101043701172
In grad_steps = 4317, loss = 0.5144046545028687
In grad_steps = 4318, loss = 0.04245613142848015
In grad_steps = 4319, loss = 1.6136128902435303
In grad_steps = 4320, loss = 0.03457782045006752
In grad_steps = 4321, loss = 0.0457242876291275
In grad_steps = 4322, loss = 0.12875282764434814
In grad_steps = 4323, loss = 0.31358110904693604
In grad_steps = 4324, loss = 0.6963887810707092
In grad_steps = 4325, loss = 0.5206340551376343
In grad_steps = 4326, loss = 0.16053150594234467
In grad_steps = 4327, loss = 0.1758265495300293
In grad_steps = 4328, loss = 0.14004528522491455
In grad_steps = 4329, loss = 0.10320648550987244
In grad_steps = 4330, loss = 0.442844957113266
In grad_steps = 4331, loss = 0.2147793173789978
In grad_steps = 4332, loss = 0.07513384521007538
In grad_steps = 4333, loss = 0.2161148190498352
In grad_steps = 4334, loss = 0.4269523620605469
In grad_steps = 4335, loss = 0.04205464944243431
In grad_steps = 4336, loss = 0.06421181559562683
In grad_steps = 4337, loss = 0.07828287780284882
In grad_steps = 4338, loss = 0.11048644036054611
In grad_steps = 4339, loss = 0.12359154224395752
In grad_steps = 4340, loss = 0.034348927438259125
In grad_steps = 4341, loss = 0.805924654006958
In grad_steps = 4342, loss = 0.19423358142375946
In grad_steps = 4343, loss = 0.14105848968029022
In grad_steps = 4344, loss = 0.02961108833551407
In grad_steps = 4345, loss = 0.19358491897583008
In grad_steps = 4346, loss = 0.09061950445175171
In grad_steps = 4347, loss = 0.7880790829658508
In grad_steps = 4348, loss = 0.37674036622047424
In grad_steps = 4349, loss = 0.03553025424480438
In grad_steps = 4350, loss = 0.13536109030246735
In grad_steps = 4351, loss = 0.10839633643627167
In grad_steps = 4352, loss = 0.04625099152326584
In grad_steps = 4353, loss = 0.058504618704319
In grad_steps = 4354, loss = 0.07228133082389832
In grad_steps = 4355, loss = 0.22153836488723755
In grad_steps = 4356, loss = 0.049874916672706604
In grad_steps = 4357, loss = 0.02981475181877613
In grad_steps = 4358, loss = 0.0291176475584507
In grad_steps = 4359, loss = 0.07437052577733994
In grad_steps = 4360, loss = 0.07789746671915054
In grad_steps = 4361, loss = 0.01332532986998558
In grad_steps = 4362, loss = 0.8346021771430969
In grad_steps = 4363, loss = 0.28003987669944763
In grad_steps = 4364, loss = 0.07645776867866516
In grad_steps = 4365, loss = 0.1042533814907074
In grad_steps = 4366, loss = 0.022465787827968597
In grad_steps = 4367, loss = 0.05181560665369034
In grad_steps = 4368, loss = 0.00642813416197896
In grad_steps = 4369, loss = 0.009950698353350163
In grad_steps = 4370, loss = 0.02155742235481739
In grad_steps = 4371, loss = 0.07490593940019608
In grad_steps = 4372, loss = 0.6534366011619568
In grad_steps = 4373, loss = 0.39873969554901123
In grad_steps = 4374, loss = 0.057897064834833145
In grad_steps = 4375, loss = 1.0328201055526733
In grad_steps = 4376, loss = 0.08581271767616272
In grad_steps = 4377, loss = 0.023039303719997406
In grad_steps = 4378, loss = 0.018072189763188362
In grad_steps = 4379, loss = 0.017590682953596115
In grad_steps = 4380, loss = 0.06847603619098663
In grad_steps = 4381, loss = 0.04211418703198433
In grad_steps = 4382, loss = 0.33543893694877625
In grad_steps = 4383, loss = 0.04396284371614456
In grad_steps = 4384, loss = 0.012960577383637428
In grad_steps = 4385, loss = 0.007958395406603813
In grad_steps = 4386, loss = 0.008862247690558434
In grad_steps = 4387, loss = 0.023505866527557373
In grad_steps = 4388, loss = 0.43894267082214355
In grad_steps = 4389, loss = 0.010831378400325775
In grad_steps = 4390, loss = 0.02543569914996624
In grad_steps = 4391, loss = 0.7750179171562195
In grad_steps = 4392, loss = 1.6057941913604736
In grad_steps = 4393, loss = 0.052975915372371674
In grad_steps = 4394, loss = 0.049486514180898666
In grad_steps = 4395, loss = 0.17404797673225403
In grad_steps = 4396, loss = 0.06707826256752014
In grad_steps = 4397, loss = 0.09537173807621002
In grad_steps = 4398, loss = 0.2361319661140442
In grad_steps = 4399, loss = 0.1995629221200943
In grad_steps = 4400, loss = 0.02719304710626602
In grad_steps = 4401, loss = 0.0767545998096466
In grad_steps = 4402, loss = 1.8519362211227417
In grad_steps = 4403, loss = 0.19143739342689514
In grad_steps = 4404, loss = 0.2530937194824219
In grad_steps = 4405, loss = 0.06263162940740585
In grad_steps = 4406, loss = 0.8859057426452637
In grad_steps = 4407, loss = 0.13701115548610687
In grad_steps = 4408, loss = 0.08094100654125214
In grad_steps = 4409, loss = 0.07767707854509354
In grad_steps = 4410, loss = 0.4433167576789856
In grad_steps = 4411, loss = 0.07499399036169052
In grad_steps = 4412, loss = 0.1962885558605194
In grad_steps = 4413, loss = 0.15106073021888733
In grad_steps = 4414, loss = 0.07436016947031021
In grad_steps = 4415, loss = 0.09069566428661346
In grad_steps = 4416, loss = 0.024119827896356583
In grad_steps = 4417, loss = 0.4803786873817444
In grad_steps = 4418, loss = 0.028524912893772125
In grad_steps = 4419, loss = 0.07672154903411865
In grad_steps = 4420, loss = 0.01674199476838112
In grad_steps = 4421, loss = 0.031864624470472336
In grad_steps = 4422, loss = 0.019494226202368736
In grad_steps = 4423, loss = 0.0814327746629715
In grad_steps = 4424, loss = 0.1402294784784317
In grad_steps = 4425, loss = 0.016417233273386955
In grad_steps = 4426, loss = 0.5251301527023315
In grad_steps = 4427, loss = 0.20548254251480103
In grad_steps = 4428, loss = 0.0357198640704155
In grad_steps = 4429, loss = 0.01981271617114544
In grad_steps = 4430, loss = 0.2315773069858551
In grad_steps = 4431, loss = 0.5999041795730591
In grad_steps = 4432, loss = 0.20997244119644165
In grad_steps = 4433, loss = 0.6930965185165405
In grad_steps = 4434, loss = 0.007989834062755108
In grad_steps = 4435, loss = 0.03136096149682999
In grad_steps = 4436, loss = 0.20021769404411316
In grad_steps = 4437, loss = 0.10780565440654755
In grad_steps = 4438, loss = 0.19681863486766815
In grad_steps = 4439, loss = 0.13431163132190704
In grad_steps = 4440, loss = 0.12318068742752075
In grad_steps = 4441, loss = 0.012984205037355423
In grad_steps = 4442, loss = 0.02171105146408081
In grad_steps = 4443, loss = 0.028563061729073524
In grad_steps = 4444, loss = 0.01518649235367775
In grad_steps = 4445, loss = 0.5995689630508423
In grad_steps = 4446, loss = 0.015917416661977768
In grad_steps = 4447, loss = 0.13155120611190796
In grad_steps = 4448, loss = 0.025357907637953758
In grad_steps = 4449, loss = 0.010532710701227188
In grad_steps = 4450, loss = 0.22919604182243347
In grad_steps = 4451, loss = 0.012983039021492004
In grad_steps = 4452, loss = 0.014452101662755013
In grad_steps = 4453, loss = 0.041350219398736954
In grad_steps = 4454, loss = 0.33123600482940674
In grad_steps = 4455, loss = 0.039278559386730194
In grad_steps = 4456, loss = 0.006519234273582697
In grad_steps = 4457, loss = 0.08161826431751251
In grad_steps = 4458, loss = 0.7749367356300354
In grad_steps = 4459, loss = 0.05091675743460655
In grad_steps = 4460, loss = 0.48860713839530945
In grad_steps = 4461, loss = 0.10196324437856674
In grad_steps = 4462, loss = 0.007546633016318083
In grad_steps = 4463, loss = 0.7181345224380493
In grad_steps = 4464, loss = 0.00649745250120759
In grad_steps = 4465, loss = 0.8432194590568542
In grad_steps = 4466, loss = 0.04616018384695053
In grad_steps = 4467, loss = 0.4373493790626526
In grad_steps = 4468, loss = 0.033145722001791
In grad_steps = 4469, loss = 0.033357031643390656
In grad_steps = 4470, loss = 0.24588492512702942
In grad_steps = 4471, loss = 0.304202675819397
In grad_steps = 4472, loss = 0.026168201118707657
In grad_steps = 4473, loss = 0.006798041518777609
In grad_steps = 4474, loss = 0.025011027231812477
In grad_steps = 4475, loss = 0.08332549035549164
In grad_steps = 4476, loss = 0.4358850121498108
In grad_steps = 4477, loss = 0.12752479314804077
In grad_steps = 4478, loss = 0.01732710935175419
In grad_steps = 4479, loss = 1.3287855386734009
In grad_steps = 4480, loss = 0.09678825736045837
In grad_steps = 4481, loss = 0.1401442587375641
In grad_steps = 4482, loss = 0.43837690353393555
In grad_steps = 4483, loss = 0.2976559102535248
In grad_steps = 4484, loss = 0.3006662428379059
In grad_steps = 4485, loss = 0.0976647436618805
In grad_steps = 4486, loss = 0.039427269250154495
In grad_steps = 4487, loss = 0.17825733125209808
In grad_steps = 4488, loss = 0.26343443989753723
In grad_steps = 4489, loss = 0.4655383825302124
In grad_steps = 4490, loss = 0.23282741010189056
In grad_steps = 4491, loss = 0.32745614647865295
In grad_steps = 4492, loss = 0.020760565996170044
In grad_steps = 4493, loss = 0.11707786470651627
In grad_steps = 4494, loss = 0.21541571617126465
In grad_steps = 4495, loss = 0.059965506196022034
In grad_steps = 4496, loss = 0.1991616189479828
In grad_steps = 4497, loss = 0.7469983696937561
In grad_steps = 4498, loss = 0.14898554980754852
In grad_steps = 4499, loss = 0.06461130082607269
In grad_steps = 4500, loss = 0.20673052966594696
In grad_steps = 4501, loss = 0.31428417563438416
In grad_steps = 4502, loss = 0.036396048963069916
In grad_steps = 4503, loss = 0.0678824931383133
In grad_steps = 4504, loss = 0.24543079733848572
In grad_steps = 4505, loss = 0.2006341516971588
In grad_steps = 4506, loss = 0.17148205637931824
In grad_steps = 4507, loss = 0.03744479641318321
In grad_steps = 4508, loss = 0.3131010830402374
In grad_steps = 4509, loss = 0.021185867488384247
In grad_steps = 4510, loss = 0.011099182069301605
In grad_steps = 4511, loss = 0.0614490769803524
In grad_steps = 4512, loss = 1.169676423072815
In grad_steps = 4513, loss = 0.1258542835712433
In grad_steps = 4514, loss = 0.013078187592327595
In grad_steps = 4515, loss = 1.0681592226028442
In grad_steps = 4516, loss = 0.01673395186662674
In grad_steps = 4517, loss = 0.022534310817718506
In grad_steps = 4518, loss = 0.11981196701526642
In grad_steps = 4519, loss = 1.0114258527755737
In grad_steps = 4520, loss = 0.024002425372600555
In grad_steps = 4521, loss = 0.1152227371931076
In grad_steps = 4522, loss = 0.4456532597541809
In grad_steps = 4523, loss = 0.008242042735219002
In grad_steps = 4524, loss = 0.7315983176231384
In grad_steps = 4525, loss = 0.8510300517082214
In grad_steps = 4526, loss = 0.004509676247835159
In grad_steps = 4527, loss = 0.020860791206359863
In grad_steps = 4528, loss = 0.15530449151992798
In grad_steps = 4529, loss = 0.24987190961837769
In grad_steps = 4530, loss = 0.3511364459991455
In grad_steps = 4531, loss = 0.08109956979751587
In grad_steps = 4532, loss = 0.31135472655296326
In grad_steps = 4533, loss = 0.07370505481958389
In grad_steps = 4534, loss = 0.1085410937666893
In grad_steps = 4535, loss = 0.07003215700387955
In grad_steps = 4536, loss = 0.32053470611572266
In grad_steps = 4537, loss = 0.11533363163471222
In grad_steps = 4538, loss = 0.1414799839258194
In grad_steps = 4539, loss = 0.21365906298160553
In grad_steps = 4540, loss = 0.041891906410455704
In grad_steps = 4541, loss = 0.027400938794016838
In grad_steps = 4542, loss = 0.15630555152893066
In grad_steps = 4543, loss = 0.033776961266994476
In grad_steps = 4544, loss = 0.026347622275352478
In grad_steps = 4545, loss = 0.4267388582229614
In grad_steps = 4546, loss = 0.3367662727832794
In grad_steps = 4547, loss = 0.03287374973297119
In grad_steps = 4548, loss = 0.015336807817220688
In grad_steps = 4549, loss = 0.0321568138897419
In grad_steps = 4550, loss = 0.012920958921313286
In grad_steps = 4551, loss = 1.4511443376541138
In grad_steps = 4552, loss = 0.042956795543432236
In grad_steps = 4553, loss = 0.521436333656311
In grad_steps = 4554, loss = 0.037462279200553894
In grad_steps = 4555, loss = 0.2798590660095215
In grad_steps = 4556, loss = 0.109031081199646
In grad_steps = 4557, loss = 0.1156860888004303
In grad_steps = 4558, loss = 0.055854715406894684
In grad_steps = 4559, loss = 0.4969494938850403
In grad_steps = 4560, loss = 0.03271865472197533
In grad_steps = 4561, loss = 0.33012789487838745
In grad_steps = 4562, loss = 0.13108161091804504
In grad_steps = 4563, loss = 0.12235983461141586
In grad_steps = 4564, loss = 0.06727506220340729
In grad_steps = 4565, loss = 0.6084136366844177
In grad_steps = 4566, loss = 0.355852335691452
In grad_steps = 4567, loss = 0.11549167335033417
In grad_steps = 4568, loss = 0.03209041804075241
In grad_steps = 4569, loss = 0.036293163895606995
In grad_steps = 4570, loss = 1.6285325288772583
In grad_steps = 4571, loss = 0.1027052104473114
In grad_steps = 4572, loss = 0.18753600120544434
In grad_steps = 4573, loss = 0.06581238657236099
In grad_steps = 4574, loss = 0.7100555300712585
In grad_steps = 4575, loss = 0.089262455701828
In grad_steps = 4576, loss = 0.09260392934083939
In grad_steps = 4577, loss = 0.021237358450889587
In grad_steps = 4578, loss = 0.24608677625656128
In grad_steps = 4579, loss = 0.09963853657245636
In grad_steps = 4580, loss = 0.6187808513641357
In grad_steps = 4581, loss = 0.034385740756988525
In grad_steps = 4582, loss = 0.14153052866458893
In grad_steps = 4583, loss = 0.02632754296064377
In grad_steps = 4584, loss = 0.023957088589668274
In grad_steps = 4585, loss = 0.04180705174803734
In grad_steps = 4586, loss = 0.021232977509498596
In grad_steps = 4587, loss = 0.8306772708892822
In grad_steps = 4588, loss = 0.02637321688234806
In grad_steps = 4589, loss = 0.04913107305765152
In grad_steps = 4590, loss = 0.3038749694824219
In grad_steps = 4591, loss = 0.09841516613960266
In grad_steps = 4592, loss = 0.09789001941680908
In grad_steps = 4593, loss = 0.07059000432491302
In grad_steps = 4594, loss = 0.04551989212632179
In grad_steps = 4595, loss = 0.4372474253177643
In grad_steps = 4596, loss = 0.026284266263246536
In grad_steps = 4597, loss = 0.1046806126832962
In grad_steps = 4598, loss = 1.0249204635620117
In grad_steps = 4599, loss = 0.22962351143360138
In grad_steps = 4600, loss = 0.1031251773238182
In grad_steps = 4601, loss = 0.7284652590751648
In grad_steps = 4602, loss = 0.11048409342765808
In grad_steps = 4603, loss = 0.03893222659826279
In grad_steps = 4604, loss = 0.09329447895288467
In grad_steps = 4605, loss = 0.5696190595626831
In grad_steps = 4606, loss = 0.037799201905727386
In grad_steps = 4607, loss = 0.12794679403305054
In grad_steps = 4608, loss = 0.19300460815429688
In grad_steps = 4609, loss = 0.624992311000824
In grad_steps = 4610, loss = 0.020617978647351265
In grad_steps = 4611, loss = 0.04243849962949753
In grad_steps = 4612, loss = 0.6334593892097473
In grad_steps = 4613, loss = 0.04375036060810089
In grad_steps = 4614, loss = 0.22408565878868103
In grad_steps = 4615, loss = 0.09635109454393387
In grad_steps = 4616, loss = 0.08679140359163284
In grad_steps = 4617, loss = 0.627752959728241
In grad_steps = 4618, loss = 0.02193586155772209
In grad_steps = 4619, loss = 0.22890114784240723
In grad_steps = 4620, loss = 0.11488372832536697
In grad_steps = 4621, loss = 0.01587732322514057
In grad_steps = 4622, loss = 0.5869700908660889
In grad_steps = 4623, loss = 0.09549162536859512
In grad_steps = 4624, loss = 0.04492496699094772
In grad_steps = 4625, loss = 0.01952485740184784
In grad_steps = 4626, loss = 0.047278694808483124
In grad_steps = 4627, loss = 0.020149366930127144
In grad_steps = 4628, loss = 0.021920230239629745
In grad_steps = 4629, loss = 0.09359218925237656
In grad_steps = 4630, loss = 0.03920942544937134
In grad_steps = 4631, loss = 0.09780867397785187
In grad_steps = 4632, loss = 0.008354210294783115
In grad_steps = 4633, loss = 0.021707233041524887
In grad_steps = 4634, loss = 0.1976637840270996
In grad_steps = 4635, loss = 0.07654544711112976
In grad_steps = 4636, loss = 0.3538877069950104
In grad_steps = 4637, loss = 0.07086023688316345
In grad_steps = 4638, loss = 0.022656209766864777
In grad_steps = 4639, loss = 0.03854221850633621
In grad_steps = 4640, loss = 0.0370018444955349
In grad_steps = 4641, loss = 0.028787437826395035
In grad_steps = 4642, loss = 0.04282623156905174
In grad_steps = 4643, loss = 0.06547480076551437
In grad_steps = 4644, loss = 0.031404003500938416
In grad_steps = 4645, loss = 0.02040431834757328
In grad_steps = 4646, loss = 0.14306312799453735
In grad_steps = 4647, loss = 0.022582145407795906
In grad_steps = 4648, loss = 0.024050505831837654
In grad_steps = 4649, loss = 0.01811295934021473
In grad_steps = 4650, loss = 0.12833760678768158
In grad_steps = 4651, loss = 0.08535046875476837
In grad_steps = 4652, loss = 0.019071025773882866
In grad_steps = 4653, loss = 0.015704015269875526
In grad_steps = 4654, loss = 0.008643826469779015
In grad_steps = 4655, loss = 1.1610655784606934
In grad_steps = 4656, loss = 0.019838910549879074
In grad_steps = 4657, loss = 0.005689315032213926
In grad_steps = 4658, loss = 0.005095246713608503
In grad_steps = 4659, loss = 0.009453266859054565
In grad_steps = 4660, loss = 0.5655991435050964
In grad_steps = 4661, loss = 0.012481266632676125
In grad_steps = 4662, loss = 0.009158151224255562
In grad_steps = 4663, loss = 1.209105134010315
In grad_steps = 4664, loss = 0.3587528467178345
In grad_steps = 4665, loss = 0.3081287145614624
In grad_steps = 4666, loss = 0.3195892870426178
In grad_steps = 4667, loss = 0.04539605230093002
In grad_steps = 4668, loss = 0.07195189595222473
In grad_steps = 4669, loss = 0.07301702350378036
In grad_steps = 4670, loss = 0.047801949083805084
In grad_steps = 4671, loss = 0.5363596081733704
In grad_steps = 4672, loss = 1.846968412399292
In grad_steps = 4673, loss = 0.20840242505073547
In grad_steps = 4674, loss = 0.018482603132724762
In grad_steps = 4675, loss = 0.33600717782974243
In grad_steps = 4676, loss = 0.040474142879247665
In grad_steps = 4677, loss = 0.6122674942016602
In grad_steps = 4678, loss = 0.06432424485683441
In grad_steps = 4679, loss = 0.012883810326457024
In grad_steps = 4680, loss = 0.6097812056541443
In grad_steps = 4681, loss = 0.19375872611999512
In grad_steps = 4682, loss = 0.13106992840766907
In grad_steps = 4683, loss = 0.1685735285282135
In grad_steps = 4684, loss = 0.22474735975265503
In grad_steps = 4685, loss = 0.04315030947327614
In grad_steps = 4686, loss = 0.7502613067626953
In grad_steps = 4687, loss = 0.051042649894952774
In grad_steps = 4688, loss = 0.15125004947185516
In grad_steps = 4689, loss = 0.14413033425807953
In grad_steps = 4690, loss = 0.06583663076162338
In grad_steps = 4691, loss = 0.05456460267305374
In grad_steps = 4692, loss = 0.11436524987220764
In grad_steps = 4693, loss = 0.1835068315267563
In grad_steps = 4694, loss = 0.07182549685239792
In grad_steps = 4695, loss = 0.7675368189811707
In grad_steps = 4696, loss = 0.060934893786907196
In grad_steps = 4697, loss = 0.4318045973777771
In grad_steps = 4698, loss = 0.10810326039791107
In grad_steps = 4699, loss = 0.11715206503868103
In grad_steps = 4700, loss = 0.027576781809329987
In grad_steps = 4701, loss = 0.03292636200785637
In grad_steps = 4702, loss = 0.2135770171880722
In grad_steps = 4703, loss = 0.06527097523212433
In grad_steps = 4704, loss = 0.4418625831604004
In grad_steps = 4705, loss = 0.0025680442340672016
Elapsed time: 2678.240036725998 seconds for ensemble 1 with 2 epochs
LoRA instance 1 evaluation complete. Data saved to /pscratch/sd/t/tianle/lucid/other_source/SURP_2024/results/tmp_lora_ensemble/Llama3-8B-set-4/test_data_instance_1_seed_10094.npz.
lora instance i = 1 Successfully finished.
Training lora instance 2
DEBUG: Total parameters: 8037076992
DEBUG: Trainable parameters: 6815744
Beginning epoch 1
In grad_steps = 0, loss = 0.5826846361160278
In grad_steps = 1, loss = 0.48196113109588623
In grad_steps = 2, loss = 1.7282915115356445
In grad_steps = 3, loss = 0.7388997077941895
In grad_steps = 4, loss = 0.932083249092102
In grad_steps = 5, loss = 0.7281535863876343
In grad_steps = 6, loss = 1.0789097547531128
In grad_steps = 7, loss = 0.8264557123184204
In grad_steps = 8, loss = 0.45475882291793823
In grad_steps = 9, loss = 0.701374888420105
In grad_steps = 10, loss = 0.7737725377082825
In grad_steps = 11, loss = 1.011564016342163
In grad_steps = 12, loss = 1.0187420845031738
In grad_steps = 13, loss = 0.6896610260009766
In grad_steps = 14, loss = 0.6333690285682678
In grad_steps = 15, loss = 0.7550512552261353
In grad_steps = 16, loss = 0.7413047552108765
In grad_steps = 17, loss = 0.5728079080581665
In grad_steps = 18, loss = 1.1379961967468262
In grad_steps = 19, loss = 1.0351791381835938
In grad_steps = 20, loss = 0.9917740821838379
In grad_steps = 21, loss = 0.5884708166122437
In grad_steps = 22, loss = 0.702961266040802
In grad_steps = 23, loss = 0.5790910720825195
In grad_steps = 24, loss = 0.7547868490219116
In grad_steps = 25, loss = 0.6732140183448792
In grad_steps = 26, loss = 0.6546050310134888
In grad_steps = 27, loss = 0.6662196516990662
In grad_steps = 28, loss = 0.7065141201019287
In grad_steps = 29, loss = 0.7248479723930359
In grad_steps = 30, loss = 0.7192060947418213
In grad_steps = 31, loss = 0.6790924072265625
In grad_steps = 32, loss = 0.6854516267776489
In grad_steps = 33, loss = 0.7357378005981445
In grad_steps = 34, loss = 0.7284418940544128
In grad_steps = 35, loss = 0.58319091796875
In grad_steps = 36, loss = 0.6999068260192871
In grad_steps = 37, loss = 0.6832988262176514
In grad_steps = 38, loss = 0.7357661128044128
In grad_steps = 39, loss = 0.6398614048957825
In grad_steps = 40, loss = 0.6557890772819519
In grad_steps = 41, loss = 0.5625994801521301
In grad_steps = 42, loss = 0.5326100587844849
In grad_steps = 43, loss = 0.5386264324188232
In grad_steps = 44, loss = 0.7521379590034485
In grad_steps = 45, loss = 0.7493720054626465
In grad_steps = 46, loss = 0.9201571941375732
In grad_steps = 47, loss = 0.6114462614059448
In grad_steps = 48, loss = 0.9289056658744812
In grad_steps = 49, loss = 0.5265735983848572
In grad_steps = 50, loss = 0.9800001382827759
In grad_steps = 51, loss = 0.9899970293045044
In grad_steps = 52, loss = 0.7140425443649292
In grad_steps = 53, loss = 0.6881328821182251
In grad_steps = 54, loss = 0.7014180421829224
In grad_steps = 55, loss = 0.6151170134544373
In grad_steps = 56, loss = 0.6394250988960266
In grad_steps = 57, loss = 2.9205329418182373
In grad_steps = 58, loss = 0.6636761426925659
In grad_steps = 59, loss = 0.70108962059021
In grad_steps = 60, loss = 0.684230625629425
In grad_steps = 61, loss = 0.739718496799469
In grad_steps = 62, loss = 0.7951288223266602
In grad_steps = 63, loss = 0.636160135269165
In grad_steps = 64, loss = 0.7063872814178467
In grad_steps = 65, loss = 0.7392981052398682
In grad_steps = 66, loss = 0.686241626739502
In grad_steps = 67, loss = 0.7028337717056274
In grad_steps = 68, loss = 0.6873128414154053
In grad_steps = 69, loss = 0.7727156281471252
In grad_steps = 70, loss = 0.6822308897972107
In grad_steps = 71, loss = 0.6838729977607727
In grad_steps = 72, loss = 0.6330369710922241
In grad_steps = 73, loss = 0.7405736446380615
In grad_steps = 74, loss = 1.7379244565963745
In grad_steps = 75, loss = 0.5817069411277771
In grad_steps = 76, loss = 0.7209609746932983
In grad_steps = 77, loss = 0.7987371683120728
In grad_steps = 78, loss = 0.6471408605575562
In grad_steps = 79, loss = 0.6086980104446411
In grad_steps = 80, loss = 0.7021018266677856
In grad_steps = 81, loss = 0.6010286211967468
In grad_steps = 82, loss = 0.6766178011894226
In grad_steps = 83, loss = 0.7091076374053955
In grad_steps = 84, loss = 0.6747727990150452
In grad_steps = 85, loss = 0.7380111217498779
In grad_steps = 86, loss = 0.6147462129592896
In grad_steps = 87, loss = 0.6053698658943176
In grad_steps = 88, loss = 0.6647893786430359
In grad_steps = 89, loss = 0.718145489692688
In grad_steps = 90, loss = 0.6663899421691895
In grad_steps = 91, loss = 0.7090276479721069
In grad_steps = 92, loss = 0.7182438373565674
In grad_steps = 93, loss = 0.595091700553894
In grad_steps = 94, loss = 0.6201502084732056
In grad_steps = 95, loss = 0.58608478307724
In grad_steps = 96, loss = 0.7786767482757568
In grad_steps = 97, loss = 0.6680858731269836
In grad_steps = 98, loss = 0.5629123449325562
In grad_steps = 99, loss = 0.6233894228935242
In grad_steps = 100, loss = 0.5504977703094482
In grad_steps = 101, loss = 0.544693112373352
In grad_steps = 102, loss = 0.8082882165908813
In grad_steps = 103, loss = 0.6599454879760742
In grad_steps = 104, loss = 0.7130553722381592
In grad_steps = 105, loss = 0.6898576617240906
In grad_steps = 106, loss = 0.5697230696678162
In grad_steps = 107, loss = 0.8369405269622803
In grad_steps = 108, loss = 0.45550504326820374
In grad_steps = 109, loss = 0.608601987361908
In grad_steps = 110, loss = 0.5157992839813232
In grad_steps = 111, loss = 0.49775293469429016
In grad_steps = 112, loss = 0.7797591090202332
In grad_steps = 113, loss = 0.8320451378822327
In grad_steps = 114, loss = 0.5402070879936218
In grad_steps = 115, loss = 0.414307177066803
In grad_steps = 116, loss = 0.40996280312538147
In grad_steps = 117, loss = 0.5878346562385559
In grad_steps = 118, loss = 1.3071022033691406
In grad_steps = 119, loss = 1.4036636352539062
In grad_steps = 120, loss = 0.9026677012443542
In grad_steps = 121, loss = 0.8977749347686768
In grad_steps = 122, loss = 0.7597060799598694
In grad_steps = 123, loss = 0.6685987710952759
In grad_steps = 124, loss = 0.6352190971374512
In grad_steps = 125, loss = 0.7465498447418213
In grad_steps = 126, loss = 0.4326452314853668
In grad_steps = 127, loss = 0.7575179934501648
In grad_steps = 128, loss = 0.6944486498832703
In grad_steps = 129, loss = 0.8048456907272339
In grad_steps = 130, loss = 0.5726232528686523
In grad_steps = 131, loss = 0.8258301019668579
In grad_steps = 132, loss = 0.6371687650680542
In grad_steps = 133, loss = 0.7195389866828918
In grad_steps = 134, loss = 0.7168002724647522
In grad_steps = 135, loss = 0.6520251631736755
In grad_steps = 136, loss = 0.6818346381187439
In grad_steps = 137, loss = 0.6957457065582275
In grad_steps = 138, loss = 0.6460078954696655
In grad_steps = 139, loss = 0.5644098520278931
In grad_steps = 140, loss = 0.6247251033782959
In grad_steps = 141, loss = 0.6588488221168518
In grad_steps = 142, loss = 0.6981920003890991
In grad_steps = 143, loss = 0.6190199851989746
In grad_steps = 144, loss = 0.6237999200820923
In grad_steps = 145, loss = 0.7913750410079956
In grad_steps = 146, loss = 0.5897719860076904
In grad_steps = 147, loss = 0.632990837097168
In grad_steps = 148, loss = 0.6491479873657227
In grad_steps = 149, loss = 0.44587379693984985
In grad_steps = 150, loss = 0.4455625116825104
In grad_steps = 151, loss = 0.8312022686004639
In grad_steps = 152, loss = 0.1815231442451477
In grad_steps = 153, loss = 1.0814341306686401
In grad_steps = 154, loss = 1.248929738998413
In grad_steps = 155, loss = 0.5358486771583557
In grad_steps = 156, loss = 0.6089849472045898
In grad_steps = 157, loss = 0.7208189964294434
In grad_steps = 158, loss = 0.5616844892501831
In grad_steps = 159, loss = 0.6487373113632202
In grad_steps = 160, loss = 0.5945025682449341
In grad_steps = 161, loss = 0.420010507106781
In grad_steps = 162, loss = 0.87675541639328
In grad_steps = 163, loss = 0.49579501152038574
In grad_steps = 164, loss = 1.0073950290679932
In grad_steps = 165, loss = 0.5970045328140259
In grad_steps = 166, loss = 0.6990739107131958
In grad_steps = 167, loss = 0.40423762798309326
In grad_steps = 168, loss = 0.4758697748184204
In grad_steps = 169, loss = 0.748863697052002
In grad_steps = 170, loss = 0.6008599996566772
In grad_steps = 171, loss = 0.7065288424491882
In grad_steps = 172, loss = 0.6727938652038574
In grad_steps = 173, loss = 0.47926583886146545
In grad_steps = 174, loss = 0.8363161683082581
In grad_steps = 175, loss = 0.5739129781723022
In grad_steps = 176, loss = 0.45074671506881714
In grad_steps = 177, loss = 0.5181871056556702
In grad_steps = 178, loss = 0.5825718641281128
In grad_steps = 179, loss = 0.55412757396698
In grad_steps = 180, loss = 0.5157418847084045
In grad_steps = 181, loss = 0.5169414281845093
In grad_steps = 182, loss = 0.556669294834137
In grad_steps = 183, loss = 0.5195509195327759
In grad_steps = 184, loss = 0.4694293737411499
In grad_steps = 185, loss = 0.5459579229354858
In grad_steps = 186, loss = 1.133002519607544
In grad_steps = 187, loss = 0.257977694272995
In grad_steps = 188, loss = 0.7902462482452393
In grad_steps = 189, loss = 0.19219963252544403
In grad_steps = 190, loss = 0.5748111605644226
In grad_steps = 191, loss = 0.6524320840835571
In grad_steps = 192, loss = 0.33459895849227905
In grad_steps = 193, loss = 0.3176848888397217
In grad_steps = 194, loss = 0.6618558168411255
In grad_steps = 195, loss = 0.2615010142326355
In grad_steps = 196, loss = 0.4206559658050537
In grad_steps = 197, loss = 0.16272467374801636
In grad_steps = 198, loss = 0.754798412322998
In grad_steps = 199, loss = 0.6639323234558105
In grad_steps = 200, loss = 0.3332328200340271
In grad_steps = 201, loss = 0.9062261581420898
In grad_steps = 202, loss = 0.2522725760936737
In grad_steps = 203, loss = 0.2496829628944397
In grad_steps = 204, loss = 0.3791787028312683
In grad_steps = 205, loss = 0.7680569887161255
In grad_steps = 206, loss = 1.132159948348999
In grad_steps = 207, loss = 0.4028472900390625
In grad_steps = 208, loss = 0.24692490696907043
In grad_steps = 209, loss = 0.7011696100234985
In grad_steps = 210, loss = 0.4575307071208954
In grad_steps = 211, loss = 0.38702189922332764
In grad_steps = 212, loss = 0.32926881313323975
In grad_steps = 213, loss = 0.5265612006187439
In grad_steps = 214, loss = 0.5028942227363586
In grad_steps = 215, loss = 0.42615073919296265
In grad_steps = 216, loss = 0.5183943510055542
In grad_steps = 217, loss = 0.5450295209884644
In grad_steps = 218, loss = 0.5165153741836548
In grad_steps = 219, loss = 0.34051522612571716
In grad_steps = 220, loss = 0.32309556007385254
In grad_steps = 221, loss = 0.2539692223072052
In grad_steps = 222, loss = 0.20187610387802124
In grad_steps = 223, loss = 0.5007956027984619
In grad_steps = 224, loss = 0.15913061797618866
In grad_steps = 225, loss = 1.1753467321395874
In grad_steps = 226, loss = 0.6211361885070801
In grad_steps = 227, loss = 1.2286113500595093
In grad_steps = 228, loss = 0.2854977548122406
In grad_steps = 229, loss = 0.40019142627716064
In grad_steps = 230, loss = 0.5192087292671204
In grad_steps = 231, loss = 0.31891632080078125
In grad_steps = 232, loss = 0.22129978239536285
In grad_steps = 233, loss = 0.5015147924423218
In grad_steps = 234, loss = 0.6548793911933899
In grad_steps = 235, loss = 0.39520013332366943
In grad_steps = 236, loss = 0.5879319310188293
In grad_steps = 237, loss = 0.5819739103317261
In grad_steps = 238, loss = 0.2796194851398468
In grad_steps = 239, loss = 0.3747612237930298
In grad_steps = 240, loss = 0.7889622449874878
In grad_steps = 241, loss = 0.36560097336769104
In grad_steps = 242, loss = 0.6420069336891174
In grad_steps = 243, loss = 0.5765062570571899
In grad_steps = 244, loss = 0.3071190416812897
In grad_steps = 245, loss = 0.28417298197746277
In grad_steps = 246, loss = 0.2690269351005554
In grad_steps = 247, loss = 0.5463176369667053
In grad_steps = 248, loss = 0.1274476945400238
In grad_steps = 249, loss = 0.31661462783813477
In grad_steps = 250, loss = 0.3213523030281067
In grad_steps = 251, loss = 0.8494833707809448
In grad_steps = 252, loss = 1.4329673051834106
In grad_steps = 253, loss = 0.25088635087013245
In grad_steps = 254, loss = 0.8695632815361023
In grad_steps = 255, loss = 0.5741667747497559
In grad_steps = 256, loss = 0.44011807441711426
In grad_steps = 257, loss = 0.5881340503692627
In grad_steps = 258, loss = 0.23034316301345825
In grad_steps = 259, loss = 0.7767925262451172
In grad_steps = 260, loss = 0.7452729344367981
In grad_steps = 261, loss = 0.5418869256973267
In grad_steps = 262, loss = 0.3711226284503937
In grad_steps = 263, loss = 0.7895326614379883
In grad_steps = 264, loss = 0.33122920989990234
In grad_steps = 265, loss = 0.4269134998321533
In grad_steps = 266, loss = 0.5633701682090759
In grad_steps = 267, loss = 0.4407331943511963
In grad_steps = 268, loss = 0.46432065963745117
In grad_steps = 269, loss = 0.4101991653442383
In grad_steps = 270, loss = 0.3361440896987915
In grad_steps = 271, loss = 0.36665305495262146
In grad_steps = 272, loss = 0.24989095330238342
In grad_steps = 273, loss = 0.5663005709648132
In grad_steps = 274, loss = 0.20247571170330048
In grad_steps = 275, loss = 0.27481216192245483
In grad_steps = 276, loss = 0.45399656891822815
In grad_steps = 277, loss = 0.6804969310760498
In grad_steps = 278, loss = 0.12816926836967468
In grad_steps = 279, loss = 1.0279560089111328
In grad_steps = 280, loss = 0.28569167852401733
In grad_steps = 281, loss = 0.5113893151283264
In grad_steps = 282, loss = 0.4587494134902954
In grad_steps = 283, loss = 0.9066027402877808
In grad_steps = 284, loss = 0.2757858335971832
In grad_steps = 285, loss = 0.5702357888221741
In grad_steps = 286, loss = 0.72319495677948
In grad_steps = 287, loss = 0.06985855102539062
In grad_steps = 288, loss = 0.0691613182425499
In grad_steps = 289, loss = 0.5697870254516602
In grad_steps = 290, loss = 0.09645025432109833
In grad_steps = 291, loss = 0.39457666873931885
In grad_steps = 292, loss = 0.639814555644989
In grad_steps = 293, loss = 0.4560425877571106
In grad_steps = 294, loss = 0.3328248858451843
In grad_steps = 295, loss = 0.24947121739387512
In grad_steps = 296, loss = 0.19958405196666718
In grad_steps = 297, loss = 0.3344607949256897
In grad_steps = 298, loss = 0.39020848274230957
In grad_steps = 299, loss = 0.5758354663848877
In grad_steps = 300, loss = 0.5015695691108704
In grad_steps = 301, loss = 0.3818782866001129
In grad_steps = 302, loss = 0.3965342342853546
In grad_steps = 303, loss = 0.2584415078163147
In grad_steps = 304, loss = 0.38264089822769165
In grad_steps = 305, loss = 1.5894075632095337
In grad_steps = 306, loss = 0.1190887838602066
In grad_steps = 307, loss = 0.31472745537757874
In grad_steps = 308, loss = 0.4155024290084839
In grad_steps = 309, loss = 0.7759870886802673
In grad_steps = 310, loss = 0.4613550305366516
In grad_steps = 311, loss = 0.8996451497077942
In grad_steps = 312, loss = 0.5815937519073486
In grad_steps = 313, loss = 0.8276572823524475
In grad_steps = 314, loss = 0.6366279721260071
In grad_steps = 315, loss = 0.31072643399238586
In grad_steps = 316, loss = 0.4025508165359497
In grad_steps = 317, loss = 0.4965384304523468
In grad_steps = 318, loss = 0.3030274510383606
In grad_steps = 319, loss = 0.4503265619277954
In grad_steps = 320, loss = 0.2134627103805542
In grad_steps = 321, loss = 0.40688586235046387
In grad_steps = 322, loss = 0.6552955508232117
In grad_steps = 323, loss = 0.5484768152236938
In grad_steps = 324, loss = 0.27859705686569214
In grad_steps = 325, loss = 0.19715744256973267
In grad_steps = 326, loss = 0.17879164218902588
In grad_steps = 327, loss = 0.29382920265197754
In grad_steps = 328, loss = 0.19166824221611023
In grad_steps = 329, loss = 0.16424116492271423
In grad_steps = 330, loss = 0.7680972814559937
In grad_steps = 331, loss = 0.24435338377952576
In grad_steps = 332, loss = 0.159478098154068
In grad_steps = 333, loss = 0.421160489320755
In grad_steps = 334, loss = 0.07367181777954102
In grad_steps = 335, loss = 0.32781335711479187
In grad_steps = 336, loss = 0.5561569333076477
In grad_steps = 337, loss = 0.10442762821912766
In grad_steps = 338, loss = 0.2803107500076294
In grad_steps = 339, loss = 0.8361061215400696
In grad_steps = 340, loss = 0.6615599989891052
In grad_steps = 341, loss = 0.051828399300575256
In grad_steps = 342, loss = 1.044257402420044
In grad_steps = 343, loss = 0.9046130180358887
In grad_steps = 344, loss = 0.12169142067432404
In grad_steps = 345, loss = 0.08982910215854645
In grad_steps = 346, loss = 0.4865916967391968
In grad_steps = 347, loss = 1.107047438621521
In grad_steps = 348, loss = 0.794349193572998
In grad_steps = 349, loss = 1.0123389959335327
In grad_steps = 350, loss = 0.8208941221237183
In grad_steps = 351, loss = 0.5517892837524414
In grad_steps = 352, loss = 0.6124625205993652
In grad_steps = 353, loss = 0.3684699833393097
In grad_steps = 354, loss = 0.49334192276000977
In grad_steps = 355, loss = 1.0011993646621704
In grad_steps = 356, loss = 0.19672371447086334
In grad_steps = 357, loss = 0.5426695942878723
In grad_steps = 358, loss = 0.1722506731748581
In grad_steps = 359, loss = 0.6947492361068726
In grad_steps = 360, loss = 0.8194383382797241
In grad_steps = 361, loss = 0.6112911105155945
In grad_steps = 362, loss = 0.22418439388275146
In grad_steps = 363, loss = 0.34314048290252686
In grad_steps = 364, loss = 0.4633132815361023
In grad_steps = 365, loss = 0.3462905287742615
In grad_steps = 366, loss = 0.2447354942560196
In grad_steps = 367, loss = 0.48996418714523315
In grad_steps = 368, loss = 0.24803170561790466
In grad_steps = 369, loss = 0.31418663263320923
In grad_steps = 370, loss = 0.5674737691879272
In grad_steps = 371, loss = 0.29241856932640076
In grad_steps = 372, loss = 0.13551731407642365
In grad_steps = 373, loss = 0.48516845703125
In grad_steps = 374, loss = 0.26953741908073425
In grad_steps = 375, loss = 0.4160001575946808
In grad_steps = 376, loss = 0.041262686252593994
In grad_steps = 377, loss = 0.6039159297943115
In grad_steps = 378, loss = 0.517383873462677
In grad_steps = 379, loss = 0.13229870796203613
In grad_steps = 380, loss = 0.29630738496780396
In grad_steps = 381, loss = 0.8275870084762573
In grad_steps = 382, loss = 0.22418497502803802
In grad_steps = 383, loss = 0.18858793377876282
In grad_steps = 384, loss = 0.21542267501354218
In grad_steps = 385, loss = 0.17809243500232697
In grad_steps = 386, loss = 1.805665135383606
In grad_steps = 387, loss = 0.514052152633667
In grad_steps = 388, loss = 0.7242594361305237
In grad_steps = 389, loss = 0.8019604682922363
In grad_steps = 390, loss = 0.3421001434326172
In grad_steps = 391, loss = 0.27888035774230957
In grad_steps = 392, loss = 0.14585039019584656
In grad_steps = 393, loss = 0.5481595396995544
In grad_steps = 394, loss = 0.266512930393219
In grad_steps = 395, loss = 0.4450499415397644
In grad_steps = 396, loss = 0.4316444396972656
In grad_steps = 397, loss = 0.49561506509780884
In grad_steps = 398, loss = 0.19717097282409668
In grad_steps = 399, loss = 0.3839474320411682
In grad_steps = 400, loss = 0.4432087540626526
In grad_steps = 401, loss = 0.15653513371944427
In grad_steps = 402, loss = 0.5030931830406189
In grad_steps = 403, loss = 0.7947984933853149
In grad_steps = 404, loss = 0.16296663880348206
In grad_steps = 405, loss = 0.2072492390871048
In grad_steps = 406, loss = 0.962652325630188
In grad_steps = 407, loss = 0.39903634786605835
In grad_steps = 408, loss = 0.10613948851823807
In grad_steps = 409, loss = 0.5121515989303589
In grad_steps = 410, loss = 0.23365086317062378
In grad_steps = 411, loss = 0.7268761992454529
In grad_steps = 412, loss = 0.31071460247039795
In grad_steps = 413, loss = 0.19677060842514038
In grad_steps = 414, loss = 0.568672776222229
In grad_steps = 415, loss = 0.49349886178970337
In grad_steps = 416, loss = 0.16510353982448578
In grad_steps = 417, loss = 0.26658061146736145
In grad_steps = 418, loss = 0.4205423593521118
In grad_steps = 419, loss = 0.27243947982788086
In grad_steps = 420, loss = 0.05983424186706543
In grad_steps = 421, loss = 0.997013509273529
In grad_steps = 422, loss = 0.6715933680534363
In grad_steps = 423, loss = 0.20947960019111633
In grad_steps = 424, loss = 0.4813533127307892
In grad_steps = 425, loss = 0.07029275596141815
In grad_steps = 426, loss = 0.6838398575782776
In grad_steps = 427, loss = 0.24452248215675354
In grad_steps = 428, loss = 0.07591664791107178
In grad_steps = 429, loss = 0.6648485064506531
In grad_steps = 430, loss = 0.5704463124275208
In grad_steps = 431, loss = 0.42030054330825806
In grad_steps = 432, loss = 0.16786962747573853
In grad_steps = 433, loss = 0.40854448080062866
In grad_steps = 434, loss = 1.0504907369613647
In grad_steps = 435, loss = 0.06870876997709274
In grad_steps = 436, loss = 0.255021870136261
In grad_steps = 437, loss = 0.3387593626976013
In grad_steps = 438, loss = 0.9405548572540283
In grad_steps = 439, loss = 0.16244149208068848
In grad_steps = 440, loss = 0.09767096489667892
In grad_steps = 441, loss = 0.6726029515266418
In grad_steps = 442, loss = 0.5715569257736206
In grad_steps = 443, loss = 0.12400683015584946
In grad_steps = 444, loss = 0.5952935814857483
In grad_steps = 445, loss = 0.1474829912185669
In grad_steps = 446, loss = 0.8075585961341858
In grad_steps = 447, loss = 0.7186119556427002
In grad_steps = 448, loss = 0.6697636842727661
In grad_steps = 449, loss = 0.1365661323070526
In grad_steps = 450, loss = 0.2806929349899292
In grad_steps = 451, loss = 0.2885592579841614
In grad_steps = 452, loss = 0.27204591035842896
In grad_steps = 453, loss = 0.335966020822525
In grad_steps = 454, loss = 0.38316988945007324
In grad_steps = 455, loss = 0.515773355960846
In grad_steps = 456, loss = 0.6543731093406677
In grad_steps = 457, loss = 0.2478380650281906
In grad_steps = 458, loss = 0.6315591335296631
In grad_steps = 459, loss = 0.3168942928314209
In grad_steps = 460, loss = 0.38939154148101807
In grad_steps = 461, loss = 0.0632738471031189
In grad_steps = 462, loss = 0.6505922675132751
In grad_steps = 463, loss = 0.33202922344207764
In grad_steps = 464, loss = 0.8787778615951538
In grad_steps = 465, loss = 0.679130494594574
In grad_steps = 466, loss = 0.509660005569458
In grad_steps = 467, loss = 0.5764801502227783
In grad_steps = 468, loss = 0.9544017314910889
In grad_steps = 469, loss = 0.4047456383705139
In grad_steps = 470, loss = 0.4723444879055023
In grad_steps = 471, loss = 0.12190156430006027
In grad_steps = 472, loss = 0.16230469942092896
In grad_steps = 473, loss = 0.2645551562309265
In grad_steps = 474, loss = 0.8076799511909485
In grad_steps = 475, loss = 0.7259513735771179
In grad_steps = 476, loss = 0.2112465798854828
In grad_steps = 477, loss = 0.34097033739089966
In grad_steps = 478, loss = 0.2842753529548645
In grad_steps = 479, loss = 0.6873518228530884
In grad_steps = 480, loss = 0.30371418595314026
In grad_steps = 481, loss = 0.7256162166595459
In grad_steps = 482, loss = 0.15788120031356812
In grad_steps = 483, loss = 0.2421523481607437
In grad_steps = 484, loss = 0.2570224106311798
In grad_steps = 485, loss = 0.5260775685310364
In grad_steps = 486, loss = 0.29818040132522583
In grad_steps = 487, loss = 0.28057482838630676
In grad_steps = 488, loss = 0.916005551815033
In grad_steps = 489, loss = 0.858180046081543
In grad_steps = 490, loss = 0.1811373233795166
In grad_steps = 491, loss = 0.5429781079292297
In grad_steps = 492, loss = 0.2827662229537964
In grad_steps = 493, loss = 0.054641153663396835
In grad_steps = 494, loss = 0.9505687952041626
In grad_steps = 495, loss = 0.0977722629904747
In grad_steps = 496, loss = 0.6172447204589844
In grad_steps = 497, loss = 0.15796709060668945
In grad_steps = 498, loss = 0.5105603933334351
In grad_steps = 499, loss = 0.2584170699119568
In grad_steps = 500, loss = 0.13268457353115082
In grad_steps = 501, loss = 0.44136935472488403
In grad_steps = 502, loss = 0.6850497126579285
In grad_steps = 503, loss = 0.540343165397644
In grad_steps = 504, loss = 0.44567710161209106
In grad_steps = 505, loss = 0.5869648456573486
In grad_steps = 506, loss = 0.314877450466156
In grad_steps = 507, loss = 0.4771853983402252
In grad_steps = 508, loss = 0.8563604950904846
In grad_steps = 509, loss = 0.4448113739490509
In grad_steps = 510, loss = 0.09116849303245544
In grad_steps = 511, loss = 0.3282531499862671
In grad_steps = 512, loss = 0.18951714038848877
In grad_steps = 513, loss = 0.4554513096809387
In grad_steps = 514, loss = 0.4669348895549774
In grad_steps = 515, loss = 0.514347493648529
In grad_steps = 516, loss = 0.6169894933700562
In grad_steps = 517, loss = 0.39468610286712646
In grad_steps = 518, loss = 0.561969518661499
In grad_steps = 519, loss = 0.31424129009246826
In grad_steps = 520, loss = 0.15491211414337158
In grad_steps = 521, loss = 0.5317735075950623
In grad_steps = 522, loss = 0.15432992577552795
In grad_steps = 523, loss = 0.2813096344470978
In grad_steps = 524, loss = 0.30085790157318115
In grad_steps = 525, loss = 0.3462008237838745
In grad_steps = 526, loss = 0.13407988846302032
In grad_steps = 527, loss = 0.574830174446106
In grad_steps = 528, loss = 0.2300245761871338
In grad_steps = 529, loss = 0.09597787261009216
In grad_steps = 530, loss = 0.08611252158880234
In grad_steps = 531, loss = 0.6558235287666321
In grad_steps = 532, loss = 0.19964489340782166
In grad_steps = 533, loss = 0.3248608708381653
In grad_steps = 534, loss = 1.001039743423462
In grad_steps = 535, loss = 0.5198725461959839
In grad_steps = 536, loss = 0.266849160194397
In grad_steps = 537, loss = 0.49622485041618347
In grad_steps = 538, loss = 1.4066848754882812
In grad_steps = 539, loss = 0.3845740556716919
In grad_steps = 540, loss = 0.11551013588905334
In grad_steps = 541, loss = 0.4747469127178192
In grad_steps = 542, loss = 0.8548920154571533
In grad_steps = 543, loss = 0.12760460376739502
In grad_steps = 544, loss = 0.7277482151985168
In grad_steps = 545, loss = 0.9764975309371948
In grad_steps = 546, loss = 0.2258397489786148
In grad_steps = 547, loss = 0.36298424005508423
In grad_steps = 548, loss = 0.35290923714637756
In grad_steps = 549, loss = 0.549298882484436
In grad_steps = 550, loss = 0.7486965656280518
In grad_steps = 551, loss = 0.2367458939552307
In grad_steps = 552, loss = 0.29497474431991577
In grad_steps = 553, loss = 0.44181394577026367
In grad_steps = 554, loss = 0.6042421460151672
In grad_steps = 555, loss = 0.6023146510124207
In grad_steps = 556, loss = 0.7632010579109192
In grad_steps = 557, loss = 0.3455023169517517
In grad_steps = 558, loss = 0.46000099182128906
In grad_steps = 559, loss = 0.3000064492225647
In grad_steps = 560, loss = 1.0786716938018799
In grad_steps = 561, loss = 0.33432653546333313
In grad_steps = 562, loss = 0.42371973395347595
In grad_steps = 563, loss = 0.3425140678882599
In grad_steps = 564, loss = 0.8553282022476196
In grad_steps = 565, loss = 0.41728803515434265
In grad_steps = 566, loss = 0.2812967598438263
In grad_steps = 567, loss = 0.7480964660644531
In grad_steps = 568, loss = 0.7108372449874878
In grad_steps = 569, loss = 0.15209272503852844
In grad_steps = 570, loss = 0.39193999767303467
In grad_steps = 571, loss = 0.16774004697799683
In grad_steps = 572, loss = 0.4045184552669525
In grad_steps = 573, loss = 0.23164242506027222
In grad_steps = 574, loss = 0.1500329077243805
In grad_steps = 575, loss = 0.308520644903183
In grad_steps = 576, loss = 0.21238966286182404
In grad_steps = 577, loss = 0.4608340263366699
In grad_steps = 578, loss = 0.4282408058643341
In grad_steps = 579, loss = 0.26829153299331665
In grad_steps = 580, loss = 0.6358169913291931
In grad_steps = 581, loss = 0.46809279918670654
In grad_steps = 582, loss = 0.6952555775642395
In grad_steps = 583, loss = 0.14146670699119568
In grad_steps = 584, loss = 0.0444159135222435
In grad_steps = 585, loss = 0.06548205018043518
In grad_steps = 586, loss = 0.46486762166023254
In grad_steps = 587, loss = 0.196383535861969
In grad_steps = 588, loss = 0.37202543020248413
In grad_steps = 589, loss = 1.1693975925445557
In grad_steps = 590, loss = 0.22548112273216248
In grad_steps = 591, loss = 0.29060620069503784
In grad_steps = 592, loss = 1.4760267734527588
In grad_steps = 593, loss = 0.0943514034152031
In grad_steps = 594, loss = 0.12028835713863373
In grad_steps = 595, loss = 0.45528778433799744
In grad_steps = 596, loss = 0.6435797214508057
In grad_steps = 597, loss = 0.5378866791725159
In grad_steps = 598, loss = 0.31343233585357666
In grad_steps = 599, loss = 0.42321261763572693
In grad_steps = 600, loss = 0.5907761454582214
In grad_steps = 601, loss = 0.16596339643001556
In grad_steps = 602, loss = 0.2686726450920105
In grad_steps = 603, loss = 0.3613560199737549
In grad_steps = 604, loss = 0.09916140884160995
In grad_steps = 605, loss = 0.41479653120040894
In grad_steps = 606, loss = 0.20004573464393616
In grad_steps = 607, loss = 0.18362432718276978
In grad_steps = 608, loss = 0.417248010635376
In grad_steps = 609, loss = 0.4243500232696533
In grad_steps = 610, loss = 0.49830883741378784
In grad_steps = 611, loss = 0.5078423023223877
In grad_steps = 612, loss = 0.287893682718277
In grad_steps = 613, loss = 0.6873936653137207
In grad_steps = 614, loss = 0.27055293321609497
In grad_steps = 615, loss = 0.5162048935890198
In grad_steps = 616, loss = 0.32420799136161804
In grad_steps = 617, loss = 0.08168527483940125
In grad_steps = 618, loss = 0.23764842748641968
In grad_steps = 619, loss = 0.08311963826417923
In grad_steps = 620, loss = 0.13948500156402588
In grad_steps = 621, loss = 1.3691486120224
In grad_steps = 622, loss = 0.3381882309913635
In grad_steps = 623, loss = 0.12010496109724045
In grad_steps = 624, loss = 0.1198810413479805
In grad_steps = 625, loss = 0.5769845843315125
In grad_steps = 626, loss = 0.21443399786949158
In grad_steps = 627, loss = 0.06289376318454742
In grad_steps = 628, loss = 0.16725711524486542
In grad_steps = 629, loss = 0.07724428921937943
In grad_steps = 630, loss = 0.15370260179042816
In grad_steps = 631, loss = 0.11951462179422379
In grad_steps = 632, loss = 0.08897048234939575
In grad_steps = 633, loss = 0.8056067824363708
In grad_steps = 634, loss = 0.262527734041214
In grad_steps = 635, loss = 0.30096060037612915
In grad_steps = 636, loss = 0.24717724323272705
In grad_steps = 637, loss = 0.4179545044898987
In grad_steps = 638, loss = 0.02607688307762146
In grad_steps = 639, loss = 0.509820282459259
In grad_steps = 640, loss = 0.08742683380842209
In grad_steps = 641, loss = 0.14249370992183685
In grad_steps = 642, loss = 0.3880265951156616
In grad_steps = 643, loss = 0.6042826175689697
In grad_steps = 644, loss = 1.0654112100601196
In grad_steps = 645, loss = 1.2284733057022095
In grad_steps = 646, loss = 0.09185951203107834
In grad_steps = 647, loss = 0.2806675136089325
In grad_steps = 648, loss = 0.18554262816905975
In grad_steps = 649, loss = 0.4740050733089447
In grad_steps = 650, loss = 0.8486858606338501
In grad_steps = 651, loss = 0.24998702108860016
In grad_steps = 652, loss = 0.9510485529899597
In grad_steps = 653, loss = 0.6330621242523193
In grad_steps = 654, loss = 0.6914433240890503
In grad_steps = 655, loss = 0.6474288105964661
In grad_steps = 656, loss = 0.19696711003780365
In grad_steps = 657, loss = 0.16534417867660522
In grad_steps = 658, loss = 0.4680466055870056
In grad_steps = 659, loss = 0.4563002288341522
In grad_steps = 660, loss = 0.28719377517700195
In grad_steps = 661, loss = 0.23282945156097412
In grad_steps = 662, loss = 0.36509913206100464
In grad_steps = 663, loss = 0.22665564715862274
In grad_steps = 664, loss = 0.1599438488483429
In grad_steps = 665, loss = 0.277351975440979
In grad_steps = 666, loss = 0.2778739929199219
In grad_steps = 667, loss = 0.5217137932777405
In grad_steps = 668, loss = 0.11893759667873383
In grad_steps = 669, loss = 0.40867316722869873
In grad_steps = 670, loss = 0.755368709564209
In grad_steps = 671, loss = 0.6582480669021606
In grad_steps = 672, loss = 0.1004110723733902
In grad_steps = 673, loss = 0.9590346813201904
In grad_steps = 674, loss = 0.14883942902088165
In grad_steps = 675, loss = 0.8584573864936829
In grad_steps = 676, loss = 0.1577153503894806
In grad_steps = 677, loss = 0.49038776755332947
In grad_steps = 678, loss = 0.06891346722841263
In grad_steps = 679, loss = 0.18124131858348846
In grad_steps = 680, loss = 0.04486123472452164
In grad_steps = 681, loss = 0.9697034358978271
In grad_steps = 682, loss = 0.6519854068756104
In grad_steps = 683, loss = 0.24104228615760803
In grad_steps = 684, loss = 0.2268705666065216
In grad_steps = 685, loss = 0.3108370900154114
In grad_steps = 686, loss = 0.25816139578819275
In grad_steps = 687, loss = 0.39957696199417114
In grad_steps = 688, loss = 0.6658760905265808
In grad_steps = 689, loss = 0.1636277139186859
In grad_steps = 690, loss = 0.33352357149124146
In grad_steps = 691, loss = 0.2041434496641159
In grad_steps = 692, loss = 0.3725382089614868
In grad_steps = 693, loss = 0.43356072902679443
In grad_steps = 694, loss = 0.3065796494483948
In grad_steps = 695, loss = 0.394621342420578
In grad_steps = 696, loss = 0.737500786781311
In grad_steps = 697, loss = 0.4258387088775635
In grad_steps = 698, loss = 0.2804237902164459
In grad_steps = 699, loss = 0.677502453327179
In grad_steps = 700, loss = 0.5613029599189758
In grad_steps = 701, loss = 0.09025399386882782
In grad_steps = 702, loss = 0.13314133882522583
In grad_steps = 703, loss = 0.20169737935066223
In grad_steps = 704, loss = 0.35654979944229126
In grad_steps = 705, loss = 0.09995296597480774
In grad_steps = 706, loss = 0.5046216249465942
In grad_steps = 707, loss = 0.10913171619176865
In grad_steps = 708, loss = 0.02729557454586029
In grad_steps = 709, loss = 0.4202162027359009
In grad_steps = 710, loss = 0.45653265714645386
In grad_steps = 711, loss = 1.2530744075775146
In grad_steps = 712, loss = 0.44938036799430847
In grad_steps = 713, loss = 1.5646400451660156
In grad_steps = 714, loss = 0.2664491832256317
In grad_steps = 715, loss = 1.012768268585205
In grad_steps = 716, loss = 0.39015358686447144
In grad_steps = 717, loss = 0.17480134963989258
In grad_steps = 718, loss = 0.38065439462661743
In grad_steps = 719, loss = 0.07174137979745865
In grad_steps = 720, loss = 0.06298021227121353
In grad_steps = 721, loss = 1.2967524528503418
In grad_steps = 722, loss = 0.36788278818130493
In grad_steps = 723, loss = 0.5930418372154236
In grad_steps = 724, loss = 0.10537172853946686
In grad_steps = 725, loss = 0.5435069799423218
In grad_steps = 726, loss = 0.22726094722747803
In grad_steps = 727, loss = 0.41374003887176514
In grad_steps = 728, loss = 0.11183416843414307
In grad_steps = 729, loss = 0.11146298795938492
In grad_steps = 730, loss = 0.2566979229450226
In grad_steps = 731, loss = 0.6214410066604614
In grad_steps = 732, loss = 0.4102177917957306
In grad_steps = 733, loss = 1.1113145351409912
In grad_steps = 734, loss = 0.4109387993812561
In grad_steps = 735, loss = 0.4007241725921631
In grad_steps = 736, loss = 0.29695314168930054
In grad_steps = 737, loss = 0.13495203852653503
In grad_steps = 738, loss = 0.8223255276679993
In grad_steps = 739, loss = 0.13288787007331848
In grad_steps = 740, loss = 0.20569837093353271
In grad_steps = 741, loss = 0.3877183794975281
In grad_steps = 742, loss = 0.11387472599744797
In grad_steps = 743, loss = 0.19781017303466797
In grad_steps = 744, loss = 1.0363764762878418
In grad_steps = 745, loss = 0.692650556564331
In grad_steps = 746, loss = 0.22927862405776978
In grad_steps = 747, loss = 0.46381866931915283
In grad_steps = 748, loss = 0.09165478497743607
In grad_steps = 749, loss = 0.7659279108047485
In grad_steps = 750, loss = 0.5978407859802246
In grad_steps = 751, loss = 0.18310612440109253
In grad_steps = 752, loss = 0.09352521598339081
In grad_steps = 753, loss = 0.16065286099910736
In grad_steps = 754, loss = 0.5357978940010071
In grad_steps = 755, loss = 0.7007307410240173
In grad_steps = 756, loss = 0.1604418307542801
In grad_steps = 757, loss = 0.17346149682998657
In grad_steps = 758, loss = 0.8111668229103088
In grad_steps = 759, loss = 0.5912491083145142
In grad_steps = 760, loss = 0.2974609136581421
In grad_steps = 761, loss = 0.5037199854850769
In grad_steps = 762, loss = 0.19841784238815308
In grad_steps = 763, loss = 0.5598241090774536
In grad_steps = 764, loss = 0.8415363430976868
In grad_steps = 765, loss = 0.6800750494003296
In grad_steps = 766, loss = 0.08892308175563812
In grad_steps = 767, loss = 0.2709420323371887
In grad_steps = 768, loss = 0.20531564950942993
In grad_steps = 769, loss = 0.8721872568130493
In grad_steps = 770, loss = 0.21983987092971802
In grad_steps = 771, loss = 0.13533589243888855
In grad_steps = 772, loss = 0.42597663402557373
In grad_steps = 773, loss = 1.0825661420822144
In grad_steps = 774, loss = 0.38733941316604614
In grad_steps = 775, loss = 0.11882627010345459
In grad_steps = 776, loss = 0.4010595381259918
In grad_steps = 777, loss = 0.5962190628051758
In grad_steps = 778, loss = 0.1607578992843628
In grad_steps = 779, loss = 0.19928723573684692
In grad_steps = 780, loss = 0.4481860399246216
In grad_steps = 781, loss = 0.47559720277786255
In grad_steps = 782, loss = 0.20838910341262817
In grad_steps = 783, loss = 0.9099761247634888
In grad_steps = 784, loss = 0.5744667053222656
In grad_steps = 785, loss = 0.08064950257539749
In grad_steps = 786, loss = 0.2527455985546112
In grad_steps = 787, loss = 0.3572444021701813
In grad_steps = 788, loss = 0.5929802656173706
In grad_steps = 789, loss = 0.4027959406375885
In grad_steps = 790, loss = 0.5260081887245178
In grad_steps = 791, loss = 0.16839030385017395
In grad_steps = 792, loss = 0.15368704497814178
In grad_steps = 793, loss = 0.754440426826477
In grad_steps = 794, loss = 1.2124745845794678
In grad_steps = 795, loss = 0.732657790184021
In grad_steps = 796, loss = 0.2794835567474365
In grad_steps = 797, loss = 0.6144843101501465
In grad_steps = 798, loss = 0.206426203250885
In grad_steps = 799, loss = 0.5116137266159058
In grad_steps = 800, loss = 0.43340927362442017
In grad_steps = 801, loss = 0.3720959722995758
In grad_steps = 802, loss = 0.4339469373226166
In grad_steps = 803, loss = 0.30280882120132446
In grad_steps = 804, loss = 0.11383175104856491
In grad_steps = 805, loss = 0.5215082764625549
In grad_steps = 806, loss = 0.186529278755188
In grad_steps = 807, loss = 0.24178951978683472
In grad_steps = 808, loss = 0.31422728300094604
In grad_steps = 809, loss = 0.11244051158428192
In grad_steps = 810, loss = 0.6801682710647583
In grad_steps = 811, loss = 0.15711353719234467
In grad_steps = 812, loss = 0.4553377330303192
In grad_steps = 813, loss = 0.39502549171447754
In grad_steps = 814, loss = 0.15678904950618744
In grad_steps = 815, loss = 0.11935462057590485
In grad_steps = 816, loss = 1.1608097553253174
In grad_steps = 817, loss = 0.05897323414683342
In grad_steps = 818, loss = 0.16514426469802856
In grad_steps = 819, loss = 0.06529340893030167
In grad_steps = 820, loss = 0.0990103930234909
In grad_steps = 821, loss = 0.16203267872333527
In grad_steps = 822, loss = 0.6227850914001465
In grad_steps = 823, loss = 0.2053302526473999
In grad_steps = 824, loss = 0.04452945664525032
In grad_steps = 825, loss = 0.05017733946442604
In grad_steps = 826, loss = 0.8345283269882202
In grad_steps = 827, loss = 0.1436460018157959
In grad_steps = 828, loss = 1.08152174949646
In grad_steps = 829, loss = 0.3682789206504822
In grad_steps = 830, loss = 0.14345255494117737
In grad_steps = 831, loss = 0.031141314655542374
In grad_steps = 832, loss = 0.920870304107666
In grad_steps = 833, loss = 0.6296820640563965
In grad_steps = 834, loss = 0.6911823153495789
In grad_steps = 835, loss = 1.704780101776123
In grad_steps = 836, loss = 0.6156449317932129
In grad_steps = 837, loss = 0.9390919804573059
In grad_steps = 838, loss = 1.3383021354675293
In grad_steps = 839, loss = 0.5842537879943848
In grad_steps = 840, loss = 0.21831244230270386
In grad_steps = 841, loss = 0.748226523399353
In grad_steps = 842, loss = 0.186419278383255
In grad_steps = 843, loss = 0.6026538610458374
In grad_steps = 844, loss = 0.5774426460266113
In grad_steps = 845, loss = 0.36406466364860535
In grad_steps = 846, loss = 0.7980431914329529
In grad_steps = 847, loss = 0.6435850858688354
In grad_steps = 848, loss = 0.5972425937652588
In grad_steps = 849, loss = 0.8955790996551514
In grad_steps = 850, loss = 0.6755825281143188
In grad_steps = 851, loss = 0.573738157749176
In grad_steps = 852, loss = 0.4350552558898926
In grad_steps = 853, loss = 0.36056530475616455
In grad_steps = 854, loss = 0.3702666163444519
In grad_steps = 855, loss = 0.6102287769317627
In grad_steps = 856, loss = 0.45641207695007324
In grad_steps = 857, loss = 0.3014920651912689
In grad_steps = 858, loss = 0.5635616183280945
In grad_steps = 859, loss = 0.40309780836105347
In grad_steps = 860, loss = 0.42024359107017517
In grad_steps = 861, loss = 0.36036649346351624
In grad_steps = 862, loss = 0.2229217290878296
In grad_steps = 863, loss = 0.2564884126186371
In grad_steps = 864, loss = 0.2623671293258667
In grad_steps = 865, loss = 0.4616534113883972
In grad_steps = 866, loss = 0.1929321587085724
In grad_steps = 867, loss = 0.1930859088897705
In grad_steps = 868, loss = 0.09303547441959381
In grad_steps = 869, loss = 0.35488438606262207
In grad_steps = 870, loss = 0.0515349917113781
In grad_steps = 871, loss = 0.18322879076004028
In grad_steps = 872, loss = 0.30516788363456726
In grad_steps = 873, loss = 0.6775042414665222
In grad_steps = 874, loss = 0.030523106455802917
In grad_steps = 875, loss = 0.03683082014322281
In grad_steps = 876, loss = 0.5298629999160767
In grad_steps = 877, loss = 0.1913364976644516
In grad_steps = 878, loss = 0.3639499545097351
In grad_steps = 879, loss = 0.4435802400112152
In grad_steps = 880, loss = 1.3255269527435303
In grad_steps = 881, loss = 0.32897335290908813
In grad_steps = 882, loss = 0.14941725134849548
In grad_steps = 883, loss = 0.16926464438438416
In grad_steps = 884, loss = 0.10109560191631317
In grad_steps = 885, loss = 0.21206670999526978
In grad_steps = 886, loss = 0.8668605089187622
In grad_steps = 887, loss = 0.08157747238874435
In grad_steps = 888, loss = 0.34151941537857056
In grad_steps = 889, loss = 0.8863815069198608
In grad_steps = 890, loss = 0.2728421688079834
In grad_steps = 891, loss = 0.5574429035186768
In grad_steps = 892, loss = 1.3683593273162842
In grad_steps = 893, loss = 0.3750554025173187
In grad_steps = 894, loss = 0.162332221865654
In grad_steps = 895, loss = 0.27684301137924194
In grad_steps = 896, loss = 0.41546234488487244
In grad_steps = 897, loss = 0.5550590753555298
In grad_steps = 898, loss = 0.6215593814849854
In grad_steps = 899, loss = 0.2642931044101715
In grad_steps = 900, loss = 0.344556987285614
In grad_steps = 901, loss = 0.6032400131225586
In grad_steps = 902, loss = 0.30176156759262085
In grad_steps = 903, loss = 0.15847563743591309
In grad_steps = 904, loss = 0.2758576273918152
In grad_steps = 905, loss = 0.4563359022140503
In grad_steps = 906, loss = 0.23482537269592285
In grad_steps = 907, loss = 0.26528242230415344
In grad_steps = 908, loss = 0.15381771326065063
In grad_steps = 909, loss = 0.5340148210525513
In grad_steps = 910, loss = 0.09872965514659882
In grad_steps = 911, loss = 0.13980484008789062
In grad_steps = 912, loss = 0.105275459587574
In grad_steps = 913, loss = 0.6464803218841553
In grad_steps = 914, loss = 0.5466238260269165
In grad_steps = 915, loss = 0.14555281400680542
In grad_steps = 916, loss = 0.1388271152973175
In grad_steps = 917, loss = 0.13853785395622253
In grad_steps = 918, loss = 1.0596837997436523
In grad_steps = 919, loss = 1.6945216655731201
In grad_steps = 920, loss = 0.9506683349609375
In grad_steps = 921, loss = 0.494855135679245
In grad_steps = 922, loss = 0.04595094919204712
In grad_steps = 923, loss = 1.0936695337295532
In grad_steps = 924, loss = 0.9409855604171753
In grad_steps = 925, loss = 0.13623419404029846
In grad_steps = 926, loss = 0.24802997708320618
In grad_steps = 927, loss = 0.4049225449562073
In grad_steps = 928, loss = 0.8992612957954407
In grad_steps = 929, loss = 0.21827760338783264
In grad_steps = 930, loss = 0.35290223360061646
In grad_steps = 931, loss = 0.4352952837944031
In grad_steps = 932, loss = 0.5680553317070007
In grad_steps = 933, loss = 0.5427572727203369
In grad_steps = 934, loss = 0.1703115850687027
In grad_steps = 935, loss = 0.1844174563884735
In grad_steps = 936, loss = 0.24749460816383362
In grad_steps = 937, loss = 0.41151830554008484
In grad_steps = 938, loss = 0.23493634164333344
In grad_steps = 939, loss = 0.4594283699989319
In grad_steps = 940, loss = 0.5469707250595093
In grad_steps = 941, loss = 0.26543810963630676
In grad_steps = 942, loss = 0.18347463011741638
In grad_steps = 943, loss = 0.5821502208709717
In grad_steps = 944, loss = 0.24301806092262268
In grad_steps = 945, loss = 0.1534610539674759
In grad_steps = 946, loss = 0.29159173369407654
In grad_steps = 947, loss = 0.6400958299636841
In grad_steps = 948, loss = 0.4776095449924469
In grad_steps = 949, loss = 0.6782035231590271
In grad_steps = 950, loss = 0.2542065680027008
In grad_steps = 951, loss = 0.8267344832420349
In grad_steps = 952, loss = 0.1794525682926178
In grad_steps = 953, loss = 0.7485485076904297
In grad_steps = 954, loss = 0.6174845695495605
In grad_steps = 955, loss = 0.4604456126689911
In grad_steps = 956, loss = 0.8361942172050476
In grad_steps = 957, loss = 0.6224893927574158
In grad_steps = 958, loss = 0.3677018880844116
In grad_steps = 959, loss = 0.20974040031433105
In grad_steps = 960, loss = 0.22468793392181396
In grad_steps = 961, loss = 0.3670876622200012
In grad_steps = 962, loss = 0.5886797904968262
In grad_steps = 963, loss = 0.9208069443702698
In grad_steps = 964, loss = 0.25378262996673584
In grad_steps = 965, loss = 0.553458571434021
In grad_steps = 966, loss = 0.4197329878807068
In grad_steps = 967, loss = 0.7569824457168579
In grad_steps = 968, loss = 0.2539502680301666
In grad_steps = 969, loss = 0.26788437366485596
In grad_steps = 970, loss = 0.32027849555015564
In grad_steps = 971, loss = 0.3844481110572815
In grad_steps = 972, loss = 0.34184104204177856
In grad_steps = 973, loss = 0.424275279045105
In grad_steps = 974, loss = 0.11142496764659882
In grad_steps = 975, loss = 0.3585963547229767
In grad_steps = 976, loss = 0.23452329635620117
In grad_steps = 977, loss = 0.21040701866149902
In grad_steps = 978, loss = 0.1599261611700058
In grad_steps = 979, loss = 0.5773975849151611
In grad_steps = 980, loss = 0.25551339983940125
In grad_steps = 981, loss = 0.46574264764785767
In grad_steps = 982, loss = 0.25502997636795044
In grad_steps = 983, loss = 0.23967108130455017
In grad_steps = 984, loss = 0.03598703071475029
In grad_steps = 985, loss = 0.2485862821340561
In grad_steps = 986, loss = 0.9097261428833008
In grad_steps = 987, loss = 0.17262046039104462
In grad_steps = 988, loss = 0.03758532181382179
In grad_steps = 989, loss = 0.022558873519301414
In grad_steps = 990, loss = 0.3760105073451996
In grad_steps = 991, loss = 0.24740366637706757
In grad_steps = 992, loss = 1.286933183670044
In grad_steps = 993, loss = 0.4919412136077881
In grad_steps = 994, loss = 0.4292300045490265
In grad_steps = 995, loss = 0.028445998206734657
In grad_steps = 996, loss = 0.12801216542720795
In grad_steps = 997, loss = 0.11232563853263855
In grad_steps = 998, loss = 0.0435774065554142
In grad_steps = 999, loss = 0.5260590314865112
In grad_steps = 1000, loss = 0.30924445390701294
In grad_steps = 1001, loss = 0.13868562877178192
In grad_steps = 1002, loss = 0.09913889318704605
In grad_steps = 1003, loss = 0.7360978126525879
In grad_steps = 1004, loss = 0.37153929471969604
In grad_steps = 1005, loss = 0.15107348561286926
In grad_steps = 1006, loss = 0.11847809702157974
In grad_steps = 1007, loss = 0.1765957623720169
In grad_steps = 1008, loss = 0.05611847713589668
In grad_steps = 1009, loss = 0.03500698506832123
In grad_steps = 1010, loss = 0.017534207552671432
In grad_steps = 1011, loss = 0.5983753800392151
In grad_steps = 1012, loss = 1.034629225730896
In grad_steps = 1013, loss = 0.08187631517648697
In grad_steps = 1014, loss = 0.7373148202896118
In grad_steps = 1015, loss = 0.56768399477005
In grad_steps = 1016, loss = 0.22296108305454254
In grad_steps = 1017, loss = 1.3486534357070923
In grad_steps = 1018, loss = 0.20974671840667725
In grad_steps = 1019, loss = 1.1336404085159302
In grad_steps = 1020, loss = 0.3053170442581177
In grad_steps = 1021, loss = 0.3164682388305664
In grad_steps = 1022, loss = 0.6030339002609253
In grad_steps = 1023, loss = 0.14895643293857574
In grad_steps = 1024, loss = 0.72525954246521
In grad_steps = 1025, loss = 0.8787814974784851
In grad_steps = 1026, loss = 0.9035132527351379
In grad_steps = 1027, loss = 0.2703363299369812
In grad_steps = 1028, loss = 0.32863378524780273
In grad_steps = 1029, loss = 0.6807859539985657
In grad_steps = 1030, loss = 0.5382219552993774
In grad_steps = 1031, loss = 0.6051579713821411
In grad_steps = 1032, loss = 0.17924413084983826
In grad_steps = 1033, loss = 0.665667712688446
In grad_steps = 1034, loss = 0.28823044896125793
In grad_steps = 1035, loss = 0.5108770132064819
In grad_steps = 1036, loss = 0.35349300503730774
In grad_steps = 1037, loss = 0.30465784668922424
In grad_steps = 1038, loss = 0.3548963963985443
In grad_steps = 1039, loss = 0.19479985535144806
In grad_steps = 1040, loss = 0.5217006802558899
In grad_steps = 1041, loss = 0.11513593047857285
In grad_steps = 1042, loss = 0.16406826674938202
In grad_steps = 1043, loss = 0.17550501227378845
In grad_steps = 1044, loss = 0.7828066945075989
In grad_steps = 1045, loss = 0.06721913814544678
In grad_steps = 1046, loss = 0.6055868268013
In grad_steps = 1047, loss = 0.06139293685555458
In grad_steps = 1048, loss = 0.19517940282821655
In grad_steps = 1049, loss = 0.18353623151779175
In grad_steps = 1050, loss = 0.7021413445472717
In grad_steps = 1051, loss = 0.3196331560611725
In grad_steps = 1052, loss = 0.8270354270935059
In grad_steps = 1053, loss = 0.04341235011816025
In grad_steps = 1054, loss = 0.573419451713562
In grad_steps = 1055, loss = 0.7766509652137756
In grad_steps = 1056, loss = 0.11641241610050201
In grad_steps = 1057, loss = 0.20127171277999878
In grad_steps = 1058, loss = 0.2669619917869568
In grad_steps = 1059, loss = 0.4420480728149414
In grad_steps = 1060, loss = 0.09211767464876175
In grad_steps = 1061, loss = 0.1186678558588028
In grad_steps = 1062, loss = 0.056731611490249634
In grad_steps = 1063, loss = 0.6605648398399353
In grad_steps = 1064, loss = 0.3105928599834442
In grad_steps = 1065, loss = 0.3608487844467163
In grad_steps = 1066, loss = 0.5822480916976929
In grad_steps = 1067, loss = 0.33129626512527466
In grad_steps = 1068, loss = 0.032175518572330475
In grad_steps = 1069, loss = 0.6913895010948181
In grad_steps = 1070, loss = 0.37937113642692566
In grad_steps = 1071, loss = 0.18384280800819397
In grad_steps = 1072, loss = 0.1377449482679367
In grad_steps = 1073, loss = 1.0461745262145996
In grad_steps = 1074, loss = 0.4745838940143585
In grad_steps = 1075, loss = 0.05275143310427666
In grad_steps = 1076, loss = 0.23754030466079712
In grad_steps = 1077, loss = 0.6912127733230591
In grad_steps = 1078, loss = 0.43856281042099
In grad_steps = 1079, loss = 0.7981299161911011
In grad_steps = 1080, loss = 0.1295548975467682
In grad_steps = 1081, loss = 0.19500333070755005
In grad_steps = 1082, loss = 0.6541455984115601
In grad_steps = 1083, loss = 0.19880971312522888
In grad_steps = 1084, loss = 0.23271173238754272
In grad_steps = 1085, loss = 0.36947140097618103
In grad_steps = 1086, loss = 0.17435401678085327
In grad_steps = 1087, loss = 0.2664185166358948
In grad_steps = 1088, loss = 0.4782524108886719
In grad_steps = 1089, loss = 0.21143023669719696
In grad_steps = 1090, loss = 0.3845883011817932
In grad_steps = 1091, loss = 0.4151749312877655
In grad_steps = 1092, loss = 0.5324830412864685
In grad_steps = 1093, loss = 0.2802702784538269
In grad_steps = 1094, loss = 0.44889992475509644
In grad_steps = 1095, loss = 0.24713864922523499
In grad_steps = 1096, loss = 0.12757590413093567
In grad_steps = 1097, loss = 0.09684623777866364
In grad_steps = 1098, loss = 0.2300909161567688
In grad_steps = 1099, loss = 1.0873693227767944
In grad_steps = 1100, loss = 0.38548174500465393
In grad_steps = 1101, loss = 0.1952480673789978
In grad_steps = 1102, loss = 0.2552666664123535
In grad_steps = 1103, loss = 0.4773588180541992
In grad_steps = 1104, loss = 0.12151645869016647
In grad_steps = 1105, loss = 0.07474011182785034
In grad_steps = 1106, loss = 0.4195204973220825
In grad_steps = 1107, loss = 0.36312851309776306
In grad_steps = 1108, loss = 0.5171781778335571
In grad_steps = 1109, loss = 0.15891411900520325
In grad_steps = 1110, loss = 0.5508989095687866
In grad_steps = 1111, loss = 0.11231103539466858
In grad_steps = 1112, loss = 1.296736717224121
In grad_steps = 1113, loss = 0.8408259153366089
In grad_steps = 1114, loss = 1.3502891063690186
In grad_steps = 1115, loss = 0.15252481400966644
In grad_steps = 1116, loss = 0.18262481689453125
In grad_steps = 1117, loss = 0.29268914461135864
In grad_steps = 1118, loss = 0.16556057333946228
In grad_steps = 1119, loss = 0.24485507607460022
In grad_steps = 1120, loss = 0.12468481063842773
In grad_steps = 1121, loss = 0.13280439376831055
In grad_steps = 1122, loss = 0.22389855980873108
In grad_steps = 1123, loss = 0.5273928642272949
In grad_steps = 1124, loss = 0.6691553592681885
In grad_steps = 1125, loss = 0.13277733325958252
In grad_steps = 1126, loss = 0.14933735132217407
In grad_steps = 1127, loss = 0.23016563057899475
In grad_steps = 1128, loss = 0.05228331685066223
In grad_steps = 1129, loss = 0.3560788035392761
In grad_steps = 1130, loss = 0.7458964586257935
In grad_steps = 1131, loss = 0.8760459423065186
In grad_steps = 1132, loss = 0.22205263376235962
In grad_steps = 1133, loss = 0.0783342570066452
In grad_steps = 1134, loss = 0.5815753936767578
In grad_steps = 1135, loss = 0.8821324110031128
In grad_steps = 1136, loss = 0.15676862001419067
In grad_steps = 1137, loss = 0.3761720359325409
In grad_steps = 1138, loss = 0.47320225834846497
In grad_steps = 1139, loss = 0.29106438159942627
In grad_steps = 1140, loss = 0.12908251583576202
In grad_steps = 1141, loss = 0.41747161746025085
In grad_steps = 1142, loss = 0.5400311350822449
In grad_steps = 1143, loss = 0.24387191236019135
In grad_steps = 1144, loss = 0.1351298838853836
In grad_steps = 1145, loss = 0.2975695729255676
In grad_steps = 1146, loss = 0.21539174020290375
In grad_steps = 1147, loss = 0.37540900707244873
In grad_steps = 1148, loss = 0.4410628378391266
In grad_steps = 1149, loss = 0.13903746008872986
In grad_steps = 1150, loss = 0.07807114720344543
In grad_steps = 1151, loss = 0.7662084698677063
In grad_steps = 1152, loss = 0.048180192708969116
In grad_steps = 1153, loss = 0.2786145508289337
In grad_steps = 1154, loss = 0.05441157892346382
In grad_steps = 1155, loss = 0.48169147968292236
In grad_steps = 1156, loss = 0.7004931569099426
In grad_steps = 1157, loss = 0.05857906863093376
In grad_steps = 1158, loss = 0.2026565670967102
In grad_steps = 1159, loss = 0.050508324056863785
In grad_steps = 1160, loss = 0.24085386097431183
In grad_steps = 1161, loss = 0.2719367444515228
In grad_steps = 1162, loss = 0.07079731673002243
In grad_steps = 1163, loss = 0.12484700977802277
In grad_steps = 1164, loss = 0.07752171158790588
In grad_steps = 1165, loss = 0.6453583836555481
In grad_steps = 1166, loss = 0.32125231623649597
In grad_steps = 1167, loss = 0.11229436099529266
In grad_steps = 1168, loss = 0.8125066757202148
In grad_steps = 1169, loss = 0.6319065093994141
In grad_steps = 1170, loss = 0.2055140733718872
In grad_steps = 1171, loss = 0.3152661919593811
In grad_steps = 1172, loss = 0.060228172689676285
In grad_steps = 1173, loss = 0.32636576890945435
In grad_steps = 1174, loss = 1.519221544265747
In grad_steps = 1175, loss = 1.0797185897827148
In grad_steps = 1176, loss = 0.7101965546607971
In grad_steps = 1177, loss = 0.18504822254180908
In grad_steps = 1178, loss = 0.0580110102891922
In grad_steps = 1179, loss = 0.5865006446838379
In grad_steps = 1180, loss = 0.38953253626823425
In grad_steps = 1181, loss = 0.852858304977417
In grad_steps = 1182, loss = 0.6544227004051208
In grad_steps = 1183, loss = 0.5439008474349976
In grad_steps = 1184, loss = 0.17034085094928741
In grad_steps = 1185, loss = 0.8787090182304382
In grad_steps = 1186, loss = 0.5727434158325195
In grad_steps = 1187, loss = 0.8229215145111084
In grad_steps = 1188, loss = 0.16157737374305725
In grad_steps = 1189, loss = 0.640398383140564
In grad_steps = 1190, loss = 0.4938003122806549
In grad_steps = 1191, loss = 0.4635316729545593
In grad_steps = 1192, loss = 0.38587477803230286
In grad_steps = 1193, loss = 0.14593060314655304
In grad_steps = 1194, loss = 0.2269642949104309
In grad_steps = 1195, loss = 0.7483497262001038
In grad_steps = 1196, loss = 0.8076563477516174
In grad_steps = 1197, loss = 0.3091440796852112
In grad_steps = 1198, loss = 0.2231963872909546
In grad_steps = 1199, loss = 0.623085618019104
In grad_steps = 1200, loss = 0.4650149941444397
In grad_steps = 1201, loss = 0.3680894374847412
In grad_steps = 1202, loss = 0.13092362880706787
In grad_steps = 1203, loss = 0.28522491455078125
In grad_steps = 1204, loss = 0.21603497862815857
In grad_steps = 1205, loss = 0.16924305260181427
In grad_steps = 1206, loss = 0.6476043462753296
In grad_steps = 1207, loss = 0.3192616105079651
In grad_steps = 1208, loss = 0.09170868992805481
In grad_steps = 1209, loss = 0.6881958842277527
In grad_steps = 1210, loss = 0.0860312208533287
In grad_steps = 1211, loss = 0.5063472986221313
In grad_steps = 1212, loss = 0.04556354880332947
In grad_steps = 1213, loss = 0.5524309277534485
In grad_steps = 1214, loss = 0.4196022152900696
In grad_steps = 1215, loss = 0.2548265755176544
In grad_steps = 1216, loss = 0.07658591866493225
In grad_steps = 1217, loss = 0.04486808180809021
In grad_steps = 1218, loss = 0.03697824850678444
In grad_steps = 1219, loss = 0.04116152971982956
In grad_steps = 1220, loss = 0.5887489914894104
In grad_steps = 1221, loss = 0.4235747456550598
In grad_steps = 1222, loss = 0.23646759986877441
In grad_steps = 1223, loss = 1.0255295038223267
In grad_steps = 1224, loss = 0.019474269822239876
In grad_steps = 1225, loss = 0.3907651901245117
In grad_steps = 1226, loss = 0.09754186123609543
In grad_steps = 1227, loss = 0.03529330715537071
In grad_steps = 1228, loss = 0.617713987827301
In grad_steps = 1229, loss = 0.2277182638645172
In grad_steps = 1230, loss = 0.04985271021723747
In grad_steps = 1231, loss = 0.8144000768661499
In grad_steps = 1232, loss = 0.37786802649497986
In grad_steps = 1233, loss = 0.0407387837767601
In grad_steps = 1234, loss = 0.262920618057251
In grad_steps = 1235, loss = 1.051877737045288
In grad_steps = 1236, loss = 0.16355754435062408
In grad_steps = 1237, loss = 0.5992467999458313
In grad_steps = 1238, loss = 0.5014927983283997
In grad_steps = 1239, loss = 0.5957486629486084
In grad_steps = 1240, loss = 0.8010625839233398
In grad_steps = 1241, loss = 0.5425205826759338
In grad_steps = 1242, loss = 0.25651079416275024
In grad_steps = 1243, loss = 0.9998883008956909
In grad_steps = 1244, loss = 0.4532526731491089
In grad_steps = 1245, loss = 0.26659467816352844
In grad_steps = 1246, loss = 0.3478260636329651
In grad_steps = 1247, loss = 0.4077233076095581
In grad_steps = 1248, loss = 0.8251941204071045
In grad_steps = 1249, loss = 0.24797707796096802
In grad_steps = 1250, loss = 0.24797844886779785
In grad_steps = 1251, loss = 0.3143625557422638
In grad_steps = 1252, loss = 0.4939557611942291
In grad_steps = 1253, loss = 0.8632014393806458
In grad_steps = 1254, loss = 0.2368607521057129
In grad_steps = 1255, loss = 0.19551733136177063
In grad_steps = 1256, loss = 0.49036136269569397
In grad_steps = 1257, loss = 0.08101767301559448
In grad_steps = 1258, loss = 0.860436737537384
In grad_steps = 1259, loss = 0.22756384313106537
In grad_steps = 1260, loss = 0.5220783352851868
In grad_steps = 1261, loss = 0.24526625871658325
In grad_steps = 1262, loss = 0.08296018838882446
In grad_steps = 1263, loss = 0.3111063241958618
In grad_steps = 1264, loss = 0.7956632375717163
In grad_steps = 1265, loss = 0.20604464411735535
In grad_steps = 1266, loss = 0.8683887124061584
In grad_steps = 1267, loss = 0.2324313223361969
In grad_steps = 1268, loss = 0.1520462930202484
In grad_steps = 1269, loss = 0.29307955503463745
In grad_steps = 1270, loss = 0.45379114151000977
In grad_steps = 1271, loss = 0.6168569922447205
In grad_steps = 1272, loss = 0.03759118169546127
In grad_steps = 1273, loss = 0.19064933061599731
In grad_steps = 1274, loss = 0.2116057127714157
In grad_steps = 1275, loss = 0.06512659788131714
In grad_steps = 1276, loss = 0.4367285370826721
In grad_steps = 1277, loss = 0.15669693052768707
In grad_steps = 1278, loss = 0.051501527428627014
In grad_steps = 1279, loss = 0.029109816998243332
In grad_steps = 1280, loss = 0.054314985871315
In grad_steps = 1281, loss = 0.08417712152004242
In grad_steps = 1282, loss = 0.4938150644302368
In grad_steps = 1283, loss = 0.03758544474840164
In grad_steps = 1284, loss = 0.0337115079164505
In grad_steps = 1285, loss = 0.7713125944137573
In grad_steps = 1286, loss = 0.3494669795036316
In grad_steps = 1287, loss = 0.708297073841095
In grad_steps = 1288, loss = 0.2147008627653122
In grad_steps = 1289, loss = 0.22283536195755005
In grad_steps = 1290, loss = 0.6723121404647827
In grad_steps = 1291, loss = 0.07651650160551071
In grad_steps = 1292, loss = 0.4156368672847748
In grad_steps = 1293, loss = 0.12098383903503418
In grad_steps = 1294, loss = 0.7831500172615051
In grad_steps = 1295, loss = 0.22777879238128662
In grad_steps = 1296, loss = 0.5659008026123047
In grad_steps = 1297, loss = 0.24144816398620605
In grad_steps = 1298, loss = 0.1423606276512146
In grad_steps = 1299, loss = 0.5267508029937744
In grad_steps = 1300, loss = 0.7980233430862427
In grad_steps = 1301, loss = 0.027568669989705086
In grad_steps = 1302, loss = 0.48161688446998596
In grad_steps = 1303, loss = 0.20672361552715302
In grad_steps = 1304, loss = 0.8376539349555969
In grad_steps = 1305, loss = 0.0494801327586174
In grad_steps = 1306, loss = 0.3226456940174103
In grad_steps = 1307, loss = 0.13753747940063477
In grad_steps = 1308, loss = 0.3386392295360565
In grad_steps = 1309, loss = 0.3164011240005493
In grad_steps = 1310, loss = 0.29702579975128174
In grad_steps = 1311, loss = 0.029545478522777557
In grad_steps = 1312, loss = 0.838600754737854
In grad_steps = 1313, loss = 0.0229979045689106
In grad_steps = 1314, loss = 0.4136315584182739
In grad_steps = 1315, loss = 0.050132911652326584
In grad_steps = 1316, loss = 1.1546053886413574
In grad_steps = 1317, loss = 0.2605833411216736
In grad_steps = 1318, loss = 0.41075533628463745
In grad_steps = 1319, loss = 0.41866055130958557
In grad_steps = 1320, loss = 0.34846311807632446
In grad_steps = 1321, loss = 0.38302862644195557
In grad_steps = 1322, loss = 0.5542885661125183
In grad_steps = 1323, loss = 0.06692689657211304
In grad_steps = 1324, loss = 0.6726852059364319
In grad_steps = 1325, loss = 0.3810092806816101
In grad_steps = 1326, loss = 0.5425198078155518
In grad_steps = 1327, loss = 0.3261184096336365
In grad_steps = 1328, loss = 0.9285763502120972
In grad_steps = 1329, loss = 0.07290489971637726
In grad_steps = 1330, loss = 0.25920429825782776
In grad_steps = 1331, loss = 0.13750320672988892
In grad_steps = 1332, loss = 0.4737934470176697
In grad_steps = 1333, loss = 0.06481407582759857
In grad_steps = 1334, loss = 0.12902803719043732
In grad_steps = 1335, loss = 0.2615341246128082
In grad_steps = 1336, loss = 0.099813312292099
In grad_steps = 1337, loss = 0.28802987933158875
In grad_steps = 1338, loss = 0.8396787047386169
In grad_steps = 1339, loss = 0.6990710496902466
In grad_steps = 1340, loss = 0.5746361017227173
In grad_steps = 1341, loss = 0.3015950322151184
In grad_steps = 1342, loss = 0.1193830668926239
In grad_steps = 1343, loss = 0.24128657579421997
In grad_steps = 1344, loss = 0.16856248676776886
In grad_steps = 1345, loss = 0.49267393350601196
In grad_steps = 1346, loss = 0.12342041730880737
In grad_steps = 1347, loss = 0.1662347912788391
In grad_steps = 1348, loss = 0.2856563329696655
In grad_steps = 1349, loss = 0.05543479323387146
In grad_steps = 1350, loss = 0.05700470507144928
In grad_steps = 1351, loss = 0.1419738233089447
In grad_steps = 1352, loss = 0.12540364265441895
In grad_steps = 1353, loss = 0.1303253471851349
In grad_steps = 1354, loss = 0.5269145965576172
In grad_steps = 1355, loss = 0.36656755208969116
In grad_steps = 1356, loss = 0.024152155965566635
In grad_steps = 1357, loss = 0.19386282563209534
In grad_steps = 1358, loss = 0.8760446906089783
In grad_steps = 1359, loss = 0.05472826957702637
In grad_steps = 1360, loss = 0.6308079361915588
In grad_steps = 1361, loss = 0.0293874554336071
In grad_steps = 1362, loss = 0.024901287630200386
In grad_steps = 1363, loss = 0.028277793899178505
In grad_steps = 1364, loss = 0.41870224475860596
In grad_steps = 1365, loss = 0.16487854719161987
In grad_steps = 1366, loss = 0.35288870334625244
In grad_steps = 1367, loss = 1.0579249858856201
In grad_steps = 1368, loss = 0.21101394295692444
In grad_steps = 1369, loss = 0.2631218433380127
In grad_steps = 1370, loss = 0.4249778687953949
In grad_steps = 1371, loss = 0.13280469179153442
In grad_steps = 1372, loss = 1.452765703201294
In grad_steps = 1373, loss = 0.4975023865699768
In grad_steps = 1374, loss = 0.030054979026317596
In grad_steps = 1375, loss = 0.311563640832901
In grad_steps = 1376, loss = 0.14907348155975342
In grad_steps = 1377, loss = 0.5241650938987732
In grad_steps = 1378, loss = 0.05583152547478676
In grad_steps = 1379, loss = 0.6562021970748901
In grad_steps = 1380, loss = 0.11301020532846451
In grad_steps = 1381, loss = 0.1943741887807846
In grad_steps = 1382, loss = 1.3529326915740967
In grad_steps = 1383, loss = 0.4030812382698059
In grad_steps = 1384, loss = 0.036467295140028
In grad_steps = 1385, loss = 0.37551790475845337
In grad_steps = 1386, loss = 0.0705321803689003
In grad_steps = 1387, loss = 0.2208479940891266
In grad_steps = 1388, loss = 0.7994431853294373
In grad_steps = 1389, loss = 0.03170322626829147
In grad_steps = 1390, loss = 0.10386183112859726
In grad_steps = 1391, loss = 0.0639522522687912
In grad_steps = 1392, loss = 0.05674697831273079
In grad_steps = 1393, loss = 0.8592333197593689
In grad_steps = 1394, loss = 0.2328818142414093
In grad_steps = 1395, loss = 0.28792086243629456
In grad_steps = 1396, loss = 0.28530994057655334
In grad_steps = 1397, loss = 0.8859533667564392
In grad_steps = 1398, loss = 0.10988076776266098
In grad_steps = 1399, loss = 0.48533713817596436
In grad_steps = 1400, loss = 0.4139927923679352
In grad_steps = 1401, loss = 0.7325782179832458
In grad_steps = 1402, loss = 0.08789297938346863
In grad_steps = 1403, loss = 0.27874988317489624
In grad_steps = 1404, loss = 0.2209416925907135
In grad_steps = 1405, loss = 0.08538901805877686
In grad_steps = 1406, loss = 0.3838557302951813
In grad_steps = 1407, loss = 0.5332427024841309
In grad_steps = 1408, loss = 0.7576581239700317
In grad_steps = 1409, loss = 0.3168286979198456
In grad_steps = 1410, loss = 0.3749659061431885
In grad_steps = 1411, loss = 0.11044642329216003
In grad_steps = 1412, loss = 0.19381658732891083
In grad_steps = 1413, loss = 0.19936330616474152
In grad_steps = 1414, loss = 0.061605483293533325
In grad_steps = 1415, loss = 0.12502653896808624
In grad_steps = 1416, loss = 0.30515021085739136
In grad_steps = 1417, loss = 0.5414702892303467
In grad_steps = 1418, loss = 0.09521520137786865
In grad_steps = 1419, loss = 0.28740930557250977
In grad_steps = 1420, loss = 0.32192549109458923
In grad_steps = 1421, loss = 0.7308271527290344
In grad_steps = 1422, loss = 0.13208484649658203
In grad_steps = 1423, loss = 0.19636058807373047
In grad_steps = 1424, loss = 0.5873457193374634
In grad_steps = 1425, loss = 0.24114471673965454
In grad_steps = 1426, loss = 0.09635628759860992
In grad_steps = 1427, loss = 0.048658955842256546
In grad_steps = 1428, loss = 0.007253466174006462
In grad_steps = 1429, loss = 1.2015070915222168
In grad_steps = 1430, loss = 1.3488095998764038
In grad_steps = 1431, loss = 0.275704950094223
In grad_steps = 1432, loss = 0.9098694324493408
In grad_steps = 1433, loss = 0.4860561788082123
In grad_steps = 1434, loss = 0.5516145825386047
In grad_steps = 1435, loss = 0.09928151965141296
In grad_steps = 1436, loss = 1.5342129468917847
In grad_steps = 1437, loss = 0.7055897116661072
In grad_steps = 1438, loss = 0.08212190866470337
In grad_steps = 1439, loss = 0.14715686440467834
In grad_steps = 1440, loss = 0.6227893829345703
In grad_steps = 1441, loss = 0.2263360619544983
In grad_steps = 1442, loss = 0.07750953733921051
In grad_steps = 1443, loss = 0.34855055809020996
In grad_steps = 1444, loss = 0.6522202491760254
In grad_steps = 1445, loss = 0.32306885719299316
In grad_steps = 1446, loss = 0.18599826097488403
In grad_steps = 1447, loss = 0.25747373700141907
In grad_steps = 1448, loss = 0.2790667414665222
In grad_steps = 1449, loss = 0.6218807101249695
In grad_steps = 1450, loss = 0.20282764732837677
In grad_steps = 1451, loss = 0.2606761157512665
In grad_steps = 1452, loss = 0.4569806158542633
In grad_steps = 1453, loss = 0.09804017841815948
In grad_steps = 1454, loss = 0.37171319127082825
In grad_steps = 1455, loss = 0.470417320728302
In grad_steps = 1456, loss = 0.5010660290718079
In grad_steps = 1457, loss = 0.6068036556243896
In grad_steps = 1458, loss = 0.358276903629303
In grad_steps = 1459, loss = 0.2533263862133026
In grad_steps = 1460, loss = 0.042275216430425644
In grad_steps = 1461, loss = 0.5294706225395203
In grad_steps = 1462, loss = 0.2571920156478882
In grad_steps = 1463, loss = 0.46277695894241333
In grad_steps = 1464, loss = 0.12300410121679306
In grad_steps = 1465, loss = 1.1149520874023438
In grad_steps = 1466, loss = 0.3401440978050232
In grad_steps = 1467, loss = 0.06339386105537415
In grad_steps = 1468, loss = 0.16672776639461517
In grad_steps = 1469, loss = 0.5756868720054626
In grad_steps = 1470, loss = 0.05221458896994591
In grad_steps = 1471, loss = 0.2966248393058777
In grad_steps = 1472, loss = 0.728155255317688
In grad_steps = 1473, loss = 0.11484265327453613
In grad_steps = 1474, loss = 0.23629917204380035
In grad_steps = 1475, loss = 1.2905495166778564
In grad_steps = 1476, loss = 0.3196324110031128
In grad_steps = 1477, loss = 0.22936831414699554
In grad_steps = 1478, loss = 0.8256050944328308
In grad_steps = 1479, loss = 0.35480305552482605
In grad_steps = 1480, loss = 0.42593878507614136
In grad_steps = 1481, loss = 0.26931771636009216
In grad_steps = 1482, loss = 0.0868300050497055
In grad_steps = 1483, loss = 0.5078659057617188
In grad_steps = 1484, loss = 0.12466628104448318
In grad_steps = 1485, loss = 0.12522538006305695
In grad_steps = 1486, loss = 0.4821934401988983
In grad_steps = 1487, loss = 0.21471047401428223
In grad_steps = 1488, loss = 0.09613919258117676
In grad_steps = 1489, loss = 0.6700819730758667
In grad_steps = 1490, loss = 0.10788805782794952
In grad_steps = 1491, loss = 0.34092748165130615
In grad_steps = 1492, loss = 0.1055525541305542
In grad_steps = 1493, loss = 0.2798379063606262
In grad_steps = 1494, loss = 0.4958875775337219
In grad_steps = 1495, loss = 0.35308024287223816
In grad_steps = 1496, loss = 0.32648929953575134
In grad_steps = 1497, loss = 0.327121764421463
In grad_steps = 1498, loss = 0.3030613958835602
In grad_steps = 1499, loss = 0.06292147934436798
In grad_steps = 1500, loss = 0.42306438088417053
In grad_steps = 1501, loss = 0.5523320436477661
In grad_steps = 1502, loss = 0.05589524656534195
In grad_steps = 1503, loss = 0.6502566337585449
In grad_steps = 1504, loss = 0.12341497093439102
In grad_steps = 1505, loss = 0.15113583207130432
In grad_steps = 1506, loss = 0.1665964126586914
In grad_steps = 1507, loss = 0.14509300887584686
In grad_steps = 1508, loss = 0.44777119159698486
In grad_steps = 1509, loss = 0.8991448283195496
In grad_steps = 1510, loss = 0.6868644952774048
In grad_steps = 1511, loss = 0.22753602266311646
In grad_steps = 1512, loss = 0.030817896127700806
In grad_steps = 1513, loss = 1.3199360370635986
In grad_steps = 1514, loss = 0.5413561463356018
In grad_steps = 1515, loss = 0.0979754626750946
In grad_steps = 1516, loss = 0.10429390519857407
In grad_steps = 1517, loss = 0.5418854355812073
In grad_steps = 1518, loss = 0.39575904607772827
In grad_steps = 1519, loss = 0.5608037710189819
In grad_steps = 1520, loss = 0.22825439274311066
In grad_steps = 1521, loss = 0.4336174428462982
In grad_steps = 1522, loss = 0.24373763799667358
In grad_steps = 1523, loss = 0.20853488147258759
In grad_steps = 1524, loss = 0.06991474330425262
In grad_steps = 1525, loss = 0.827754020690918
In grad_steps = 1526, loss = 0.07829894870519638
In grad_steps = 1527, loss = 0.0449698232114315
In grad_steps = 1528, loss = 0.39783865213394165
In grad_steps = 1529, loss = 0.15363259613513947
In grad_steps = 1530, loss = 0.23137766122817993
In grad_steps = 1531, loss = 0.09711989015340805
In grad_steps = 1532, loss = 0.48270413279533386
In grad_steps = 1533, loss = 1.1004728078842163
In grad_steps = 1534, loss = 0.2341189682483673
In grad_steps = 1535, loss = 0.0520811453461647
In grad_steps = 1536, loss = 0.1594265252351761
In grad_steps = 1537, loss = 0.14313402771949768
In grad_steps = 1538, loss = 0.4993319511413574
In grad_steps = 1539, loss = 0.04818153381347656
In grad_steps = 1540, loss = 0.09266471117734909
In grad_steps = 1541, loss = 0.8655192255973816
In grad_steps = 1542, loss = 0.42575064301490784
In grad_steps = 1543, loss = 0.3141416013240814
In grad_steps = 1544, loss = 0.19461648166179657
In grad_steps = 1545, loss = 0.027480699121952057
In grad_steps = 1546, loss = 0.8857686519622803
In grad_steps = 1547, loss = 0.05336768552660942
In grad_steps = 1548, loss = 0.154913991689682
In grad_steps = 1549, loss = 0.08754472434520721
In grad_steps = 1550, loss = 0.06447476148605347
In grad_steps = 1551, loss = 0.17351402342319489
In grad_steps = 1552, loss = 0.40943485498428345
In grad_steps = 1553, loss = 0.10117530822753906
In grad_steps = 1554, loss = 0.7234185934066772
In grad_steps = 1555, loss = 0.03965676948428154
In grad_steps = 1556, loss = 0.0622006393969059
In grad_steps = 1557, loss = 0.7278710603713989
In grad_steps = 1558, loss = 0.43953394889831543
In grad_steps = 1559, loss = 0.6034903526306152
In grad_steps = 1560, loss = 0.6875020265579224
In grad_steps = 1561, loss = 0.1593928337097168
In grad_steps = 1562, loss = 0.10010766983032227
In grad_steps = 1563, loss = 0.19024130702018738
In grad_steps = 1564, loss = 0.10044892132282257
In grad_steps = 1565, loss = 0.24076175689697266
In grad_steps = 1566, loss = 0.9709538221359253
In grad_steps = 1567, loss = 0.16291745007038116
In grad_steps = 1568, loss = 0.36034882068634033
In grad_steps = 1569, loss = 0.254218727350235
In grad_steps = 1570, loss = 0.48849475383758545
In grad_steps = 1571, loss = 0.30272790789604187
In grad_steps = 1572, loss = 0.7237715721130371
In grad_steps = 1573, loss = 0.12881800532341003
In grad_steps = 1574, loss = 0.4486906826496124
In grad_steps = 1575, loss = 0.07472749054431915
In grad_steps = 1576, loss = 0.19391652941703796
In grad_steps = 1577, loss = 0.37450453639030457
In grad_steps = 1578, loss = 0.7888054847717285
In grad_steps = 1579, loss = 0.1131448745727539
In grad_steps = 1580, loss = 0.06196650117635727
In grad_steps = 1581, loss = 0.15751318633556366
In grad_steps = 1582, loss = 0.0891626700758934
In grad_steps = 1583, loss = 0.9258180856704712
In grad_steps = 1584, loss = 0.5449466109275818
In grad_steps = 1585, loss = 1.1569567918777466
In grad_steps = 1586, loss = 0.7211672067642212
In grad_steps = 1587, loss = 0.558403730392456
In grad_steps = 1588, loss = 0.27398136258125305
In grad_steps = 1589, loss = 0.170205220580101
In grad_steps = 1590, loss = 0.17811720073223114
In grad_steps = 1591, loss = 0.03869718313217163
In grad_steps = 1592, loss = 0.5386893153190613
In grad_steps = 1593, loss = 0.1439800262451172
In grad_steps = 1594, loss = 0.10626934468746185
In grad_steps = 1595, loss = 0.5418031215667725
In grad_steps = 1596, loss = 0.35198986530303955
In grad_steps = 1597, loss = 0.26703494787216187
In grad_steps = 1598, loss = 0.14948730170726776
In grad_steps = 1599, loss = 0.5526937246322632
In grad_steps = 1600, loss = 0.49621856212615967
In grad_steps = 1601, loss = 0.38394588232040405
In grad_steps = 1602, loss = 0.36599090695381165
In grad_steps = 1603, loss = 0.0724973976612091
In grad_steps = 1604, loss = 0.2579508125782013
In grad_steps = 1605, loss = 0.11827141046524048
In grad_steps = 1606, loss = 1.317426085472107
In grad_steps = 1607, loss = 0.2461288720369339
In grad_steps = 1608, loss = 0.18802012503147125
In grad_steps = 1609, loss = 0.2994999289512634
In grad_steps = 1610, loss = 0.30433928966522217
In grad_steps = 1611, loss = 0.1446160078048706
In grad_steps = 1612, loss = 0.9801576137542725
In grad_steps = 1613, loss = 0.3181101083755493
In grad_steps = 1614, loss = 0.2850680947303772
In grad_steps = 1615, loss = 0.2847801148891449
In grad_steps = 1616, loss = 0.5078886151313782
In grad_steps = 1617, loss = 0.022248651832342148
In grad_steps = 1618, loss = 0.22109514474868774
In grad_steps = 1619, loss = 0.3433135449886322
In grad_steps = 1620, loss = 0.04275723174214363
In grad_steps = 1621, loss = 0.20030155777931213
In grad_steps = 1622, loss = 0.423154354095459
In grad_steps = 1623, loss = 0.18517601490020752
In grad_steps = 1624, loss = 0.20911432802677155
In grad_steps = 1625, loss = 0.17023944854736328
In grad_steps = 1626, loss = 0.028057042509317398
In grad_steps = 1627, loss = 0.17392638325691223
In grad_steps = 1628, loss = 0.21250012516975403
In grad_steps = 1629, loss = 0.08716882765293121
In grad_steps = 1630, loss = 0.49428001046180725
In grad_steps = 1631, loss = 0.2031823694705963
In grad_steps = 1632, loss = 0.8337733745574951
In grad_steps = 1633, loss = 0.04961265251040459
In grad_steps = 1634, loss = 0.03311111778020859
In grad_steps = 1635, loss = 0.35084211826324463
In grad_steps = 1636, loss = 0.818602979183197
In grad_steps = 1637, loss = 0.01776709221303463
In grad_steps = 1638, loss = 0.050372861325740814
In grad_steps = 1639, loss = 1.8045710325241089
In grad_steps = 1640, loss = 0.2715620696544647
In grad_steps = 1641, loss = 0.07961913198232651
In grad_steps = 1642, loss = 0.77198326587677
In grad_steps = 1643, loss = 0.10523245483636856
In grad_steps = 1644, loss = 0.6602175235748291
In grad_steps = 1645, loss = 0.16679181158542633
In grad_steps = 1646, loss = 0.4749535918235779
In grad_steps = 1647, loss = 0.4129441976547241
In grad_steps = 1648, loss = 0.21452483534812927
In grad_steps = 1649, loss = 0.13052931427955627
In grad_steps = 1650, loss = 0.5209162831306458
In grad_steps = 1651, loss = 0.2745247483253479
In grad_steps = 1652, loss = 0.18812529742717743
In grad_steps = 1653, loss = 0.3182529807090759
In grad_steps = 1654, loss = 0.6652659773826599
In grad_steps = 1655, loss = 0.34451285004615784
In grad_steps = 1656, loss = 0.4662206172943115
In grad_steps = 1657, loss = 0.11317852139472961
In grad_steps = 1658, loss = 0.19085776805877686
In grad_steps = 1659, loss = 0.6841084957122803
In grad_steps = 1660, loss = 0.4795161187648773
In grad_steps = 1661, loss = 0.527778685092926
In grad_steps = 1662, loss = 1.0734758377075195
In grad_steps = 1663, loss = 0.3239615559577942
In grad_steps = 1664, loss = 0.23312777280807495
In grad_steps = 1665, loss = 0.48750174045562744
In grad_steps = 1666, loss = 0.058870818465948105
In grad_steps = 1667, loss = 0.19442959129810333
In grad_steps = 1668, loss = 1.019308090209961
In grad_steps = 1669, loss = 0.34088951349258423
In grad_steps = 1670, loss = 0.12947410345077515
In grad_steps = 1671, loss = 0.2182817906141281
In grad_steps = 1672, loss = 0.7164561152458191
In grad_steps = 1673, loss = 0.4973703622817993
In grad_steps = 1674, loss = 0.15686050057411194
In grad_steps = 1675, loss = 0.7591345310211182
In grad_steps = 1676, loss = 0.400871217250824
In grad_steps = 1677, loss = 0.3254682719707489
In grad_steps = 1678, loss = 0.28586575388908386
In grad_steps = 1679, loss = 0.6277133226394653
In grad_steps = 1680, loss = 0.28669700026512146
In grad_steps = 1681, loss = 0.380191445350647
In grad_steps = 1682, loss = 0.15245266258716583
In grad_steps = 1683, loss = 0.2524564862251282
In grad_steps = 1684, loss = 0.46005019545555115
In grad_steps = 1685, loss = 0.141647070646286
In grad_steps = 1686, loss = 0.4318477213382721
In grad_steps = 1687, loss = 0.3823961019515991
In grad_steps = 1688, loss = 0.4059586226940155
In grad_steps = 1689, loss = 0.1808578372001648
In grad_steps = 1690, loss = 0.09933092445135117
In grad_steps = 1691, loss = 0.2723432183265686
In grad_steps = 1692, loss = 0.20177915692329407
In grad_steps = 1693, loss = 0.3083588182926178
In grad_steps = 1694, loss = 0.24523352086544037
In grad_steps = 1695, loss = 0.11936482042074203
In grad_steps = 1696, loss = 0.20104624330997467
In grad_steps = 1697, loss = 0.23434031009674072
In grad_steps = 1698, loss = 0.02883562445640564
In grad_steps = 1699, loss = 0.13446056842803955
In grad_steps = 1700, loss = 0.09226853400468826
In grad_steps = 1701, loss = 0.8504397869110107
In grad_steps = 1702, loss = 0.02071550488471985
In grad_steps = 1703, loss = 0.1777980923652649
In grad_steps = 1704, loss = 0.10691682994365692
In grad_steps = 1705, loss = 0.2153170108795166
In grad_steps = 1706, loss = 0.4097864627838135
In grad_steps = 1707, loss = 1.2777225971221924
In grad_steps = 1708, loss = 0.6696175336837769
In grad_steps = 1709, loss = 0.057415641844272614
In grad_steps = 1710, loss = 0.13156333565711975
In grad_steps = 1711, loss = 0.030710183084011078
In grad_steps = 1712, loss = 0.31822994351387024
In grad_steps = 1713, loss = 0.9068982005119324
In grad_steps = 1714, loss = 0.11612046509981155
In grad_steps = 1715, loss = 0.521148681640625
In grad_steps = 1716, loss = 0.9480766654014587
In grad_steps = 1717, loss = 0.7988449931144714
In grad_steps = 1718, loss = 0.03274643421173096
In grad_steps = 1719, loss = 0.5564738512039185
In grad_steps = 1720, loss = 0.3725409507751465
In grad_steps = 1721, loss = 0.059538643807172775
In grad_steps = 1722, loss = 0.4680957794189453
In grad_steps = 1723, loss = 0.09464636445045471
In grad_steps = 1724, loss = 0.8473127484321594
In grad_steps = 1725, loss = 0.1155446469783783
In grad_steps = 1726, loss = 0.3272130489349365
In grad_steps = 1727, loss = 0.3090945780277252
In grad_steps = 1728, loss = 0.22220450639724731
In grad_steps = 1729, loss = 0.5094783306121826
In grad_steps = 1730, loss = 0.12893445789813995
In grad_steps = 1731, loss = 0.2533802092075348
In grad_steps = 1732, loss = 0.06989994645118713
In grad_steps = 1733, loss = 0.06394287943840027
In grad_steps = 1734, loss = 0.23424364626407623
In grad_steps = 1735, loss = 0.19742758572101593
In grad_steps = 1736, loss = 0.3216491937637329
In grad_steps = 1737, loss = 0.5791402459144592
In grad_steps = 1738, loss = 0.16092228889465332
In grad_steps = 1739, loss = 0.07158416509628296
In grad_steps = 1740, loss = 0.035272665321826935
In grad_steps = 1741, loss = 0.33264273405075073
In grad_steps = 1742, loss = 0.367418497800827
In grad_steps = 1743, loss = 0.0327305905520916
In grad_steps = 1744, loss = 0.82992023229599
In grad_steps = 1745, loss = 0.2896779179573059
In grad_steps = 1746, loss = 0.7416470646858215
In grad_steps = 1747, loss = 0.03804768621921539
In grad_steps = 1748, loss = 0.6580043435096741
In grad_steps = 1749, loss = 0.02669495716691017
In grad_steps = 1750, loss = 0.9109233617782593
In grad_steps = 1751, loss = 0.20340576767921448
In grad_steps = 1752, loss = 0.1929628849029541
In grad_steps = 1753, loss = 0.15772943198680878
In grad_steps = 1754, loss = 0.10060537606477737
In grad_steps = 1755, loss = 0.06671904772520065
In grad_steps = 1756, loss = 0.595924437046051
In grad_steps = 1757, loss = 1.0308022499084473
In grad_steps = 1758, loss = 0.07704393565654755
In grad_steps = 1759, loss = 0.13881558179855347
In grad_steps = 1760, loss = 0.31773853302001953
In grad_steps = 1761, loss = 0.10293862223625183
In grad_steps = 1762, loss = 0.7124285697937012
In grad_steps = 1763, loss = 0.12194470316171646
In grad_steps = 1764, loss = 0.22601160407066345
In grad_steps = 1765, loss = 0.26076433062553406
In grad_steps = 1766, loss = 0.030790016055107117
In grad_steps = 1767, loss = 0.1076565533876419
In grad_steps = 1768, loss = 0.24442577362060547
In grad_steps = 1769, loss = 0.5561385154724121
In grad_steps = 1770, loss = 0.7465653419494629
In grad_steps = 1771, loss = 0.4691164493560791
In grad_steps = 1772, loss = 0.08899389207363129
In grad_steps = 1773, loss = 0.051341474056243896
In grad_steps = 1774, loss = 0.05267166718840599
In grad_steps = 1775, loss = 0.6128573417663574
In grad_steps = 1776, loss = 0.4522944688796997
In grad_steps = 1777, loss = 0.13026411831378937
In grad_steps = 1778, loss = 0.48110002279281616
In grad_steps = 1779, loss = 0.44655412435531616
In grad_steps = 1780, loss = 0.033498235046863556
In grad_steps = 1781, loss = 0.08893325179815292
In grad_steps = 1782, loss = 0.2168363779783249
In grad_steps = 1783, loss = 0.46702414751052856
In grad_steps = 1784, loss = 1.012021780014038
In grad_steps = 1785, loss = 0.0870770737528801
In grad_steps = 1786, loss = 0.12197326123714447
In grad_steps = 1787, loss = 0.1938096433877945
In grad_steps = 1788, loss = 0.14384236931800842
In grad_steps = 1789, loss = 0.2847241759300232
In grad_steps = 1790, loss = 0.23816265165805817
In grad_steps = 1791, loss = 0.33811870217323303
In grad_steps = 1792, loss = 0.06371192634105682
In grad_steps = 1793, loss = 0.092715784907341
In grad_steps = 1794, loss = 0.17074081301689148
In grad_steps = 1795, loss = 0.13310305774211884
In grad_steps = 1796, loss = 0.053018391132354736
In grad_steps = 1797, loss = 0.3505801558494568
In grad_steps = 1798, loss = 0.05188145861029625
In grad_steps = 1799, loss = 0.9726618528366089
In grad_steps = 1800, loss = 0.529340922832489
In grad_steps = 1801, loss = 0.30512768030166626
In grad_steps = 1802, loss = 0.8076435923576355
In grad_steps = 1803, loss = 0.03291957825422287
In grad_steps = 1804, loss = 0.8005962371826172
In grad_steps = 1805, loss = 0.2647472620010376
In grad_steps = 1806, loss = 0.08269554376602173
In grad_steps = 1807, loss = 0.12982064485549927
In grad_steps = 1808, loss = 0.6277081370353699
In grad_steps = 1809, loss = 0.29770421981811523
In grad_steps = 1810, loss = 0.025930553674697876
In grad_steps = 1811, loss = 0.04131278768181801
In grad_steps = 1812, loss = 0.2455640733242035
In grad_steps = 1813, loss = 0.20506656169891357
In grad_steps = 1814, loss = 0.059315409511327744
In grad_steps = 1815, loss = 0.45387008786201477
In grad_steps = 1816, loss = 0.08322487771511078
In grad_steps = 1817, loss = 0.45424970984458923
In grad_steps = 1818, loss = 0.21751956641674042
In grad_steps = 1819, loss = 0.4821723997592926
In grad_steps = 1820, loss = 0.907421350479126
In grad_steps = 1821, loss = 0.40100231766700745
In grad_steps = 1822, loss = 0.050709452480077744
In grad_steps = 1823, loss = 1.2392381429672241
In grad_steps = 1824, loss = 0.07057970017194748
In grad_steps = 1825, loss = 0.08532088994979858
In grad_steps = 1826, loss = 0.8460358381271362
In grad_steps = 1827, loss = 1.2930538654327393
In grad_steps = 1828, loss = 0.9712965488433838
In grad_steps = 1829, loss = 0.8923612833023071
In grad_steps = 1830, loss = 0.4505162239074707
In grad_steps = 1831, loss = 0.34490707516670227
In grad_steps = 1832, loss = 0.6302803754806519
In grad_steps = 1833, loss = 1.2206014394760132
In grad_steps = 1834, loss = 0.11951003968715668
In grad_steps = 1835, loss = 0.300570011138916
In grad_steps = 1836, loss = 0.48979923129081726
In grad_steps = 1837, loss = 0.49014031887054443
In grad_steps = 1838, loss = 0.5091259479522705
In grad_steps = 1839, loss = 0.2782132029533386
In grad_steps = 1840, loss = 0.12830935418605804
In grad_steps = 1841, loss = 0.41788536310195923
In grad_steps = 1842, loss = 0.2864016890525818
In grad_steps = 1843, loss = 0.19815336167812347
In grad_steps = 1844, loss = 0.7342283725738525
In grad_steps = 1845, loss = 0.23334068059921265
In grad_steps = 1846, loss = 0.4662879705429077
In grad_steps = 1847, loss = 0.4373975098133087
In grad_steps = 1848, loss = 0.13660673797130585
In grad_steps = 1849, loss = 0.15826013684272766
In grad_steps = 1850, loss = 0.2591623067855835
In grad_steps = 1851, loss = 0.2695767879486084
In grad_steps = 1852, loss = 0.1608368158340454
In grad_steps = 1853, loss = 0.43668150901794434
In grad_steps = 1854, loss = 0.17333321273326874
In grad_steps = 1855, loss = 0.043003544211387634
In grad_steps = 1856, loss = 0.04744097590446472
In grad_steps = 1857, loss = 0.7237138152122498
In grad_steps = 1858, loss = 0.060261115431785583
In grad_steps = 1859, loss = 0.750880241394043
In grad_steps = 1860, loss = 0.47076576948165894
In grad_steps = 1861, loss = 0.04177063703536987
In grad_steps = 1862, loss = 0.4476776719093323
In grad_steps = 1863, loss = 0.15245027840137482
In grad_steps = 1864, loss = 0.1132088452577591
In grad_steps = 1865, loss = 0.43801993131637573
In grad_steps = 1866, loss = 0.15726520121097565
In grad_steps = 1867, loss = 1.1592470407485962
In grad_steps = 1868, loss = 0.09149912744760513
In grad_steps = 1869, loss = 0.08367639780044556
In grad_steps = 1870, loss = 0.052938275039196014
In grad_steps = 1871, loss = 0.15982818603515625
In grad_steps = 1872, loss = 0.18320916593074799
In grad_steps = 1873, loss = 0.0772455632686615
In grad_steps = 1874, loss = 0.08516623079776764
In grad_steps = 1875, loss = 0.10890433192253113
In grad_steps = 1876, loss = 0.06303554773330688
In grad_steps = 1877, loss = 0.38128185272216797
In grad_steps = 1878, loss = 0.20515461266040802
In grad_steps = 1879, loss = 0.7404003143310547
In grad_steps = 1880, loss = 0.06047492474317551
In grad_steps = 1881, loss = 0.19936729967594147
In grad_steps = 1882, loss = 0.07123957574367523
In grad_steps = 1883, loss = 0.3785063624382019
In grad_steps = 1884, loss = 0.07766364514827728
In grad_steps = 1885, loss = 0.24209807813167572
In grad_steps = 1886, loss = 0.6333099603652954
In grad_steps = 1887, loss = 0.033790260553359985
In grad_steps = 1888, loss = 0.2000618875026703
In grad_steps = 1889, loss = 0.03893902897834778
In grad_steps = 1890, loss = 0.04544419050216675
In grad_steps = 1891, loss = 0.11764327436685562
In grad_steps = 1892, loss = 0.1913432478904724
In grad_steps = 1893, loss = 0.02900055982172489
In grad_steps = 1894, loss = 0.04126535728573799
In grad_steps = 1895, loss = 0.41956815123558044
In grad_steps = 1896, loss = 0.5372673273086548
In grad_steps = 1897, loss = 0.013814534991979599
In grad_steps = 1898, loss = 0.011465853080153465
In grad_steps = 1899, loss = 0.07234416902065277
In grad_steps = 1900, loss = 0.046121418476104736
In grad_steps = 1901, loss = 0.21412909030914307
In grad_steps = 1902, loss = 0.12956827878952026
In grad_steps = 1903, loss = 0.22930818796157837
In grad_steps = 1904, loss = 0.31180691719055176
In grad_steps = 1905, loss = 0.019016578793525696
In grad_steps = 1906, loss = 0.05927782878279686
In grad_steps = 1907, loss = 1.759310245513916
In grad_steps = 1908, loss = 0.48044028878211975
In grad_steps = 1909, loss = 0.030667118728160858
In grad_steps = 1910, loss = 0.7396363019943237
In grad_steps = 1911, loss = 0.06279662251472473
In grad_steps = 1912, loss = 0.1649416983127594
In grad_steps = 1913, loss = 0.047089431434869766
In grad_steps = 1914, loss = 0.049551866948604584
In grad_steps = 1915, loss = 0.38200312852859497
In grad_steps = 1916, loss = 0.4062665104866028
In grad_steps = 1917, loss = 0.0706963837146759
In grad_steps = 1918, loss = 0.34299880266189575
In grad_steps = 1919, loss = 0.3234122097492218
In grad_steps = 1920, loss = 0.7109646201133728
In grad_steps = 1921, loss = 0.4086802899837494
In grad_steps = 1922, loss = 0.2448391169309616
In grad_steps = 1923, loss = 0.01218372117727995
In grad_steps = 1924, loss = 0.45094436407089233
In grad_steps = 1925, loss = 0.6116716861724854
In grad_steps = 1926, loss = 0.5546755194664001
In grad_steps = 1927, loss = 0.26958563923835754
In grad_steps = 1928, loss = 1.1698087453842163
In grad_steps = 1929, loss = 0.3419436812400818
In grad_steps = 1930, loss = 0.24226954579353333
In grad_steps = 1931, loss = 0.8078367710113525
In grad_steps = 1932, loss = 0.20240391790866852
In grad_steps = 1933, loss = 0.9596130847930908
In grad_steps = 1934, loss = 0.3012427091598511
In grad_steps = 1935, loss = 0.21020513772964478
In grad_steps = 1936, loss = 0.16022132337093353
In grad_steps = 1937, loss = 0.15568804740905762
In grad_steps = 1938, loss = 0.39370378851890564
In grad_steps = 1939, loss = 0.4618610441684723
In grad_steps = 1940, loss = 0.20496594905853271
In grad_steps = 1941, loss = 0.29554659128189087
In grad_steps = 1942, loss = 0.2605765461921692
In grad_steps = 1943, loss = 0.9495891332626343
In grad_steps = 1944, loss = 0.13007113337516785
In grad_steps = 1945, loss = 0.33173733949661255
In grad_steps = 1946, loss = 0.22495627403259277
In grad_steps = 1947, loss = 0.3622376024723053
In grad_steps = 1948, loss = 0.08731698989868164
In grad_steps = 1949, loss = 0.2859787046909332
In grad_steps = 1950, loss = 0.29492780566215515
In grad_steps = 1951, loss = 0.5418601036071777
In grad_steps = 1952, loss = 0.12868331372737885
In grad_steps = 1953, loss = 0.10260980576276779
In grad_steps = 1954, loss = 0.6051585078239441
In grad_steps = 1955, loss = 0.016948699951171875
In grad_steps = 1956, loss = 0.18189334869384766
In grad_steps = 1957, loss = 0.04382586106657982
In grad_steps = 1958, loss = 0.09816990792751312
In grad_steps = 1959, loss = 0.08945336192846298
In grad_steps = 1960, loss = 0.177426278591156
In grad_steps = 1961, loss = 0.38908684253692627
In grad_steps = 1962, loss = 0.032821256667375565
In grad_steps = 1963, loss = 1.1679149866104126
In grad_steps = 1964, loss = 0.6105296015739441
In grad_steps = 1965, loss = 0.3850385844707489
In grad_steps = 1966, loss = 1.8795228004455566
In grad_steps = 1967, loss = 0.6658992767333984
In grad_steps = 1968, loss = 0.2237473875284195
In grad_steps = 1969, loss = 0.15343181788921356
In grad_steps = 1970, loss = 0.3439929783344269
In grad_steps = 1971, loss = 0.437887966632843
In grad_steps = 1972, loss = 0.6002901196479797
In grad_steps = 1973, loss = 0.4220983386039734
In grad_steps = 1974, loss = 0.39122554659843445
In grad_steps = 1975, loss = 0.22452017664909363
In grad_steps = 1976, loss = 0.21323464810848236
In grad_steps = 1977, loss = 0.7564637064933777
In grad_steps = 1978, loss = 0.31289422512054443
In grad_steps = 1979, loss = 0.09455312043428421
In grad_steps = 1980, loss = 0.31212306022644043
In grad_steps = 1981, loss = 0.5032499432563782
In grad_steps = 1982, loss = 0.07466740906238556
In grad_steps = 1983, loss = 0.12467360496520996
In grad_steps = 1984, loss = 0.1979251503944397
In grad_steps = 1985, loss = 0.40441954135894775
In grad_steps = 1986, loss = 0.29473596811294556
In grad_steps = 1987, loss = 0.21439975500106812
In grad_steps = 1988, loss = 0.34640225768089294
In grad_steps = 1989, loss = 0.2961084842681885
In grad_steps = 1990, loss = 0.41131457686424255
In grad_steps = 1991, loss = 0.15548980236053467
In grad_steps = 1992, loss = 0.4161377251148224
In grad_steps = 1993, loss = 0.1543852686882019
In grad_steps = 1994, loss = 0.3604288697242737
In grad_steps = 1995, loss = 0.671987771987915
In grad_steps = 1996, loss = 0.1209724023938179
In grad_steps = 1997, loss = 0.16512778401374817
In grad_steps = 1998, loss = 0.17901088297367096
In grad_steps = 1999, loss = 0.08181881159543991
In grad_steps = 2000, loss = 0.07180505990982056
In grad_steps = 2001, loss = 0.12925460934638977
In grad_steps = 2002, loss = 0.461738646030426
In grad_steps = 2003, loss = 0.09849132597446442
In grad_steps = 2004, loss = 0.08516322821378708
In grad_steps = 2005, loss = 0.043080125004053116
In grad_steps = 2006, loss = 0.3512684106826782
In grad_steps = 2007, loss = 0.12255699932575226
In grad_steps = 2008, loss = 0.08174692839384079
In grad_steps = 2009, loss = 0.6180421710014343
In grad_steps = 2010, loss = 0.20560204982757568
In grad_steps = 2011, loss = 0.32676002383232117
In grad_steps = 2012, loss = 0.3462502062320709
In grad_steps = 2013, loss = 0.05636146292090416
In grad_steps = 2014, loss = 0.1414441019296646
In grad_steps = 2015, loss = 0.008182148449122906
In grad_steps = 2016, loss = 0.023071495816111565
In grad_steps = 2017, loss = 0.05244016274809837
In grad_steps = 2018, loss = 0.05581725388765335
In grad_steps = 2019, loss = 0.7656683921813965
In grad_steps = 2020, loss = 0.2048129439353943
In grad_steps = 2021, loss = 0.10540090501308441
In grad_steps = 2022, loss = 1.1193739175796509
In grad_steps = 2023, loss = 0.4050094187259674
In grad_steps = 2024, loss = 0.03707237169146538
In grad_steps = 2025, loss = 0.2097051441669464
In grad_steps = 2026, loss = 0.13569360971450806
In grad_steps = 2027, loss = 0.7855018973350525
In grad_steps = 2028, loss = 0.11203870922327042
In grad_steps = 2029, loss = 0.22064360976219177
In grad_steps = 2030, loss = 0.22151219844818115
In grad_steps = 2031, loss = 0.05528685078024864
In grad_steps = 2032, loss = 0.010797867551445961
In grad_steps = 2033, loss = 0.06753194332122803
In grad_steps = 2034, loss = 0.03257298097014427
In grad_steps = 2035, loss = 0.0672224760055542
In grad_steps = 2036, loss = 0.02124261111021042
In grad_steps = 2037, loss = 0.19948504865169525
In grad_steps = 2038, loss = 0.6213136315345764
In grad_steps = 2039, loss = 0.6620428562164307
In grad_steps = 2040, loss = 0.6799540519714355
In grad_steps = 2041, loss = 0.055593449622392654
In grad_steps = 2042, loss = 0.1536470204591751
In grad_steps = 2043, loss = 0.2150767743587494
In grad_steps = 2044, loss = 0.049803476780653
In grad_steps = 2045, loss = 0.6904940605163574
In grad_steps = 2046, loss = 0.5485976934432983
In grad_steps = 2047, loss = 0.02745046839118004
In grad_steps = 2048, loss = 0.12334447354078293
In grad_steps = 2049, loss = 2.1440203189849854
In grad_steps = 2050, loss = 0.5269761085510254
In grad_steps = 2051, loss = 0.5257658958435059
In grad_steps = 2052, loss = 0.06961988657712936
In grad_steps = 2053, loss = 0.9717506766319275
In grad_steps = 2054, loss = 0.1605587750673294
In grad_steps = 2055, loss = 0.1381179690361023
In grad_steps = 2056, loss = 0.13334035873413086
In grad_steps = 2057, loss = 0.07802482694387436
In grad_steps = 2058, loss = 0.36978286504745483
In grad_steps = 2059, loss = 0.0858146995306015
In grad_steps = 2060, loss = 0.47332310676574707
In grad_steps = 2061, loss = 0.21493256092071533
In grad_steps = 2062, loss = 0.08007794618606567
In grad_steps = 2063, loss = 0.08115650713443756
In grad_steps = 2064, loss = 0.5516327023506165
In grad_steps = 2065, loss = 0.10458852350711823
In grad_steps = 2066, loss = 0.16349761188030243
In grad_steps = 2067, loss = 0.07709161937236786
In grad_steps = 2068, loss = 0.09410174190998077
In grad_steps = 2069, loss = 0.1943970024585724
In grad_steps = 2070, loss = 0.49264785647392273
In grad_steps = 2071, loss = 0.152884840965271
In grad_steps = 2072, loss = 0.03907075151801109
In grad_steps = 2073, loss = 0.76723313331604
In grad_steps = 2074, loss = 0.19611771404743195
In grad_steps = 2075, loss = 0.06203219294548035
In grad_steps = 2076, loss = 0.03054690733551979
In grad_steps = 2077, loss = 0.15719106793403625
In grad_steps = 2078, loss = 0.9166335463523865
In grad_steps = 2079, loss = 0.3212566077709198
In grad_steps = 2080, loss = 0.5320571660995483
In grad_steps = 2081, loss = 0.016660019755363464
In grad_steps = 2082, loss = 0.03830031678080559
In grad_steps = 2083, loss = 0.04843544214963913
In grad_steps = 2084, loss = 0.07486148923635483
In grad_steps = 2085, loss = 0.14659765362739563
In grad_steps = 2086, loss = 0.24949894845485687
In grad_steps = 2087, loss = 0.20350174605846405
In grad_steps = 2088, loss = 0.01972014829516411
In grad_steps = 2089, loss = 0.05636972934007645
In grad_steps = 2090, loss = 0.04729030653834343
In grad_steps = 2091, loss = 0.04150324687361717
In grad_steps = 2092, loss = 0.4959389865398407
In grad_steps = 2093, loss = 0.0453350767493248
In grad_steps = 2094, loss = 0.22340857982635498
In grad_steps = 2095, loss = 0.04810124635696411
In grad_steps = 2096, loss = 0.012526197358965874
In grad_steps = 2097, loss = 1.6344873905181885
In grad_steps = 2098, loss = 0.02809860184788704
In grad_steps = 2099, loss = 0.022085854783654213
In grad_steps = 2100, loss = 0.058382321149110794
In grad_steps = 2101, loss = 0.11363542079925537
In grad_steps = 2102, loss = 0.04208584129810333
In grad_steps = 2103, loss = 0.12702035903930664
In grad_steps = 2104, loss = 0.8528881669044495
In grad_steps = 2105, loss = 0.56110018491745
In grad_steps = 2106, loss = 0.09412671625614166
In grad_steps = 2107, loss = 1.4647289514541626
In grad_steps = 2108, loss = 0.16839417815208435
In grad_steps = 2109, loss = 0.11635678261518478
In grad_steps = 2110, loss = 0.36895060539245605
In grad_steps = 2111, loss = 0.192551389336586
In grad_steps = 2112, loss = 0.6680764555931091
In grad_steps = 2113, loss = 0.5365869402885437
In grad_steps = 2114, loss = 0.22989076375961304
In grad_steps = 2115, loss = 0.09840749204158783
In grad_steps = 2116, loss = 0.08780718594789505
In grad_steps = 2117, loss = 0.40991100668907166
In grad_steps = 2118, loss = 0.18083511292934418
In grad_steps = 2119, loss = 0.03419994190335274
In grad_steps = 2120, loss = 0.034525759518146515
In grad_steps = 2121, loss = 0.10932154953479767
In grad_steps = 2122, loss = 0.08001723885536194
In grad_steps = 2123, loss = 0.35449734330177307
In grad_steps = 2124, loss = 0.12236601114273071
In grad_steps = 2125, loss = 0.028084466233849525
In grad_steps = 2126, loss = 1.597753643989563
In grad_steps = 2127, loss = 0.17978951334953308
In grad_steps = 2128, loss = 0.07719799131155014
In grad_steps = 2129, loss = 0.25624531507492065
In grad_steps = 2130, loss = 0.2009102702140808
In grad_steps = 2131, loss = 0.4792419373989105
In grad_steps = 2132, loss = 0.20904482901096344
In grad_steps = 2133, loss = 0.03220786526799202
In grad_steps = 2134, loss = 0.11305376142263412
In grad_steps = 2135, loss = 0.5219799876213074
In grad_steps = 2136, loss = 0.47255170345306396
In grad_steps = 2137, loss = 0.17285214364528656
In grad_steps = 2138, loss = 0.5481687784194946
In grad_steps = 2139, loss = 0.10770615190267563
In grad_steps = 2140, loss = 0.08760352432727814
In grad_steps = 2141, loss = 0.24838121235370636
In grad_steps = 2142, loss = 0.02709045447409153
In grad_steps = 2143, loss = 0.20149914920330048
In grad_steps = 2144, loss = 1.0355987548828125
In grad_steps = 2145, loss = 0.32189831137657166
In grad_steps = 2146, loss = 0.1722830832004547
In grad_steps = 2147, loss = 0.5256531238555908
In grad_steps = 2148, loss = 0.9723733067512512
In grad_steps = 2149, loss = 0.03093307837843895
In grad_steps = 2150, loss = 0.20837844908237457
In grad_steps = 2151, loss = 0.4396301805973053
In grad_steps = 2152, loss = 0.23808559775352478
In grad_steps = 2153, loss = 0.1984695941209793
In grad_steps = 2154, loss = 0.06889104843139648
In grad_steps = 2155, loss = 0.2813009023666382
In grad_steps = 2156, loss = 0.5270094275474548
In grad_steps = 2157, loss = 0.05837031453847885
In grad_steps = 2158, loss = 0.23227669298648834
In grad_steps = 2159, loss = 0.8127843141555786
In grad_steps = 2160, loss = 0.10238784551620483
In grad_steps = 2161, loss = 0.05344533175230026
In grad_steps = 2162, loss = 0.803548276424408
In grad_steps = 2163, loss = 0.055825125426054
In grad_steps = 2164, loss = 0.06731016933917999
In grad_steps = 2165, loss = 0.4184189438819885
In grad_steps = 2166, loss = 0.6049885749816895
In grad_steps = 2167, loss = 0.056123435497283936
In grad_steps = 2168, loss = 0.10297354310750961
In grad_steps = 2169, loss = 0.4431944489479065
In grad_steps = 2170, loss = 0.02340233325958252
In grad_steps = 2171, loss = 0.7792528867721558
In grad_steps = 2172, loss = 0.9072058200836182
In grad_steps = 2173, loss = 0.01809638924896717
In grad_steps = 2174, loss = 0.16517165303230286
In grad_steps = 2175, loss = 0.3100241422653198
In grad_steps = 2176, loss = 0.20388062298297882
In grad_steps = 2177, loss = 0.11026877164840698
In grad_steps = 2178, loss = 0.0790863186120987
In grad_steps = 2179, loss = 0.40283215045928955
In grad_steps = 2180, loss = 0.06853394210338593
In grad_steps = 2181, loss = 0.059546828269958496
In grad_steps = 2182, loss = 0.20890332758426666
In grad_steps = 2183, loss = 0.23697790503501892
In grad_steps = 2184, loss = 0.20442460477352142
In grad_steps = 2185, loss = 0.16355234384536743
In grad_steps = 2186, loss = 0.1896212100982666
In grad_steps = 2187, loss = 0.1809130162000656
In grad_steps = 2188, loss = 0.03851785510778427
In grad_steps = 2189, loss = 0.11039579659700394
In grad_steps = 2190, loss = 0.04542709141969681
In grad_steps = 2191, loss = 0.197829931974411
In grad_steps = 2192, loss = 0.49993303418159485
In grad_steps = 2193, loss = 0.09799712896347046
In grad_steps = 2194, loss = 0.04133665934205055
In grad_steps = 2195, loss = 0.021331891417503357
In grad_steps = 2196, loss = 0.08433080464601517
In grad_steps = 2197, loss = 0.014315401203930378
In grad_steps = 2198, loss = 1.257824182510376
In grad_steps = 2199, loss = 0.05904345214366913
In grad_steps = 2200, loss = 0.48328033089637756
In grad_steps = 2201, loss = 0.12009994685649872
In grad_steps = 2202, loss = 0.2021719515323639
In grad_steps = 2203, loss = 0.08634732663631439
In grad_steps = 2204, loss = 0.3094451427459717
In grad_steps = 2205, loss = 0.013612102717161179
In grad_steps = 2206, loss = 0.2690585255622864
In grad_steps = 2207, loss = 0.045792631804943085
In grad_steps = 2208, loss = 0.12336830794811249
In grad_steps = 2209, loss = 0.027012374252080917
In grad_steps = 2210, loss = 0.8980386853218079
In grad_steps = 2211, loss = 0.17747241258621216
In grad_steps = 2212, loss = 1.4620139598846436
In grad_steps = 2213, loss = 1.3655898571014404
In grad_steps = 2214, loss = 0.32752662897109985
In grad_steps = 2215, loss = 0.3084149956703186
In grad_steps = 2216, loss = 0.1091264933347702
In grad_steps = 2217, loss = 1.2812813520431519
In grad_steps = 2218, loss = 0.35354307293891907
In grad_steps = 2219, loss = 0.47990381717681885
In grad_steps = 2220, loss = 0.19349709153175354
In grad_steps = 2221, loss = 0.12071561813354492
In grad_steps = 2222, loss = 0.3866170346736908
In grad_steps = 2223, loss = 0.5143580436706543
In grad_steps = 2224, loss = 0.06526646018028259
In grad_steps = 2225, loss = 0.4208492636680603
In grad_steps = 2226, loss = 0.25020092725753784
In grad_steps = 2227, loss = 0.44168052077293396
In grad_steps = 2228, loss = 0.4861541986465454
In grad_steps = 2229, loss = 0.13551142811775208
In grad_steps = 2230, loss = 0.16837218403816223
In grad_steps = 2231, loss = 0.12843313813209534
In grad_steps = 2232, loss = 0.2655290961265564
In grad_steps = 2233, loss = 0.25654149055480957
In grad_steps = 2234, loss = 0.36249810457229614
In grad_steps = 2235, loss = 0.030089225620031357
In grad_steps = 2236, loss = 0.3866613507270813
In grad_steps = 2237, loss = 0.5244234800338745
In grad_steps = 2238, loss = 0.3030252754688263
In grad_steps = 2239, loss = 0.5718578696250916
In grad_steps = 2240, loss = 0.15243837237358093
In grad_steps = 2241, loss = 0.06026093289256096
In grad_steps = 2242, loss = 0.18260280787944794
In grad_steps = 2243, loss = 0.07590506225824356
In grad_steps = 2244, loss = 0.14312876760959625
In grad_steps = 2245, loss = 1.1413569450378418
In grad_steps = 2246, loss = 0.7972293496131897
In grad_steps = 2247, loss = 0.056855134665966034
In grad_steps = 2248, loss = 0.92901211977005
In grad_steps = 2249, loss = 1.2734723091125488
In grad_steps = 2250, loss = 0.09317003935575485
In grad_steps = 2251, loss = 0.07937312126159668
In grad_steps = 2252, loss = 0.7328895330429077
In grad_steps = 2253, loss = 0.7009990215301514
In grad_steps = 2254, loss = 0.26949456334114075
In grad_steps = 2255, loss = 0.2512528598308563
In grad_steps = 2256, loss = 0.5624001026153564
In grad_steps = 2257, loss = 0.42581823468208313
In grad_steps = 2258, loss = 0.25465166568756104
In grad_steps = 2259, loss = 0.3087843358516693
In grad_steps = 2260, loss = 0.28305843472480774
In grad_steps = 2261, loss = 0.28354761004447937
In grad_steps = 2262, loss = 0.3700077533721924
In grad_steps = 2263, loss = 0.2616140842437744
In grad_steps = 2264, loss = 0.4759219288825989
In grad_steps = 2265, loss = 0.08946800231933594
In grad_steps = 2266, loss = 0.30509963631629944
In grad_steps = 2267, loss = 0.1809038519859314
In grad_steps = 2268, loss = 0.04104200005531311
In grad_steps = 2269, loss = 0.8073700070381165
In grad_steps = 2270, loss = 0.055286817252635956
In grad_steps = 2271, loss = 0.16806279122829437
In grad_steps = 2272, loss = 0.044179413467645645
In grad_steps = 2273, loss = 0.23012498021125793
In grad_steps = 2274, loss = 0.05467883124947548
In grad_steps = 2275, loss = 0.03195876628160477
In grad_steps = 2276, loss = 0.03198094293475151
In grad_steps = 2277, loss = 0.1851329803466797
In grad_steps = 2278, loss = 0.22138214111328125
In grad_steps = 2279, loss = 0.012859995476901531
In grad_steps = 2280, loss = 0.017273353412747383
In grad_steps = 2281, loss = 0.10088694840669632
In grad_steps = 2282, loss = 0.04106682166457176
In grad_steps = 2283, loss = 1.4284683465957642
In grad_steps = 2284, loss = 0.06700507551431656
In grad_steps = 2285, loss = 0.009719062596559525
In grad_steps = 2286, loss = 0.12625457346439362
In grad_steps = 2287, loss = 0.9310776591300964
In grad_steps = 2288, loss = 0.08220633864402771
In grad_steps = 2289, loss = 0.024880288168787956
In grad_steps = 2290, loss = 0.3563733994960785
In grad_steps = 2291, loss = 0.0640067309141159
In grad_steps = 2292, loss = 0.4273216426372528
In grad_steps = 2293, loss = 0.13991637527942657
In grad_steps = 2294, loss = 0.19188769161701202
In grad_steps = 2295, loss = 0.12433656305074692
In grad_steps = 2296, loss = 0.0875755175948143
In grad_steps = 2297, loss = 0.10699926316738129
In grad_steps = 2298, loss = 0.3805636167526245
In grad_steps = 2299, loss = 0.16513660550117493
In grad_steps = 2300, loss = 0.08290205895900726
In grad_steps = 2301, loss = 0.12006865441799164
In grad_steps = 2302, loss = 0.40962356328964233
In grad_steps = 2303, loss = 0.01662594825029373
In grad_steps = 2304, loss = 0.029650313779711723
In grad_steps = 2305, loss = 0.12263758480548859
In grad_steps = 2306, loss = 0.03388453647494316
In grad_steps = 2307, loss = 0.14248956739902496
In grad_steps = 2308, loss = 0.12991608679294586
In grad_steps = 2309, loss = 0.019474569708108902
In grad_steps = 2310, loss = 0.8425384163856506
In grad_steps = 2311, loss = 0.02649407833814621
In grad_steps = 2312, loss = 0.9787594079971313
In grad_steps = 2313, loss = 0.4253053069114685
In grad_steps = 2314, loss = 0.11667804419994354
In grad_steps = 2315, loss = 0.6992207765579224
In grad_steps = 2316, loss = 0.381448894739151
In grad_steps = 2317, loss = 0.04048863425850868
In grad_steps = 2318, loss = 0.1397324800491333
In grad_steps = 2319, loss = 1.4944100379943848
In grad_steps = 2320, loss = 0.04646476358175278
In grad_steps = 2321, loss = 0.04875975102186203
In grad_steps = 2322, loss = 0.08441480994224548
In grad_steps = 2323, loss = 0.03781844675540924
In grad_steps = 2324, loss = 0.5787394642829895
In grad_steps = 2325, loss = 0.11573345959186554
In grad_steps = 2326, loss = 0.05603665113449097
In grad_steps = 2327, loss = 1.547267198562622
In grad_steps = 2328, loss = 0.6277720928192139
In grad_steps = 2329, loss = 0.09407220780849457
In grad_steps = 2330, loss = 0.21254374086856842
In grad_steps = 2331, loss = 0.9773448705673218
In grad_steps = 2332, loss = 0.12532418966293335
In grad_steps = 2333, loss = 0.6378986835479736
In grad_steps = 2334, loss = 0.13438740372657776
In grad_steps = 2335, loss = 0.5107202529907227
In grad_steps = 2336, loss = 0.3912827670574188
In grad_steps = 2337, loss = 0.11912713944911957
In grad_steps = 2338, loss = 0.09681113809347153
In grad_steps = 2339, loss = 0.2890801727771759
In grad_steps = 2340, loss = 0.3153665065765381
In grad_steps = 2341, loss = 0.19155465066432953
In grad_steps = 2342, loss = 1.193464994430542
In grad_steps = 2343, loss = 0.12156820297241211
In grad_steps = 2344, loss = 0.830371081829071
In grad_steps = 2345, loss = 0.21531708538532257
In grad_steps = 2346, loss = 0.27713674306869507
In grad_steps = 2347, loss = 0.2004501074552536
In grad_steps = 2348, loss = 0.11472637951374054
In grad_steps = 2349, loss = 0.17233583331108093
In grad_steps = 2350, loss = 0.2418949007987976
In grad_steps = 2351, loss = 0.4968692362308502
In grad_steps = 2352, loss = 0.021109549328684807
Beginning epoch 2
In grad_steps = 2353, loss = 0.135352224111557
In grad_steps = 2354, loss = 1.1646294593811035
In grad_steps = 2355, loss = 0.3510608971118927
In grad_steps = 2356, loss = 0.36444762349128723
In grad_steps = 2357, loss = 0.19461029767990112
In grad_steps = 2358, loss = 0.28949999809265137
In grad_steps = 2359, loss = 0.305559366941452
In grad_steps = 2360, loss = 0.12166841328144073
In grad_steps = 2361, loss = 0.06761236488819122
In grad_steps = 2362, loss = 0.7407370209693909
In grad_steps = 2363, loss = 0.3616681396961212
In grad_steps = 2364, loss = 0.5285423398017883
In grad_steps = 2365, loss = 0.19602417945861816
In grad_steps = 2366, loss = 0.1794801652431488
In grad_steps = 2367, loss = 0.2383083999156952
In grad_steps = 2368, loss = 0.14615166187286377
In grad_steps = 2369, loss = 0.09696058928966522
In grad_steps = 2370, loss = 0.32106295228004456
In grad_steps = 2371, loss = 0.498791366815567
In grad_steps = 2372, loss = 0.2418341040611267
In grad_steps = 2373, loss = 0.07900073379278183
In grad_steps = 2374, loss = 0.07198089361190796
In grad_steps = 2375, loss = 0.15672126412391663
In grad_steps = 2376, loss = 1.1770977973937988
In grad_steps = 2377, loss = 0.6435238122940063
In grad_steps = 2378, loss = 0.1564721167087555
In grad_steps = 2379, loss = 0.12262310832738876
In grad_steps = 2380, loss = 0.0150486771017313
In grad_steps = 2381, loss = 0.04460667818784714
In grad_steps = 2382, loss = 0.04559347778558731
In grad_steps = 2383, loss = 0.06687894463539124
In grad_steps = 2384, loss = 0.12155090272426605
In grad_steps = 2385, loss = 0.16890974342823029
In grad_steps = 2386, loss = 0.048959966748952866
In grad_steps = 2387, loss = 0.17909638583660126
In grad_steps = 2388, loss = 0.18408258259296417
In grad_steps = 2389, loss = 0.023577047511935234
In grad_steps = 2390, loss = 0.01954539865255356
In grad_steps = 2391, loss = 0.7750603556632996
In grad_steps = 2392, loss = 0.5170276165008545
In grad_steps = 2393, loss = 0.0212106816470623
In grad_steps = 2394, loss = 0.07949306815862656
In grad_steps = 2395, loss = 0.2215600609779358
In grad_steps = 2396, loss = 0.03178336098790169
In grad_steps = 2397, loss = 0.4905621111392975
In grad_steps = 2398, loss = 0.09481105953454971
In grad_steps = 2399, loss = 0.06672067195177078
In grad_steps = 2400, loss = 0.8397334814071655
In grad_steps = 2401, loss = 0.045663982629776
In grad_steps = 2402, loss = 0.4243939220905304
In grad_steps = 2403, loss = 0.29300403594970703
In grad_steps = 2404, loss = 1.1381494998931885
In grad_steps = 2405, loss = 0.05139654874801636
In grad_steps = 2406, loss = 0.08149364590644836
In grad_steps = 2407, loss = 0.025239793583750725
In grad_steps = 2408, loss = 0.03502795100212097
In grad_steps = 2409, loss = 0.6101351380348206
In grad_steps = 2410, loss = 0.0655207708477974
In grad_steps = 2411, loss = 0.3039018213748932
In grad_steps = 2412, loss = 0.040321625769138336
In grad_steps = 2413, loss = 0.2443827986717224
In grad_steps = 2414, loss = 0.06681543588638306
In grad_steps = 2415, loss = 0.3259335458278656
In grad_steps = 2416, loss = 0.9103657007217407
In grad_steps = 2417, loss = 0.5629154443740845
In grad_steps = 2418, loss = 0.06611305475234985
In grad_steps = 2419, loss = 0.034271325916051865
In grad_steps = 2420, loss = 0.03946902975440025
In grad_steps = 2421, loss = 0.029584160074591637
In grad_steps = 2422, loss = 1.1929757595062256
In grad_steps = 2423, loss = 0.7602920532226562
In grad_steps = 2424, loss = 0.0558135062456131
In grad_steps = 2425, loss = 1.268160343170166
In grad_steps = 2426, loss = 0.0850861519575119
In grad_steps = 2427, loss = 0.0943528488278389
In grad_steps = 2428, loss = 0.07301151007413864
In grad_steps = 2429, loss = 0.27035143971443176
In grad_steps = 2430, loss = 0.6273078322410583
In grad_steps = 2431, loss = 0.4289323091506958
In grad_steps = 2432, loss = 0.20673868060112
In grad_steps = 2433, loss = 0.30851608514785767
In grad_steps = 2434, loss = 0.04180195555090904
In grad_steps = 2435, loss = 0.10990412533283234
In grad_steps = 2436, loss = 0.1945345252752304
In grad_steps = 2437, loss = 0.094047911465168
In grad_steps = 2438, loss = 0.6961535811424255
In grad_steps = 2439, loss = 0.9233614802360535
In grad_steps = 2440, loss = 0.06924602389335632
In grad_steps = 2441, loss = 0.3844507336616516
In grad_steps = 2442, loss = 0.13502828776836395
In grad_steps = 2443, loss = 1.2911880016326904
In grad_steps = 2444, loss = 0.47422823309898376
In grad_steps = 2445, loss = 0.35894864797592163
In grad_steps = 2446, loss = 0.17611420154571533
In grad_steps = 2447, loss = 0.08011722564697266
In grad_steps = 2448, loss = 0.5270518660545349
In grad_steps = 2449, loss = 0.19466160237789154
In grad_steps = 2450, loss = 0.07203558832406998
In grad_steps = 2451, loss = 0.3191429674625397
In grad_steps = 2452, loss = 0.5569359064102173
In grad_steps = 2453, loss = 0.3352278172969818
In grad_steps = 2454, loss = 0.128648579120636
In grad_steps = 2455, loss = 0.6110530495643616
In grad_steps = 2456, loss = 0.5156123042106628
In grad_steps = 2457, loss = 0.29268524050712585
In grad_steps = 2458, loss = 0.40534770488739014
In grad_steps = 2459, loss = 0.0713977962732315
In grad_steps = 2460, loss = 0.0545077919960022
In grad_steps = 2461, loss = 0.1952945590019226
In grad_steps = 2462, loss = 0.44372323155403137
In grad_steps = 2463, loss = 0.16271553933620453
In grad_steps = 2464, loss = 0.09237436205148697
In grad_steps = 2465, loss = 0.3399878144264221
In grad_steps = 2466, loss = 0.12713293731212616
In grad_steps = 2467, loss = 0.5907507538795471
In grad_steps = 2468, loss = 0.2387717217206955
In grad_steps = 2469, loss = 0.6608654856681824
In grad_steps = 2470, loss = 0.4025588631629944
In grad_steps = 2471, loss = 0.17866185307502747
In grad_steps = 2472, loss = 0.5889198780059814
In grad_steps = 2473, loss = 0.5000860095024109
In grad_steps = 2474, loss = 0.1507997065782547
In grad_steps = 2475, loss = 0.6357503533363342
In grad_steps = 2476, loss = 0.056841108947992325
In grad_steps = 2477, loss = 0.2300015389919281
In grad_steps = 2478, loss = 0.7673694491386414
In grad_steps = 2479, loss = 0.3924955725669861
In grad_steps = 2480, loss = 0.11181746423244476
In grad_steps = 2481, loss = 0.37382206320762634
In grad_steps = 2482, loss = 0.09113793075084686
In grad_steps = 2483, loss = 0.41809511184692383
In grad_steps = 2484, loss = 0.09642288088798523
In grad_steps = 2485, loss = 0.16943210363388062
In grad_steps = 2486, loss = 0.07303798198699951
In grad_steps = 2487, loss = 0.6042380928993225
In grad_steps = 2488, loss = 0.06588581949472427
In grad_steps = 2489, loss = 0.10813870280981064
In grad_steps = 2490, loss = 0.2149914801120758
In grad_steps = 2491, loss = 0.4602740406990051
In grad_steps = 2492, loss = 0.12178565561771393
In grad_steps = 2493, loss = 0.03322140499949455
In grad_steps = 2494, loss = 0.17055755853652954
In grad_steps = 2495, loss = 0.29201456904411316
In grad_steps = 2496, loss = 0.3135696053504944
In grad_steps = 2497, loss = 0.1470334827899933
In grad_steps = 2498, loss = 0.2892344295978546
In grad_steps = 2499, loss = 0.34740692377090454
In grad_steps = 2500, loss = 1.0749592781066895
In grad_steps = 2501, loss = 0.8559718132019043
In grad_steps = 2502, loss = 1.0170083045959473
In grad_steps = 2503, loss = 0.11052899062633514
In grad_steps = 2504, loss = 0.07247660309076309
In grad_steps = 2505, loss = 0.11592519283294678
In grad_steps = 2506, loss = 0.46345096826553345
In grad_steps = 2507, loss = 0.8816070556640625
In grad_steps = 2508, loss = 0.24258187413215637
In grad_steps = 2509, loss = 0.23883116245269775
In grad_steps = 2510, loss = 0.3113279342651367
In grad_steps = 2511, loss = 0.17176604270935059
In grad_steps = 2512, loss = 0.3557816743850708
In grad_steps = 2513, loss = 0.4994771480560303
In grad_steps = 2514, loss = 0.06252036243677139
In grad_steps = 2515, loss = 0.35015320777893066
In grad_steps = 2516, loss = 0.16036412119865417
In grad_steps = 2517, loss = 0.23256099224090576
In grad_steps = 2518, loss = 0.2888752520084381
In grad_steps = 2519, loss = 0.25299254059791565
In grad_steps = 2520, loss = 0.05463721603155136
In grad_steps = 2521, loss = 0.0434732586145401
In grad_steps = 2522, loss = 0.6970024108886719
In grad_steps = 2523, loss = 0.06514445692300797
In grad_steps = 2524, loss = 0.19737401604652405
In grad_steps = 2525, loss = 0.07878877222537994
In grad_steps = 2526, loss = 0.08292634785175323
In grad_steps = 2527, loss = 0.6862693428993225
In grad_steps = 2528, loss = 0.12497732043266296
In grad_steps = 2529, loss = 0.050099678337574005
In grad_steps = 2530, loss = 0.030671004205942154
In grad_steps = 2531, loss = 0.433368980884552
In grad_steps = 2532, loss = 0.07955680042505264
In grad_steps = 2533, loss = 0.20952504873275757
In grad_steps = 2534, loss = 0.03019723668694496
In grad_steps = 2535, loss = 0.15289145708084106
In grad_steps = 2536, loss = 0.053510647267103195
In grad_steps = 2537, loss = 0.1908072680234909
In grad_steps = 2538, loss = 0.18430006504058838
In grad_steps = 2539, loss = 0.06632381677627563
In grad_steps = 2540, loss = 0.01840244233608246
In grad_steps = 2541, loss = 0.4253619313240051
In grad_steps = 2542, loss = 0.009653439745306969
In grad_steps = 2543, loss = 0.03213946521282196
In grad_steps = 2544, loss = 0.4479459226131439
In grad_steps = 2545, loss = 0.03715573996305466
In grad_steps = 2546, loss = 0.021074306219816208
In grad_steps = 2547, loss = 0.5337931513786316
In grad_steps = 2548, loss = 0.01829204522073269
In grad_steps = 2549, loss = 0.8751363754272461
In grad_steps = 2550, loss = 0.008003434166312218
In grad_steps = 2551, loss = 0.05027192085981369
In grad_steps = 2552, loss = 0.02589365653693676
In grad_steps = 2553, loss = 0.022584885358810425
In grad_steps = 2554, loss = 0.3158450722694397
In grad_steps = 2555, loss = 0.44095030426979065
In grad_steps = 2556, loss = 0.013966665603220463
In grad_steps = 2557, loss = 0.034805286675691605
In grad_steps = 2558, loss = 0.17585870623588562
In grad_steps = 2559, loss = 1.0249524116516113
In grad_steps = 2560, loss = 0.05920723080635071
In grad_steps = 2561, loss = 0.04781695827841759
In grad_steps = 2562, loss = 0.8435279726982117
In grad_steps = 2563, loss = 0.03574107214808464
In grad_steps = 2564, loss = 0.021555952727794647
In grad_steps = 2565, loss = 0.05517578125
In grad_steps = 2566, loss = 0.057574596256017685
In grad_steps = 2567, loss = 0.5572915077209473
In grad_steps = 2568, loss = 0.13467755913734436
In grad_steps = 2569, loss = 0.05642756074666977
In grad_steps = 2570, loss = 0.6256147027015686
In grad_steps = 2571, loss = 0.47508475184440613
In grad_steps = 2572, loss = 0.06159990653395653
In grad_steps = 2573, loss = 0.08403857052326202
In grad_steps = 2574, loss = 0.22588950395584106
In grad_steps = 2575, loss = 0.0016974088503047824
In grad_steps = 2576, loss = 0.06684267520904541
In grad_steps = 2577, loss = 0.17325368523597717
In grad_steps = 2578, loss = 0.8846986889839172
In grad_steps = 2579, loss = 0.1578594148159027
In grad_steps = 2580, loss = 0.5130813121795654
In grad_steps = 2581, loss = 0.3607536554336548
In grad_steps = 2582, loss = 0.5515861511230469
In grad_steps = 2583, loss = 0.16728663444519043
In grad_steps = 2584, loss = 0.08822406828403473
In grad_steps = 2585, loss = 0.06988367438316345
In grad_steps = 2586, loss = 0.7073839902877808
In grad_steps = 2587, loss = 0.16929399967193604
In grad_steps = 2588, loss = 0.12676356732845306
In grad_steps = 2589, loss = 0.23930852115154266
In grad_steps = 2590, loss = 0.3735703229904175
In grad_steps = 2591, loss = 0.1683625727891922
In grad_steps = 2592, loss = 0.38166892528533936
In grad_steps = 2593, loss = 0.03019392117857933
In grad_steps = 2594, loss = 0.12889070808887482
In grad_steps = 2595, loss = 0.027312686666846275
In grad_steps = 2596, loss = 0.06170976907014847
In grad_steps = 2597, loss = 0.09199561923742294
In grad_steps = 2598, loss = 0.22051690518856049
In grad_steps = 2599, loss = 0.025719167664647102
In grad_steps = 2600, loss = 0.5608000755310059
In grad_steps = 2601, loss = 0.022241946309804916
In grad_steps = 2602, loss = 0.041178274899721146
In grad_steps = 2603, loss = 0.15976984798908234
In grad_steps = 2604, loss = 0.1415250152349472
In grad_steps = 2605, loss = 2.1234652996063232
In grad_steps = 2606, loss = 0.0612148679792881
In grad_steps = 2607, loss = 0.16595612466335297
In grad_steps = 2608, loss = 0.7732412219047546
In grad_steps = 2609, loss = 0.10425354540348053
In grad_steps = 2610, loss = 0.3403538763523102
In grad_steps = 2611, loss = 0.09066755324602127
In grad_steps = 2612, loss = 0.07752516120672226
In grad_steps = 2613, loss = 0.9423052072525024
In grad_steps = 2614, loss = 0.8848717212677002
In grad_steps = 2615, loss = 0.10417570173740387
In grad_steps = 2616, loss = 0.6858752369880676
In grad_steps = 2617, loss = 0.0692625641822815
In grad_steps = 2618, loss = 0.07100458443164825
In grad_steps = 2619, loss = 0.1921895295381546
In grad_steps = 2620, loss = 0.10751493275165558
In grad_steps = 2621, loss = 0.712382435798645
In grad_steps = 2622, loss = 0.12748375535011292
In grad_steps = 2623, loss = 0.16180191934108734
In grad_steps = 2624, loss = 0.11626168340444565
In grad_steps = 2625, loss = 0.054584331810474396
In grad_steps = 2626, loss = 0.7206966876983643
In grad_steps = 2627, loss = 0.045543622225522995
In grad_steps = 2628, loss = 0.055748917162418365
In grad_steps = 2629, loss = 0.7159026265144348
In grad_steps = 2630, loss = 0.6025549173355103
In grad_steps = 2631, loss = 0.03594175726175308
In grad_steps = 2632, loss = 0.9941754937171936
In grad_steps = 2633, loss = 0.404588907957077
In grad_steps = 2634, loss = 1.063942313194275
In grad_steps = 2635, loss = 0.1007080227136612
In grad_steps = 2636, loss = 0.3623800575733185
In grad_steps = 2637, loss = 0.09609862416982651
In grad_steps = 2638, loss = 0.6291605234146118
In grad_steps = 2639, loss = 0.5911082029342651
In grad_steps = 2640, loss = 0.056048180907964706
In grad_steps = 2641, loss = 0.07708622515201569
In grad_steps = 2642, loss = 0.4796833097934723
In grad_steps = 2643, loss = 0.14156465232372284
In grad_steps = 2644, loss = 0.2676394581794739
In grad_steps = 2645, loss = 0.2392643541097641
In grad_steps = 2646, loss = 0.24582672119140625
In grad_steps = 2647, loss = 0.1166854202747345
In grad_steps = 2648, loss = 0.18081243336200714
In grad_steps = 2649, loss = 0.07412591576576233
In grad_steps = 2650, loss = 0.3845521807670593
In grad_steps = 2651, loss = 0.34740039706230164
In grad_steps = 2652, loss = 0.1784769892692566
In grad_steps = 2653, loss = 0.07084370404481888
In grad_steps = 2654, loss = 0.14544185996055603
In grad_steps = 2655, loss = 0.0200671274214983
In grad_steps = 2656, loss = 0.22658532857894897
In grad_steps = 2657, loss = 0.5336177349090576
In grad_steps = 2658, loss = 0.8521177768707275
In grad_steps = 2659, loss = 0.08096262067556381
In grad_steps = 2660, loss = 0.6218756437301636
In grad_steps = 2661, loss = 0.13264097273349762
In grad_steps = 2662, loss = 0.4774608612060547
In grad_steps = 2663, loss = 0.21303436160087585
In grad_steps = 2664, loss = 0.4817262589931488
In grad_steps = 2665, loss = 0.18078652024269104
In grad_steps = 2666, loss = 0.2709645628929138
In grad_steps = 2667, loss = 0.8531284332275391
In grad_steps = 2668, loss = 0.5788227319717407
In grad_steps = 2669, loss = 0.08307497203350067
In grad_steps = 2670, loss = 0.09768546372652054
In grad_steps = 2671, loss = 0.35409215092658997
In grad_steps = 2672, loss = 0.09918537735939026
In grad_steps = 2673, loss = 0.06434915214776993
In grad_steps = 2674, loss = 0.1893010139465332
In grad_steps = 2675, loss = 0.1842878758907318
In grad_steps = 2676, loss = 0.4621852934360504
In grad_steps = 2677, loss = 0.07782863080501556
In grad_steps = 2678, loss = 0.11396228522062302
In grad_steps = 2679, loss = 0.04590403288602829
In grad_steps = 2680, loss = 0.05590614303946495
In grad_steps = 2681, loss = 0.04481589421629906
In grad_steps = 2682, loss = 0.012805802747607231
In grad_steps = 2683, loss = 0.1474703699350357
In grad_steps = 2684, loss = 0.10985658317804337
In grad_steps = 2685, loss = 0.13484686613082886
In grad_steps = 2686, loss = 0.7534964084625244
In grad_steps = 2687, loss = 0.011956164613366127
In grad_steps = 2688, loss = 0.13938893377780914
In grad_steps = 2689, loss = 0.05104280635714531
In grad_steps = 2690, loss = 0.07769817858934402
In grad_steps = 2691, loss = 0.03558710962533951
In grad_steps = 2692, loss = 0.04457845166325569
In grad_steps = 2693, loss = 0.10883895307779312
In grad_steps = 2694, loss = 0.06414879113435745
In grad_steps = 2695, loss = 1.153780221939087
In grad_steps = 2696, loss = 0.3515351116657257
In grad_steps = 2697, loss = 0.19880862534046173
In grad_steps = 2698, loss = 0.13884225487709045
In grad_steps = 2699, loss = 0.0622594878077507
In grad_steps = 2700, loss = 0.02376718260347843
In grad_steps = 2701, loss = 1.009806752204895
In grad_steps = 2702, loss = 1.060098648071289
In grad_steps = 2703, loss = 0.45922669768333435
In grad_steps = 2704, loss = 0.04596111178398132
In grad_steps = 2705, loss = 0.1535567194223404
In grad_steps = 2706, loss = 0.0427175909280777
In grad_steps = 2707, loss = 0.2241763472557068
In grad_steps = 2708, loss = 0.7531050443649292
In grad_steps = 2709, loss = 0.38395118713378906
In grad_steps = 2710, loss = 0.16102418303489685
In grad_steps = 2711, loss = 0.08387921750545502
In grad_steps = 2712, loss = 0.33662959933280945
In grad_steps = 2713, loss = 0.2823116183280945
In grad_steps = 2714, loss = 0.059011202305555344
In grad_steps = 2715, loss = 0.15919595956802368
In grad_steps = 2716, loss = 0.047877825796604156
In grad_steps = 2717, loss = 0.16455893218517303
In grad_steps = 2718, loss = 0.11457668244838715
In grad_steps = 2719, loss = 0.04662596061825752
In grad_steps = 2720, loss = 0.1436263918876648
In grad_steps = 2721, loss = 0.08078713715076447
In grad_steps = 2722, loss = 0.02983512356877327
In grad_steps = 2723, loss = 1.0966386795043945
In grad_steps = 2724, loss = 0.030018962919712067
In grad_steps = 2725, loss = 0.06491639465093613
In grad_steps = 2726, loss = 0.12545660138130188
In grad_steps = 2727, loss = 0.04322104901075363
In grad_steps = 2728, loss = 0.023849517107009888
In grad_steps = 2729, loss = 0.01629149541258812
In grad_steps = 2730, loss = 0.09088821709156036
In grad_steps = 2731, loss = 0.21100640296936035
In grad_steps = 2732, loss = 0.1430789828300476
In grad_steps = 2733, loss = 0.05994855612516403
In grad_steps = 2734, loss = 0.5144118070602417
In grad_steps = 2735, loss = 0.20915479958057404
In grad_steps = 2736, loss = 0.02744166925549507
In grad_steps = 2737, loss = 0.056400761008262634
In grad_steps = 2738, loss = 0.14370843768119812
In grad_steps = 2739, loss = 1.8716028928756714
In grad_steps = 2740, loss = 0.034784626215696335
In grad_steps = 2741, loss = 0.4669800400733948
In grad_steps = 2742, loss = 0.7255414724349976
In grad_steps = 2743, loss = 0.09163541346788406
In grad_steps = 2744, loss = 0.07157251238822937
In grad_steps = 2745, loss = 0.037348151206970215
In grad_steps = 2746, loss = 0.21747569739818573
In grad_steps = 2747, loss = 0.026639247313141823
In grad_steps = 2748, loss = 0.26474690437316895
In grad_steps = 2749, loss = 0.06613000482320786
In grad_steps = 2750, loss = 0.16046565771102905
In grad_steps = 2751, loss = 0.11278170347213745
In grad_steps = 2752, loss = 0.4663375914096832
In grad_steps = 2753, loss = 0.3211289048194885
In grad_steps = 2754, loss = 0.14063076674938202
In grad_steps = 2755, loss = 0.05381651595234871
In grad_steps = 2756, loss = 0.5885709524154663
In grad_steps = 2757, loss = 0.14787453413009644
In grad_steps = 2758, loss = 0.04798388108611107
In grad_steps = 2759, loss = 0.8824512958526611
In grad_steps = 2760, loss = 0.2052050232887268
In grad_steps = 2761, loss = 0.08687768131494522
In grad_steps = 2762, loss = 0.3689940571784973
In grad_steps = 2763, loss = 0.04989568889141083
In grad_steps = 2764, loss = 0.49917271733283997
In grad_steps = 2765, loss = 0.1990969032049179
In grad_steps = 2766, loss = 0.09170334786176682
In grad_steps = 2767, loss = 0.42835357785224915
In grad_steps = 2768, loss = 0.11342637240886688
In grad_steps = 2769, loss = 0.10366543382406235
In grad_steps = 2770, loss = 0.4333868622779846
In grad_steps = 2771, loss = 0.23892301321029663
In grad_steps = 2772, loss = 0.0463251993060112
In grad_steps = 2773, loss = 0.012339373119175434
In grad_steps = 2774, loss = 0.2888284921646118
In grad_steps = 2775, loss = 0.42606598138809204
In grad_steps = 2776, loss = 0.13393846154212952
In grad_steps = 2777, loss = 0.24198158085346222
In grad_steps = 2778, loss = 0.014648836106061935
In grad_steps = 2779, loss = 0.04471531882882118
In grad_steps = 2780, loss = 0.07328453660011292
In grad_steps = 2781, loss = 0.027977844700217247
In grad_steps = 2782, loss = 0.34570547938346863
In grad_steps = 2783, loss = 0.24873314797878265
In grad_steps = 2784, loss = 0.06504006683826447
In grad_steps = 2785, loss = 0.02835804969072342
In grad_steps = 2786, loss = 0.5972647666931152
In grad_steps = 2787, loss = 1.04361093044281
In grad_steps = 2788, loss = 0.06978420168161392
In grad_steps = 2789, loss = 0.21807995438575745
In grad_steps = 2790, loss = 0.12726923823356628
In grad_steps = 2791, loss = 0.8938652276992798
In grad_steps = 2792, loss = 0.047322828322649
In grad_steps = 2793, loss = 0.014665631577372551
In grad_steps = 2794, loss = 0.2243780493736267
In grad_steps = 2795, loss = 0.9965711236000061
In grad_steps = 2796, loss = 0.012192822992801666
In grad_steps = 2797, loss = 0.05397994816303253
In grad_steps = 2798, loss = 0.016971826553344727
In grad_steps = 2799, loss = 0.14333850145339966
In grad_steps = 2800, loss = 0.05622207745909691
In grad_steps = 2801, loss = 0.4334351718425751
In grad_steps = 2802, loss = 0.12002000212669373
In grad_steps = 2803, loss = 0.06291947513818741
In grad_steps = 2804, loss = 0.2286531627178192
In grad_steps = 2805, loss = 0.35382962226867676
In grad_steps = 2806, loss = 0.04651042819023132
In grad_steps = 2807, loss = 0.5061315298080444
In grad_steps = 2808, loss = 0.02267010137438774
In grad_steps = 2809, loss = 0.863012969493866
In grad_steps = 2810, loss = 0.0583086758852005
In grad_steps = 2811, loss = 0.3103466331958771
In grad_steps = 2812, loss = 0.11455614119768143
In grad_steps = 2813, loss = 0.036641404032707214
In grad_steps = 2814, loss = 0.01888854242861271
In grad_steps = 2815, loss = 0.7259424924850464
In grad_steps = 2816, loss = 0.05766407400369644
In grad_steps = 2817, loss = 0.13626328110694885
In grad_steps = 2818, loss = 0.7746729850769043
In grad_steps = 2819, loss = 0.09290656447410583
In grad_steps = 2820, loss = 0.34006401896476746
In grad_steps = 2821, loss = 0.160054549574852
In grad_steps = 2822, loss = 0.4145335257053375
In grad_steps = 2823, loss = 0.21069186925888062
In grad_steps = 2824, loss = 0.04321904480457306
In grad_steps = 2825, loss = 0.15474891662597656
In grad_steps = 2826, loss = 0.06645089387893677
In grad_steps = 2827, loss = 0.48803234100341797
In grad_steps = 2828, loss = 0.07410844415426254
In grad_steps = 2829, loss = 0.01644217036664486
In grad_steps = 2830, loss = 0.052946723997592926
In grad_steps = 2831, loss = 0.06793457269668579
In grad_steps = 2832, loss = 0.7600055932998657
In grad_steps = 2833, loss = 0.04286849498748779
In grad_steps = 2834, loss = 0.2704745829105377
In grad_steps = 2835, loss = 0.041258715093135834
In grad_steps = 2836, loss = 0.08468124270439148
In grad_steps = 2837, loss = 0.5852612257003784
In grad_steps = 2838, loss = 0.16237084567546844
In grad_steps = 2839, loss = 0.038158249109983444
In grad_steps = 2840, loss = 0.6665982007980347
In grad_steps = 2841, loss = 1.1385916471481323
In grad_steps = 2842, loss = 0.17123094201087952
In grad_steps = 2843, loss = 0.19366855919361115
In grad_steps = 2844, loss = 0.3486402630805969
In grad_steps = 2845, loss = 0.09684908390045166
In grad_steps = 2846, loss = 0.03591049835085869
In grad_steps = 2847, loss = 0.5358737111091614
In grad_steps = 2848, loss = 0.08281233161687851
In grad_steps = 2849, loss = 0.2109212428331375
In grad_steps = 2850, loss = 0.03874704986810684
In grad_steps = 2851, loss = 0.5962344408035278
In grad_steps = 2852, loss = 0.08346579968929291
In grad_steps = 2853, loss = 0.029239442199468613
In grad_steps = 2854, loss = 0.24368858337402344
In grad_steps = 2855, loss = 0.9325517416000366
In grad_steps = 2856, loss = 0.2439747452735901
In grad_steps = 2857, loss = 0.23973770439624786
In grad_steps = 2858, loss = 0.5458054542541504
In grad_steps = 2859, loss = 0.06275159120559692
In grad_steps = 2860, loss = 0.3694189488887787
In grad_steps = 2861, loss = 0.7987893223762512
In grad_steps = 2862, loss = 0.11575628817081451
In grad_steps = 2863, loss = 0.022423958405852318
In grad_steps = 2864, loss = 0.20880690217018127
In grad_steps = 2865, loss = 0.10090473294258118
In grad_steps = 2866, loss = 0.08879812806844711
In grad_steps = 2867, loss = 0.09169275313615799
In grad_steps = 2868, loss = 0.0995049923658371
In grad_steps = 2869, loss = 0.4222489297389984
In grad_steps = 2870, loss = 0.3642054498195648
In grad_steps = 2871, loss = 0.14381666481494904
In grad_steps = 2872, loss = 0.5191861987113953
In grad_steps = 2873, loss = 0.09900835156440735
In grad_steps = 2874, loss = 0.2877599596977234
In grad_steps = 2875, loss = 0.0804428830742836
In grad_steps = 2876, loss = 0.02385636419057846
In grad_steps = 2877, loss = 0.019436584785580635
In grad_steps = 2878, loss = 0.1374938040971756
In grad_steps = 2879, loss = 0.033515140414237976
In grad_steps = 2880, loss = 0.29801368713378906
In grad_steps = 2881, loss = 0.18319454789161682
In grad_steps = 2882, loss = 0.02549116685986519
In grad_steps = 2883, loss = 0.01788587123155594
In grad_steps = 2884, loss = 0.8530752062797546
In grad_steps = 2885, loss = 0.08664865046739578
In grad_steps = 2886, loss = 0.04142845794558525
In grad_steps = 2887, loss = 1.3850979804992676
In grad_steps = 2888, loss = 0.8912944197654724
In grad_steps = 2889, loss = 0.5367214679718018
In grad_steps = 2890, loss = 0.09510369598865509
In grad_steps = 2891, loss = 0.22186565399169922
In grad_steps = 2892, loss = 0.07905379682779312
In grad_steps = 2893, loss = 0.06375324726104736
In grad_steps = 2894, loss = 0.10127703845500946
In grad_steps = 2895, loss = 0.19890914857387543
In grad_steps = 2896, loss = 0.017421837896108627
In grad_steps = 2897, loss = 0.6871260404586792
In grad_steps = 2898, loss = 0.7536076307296753
In grad_steps = 2899, loss = 0.11361736059188843
In grad_steps = 2900, loss = 0.6822493672370911
In grad_steps = 2901, loss = 0.0989350825548172
In grad_steps = 2902, loss = 0.28023794293403625
In grad_steps = 2903, loss = 0.7951910495758057
In grad_steps = 2904, loss = 0.21963438391685486
In grad_steps = 2905, loss = 0.34017056226730347
In grad_steps = 2906, loss = 0.09254999458789825
In grad_steps = 2907, loss = 0.6044740676879883
In grad_steps = 2908, loss = 0.5521525740623474
In grad_steps = 2909, loss = 0.36714479327201843
In grad_steps = 2910, loss = 0.3819735050201416
In grad_steps = 2911, loss = 0.442804217338562
In grad_steps = 2912, loss = 0.3237763047218323
In grad_steps = 2913, loss = 0.7307626008987427
In grad_steps = 2914, loss = 0.13230451941490173
In grad_steps = 2915, loss = 0.13908007740974426
In grad_steps = 2916, loss = 0.11904681473970413
In grad_steps = 2917, loss = 0.959350049495697
In grad_steps = 2918, loss = 0.38324591517448425
In grad_steps = 2919, loss = 0.4601828157901764
In grad_steps = 2920, loss = 0.8133131265640259
In grad_steps = 2921, loss = 0.32067424058914185
In grad_steps = 2922, loss = 0.07118771970272064
In grad_steps = 2923, loss = 0.11446423083543777
In grad_steps = 2924, loss = 0.08526329696178436
In grad_steps = 2925, loss = 0.199814110994339
In grad_steps = 2926, loss = 0.07466529309749603
In grad_steps = 2927, loss = 0.12444669008255005
In grad_steps = 2928, loss = 0.110348641872406
In grad_steps = 2929, loss = 0.19043853878974915
In grad_steps = 2930, loss = 0.022905632853507996
In grad_steps = 2931, loss = 0.224543035030365
In grad_steps = 2932, loss = 0.30617624521255493
In grad_steps = 2933, loss = 0.6557437777519226
In grad_steps = 2934, loss = 0.4221118092536926
In grad_steps = 2935, loss = 0.21635545790195465
In grad_steps = 2936, loss = 0.07869479060173035
In grad_steps = 2937, loss = 0.03749232366681099
In grad_steps = 2938, loss = 0.035524092614650726
In grad_steps = 2939, loss = 0.06546627730131149
In grad_steps = 2940, loss = 0.4260387718677521
In grad_steps = 2941, loss = 0.14163470268249512
In grad_steps = 2942, loss = 0.7666153907775879
In grad_steps = 2943, loss = 0.10497292876243591
In grad_steps = 2944, loss = 0.14448115229606628
In grad_steps = 2945, loss = 0.972934365272522
In grad_steps = 2946, loss = 0.036653611809015274
In grad_steps = 2947, loss = 0.13727156817913055
In grad_steps = 2948, loss = 0.27616825699806213
In grad_steps = 2949, loss = 0.1381053626537323
In grad_steps = 2950, loss = 0.4415622353553772
In grad_steps = 2951, loss = 0.183156356215477
In grad_steps = 2952, loss = 0.32572728395462036
In grad_steps = 2953, loss = 0.22578400373458862
In grad_steps = 2954, loss = 0.021961068734526634
In grad_steps = 2955, loss = 0.05316666141152382
In grad_steps = 2956, loss = 0.06686284393072128
In grad_steps = 2957, loss = 0.15720723569393158
In grad_steps = 2958, loss = 0.054194375872612
In grad_steps = 2959, loss = 0.06447938829660416
In grad_steps = 2960, loss = 0.18478922545909882
In grad_steps = 2961, loss = 0.3074495792388916
In grad_steps = 2962, loss = 0.036974165588617325
In grad_steps = 2963, loss = 0.31296306848526
In grad_steps = 2964, loss = 0.5490881204605103
In grad_steps = 2965, loss = 0.0494079664349556
In grad_steps = 2966, loss = 0.6841283440589905
In grad_steps = 2967, loss = 0.10732124745845795
In grad_steps = 2968, loss = 0.10324172675609589
In grad_steps = 2969, loss = 0.1658317744731903
In grad_steps = 2970, loss = 0.011277252808213234
In grad_steps = 2971, loss = 0.10426666587591171
In grad_steps = 2972, loss = 0.022612029686570168
In grad_steps = 2973, loss = 0.04478510841727257
In grad_steps = 2974, loss = 1.3472514152526855
In grad_steps = 2975, loss = 0.2645390033721924
In grad_steps = 2976, loss = 0.030328629538416862
In grad_steps = 2977, loss = 0.02766408771276474
In grad_steps = 2978, loss = 0.18337872624397278
In grad_steps = 2979, loss = 0.7454846501350403
In grad_steps = 2980, loss = 0.01363803818821907
In grad_steps = 2981, loss = 0.4045145511627197
In grad_steps = 2982, loss = 0.1452234834432602
In grad_steps = 2983, loss = 0.04312204197049141
In grad_steps = 2984, loss = 0.03771067038178444
In grad_steps = 2985, loss = 0.04744233191013336
In grad_steps = 2986, loss = 0.2774464786052704
In grad_steps = 2987, loss = 0.05534815788269043
In grad_steps = 2988, loss = 0.08528555184602737
In grad_steps = 2989, loss = 0.06443022191524506
In grad_steps = 2990, loss = 0.18488813936710358
In grad_steps = 2991, loss = 0.01961178332567215
In grad_steps = 2992, loss = 0.7225151658058167
In grad_steps = 2993, loss = 0.6484031677246094
In grad_steps = 2994, loss = 0.027481338009238243
In grad_steps = 2995, loss = 0.0434732623398304
In grad_steps = 2996, loss = 0.04115879535675049
In grad_steps = 2997, loss = 0.1427108347415924
In grad_steps = 2998, loss = 1.07852041721344
In grad_steps = 2999, loss = 0.1548694223165512
In grad_steps = 3000, loss = 0.09211937338113785
In grad_steps = 3001, loss = 0.35364633798599243
In grad_steps = 3002, loss = 0.03823927789926529
In grad_steps = 3003, loss = 0.6884113550186157
In grad_steps = 3004, loss = 0.11700893938541412
In grad_steps = 3005, loss = 1.4629950523376465
In grad_steps = 3006, loss = 0.38437601923942566
In grad_steps = 3007, loss = 0.6035512685775757
In grad_steps = 3008, loss = 0.9402864575386047
In grad_steps = 3009, loss = 0.08733117580413818
In grad_steps = 3010, loss = 0.03742092847824097
In grad_steps = 3011, loss = 0.29062867164611816
In grad_steps = 3012, loss = 0.06094552204012871
In grad_steps = 3013, loss = 0.05197474732995033
In grad_steps = 3014, loss = 0.1408352553844452
In grad_steps = 3015, loss = 0.3232007920742035
In grad_steps = 3016, loss = 0.042022671550512314
In grad_steps = 3017, loss = 0.05960061773657799
In grad_steps = 3018, loss = 0.12942153215408325
In grad_steps = 3019, loss = 0.21697068214416504
In grad_steps = 3020, loss = 0.31878358125686646
In grad_steps = 3021, loss = 0.15961605310440063
In grad_steps = 3022, loss = 0.23377972841262817
In grad_steps = 3023, loss = 0.34026411175727844
In grad_steps = 3024, loss = 0.39923596382141113
In grad_steps = 3025, loss = 0.12469423562288284
In grad_steps = 3026, loss = 1.099929928779602
In grad_steps = 3027, loss = 0.054715827107429504
In grad_steps = 3028, loss = 0.54034823179245
In grad_steps = 3029, loss = 0.11711093038320541
In grad_steps = 3030, loss = 0.10060049593448639
In grad_steps = 3031, loss = 0.060096509754657745
In grad_steps = 3032, loss = 0.0903388038277626
In grad_steps = 3033, loss = 0.04064294695854187
In grad_steps = 3034, loss = 0.613874077796936
In grad_steps = 3035, loss = 0.14921383559703827
In grad_steps = 3036, loss = 0.17409516870975494
In grad_steps = 3037, loss = 0.14163178205490112
In grad_steps = 3038, loss = 0.11570598930120468
In grad_steps = 3039, loss = 0.16415375471115112
In grad_steps = 3040, loss = 0.15089958906173706
In grad_steps = 3041, loss = 0.5100657343864441
In grad_steps = 3042, loss = 0.025775525718927383
In grad_steps = 3043, loss = 0.11670982837677002
In grad_steps = 3044, loss = 0.4624263048171997
In grad_steps = 3045, loss = 0.06345408409833908
In grad_steps = 3046, loss = 0.04993806034326553
In grad_steps = 3047, loss = 0.09606441855430603
In grad_steps = 3048, loss = 0.6358191967010498
In grad_steps = 3049, loss = 0.13002726435661316
In grad_steps = 3050, loss = 0.15753208100795746
In grad_steps = 3051, loss = 0.031036103144288063
In grad_steps = 3052, loss = 0.23272015154361725
In grad_steps = 3053, loss = 0.35417306423187256
In grad_steps = 3054, loss = 0.02896234765648842
In grad_steps = 3055, loss = 0.05974357947707176
In grad_steps = 3056, loss = 0.01958939991891384
In grad_steps = 3057, loss = 0.04816137254238129
In grad_steps = 3058, loss = 0.09722493588924408
In grad_steps = 3059, loss = 0.24160048365592957
In grad_steps = 3060, loss = 0.019965292885899544
In grad_steps = 3061, loss = 0.022333504632115364
In grad_steps = 3062, loss = 0.01821208745241165
In grad_steps = 3063, loss = 0.016498683020472527
In grad_steps = 3064, loss = 0.18169626593589783
In grad_steps = 3065, loss = 0.13001945614814758
In grad_steps = 3066, loss = 1.491943597793579
In grad_steps = 3067, loss = 1.413269281387329
In grad_steps = 3068, loss = 2.0347375869750977
In grad_steps = 3069, loss = 0.057637181133031845
In grad_steps = 3070, loss = 0.019998274743556976
In grad_steps = 3071, loss = 0.031686265021562576
In grad_steps = 3072, loss = 0.024903886020183563
In grad_steps = 3073, loss = 0.0193475354462862
In grad_steps = 3074, loss = 1.0318915843963623
In grad_steps = 3075, loss = 0.4650055468082428
In grad_steps = 3076, loss = 0.2851983606815338
In grad_steps = 3077, loss = 0.04255036637187004
In grad_steps = 3078, loss = 0.0817863792181015
In grad_steps = 3079, loss = 0.06304917484521866
In grad_steps = 3080, loss = 0.1536935567855835
In grad_steps = 3081, loss = 0.04827168956398964
In grad_steps = 3082, loss = 0.03413071483373642
In grad_steps = 3083, loss = 0.4223519563674927
In grad_steps = 3084, loss = 0.29280564188957214
In grad_steps = 3085, loss = 0.5518211126327515
In grad_steps = 3086, loss = 0.7533559203147888
In grad_steps = 3087, loss = 0.08607099205255508
In grad_steps = 3088, loss = 0.36017942428588867
In grad_steps = 3089, loss = 0.1344819962978363
In grad_steps = 3090, loss = 0.07012376934289932
In grad_steps = 3091, loss = 0.3481918275356293
In grad_steps = 3092, loss = 0.1347273886203766
In grad_steps = 3093, loss = 0.24036242067813873
In grad_steps = 3094, loss = 0.24984705448150635
In grad_steps = 3095, loss = 0.060043446719646454
In grad_steps = 3096, loss = 0.06229535862803459
In grad_steps = 3097, loss = 0.407856822013855
In grad_steps = 3098, loss = 0.5345819592475891
In grad_steps = 3099, loss = 0.22270016372203827
In grad_steps = 3100, loss = 0.19430364668369293
In grad_steps = 3101, loss = 0.07783973962068558
In grad_steps = 3102, loss = 0.4187074601650238
In grad_steps = 3103, loss = 0.07354043424129486
In grad_steps = 3104, loss = 0.0538148507475853
In grad_steps = 3105, loss = 0.11814385652542114
In grad_steps = 3106, loss = 0.14227254688739777
In grad_steps = 3107, loss = 0.18820540606975555
In grad_steps = 3108, loss = 0.3228028416633606
In grad_steps = 3109, loss = 0.10821069031953812
In grad_steps = 3110, loss = 0.06171580031514168
In grad_steps = 3111, loss = 1.013836145401001
In grad_steps = 3112, loss = 0.24773579835891724
In grad_steps = 3113, loss = 0.11865529417991638
In grad_steps = 3114, loss = 0.2671952545642853
In grad_steps = 3115, loss = 0.3276803195476532
In grad_steps = 3116, loss = 0.037408918142318726
In grad_steps = 3117, loss = 0.40701884031295776
In grad_steps = 3118, loss = 0.643808901309967
In grad_steps = 3119, loss = 0.025922968983650208
In grad_steps = 3120, loss = 0.08318565785884857
In grad_steps = 3121, loss = 0.06952826678752899
In grad_steps = 3122, loss = 0.441474974155426
In grad_steps = 3123, loss = 0.05645252391695976
In grad_steps = 3124, loss = 0.08887423574924469
In grad_steps = 3125, loss = 0.17443911731243134
In grad_steps = 3126, loss = 0.8930745124816895
In grad_steps = 3127, loss = 0.02327941358089447
In grad_steps = 3128, loss = 0.061793599277734756
In grad_steps = 3129, loss = 0.05182608589529991
In grad_steps = 3130, loss = 0.04766944423317909
In grad_steps = 3131, loss = 0.07236118614673615
In grad_steps = 3132, loss = 0.01983882300555706
In grad_steps = 3133, loss = 0.057298194617033005
In grad_steps = 3134, loss = 0.05303812026977539
In grad_steps = 3135, loss = 0.02647269517183304
In grad_steps = 3136, loss = 0.38576504588127136
In grad_steps = 3137, loss = 0.42143896222114563
In grad_steps = 3138, loss = 0.012131450697779655
In grad_steps = 3139, loss = 0.030775854364037514
In grad_steps = 3140, loss = 0.47044965624809265
In grad_steps = 3141, loss = 0.19929766654968262
In grad_steps = 3142, loss = 0.034761011600494385
In grad_steps = 3143, loss = 0.2651873230934143
In grad_steps = 3144, loss = 0.026270568370819092
In grad_steps = 3145, loss = 0.017078949138522148
In grad_steps = 3146, loss = 0.22959817945957184
In grad_steps = 3147, loss = 1.2853344678878784
In grad_steps = 3148, loss = 0.43861204385757446
In grad_steps = 3149, loss = 0.14180579781532288
In grad_steps = 3150, loss = 1.102046251296997
In grad_steps = 3151, loss = 0.040472786873579025
In grad_steps = 3152, loss = 0.2051980197429657
In grad_steps = 3153, loss = 0.2465827912092209
In grad_steps = 3154, loss = 0.12312744557857513
In grad_steps = 3155, loss = 0.34252452850341797
In grad_steps = 3156, loss = 0.0529811792075634
In grad_steps = 3157, loss = 0.027252215892076492
In grad_steps = 3158, loss = 0.12280471622943878
In grad_steps = 3159, loss = 0.03256458416581154
In grad_steps = 3160, loss = 0.024334754794836044
In grad_steps = 3161, loss = 0.12429515272378922
In grad_steps = 3162, loss = 0.02308979071676731
In grad_steps = 3163, loss = 0.8095215559005737
In grad_steps = 3164, loss = 0.03077073208987713
In grad_steps = 3165, loss = 0.243336021900177
In grad_steps = 3166, loss = 0.2020466923713684
In grad_steps = 3167, loss = 0.08755219727754593
In grad_steps = 3168, loss = 0.0940791517496109
In grad_steps = 3169, loss = 0.41558516025543213
In grad_steps = 3170, loss = 0.022778568789362907
In grad_steps = 3171, loss = 0.22912351787090302
In grad_steps = 3172, loss = 0.03253354877233505
In grad_steps = 3173, loss = 0.023692604154348373
In grad_steps = 3174, loss = 0.1002974882721901
In grad_steps = 3175, loss = 0.21660210192203522
In grad_steps = 3176, loss = 0.4579393267631531
In grad_steps = 3177, loss = 0.020088590681552887
In grad_steps = 3178, loss = 0.03133389353752136
In grad_steps = 3179, loss = 0.45358550548553467
In grad_steps = 3180, loss = 0.10003219544887543
In grad_steps = 3181, loss = 0.14004755020141602
In grad_steps = 3182, loss = 0.5312415361404419
In grad_steps = 3183, loss = 0.34581440687179565
In grad_steps = 3184, loss = 0.1388883888721466
In grad_steps = 3185, loss = 0.7451452016830444
In grad_steps = 3186, loss = 0.5249313116073608
In grad_steps = 3187, loss = 0.4612973928451538
In grad_steps = 3188, loss = 1.569740891456604
In grad_steps = 3189, loss = 0.1921749860048294
In grad_steps = 3190, loss = 0.07948672771453857
In grad_steps = 3191, loss = 0.20838826894760132
In grad_steps = 3192, loss = 0.10909702628850937
In grad_steps = 3193, loss = 0.10271080583333969
In grad_steps = 3194, loss = 0.38962826132774353
In grad_steps = 3195, loss = 0.036193422973155975
In grad_steps = 3196, loss = 0.722038209438324
In grad_steps = 3197, loss = 0.350009024143219
In grad_steps = 3198, loss = 0.038594312965869904
In grad_steps = 3199, loss = 0.4848247766494751
In grad_steps = 3200, loss = 0.05427462235093117
In grad_steps = 3201, loss = 1.054736852645874
In grad_steps = 3202, loss = 0.3577200770378113
In grad_steps = 3203, loss = 0.5112389326095581
In grad_steps = 3204, loss = 0.8519041538238525
In grad_steps = 3205, loss = 0.22524508833885193
In grad_steps = 3206, loss = 0.23458652198314667
In grad_steps = 3207, loss = 0.09748519957065582
In grad_steps = 3208, loss = 0.6046774983406067
In grad_steps = 3209, loss = 0.1343524158000946
In grad_steps = 3210, loss = 0.2710452079772949
In grad_steps = 3211, loss = 0.5819112062454224
In grad_steps = 3212, loss = 0.19858402013778687
In grad_steps = 3213, loss = 0.4428110718727112
In grad_steps = 3214, loss = 0.41089004278182983
In grad_steps = 3215, loss = 0.04649338498711586
In grad_steps = 3216, loss = 0.06390778720378876
In grad_steps = 3217, loss = 0.2013901323080063
In grad_steps = 3218, loss = 0.40797680616378784
In grad_steps = 3219, loss = 0.1732349991798401
In grad_steps = 3220, loss = 0.30195316672325134
In grad_steps = 3221, loss = 0.20830124616622925
In grad_steps = 3222, loss = 0.3374926447868347
In grad_steps = 3223, loss = 0.153152734041214
In grad_steps = 3224, loss = 0.08756783604621887
In grad_steps = 3225, loss = 0.13986976444721222
In grad_steps = 3226, loss = 0.11351151764392853
In grad_steps = 3227, loss = 0.06784070283174515
In grad_steps = 3228, loss = 0.11357203125953674
In grad_steps = 3229, loss = 0.12718334794044495
In grad_steps = 3230, loss = 0.02172517590224743
In grad_steps = 3231, loss = 0.02704811468720436
In grad_steps = 3232, loss = 0.2200344204902649
In grad_steps = 3233, loss = 0.06566903740167618
In grad_steps = 3234, loss = 0.599361777305603
In grad_steps = 3235, loss = 0.053818635642528534
In grad_steps = 3236, loss = 0.024296864867210388
In grad_steps = 3237, loss = 0.012208163738250732
In grad_steps = 3238, loss = 0.029112927615642548
In grad_steps = 3239, loss = 0.09119870513677597
In grad_steps = 3240, loss = 0.017661798745393753
In grad_steps = 3241, loss = 0.07152573764324188
In grad_steps = 3242, loss = 0.7481398582458496
In grad_steps = 3243, loss = 0.6790322065353394
In grad_steps = 3244, loss = 0.0039032339118421078
In grad_steps = 3245, loss = 1.141768455505371
In grad_steps = 3246, loss = 0.10126683115959167
In grad_steps = 3247, loss = 0.054271209985017776
In grad_steps = 3248, loss = 0.3334185779094696
In grad_steps = 3249, loss = 0.12979541718959808
In grad_steps = 3250, loss = 0.27799713611602783
In grad_steps = 3251, loss = 0.9759318232536316
In grad_steps = 3252, loss = 0.04376073554158211
In grad_steps = 3253, loss = 0.14163199067115784
In grad_steps = 3254, loss = 0.18128331005573273
In grad_steps = 3255, loss = 0.04598317667841911
In grad_steps = 3256, loss = 0.19477659463882446
In grad_steps = 3257, loss = 0.2861292362213135
In grad_steps = 3258, loss = 0.12514124810695648
In grad_steps = 3259, loss = 0.11918573081493378
In grad_steps = 3260, loss = 0.06502874195575714
In grad_steps = 3261, loss = 0.08416564017534256
In grad_steps = 3262, loss = 0.1275874525308609
In grad_steps = 3263, loss = 0.050645168870687485
In grad_steps = 3264, loss = 0.030202485620975494
In grad_steps = 3265, loss = 0.057838696986436844
In grad_steps = 3266, loss = 0.487557977437973
In grad_steps = 3267, loss = 0.5452592968940735
In grad_steps = 3268, loss = 0.1782202422618866
In grad_steps = 3269, loss = 0.07375209033489227
In grad_steps = 3270, loss = 0.018293652683496475
In grad_steps = 3271, loss = 0.9007715582847595
In grad_steps = 3272, loss = 0.8812121152877808
In grad_steps = 3273, loss = 0.6992484331130981
In grad_steps = 3274, loss = 0.6107043027877808
In grad_steps = 3275, loss = 0.02243787981569767
In grad_steps = 3276, loss = 1.237450361251831
In grad_steps = 3277, loss = 0.510709822177887
In grad_steps = 3278, loss = 0.0433320626616478
In grad_steps = 3279, loss = 0.09052255004644394
In grad_steps = 3280, loss = 0.24523529410362244
In grad_steps = 3281, loss = 0.6826790571212769
In grad_steps = 3282, loss = 0.16769270598888397
In grad_steps = 3283, loss = 0.4294913113117218
In grad_steps = 3284, loss = 0.24181081354618073
In grad_steps = 3285, loss = 0.4899757504463196
In grad_steps = 3286, loss = 0.4348960518836975
In grad_steps = 3287, loss = 0.11023484170436859
In grad_steps = 3288, loss = 0.11625587940216064
In grad_steps = 3289, loss = 0.22836339473724365
In grad_steps = 3290, loss = 0.5613782405853271
In grad_steps = 3291, loss = 0.22131364047527313
In grad_steps = 3292, loss = 0.4418671727180481
In grad_steps = 3293, loss = 0.45762723684310913
In grad_steps = 3294, loss = 0.21450939774513245
In grad_steps = 3295, loss = 0.22745350003242493
In grad_steps = 3296, loss = 0.8431531190872192
In grad_steps = 3297, loss = 0.15897423028945923
In grad_steps = 3298, loss = 0.08941567689180374
In grad_steps = 3299, loss = 0.17690744996070862
In grad_steps = 3300, loss = 0.35219889879226685
In grad_steps = 3301, loss = 0.37473562359809875
In grad_steps = 3302, loss = 0.4577234983444214
In grad_steps = 3303, loss = 0.1616993099451065
In grad_steps = 3304, loss = 0.6374253034591675
In grad_steps = 3305, loss = 0.250309020280838
In grad_steps = 3306, loss = 0.44766879081726074
In grad_steps = 3307, loss = 0.22658869624137878
In grad_steps = 3308, loss = 0.3495956361293793
In grad_steps = 3309, loss = 0.7905587553977966
In grad_steps = 3310, loss = 0.35148605704307556
In grad_steps = 3311, loss = 0.26019030809402466
In grad_steps = 3312, loss = 0.17747393250465393
In grad_steps = 3313, loss = 0.09776565432548523
In grad_steps = 3314, loss = 0.023936282843351364
In grad_steps = 3315, loss = 0.16796687245368958
In grad_steps = 3316, loss = 0.7809017896652222
In grad_steps = 3317, loss = 0.059659749269485474
In grad_steps = 3318, loss = 0.8301680088043213
In grad_steps = 3319, loss = 0.39108285307884216
In grad_steps = 3320, loss = 0.23902755975723267
In grad_steps = 3321, loss = 0.05338578671216965
In grad_steps = 3322, loss = 0.08371804654598236
In grad_steps = 3323, loss = 0.11113739758729935
In grad_steps = 3324, loss = 0.08786527067422867
In grad_steps = 3325, loss = 0.08490775525569916
In grad_steps = 3326, loss = 0.28852376341819763
In grad_steps = 3327, loss = 0.033753350377082825
In grad_steps = 3328, loss = 0.11360214650630951
In grad_steps = 3329, loss = 0.1705738753080368
In grad_steps = 3330, loss = 0.06540150940418243
In grad_steps = 3331, loss = 0.09962716698646545
In grad_steps = 3332, loss = 0.0930890440940857
In grad_steps = 3333, loss = 0.1617092490196228
In grad_steps = 3334, loss = 0.0574614442884922
In grad_steps = 3335, loss = 0.3030630648136139
In grad_steps = 3336, loss = 0.010184252634644508
In grad_steps = 3337, loss = 0.008517528884112835
In grad_steps = 3338, loss = 0.07140622287988663
In grad_steps = 3339, loss = 0.11050347238779068
In grad_steps = 3340, loss = 0.01316942647099495
In grad_steps = 3341, loss = 0.19352437555789948
In grad_steps = 3342, loss = 0.017363768070936203
In grad_steps = 3343, loss = 0.01724054664373398
In grad_steps = 3344, loss = 0.02024710550904274
In grad_steps = 3345, loss = 0.11277857422828674
In grad_steps = 3346, loss = 0.019011596217751503
In grad_steps = 3347, loss = 0.5576843023300171
In grad_steps = 3348, loss = 0.004599207080900669
In grad_steps = 3349, loss = 0.04626153036952019
In grad_steps = 3350, loss = 0.20652878284454346
In grad_steps = 3351, loss = 0.21632462739944458
In grad_steps = 3352, loss = 0.12617981433868408
In grad_steps = 3353, loss = 0.253639817237854
In grad_steps = 3354, loss = 0.31348684430122375
In grad_steps = 3355, loss = 0.004130691289901733
In grad_steps = 3356, loss = 0.2189565896987915
In grad_steps = 3357, loss = 0.006383122410625219
In grad_steps = 3358, loss = 0.5488450527191162
In grad_steps = 3359, loss = 0.2427215278148651
In grad_steps = 3360, loss = 0.18394753336906433
In grad_steps = 3361, loss = 0.029339700937271118
In grad_steps = 3362, loss = 0.0029711935203522444
In grad_steps = 3363, loss = 0.0040266066789627075
In grad_steps = 3364, loss = 0.11860548704862595
In grad_steps = 3365, loss = 0.2950415015220642
In grad_steps = 3366, loss = 0.11333455890417099
In grad_steps = 3367, loss = 0.024623945355415344
In grad_steps = 3368, loss = 0.3674294948577881
In grad_steps = 3369, loss = 0.004116041120141745
In grad_steps = 3370, loss = 0.16586743295192719
In grad_steps = 3371, loss = 0.40314579010009766
In grad_steps = 3372, loss = 0.4104738235473633
In grad_steps = 3373, loss = 0.6365087032318115
In grad_steps = 3374, loss = 0.04320775717496872
In grad_steps = 3375, loss = 1.0242215394973755
In grad_steps = 3376, loss = 0.30763980746269226
In grad_steps = 3377, loss = 0.46947675943374634
In grad_steps = 3378, loss = 1.2985820770263672
In grad_steps = 3379, loss = 0.11286238580942154
In grad_steps = 3380, loss = 0.13205458223819733
In grad_steps = 3381, loss = 0.03243689611554146
In grad_steps = 3382, loss = 0.570603609085083
In grad_steps = 3383, loss = 0.7434641122817993
In grad_steps = 3384, loss = 0.7291850447654724
In grad_steps = 3385, loss = 0.09493588656187057
In grad_steps = 3386, loss = 0.2870914041996002
In grad_steps = 3387, loss = 0.09032785147428513
In grad_steps = 3388, loss = 0.19075369834899902
In grad_steps = 3389, loss = 0.10809917002916336
In grad_steps = 3390, loss = 0.10333514213562012
In grad_steps = 3391, loss = 0.1432737410068512
In grad_steps = 3392, loss = 0.2527660131454468
In grad_steps = 3393, loss = 0.5829862952232361
In grad_steps = 3394, loss = 0.12839245796203613
In grad_steps = 3395, loss = 0.08709761500358582
In grad_steps = 3396, loss = 0.06142216920852661
In grad_steps = 3397, loss = 0.8186648488044739
In grad_steps = 3398, loss = 0.41276848316192627
In grad_steps = 3399, loss = 0.13395586609840393
In grad_steps = 3400, loss = 0.04404882341623306
In grad_steps = 3401, loss = 0.07744605839252472
In grad_steps = 3402, loss = 0.24379593133926392
In grad_steps = 3403, loss = 1.955926775932312
In grad_steps = 3404, loss = 0.054891377687454224
In grad_steps = 3405, loss = 0.8915805220603943
In grad_steps = 3406, loss = 0.02655818499624729
In grad_steps = 3407, loss = 0.5593929290771484
In grad_steps = 3408, loss = 0.7354251742362976
In grad_steps = 3409, loss = 0.14521683752536774
In grad_steps = 3410, loss = 0.04366926848888397
In grad_steps = 3411, loss = 0.05375352501869202
In grad_steps = 3412, loss = 0.19902659952640533
In grad_steps = 3413, loss = 0.07046167552471161
In grad_steps = 3414, loss = 0.11152708530426025
In grad_steps = 3415, loss = 0.01924854889512062
In grad_steps = 3416, loss = 0.9472846984863281
In grad_steps = 3417, loss = 0.11136635392904282
In grad_steps = 3418, loss = 0.07091543823480606
In grad_steps = 3419, loss = 0.21393908560276031
In grad_steps = 3420, loss = 0.12314504384994507
In grad_steps = 3421, loss = 0.028979193419218063
In grad_steps = 3422, loss = 0.12549954652786255
In grad_steps = 3423, loss = 0.6148778200149536
In grad_steps = 3424, loss = 0.05582646653056145
In grad_steps = 3425, loss = 0.15291853249073029
In grad_steps = 3426, loss = 1.0410372018814087
In grad_steps = 3427, loss = 0.07412058860063553
In grad_steps = 3428, loss = 0.032442186027765274
In grad_steps = 3429, loss = 0.33118385076522827
In grad_steps = 3430, loss = 0.2631883919239044
In grad_steps = 3431, loss = 0.21366728842258453
In grad_steps = 3432, loss = 0.23177403211593628
In grad_steps = 3433, loss = 0.03779861330986023
In grad_steps = 3434, loss = 0.04653496295213699
In grad_steps = 3435, loss = 0.7410091161727905
In grad_steps = 3436, loss = 0.045488763600587845
In grad_steps = 3437, loss = 0.05200086906552315
In grad_steps = 3438, loss = 0.15183715522289276
In grad_steps = 3439, loss = 0.03057284839451313
In grad_steps = 3440, loss = 0.03745490312576294
In grad_steps = 3441, loss = 0.781507670879364
In grad_steps = 3442, loss = 0.342937171459198
In grad_steps = 3443, loss = 0.12900762259960175
In grad_steps = 3444, loss = 0.17266908288002014
In grad_steps = 3445, loss = 0.6347416043281555
In grad_steps = 3446, loss = 0.13123005628585815
In grad_steps = 3447, loss = 0.081846222281456
In grad_steps = 3448, loss = 0.09764755517244339
In grad_steps = 3449, loss = 0.03179941698908806
In grad_steps = 3450, loss = 0.023083457723259926
In grad_steps = 3451, loss = 0.0731666162610054
In grad_steps = 3452, loss = 0.10344251245260239
In grad_steps = 3453, loss = 0.07950703054666519
In grad_steps = 3454, loss = 0.07792319357395172
In grad_steps = 3455, loss = 0.06610363721847534
In grad_steps = 3456, loss = 0.7312623858451843
In grad_steps = 3457, loss = 0.026788029819726944
In grad_steps = 3458, loss = 0.027050066739320755
In grad_steps = 3459, loss = 0.5686901807785034
In grad_steps = 3460, loss = 0.3541998267173767
In grad_steps = 3461, loss = 0.3459550738334656
In grad_steps = 3462, loss = 0.37438222765922546
In grad_steps = 3463, loss = 0.16033686697483063
In grad_steps = 3464, loss = 0.25443312525749207
In grad_steps = 3465, loss = 0.12359794229269028
In grad_steps = 3466, loss = 1.0857439041137695
In grad_steps = 3467, loss = 0.6062690019607544
In grad_steps = 3468, loss = 0.04490339756011963
In grad_steps = 3469, loss = 0.41334325075149536
In grad_steps = 3470, loss = 0.2415153682231903
In grad_steps = 3471, loss = 0.5623081922531128
In grad_steps = 3472, loss = 0.05680178105831146
In grad_steps = 3473, loss = 0.022106759250164032
In grad_steps = 3474, loss = 0.09266347438097
In grad_steps = 3475, loss = 0.09189581871032715
In grad_steps = 3476, loss = 0.08167261630296707
In grad_steps = 3477, loss = 0.37705665826797485
In grad_steps = 3478, loss = 0.044256001710891724
In grad_steps = 3479, loss = 0.02873026393353939
In grad_steps = 3480, loss = 0.19457465410232544
In grad_steps = 3481, loss = 0.02284703031182289
In grad_steps = 3482, loss = 0.04269826039671898
In grad_steps = 3483, loss = 1.0376226902008057
In grad_steps = 3484, loss = 0.2693970203399658
In grad_steps = 3485, loss = 0.13478317856788635
In grad_steps = 3486, loss = 0.05320319905877113
In grad_steps = 3487, loss = 0.9586457014083862
In grad_steps = 3488, loss = 0.05655691400170326
In grad_steps = 3489, loss = 0.07615400105714798
In grad_steps = 3490, loss = 0.6126074194908142
In grad_steps = 3491, loss = 0.46739915013313293
In grad_steps = 3492, loss = 0.09057983011007309
In grad_steps = 3493, loss = 0.12931710481643677
In grad_steps = 3494, loss = 0.39368969202041626
In grad_steps = 3495, loss = 0.2387622892856598
In grad_steps = 3496, loss = 0.5863814949989319
In grad_steps = 3497, loss = 0.05827978253364563
In grad_steps = 3498, loss = 0.15902163088321686
In grad_steps = 3499, loss = 0.09178681671619415
In grad_steps = 3500, loss = 0.059111110866069794
In grad_steps = 3501, loss = 0.21018420159816742
In grad_steps = 3502, loss = 0.20095431804656982
In grad_steps = 3503, loss = 0.03226359561085701
In grad_steps = 3504, loss = 0.5369603633880615
In grad_steps = 3505, loss = 0.483873575925827
In grad_steps = 3506, loss = 0.27471843361854553
In grad_steps = 3507, loss = 0.017316602170467377
In grad_steps = 3508, loss = 0.06747302412986755
In grad_steps = 3509, loss = 0.6308181881904602
In grad_steps = 3510, loss = 0.01454935409128666
In grad_steps = 3511, loss = 0.010951939038932323
In grad_steps = 3512, loss = 0.009002532809972763
In grad_steps = 3513, loss = 0.30225029587745667
In grad_steps = 3514, loss = 0.6397542357444763
In grad_steps = 3515, loss = 0.08701011538505554
In grad_steps = 3516, loss = 0.030714167281985283
In grad_steps = 3517, loss = 0.008602812886238098
In grad_steps = 3518, loss = 0.10976138710975647
In grad_steps = 3519, loss = 0.2432655245065689
In grad_steps = 3520, loss = 0.04229576140642166
In grad_steps = 3521, loss = 0.7844493389129639
In grad_steps = 3522, loss = 1.4143909215927124
In grad_steps = 3523, loss = 0.5290321111679077
In grad_steps = 3524, loss = 0.10090407729148865
In grad_steps = 3525, loss = 0.024091150611639023
In grad_steps = 3526, loss = 0.09215812385082245
In grad_steps = 3527, loss = 0.3710392117500305
In grad_steps = 3528, loss = 0.5185884237289429
In grad_steps = 3529, loss = 0.4106178283691406
In grad_steps = 3530, loss = 0.13039809465408325
In grad_steps = 3531, loss = 0.07578384876251221
In grad_steps = 3532, loss = 0.06643398851156235
In grad_steps = 3533, loss = 0.36586472392082214
In grad_steps = 3534, loss = 0.6161222457885742
In grad_steps = 3535, loss = 0.3240394592285156
In grad_steps = 3536, loss = 0.02995787374675274
In grad_steps = 3537, loss = 0.08222898095846176
In grad_steps = 3538, loss = 0.9854497313499451
In grad_steps = 3539, loss = 0.5028119087219238
In grad_steps = 3540, loss = 0.5229871869087219
In grad_steps = 3541, loss = 0.053926169872283936
In grad_steps = 3542, loss = 0.33522099256515503
In grad_steps = 3543, loss = 0.2761910855770111
In grad_steps = 3544, loss = 0.1235450878739357
In grad_steps = 3545, loss = 0.10370325297117233
In grad_steps = 3546, loss = 0.2155103087425232
In grad_steps = 3547, loss = 0.09959418326616287
In grad_steps = 3548, loss = 1.2648340463638306
In grad_steps = 3549, loss = 0.3158441483974457
In grad_steps = 3550, loss = 0.11754421144723892
In grad_steps = 3551, loss = 0.0496516078710556
In grad_steps = 3552, loss = 0.357300341129303
In grad_steps = 3553, loss = 0.2142138034105301
In grad_steps = 3554, loss = 0.25093913078308105
In grad_steps = 3555, loss = 0.20838220417499542
In grad_steps = 3556, loss = 0.47586795687675476
In grad_steps = 3557, loss = 0.08198846131563187
In grad_steps = 3558, loss = 0.02953183837234974
In grad_steps = 3559, loss = 0.6544586420059204
In grad_steps = 3560, loss = 0.0641452819108963
In grad_steps = 3561, loss = 0.03251093253493309
In grad_steps = 3562, loss = 0.19298166036605835
In grad_steps = 3563, loss = 0.036943718791007996
In grad_steps = 3564, loss = 0.01189175620675087
In grad_steps = 3565, loss = 0.017045870423316956
In grad_steps = 3566, loss = 0.36987248063087463
In grad_steps = 3567, loss = 0.2908136546611786
In grad_steps = 3568, loss = 0.24850015342235565
In grad_steps = 3569, loss = 0.042262133210897446
In grad_steps = 3570, loss = 0.015813259407877922
In grad_steps = 3571, loss = 0.010892688296735287
In grad_steps = 3572, loss = 0.14505831897258759
In grad_steps = 3573, loss = 0.2797869145870209
In grad_steps = 3574, loss = 0.04061904922127724
In grad_steps = 3575, loss = 0.017889518290758133
In grad_steps = 3576, loss = 0.2105817347764969
In grad_steps = 3577, loss = 0.004606869537383318
In grad_steps = 3578, loss = 0.03336552530527115
In grad_steps = 3579, loss = 0.02559671550989151
In grad_steps = 3580, loss = 0.024122465401887894
In grad_steps = 3581, loss = 0.05608375370502472
In grad_steps = 3582, loss = 0.0347454696893692
In grad_steps = 3583, loss = 0.41901373863220215
In grad_steps = 3584, loss = 0.7650951743125916
In grad_steps = 3585, loss = 0.4112539291381836
In grad_steps = 3586, loss = 0.009349117055535316
In grad_steps = 3587, loss = 0.45242369174957275
In grad_steps = 3588, loss = 0.3931143879890442
In grad_steps = 3589, loss = 0.09570079296827316
In grad_steps = 3590, loss = 0.51228266954422
In grad_steps = 3591, loss = 0.49022626876831055
In grad_steps = 3592, loss = 0.46815550327301025
In grad_steps = 3593, loss = 1.8885551691055298
In grad_steps = 3594, loss = 0.29774415493011475
In grad_steps = 3595, loss = 0.012976974248886108
In grad_steps = 3596, loss = 1.465597152709961
In grad_steps = 3597, loss = 0.29832184314727783
In grad_steps = 3598, loss = 0.05544137582182884
In grad_steps = 3599, loss = 0.4545978307723999
In grad_steps = 3600, loss = 0.2404431402683258
In grad_steps = 3601, loss = 0.3363357186317444
In grad_steps = 3602, loss = 0.13669775426387787
In grad_steps = 3603, loss = 0.14726656675338745
In grad_steps = 3604, loss = 0.37441253662109375
In grad_steps = 3605, loss = 0.36472445726394653
In grad_steps = 3606, loss = 0.47391754388809204
In grad_steps = 3607, loss = 0.28941309452056885
In grad_steps = 3608, loss = 0.0802154615521431
In grad_steps = 3609, loss = 0.19956305623054504
In grad_steps = 3610, loss = 0.04704084247350693
In grad_steps = 3611, loss = 0.3011907935142517
In grad_steps = 3612, loss = 0.19050249457359314
In grad_steps = 3613, loss = 0.3370191156864166
In grad_steps = 3614, loss = 0.1586764007806778
In grad_steps = 3615, loss = 0.06133952736854553
In grad_steps = 3616, loss = 0.1644904613494873
In grad_steps = 3617, loss = 0.4422096610069275
In grad_steps = 3618, loss = 0.09877699613571167
In grad_steps = 3619, loss = 0.7040570974349976
In grad_steps = 3620, loss = 0.1273951679468155
In grad_steps = 3621, loss = 0.08505266159772873
In grad_steps = 3622, loss = 0.09580211341381073
In grad_steps = 3623, loss = 0.4348668158054352
In grad_steps = 3624, loss = 0.38269734382629395
In grad_steps = 3625, loss = 0.011279325932264328
In grad_steps = 3626, loss = 0.021418236196041107
In grad_steps = 3627, loss = 0.03038826584815979
In grad_steps = 3628, loss = 0.017953164875507355
In grad_steps = 3629, loss = 0.43664586544036865
In grad_steps = 3630, loss = 0.39536917209625244
In grad_steps = 3631, loss = 0.019415052607655525
In grad_steps = 3632, loss = 0.006780647672712803
In grad_steps = 3633, loss = 0.010620726272463799
In grad_steps = 3634, loss = 0.04579390585422516
In grad_steps = 3635, loss = 0.1595885157585144
In grad_steps = 3636, loss = 0.03902438282966614
In grad_steps = 3637, loss = 0.0167906042188406
In grad_steps = 3638, loss = 0.9343382120132446
In grad_steps = 3639, loss = 0.253318727016449
In grad_steps = 3640, loss = 0.6098524332046509
In grad_steps = 3641, loss = 0.1539829671382904
In grad_steps = 3642, loss = 0.09285430610179901
In grad_steps = 3643, loss = 0.16793638467788696
In grad_steps = 3644, loss = 0.08881036937236786
In grad_steps = 3645, loss = 0.24029521644115448
In grad_steps = 3646, loss = 0.034525178372859955
In grad_steps = 3647, loss = 0.6987075805664062
In grad_steps = 3648, loss = 0.044954631477594376
In grad_steps = 3649, loss = 0.13864459097385406
In grad_steps = 3650, loss = 0.12975113093852997
In grad_steps = 3651, loss = 0.04714870825409889
In grad_steps = 3652, loss = 0.13820196688175201
In grad_steps = 3653, loss = 0.17531010508537292
In grad_steps = 3654, loss = 0.08208382874727249
In grad_steps = 3655, loss = 0.3599434196949005
In grad_steps = 3656, loss = 0.03395071253180504
In grad_steps = 3657, loss = 0.5727939605712891
In grad_steps = 3658, loss = 0.013094354420900345
In grad_steps = 3659, loss = 0.13769419491291046
In grad_steps = 3660, loss = 0.07255693525075912
In grad_steps = 3661, loss = 0.19860997796058655
In grad_steps = 3662, loss = 0.2556948959827423
In grad_steps = 3663, loss = 0.11307689547538757
In grad_steps = 3664, loss = 0.0080722002312541
In grad_steps = 3665, loss = 0.3347993791103363
In grad_steps = 3666, loss = 0.0842304453253746
In grad_steps = 3667, loss = 0.030037442222237587
In grad_steps = 3668, loss = 0.027967650443315506
In grad_steps = 3669, loss = 1.195214033126831
In grad_steps = 3670, loss = 0.23894819617271423
In grad_steps = 3671, loss = 0.1338220089673996
In grad_steps = 3672, loss = 0.04329456016421318
In grad_steps = 3673, loss = 0.20549309253692627
In grad_steps = 3674, loss = 0.0386430062353611
In grad_steps = 3675, loss = 0.7365756630897522
In grad_steps = 3676, loss = 0.01918407902121544
In grad_steps = 3677, loss = 0.28353351354599
In grad_steps = 3678, loss = 0.3169364035129547
In grad_steps = 3679, loss = 0.0340060256421566
In grad_steps = 3680, loss = 0.008070738054811954
In grad_steps = 3681, loss = 1.3907016515731812
In grad_steps = 3682, loss = 0.006877844221889973
In grad_steps = 3683, loss = 0.04448125511407852
In grad_steps = 3684, loss = 0.028411351144313812
In grad_steps = 3685, loss = 0.14868448674678802
In grad_steps = 3686, loss = 0.005776740610599518
In grad_steps = 3687, loss = 0.04298220947384834
In grad_steps = 3688, loss = 0.20514139533042908
In grad_steps = 3689, loss = 0.02075648121535778
In grad_steps = 3690, loss = 0.6129517555236816
In grad_steps = 3691, loss = 0.9749957323074341
In grad_steps = 3692, loss = 0.5947115421295166
In grad_steps = 3693, loss = 0.3518052101135254
In grad_steps = 3694, loss = 0.055885884910821915
In grad_steps = 3695, loss = 0.030150989070534706
In grad_steps = 3696, loss = 0.11379794031381607
In grad_steps = 3697, loss = 0.6090156435966492
In grad_steps = 3698, loss = 0.2661857008934021
In grad_steps = 3699, loss = 0.06808644533157349
In grad_steps = 3700, loss = 0.08541618287563324
In grad_steps = 3701, loss = 0.18864071369171143
In grad_steps = 3702, loss = 0.06911146640777588
In grad_steps = 3703, loss = 0.05095697566866875
In grad_steps = 3704, loss = 0.07413197308778763
In grad_steps = 3705, loss = 0.12419337034225464
In grad_steps = 3706, loss = 0.4724346101284027
In grad_steps = 3707, loss = 0.18257112801074982
In grad_steps = 3708, loss = 0.10574948787689209
In grad_steps = 3709, loss = 0.13963204622268677
In grad_steps = 3710, loss = 0.2379310429096222
In grad_steps = 3711, loss = 0.3910428583621979
In grad_steps = 3712, loss = 0.1346423327922821
In grad_steps = 3713, loss = 0.27136629819869995
In grad_steps = 3714, loss = 0.04852884262800217
In grad_steps = 3715, loss = 0.03633737191557884
In grad_steps = 3716, loss = 0.023848462849855423
In grad_steps = 3717, loss = 0.08993818610906601
In grad_steps = 3718, loss = 0.020224403589963913
In grad_steps = 3719, loss = 0.15426063537597656
In grad_steps = 3720, loss = 1.0248585939407349
In grad_steps = 3721, loss = 0.12587329745292664
In grad_steps = 3722, loss = 0.1124279648065567
In grad_steps = 3723, loss = 0.18181034922599792
In grad_steps = 3724, loss = 0.09610415250062943
In grad_steps = 3725, loss = 1.5851964950561523
In grad_steps = 3726, loss = 0.055199556052684784
In grad_steps = 3727, loss = 0.012766274623572826
In grad_steps = 3728, loss = 0.1439829021692276
In grad_steps = 3729, loss = 0.01666058786213398
In grad_steps = 3730, loss = 0.813183069229126
In grad_steps = 3731, loss = 0.027021687477827072
In grad_steps = 3732, loss = 0.31210026144981384
In grad_steps = 3733, loss = 0.10114634037017822
In grad_steps = 3734, loss = 0.015644066035747528
In grad_steps = 3735, loss = 0.28713029623031616
In grad_steps = 3736, loss = 0.1953415721654892
In grad_steps = 3737, loss = 0.015699133276939392
In grad_steps = 3738, loss = 0.02950211986899376
In grad_steps = 3739, loss = 0.022518528625369072
In grad_steps = 3740, loss = 0.03173607587814331
In grad_steps = 3741, loss = 0.3532108962535858
In grad_steps = 3742, loss = 0.011113347485661507
In grad_steps = 3743, loss = 0.01504940539598465
In grad_steps = 3744, loss = 0.047986239194869995
In grad_steps = 3745, loss = 0.01531467866152525
In grad_steps = 3746, loss = 0.08325504511594772
In grad_steps = 3747, loss = 0.031304869800806046
In grad_steps = 3748, loss = 0.2585184872150421
In grad_steps = 3749, loss = 0.11555100232362747
In grad_steps = 3750, loss = 0.9810434579849243
In grad_steps = 3751, loss = 0.032819367945194244
In grad_steps = 3752, loss = 0.11454952508211136
In grad_steps = 3753, loss = 0.044712889939546585
In grad_steps = 3754, loss = 0.4053696095943451
In grad_steps = 3755, loss = 0.02588438056409359
In grad_steps = 3756, loss = 0.012171497568488121
In grad_steps = 3757, loss = 0.028242401778697968
In grad_steps = 3758, loss = 0.009405642747879028
In grad_steps = 3759, loss = 0.20853543281555176
In grad_steps = 3760, loss = 0.5698642730712891
In grad_steps = 3761, loss = 0.3343331813812256
In grad_steps = 3762, loss = 0.02130625769495964
In grad_steps = 3763, loss = 0.07864049077033997
In grad_steps = 3764, loss = 0.009885396808385849
In grad_steps = 3765, loss = 0.03336863964796066
In grad_steps = 3766, loss = 0.0568271204829216
In grad_steps = 3767, loss = 0.015505406074225903
In grad_steps = 3768, loss = 0.013669319450855255
In grad_steps = 3769, loss = 0.0955161601305008
In grad_steps = 3770, loss = 0.285042941570282
In grad_steps = 3771, loss = 0.009711107239127159
In grad_steps = 3772, loss = 0.014285599812865257
In grad_steps = 3773, loss = 0.025943215936422348
In grad_steps = 3774, loss = 0.2523333430290222
In grad_steps = 3775, loss = 0.022078033536672592
In grad_steps = 3776, loss = 0.007687535602599382
In grad_steps = 3777, loss = 0.31843024492263794
In grad_steps = 3778, loss = 0.025368250906467438
In grad_steps = 3779, loss = 0.010626155883073807
In grad_steps = 3780, loss = 0.010163405910134315
In grad_steps = 3781, loss = 0.003513925475999713
In grad_steps = 3782, loss = 1.14029860496521
In grad_steps = 3783, loss = 1.4214844703674316
In grad_steps = 3784, loss = 0.02105151116847992
In grad_steps = 3785, loss = 0.012873172760009766
In grad_steps = 3786, loss = 0.5514674186706543
In grad_steps = 3787, loss = 0.339687705039978
In grad_steps = 3788, loss = 0.05797030031681061
In grad_steps = 3789, loss = 1.1651558876037598
In grad_steps = 3790, loss = 0.38507694005966187
In grad_steps = 3791, loss = 0.01646341010928154
In grad_steps = 3792, loss = 0.046503812074661255
In grad_steps = 3793, loss = 0.5317026376724243
In grad_steps = 3794, loss = 0.04984278976917267
In grad_steps = 3795, loss = 0.017276376485824585
In grad_steps = 3796, loss = 0.11277268826961517
In grad_steps = 3797, loss = 0.14931152760982513
In grad_steps = 3798, loss = 0.09093830734491348
In grad_steps = 3799, loss = 0.019669685512781143
In grad_steps = 3800, loss = 0.1512293964624405
In grad_steps = 3801, loss = 0.2881053388118744
In grad_steps = 3802, loss = 0.1420302838087082
In grad_steps = 3803, loss = 0.1464717537164688
In grad_steps = 3804, loss = 0.31729212403297424
In grad_steps = 3805, loss = 0.11049193143844604
In grad_steps = 3806, loss = 0.007380031980574131
In grad_steps = 3807, loss = 0.15520137548446655
In grad_steps = 3808, loss = 0.5772755146026611
In grad_steps = 3809, loss = 0.19642390310764313
In grad_steps = 3810, loss = 0.3434849977493286
In grad_steps = 3811, loss = 0.09186559170484543
In grad_steps = 3812, loss = 0.16371744871139526
In grad_steps = 3813, loss = 0.013889752328395844
In grad_steps = 3814, loss = 0.3400273025035858
In grad_steps = 3815, loss = 0.27858227491378784
In grad_steps = 3816, loss = 0.9416748285293579
In grad_steps = 3817, loss = 0.04854854568839073
In grad_steps = 3818, loss = 1.0603210926055908
In grad_steps = 3819, loss = 0.45054900646209717
In grad_steps = 3820, loss = 0.1997678279876709
In grad_steps = 3821, loss = 0.020820263773202896
In grad_steps = 3822, loss = 0.7943167090415955
In grad_steps = 3823, loss = 0.031809426844120026
In grad_steps = 3824, loss = 0.027445480227470398
In grad_steps = 3825, loss = 0.6026361584663391
In grad_steps = 3826, loss = 0.037662263959646225
In grad_steps = 3827, loss = 0.8214778304100037
In grad_steps = 3828, loss = 0.40968382358551025
In grad_steps = 3829, loss = 0.5642632842063904
In grad_steps = 3830, loss = 0.03708653524518013
In grad_steps = 3831, loss = 0.8441944718360901
In grad_steps = 3832, loss = 0.3563992977142334
In grad_steps = 3833, loss = 0.27613765001296997
In grad_steps = 3834, loss = 0.1974041908979416
In grad_steps = 3835, loss = 0.08451366424560547
In grad_steps = 3836, loss = 0.19487477838993073
In grad_steps = 3837, loss = 0.04934627562761307
In grad_steps = 3838, loss = 0.048053622245788574
In grad_steps = 3839, loss = 0.1772841513156891
In grad_steps = 3840, loss = 0.2725756764411926
In grad_steps = 3841, loss = 0.10071936249732971
In grad_steps = 3842, loss = 0.013680013827979565
In grad_steps = 3843, loss = 0.29667991399765015
In grad_steps = 3844, loss = 0.17965663969516754
In grad_steps = 3845, loss = 0.04307638853788376
In grad_steps = 3846, loss = 0.2982443571090698
In grad_steps = 3847, loss = 0.23304946720600128
In grad_steps = 3848, loss = 0.04628598690032959
In grad_steps = 3849, loss = 0.28398534655570984
In grad_steps = 3850, loss = 0.21431654691696167
In grad_steps = 3851, loss = 0.11578886210918427
In grad_steps = 3852, loss = 0.012591722421348095
In grad_steps = 3853, loss = 0.023791350424289703
In grad_steps = 3854, loss = 0.24604292213916779
In grad_steps = 3855, loss = 0.03410795331001282
In grad_steps = 3856, loss = 0.6398680210113525
In grad_steps = 3857, loss = 0.2514537274837494
In grad_steps = 3858, loss = 0.00927799753844738
In grad_steps = 3859, loss = 0.011781243607401848
In grad_steps = 3860, loss = 0.011151722632348537
In grad_steps = 3861, loss = 0.5351148247718811
In grad_steps = 3862, loss = 1.2911244630813599
In grad_steps = 3863, loss = 0.5020014643669128
In grad_steps = 3864, loss = 0.14180225133895874
In grad_steps = 3865, loss = 0.23479120433330536
In grad_steps = 3866, loss = 1.4842973947525024
In grad_steps = 3867, loss = 0.3201201856136322
In grad_steps = 3868, loss = 0.13416193425655365
In grad_steps = 3869, loss = 0.07161352783441544
In grad_steps = 3870, loss = 0.1550704389810562
In grad_steps = 3871, loss = 0.3842701315879822
In grad_steps = 3872, loss = 0.5601246356964111
In grad_steps = 3873, loss = 0.04266747087240219
In grad_steps = 3874, loss = 0.286054790019989
In grad_steps = 3875, loss = 0.14663660526275635
In grad_steps = 3876, loss = 0.09587420523166656
In grad_steps = 3877, loss = 0.1686609387397766
In grad_steps = 3878, loss = 0.36593401432037354
In grad_steps = 3879, loss = 0.05494031682610512
In grad_steps = 3880, loss = 0.03698787838220596
In grad_steps = 3881, loss = 0.21533842384815216
In grad_steps = 3882, loss = 0.09969441592693329
In grad_steps = 3883, loss = 0.04887385293841362
In grad_steps = 3884, loss = 0.048665836453437805
In grad_steps = 3885, loss = 0.24115094542503357
In grad_steps = 3886, loss = 0.10615386068820953
In grad_steps = 3887, loss = 0.08243006467819214
In grad_steps = 3888, loss = 0.009563584811985493
In grad_steps = 3889, loss = 0.15690672397613525
In grad_steps = 3890, loss = 0.5756205916404724
In grad_steps = 3891, loss = 0.4015967845916748
In grad_steps = 3892, loss = 0.013431561179459095
In grad_steps = 3893, loss = 0.008536976762115955
In grad_steps = 3894, loss = 0.010028747841715813
In grad_steps = 3895, loss = 0.008660471066832542
In grad_steps = 3896, loss = 0.1064218059182167
In grad_steps = 3897, loss = 0.3225661814212799
In grad_steps = 3898, loss = 0.004408128559589386
In grad_steps = 3899, loss = 0.42299216985702515
In grad_steps = 3900, loss = 0.005993484053760767
In grad_steps = 3901, loss = 0.1586538553237915
In grad_steps = 3902, loss = 0.005809294991195202
In grad_steps = 3903, loss = 0.27163976430892944
In grad_steps = 3904, loss = 0.17814218997955322
In grad_steps = 3905, loss = 0.2612597942352295
In grad_steps = 3906, loss = 0.26269659399986267
In grad_steps = 3907, loss = 0.02207408845424652
In grad_steps = 3908, loss = 0.006178549490869045
In grad_steps = 3909, loss = 0.0035340883769094944
In grad_steps = 3910, loss = 1.1497914791107178
In grad_steps = 3911, loss = 0.07047759741544724
In grad_steps = 3912, loss = 0.6471203565597534
In grad_steps = 3913, loss = 0.511683464050293
In grad_steps = 3914, loss = 0.06927025318145752
In grad_steps = 3915, loss = 0.04037010669708252
In grad_steps = 3916, loss = 0.055007509887218475
In grad_steps = 3917, loss = 0.030010886490345
In grad_steps = 3918, loss = 0.3833516240119934
In grad_steps = 3919, loss = 1.3333989381790161
In grad_steps = 3920, loss = 0.027429956942796707
In grad_steps = 3921, loss = 0.0869021937251091
In grad_steps = 3922, loss = 0.09344099462032318
In grad_steps = 3923, loss = 0.5566737055778503
In grad_steps = 3924, loss = 0.18671929836273193
In grad_steps = 3925, loss = 0.3339995741844177
In grad_steps = 3926, loss = 0.15785856544971466
In grad_steps = 3927, loss = 0.33149176836013794
In grad_steps = 3928, loss = 0.11125174164772034
In grad_steps = 3929, loss = 0.090916708111763
In grad_steps = 3930, loss = 0.20807382464408875
In grad_steps = 3931, loss = 0.49980682134628296
In grad_steps = 3932, loss = 0.12415696680545807
In grad_steps = 3933, loss = 0.07437752932310104
In grad_steps = 3934, loss = 0.1967051923274994
In grad_steps = 3935, loss = 0.13939234614372253
In grad_steps = 3936, loss = 0.188961461186409
In grad_steps = 3937, loss = 0.09360603243112564
In grad_steps = 3938, loss = 0.6000872850418091
In grad_steps = 3939, loss = 0.3030618727207184
In grad_steps = 3940, loss = 0.12505750358104706
In grad_steps = 3941, loss = 0.13092464208602905
In grad_steps = 3942, loss = 0.06536990404129028
In grad_steps = 3943, loss = 0.444102942943573
In grad_steps = 3944, loss = 0.009349900297820568
In grad_steps = 3945, loss = 0.33165442943573
In grad_steps = 3946, loss = 0.018222074955701828
In grad_steps = 3947, loss = 0.015143447555601597
In grad_steps = 3948, loss = 0.19400721788406372
In grad_steps = 3949, loss = 0.6789781451225281
In grad_steps = 3950, loss = 0.02702510729432106
In grad_steps = 3951, loss = 0.22526884078979492
In grad_steps = 3952, loss = 0.6954639554023743
In grad_steps = 3953, loss = 0.1215503141283989
In grad_steps = 3954, loss = 0.07965292036533356
In grad_steps = 3955, loss = 0.7728104591369629
In grad_steps = 3956, loss = 0.04276615381240845
In grad_steps = 3957, loss = 0.10289406031370163
In grad_steps = 3958, loss = 0.08574292063713074
In grad_steps = 3959, loss = 0.771174430847168
In grad_steps = 3960, loss = 0.06802275776863098
In grad_steps = 3961, loss = 0.05732797086238861
In grad_steps = 3962, loss = 0.09132801741361618
In grad_steps = 3963, loss = 0.15128886699676514
In grad_steps = 3964, loss = 0.04614299535751343
In grad_steps = 3965, loss = 0.6252938508987427
In grad_steps = 3966, loss = 0.151339590549469
In grad_steps = 3967, loss = 0.40319570899009705
In grad_steps = 3968, loss = 0.008567635901272297
In grad_steps = 3969, loss = 0.39232879877090454
In grad_steps = 3970, loss = 0.019748982042074203
In grad_steps = 3971, loss = 0.6077085137367249
In grad_steps = 3972, loss = 0.243734210729599
In grad_steps = 3973, loss = 0.0249128807336092
In grad_steps = 3974, loss = 0.6094641089439392
In grad_steps = 3975, loss = 0.154991015791893
In grad_steps = 3976, loss = 0.46471500396728516
In grad_steps = 3977, loss = 0.21867918968200684
In grad_steps = 3978, loss = 0.21673092246055603
In grad_steps = 3979, loss = 0.03812146931886673
In grad_steps = 3980, loss = 0.104779452085495
In grad_steps = 3981, loss = 0.05148908495903015
In grad_steps = 3982, loss = 0.09407730400562286
In grad_steps = 3983, loss = 0.8680696487426758
In grad_steps = 3984, loss = 0.06509047001600266
In grad_steps = 3985, loss = 0.3079017400741577
In grad_steps = 3986, loss = 0.023431045934557915
In grad_steps = 3987, loss = 0.03926798328757286
In grad_steps = 3988, loss = 0.10270554572343826
In grad_steps = 3989, loss = 0.47691652178764343
In grad_steps = 3990, loss = 0.014875577762722969
In grad_steps = 3991, loss = 0.10437098145484924
In grad_steps = 3992, loss = 0.4666876494884491
In grad_steps = 3993, loss = 0.0435364730656147
In grad_steps = 3994, loss = 0.02473657950758934
In grad_steps = 3995, loss = 0.7687134742736816
In grad_steps = 3996, loss = 0.02592645026743412
In grad_steps = 3997, loss = 1.0111972093582153
In grad_steps = 3998, loss = 0.027117038145661354
In grad_steps = 3999, loss = 0.27783116698265076
In grad_steps = 4000, loss = 0.10953264683485031
In grad_steps = 4001, loss = 0.08112585544586182
In grad_steps = 4002, loss = 0.01904929243028164
In grad_steps = 4003, loss = 0.37116435170173645
In grad_steps = 4004, loss = 0.08842942118644714
In grad_steps = 4005, loss = 0.07828561961650848
In grad_steps = 4006, loss = 0.30950385332107544
In grad_steps = 4007, loss = 0.22192910313606262
In grad_steps = 4008, loss = 0.08352670073509216
In grad_steps = 4009, loss = 0.7438282370567322
In grad_steps = 4010, loss = 0.04990086331963539
In grad_steps = 4011, loss = 0.19731546938419342
In grad_steps = 4012, loss = 0.573901891708374
In grad_steps = 4013, loss = 0.44293251633644104
In grad_steps = 4014, loss = 0.25107765197753906
In grad_steps = 4015, loss = 0.30605176091194153
In grad_steps = 4016, loss = 0.2230919599533081
In grad_steps = 4017, loss = 0.2525857090950012
In grad_steps = 4018, loss = 0.18572482466697693
In grad_steps = 4019, loss = 0.015789946541190147
In grad_steps = 4020, loss = 0.19103096425533295
In grad_steps = 4021, loss = 0.7502281069755554
In grad_steps = 4022, loss = 0.11947640031576157
In grad_steps = 4023, loss = 0.09440883994102478
In grad_steps = 4024, loss = 0.12961691617965698
In grad_steps = 4025, loss = 0.6777610778808594
In grad_steps = 4026, loss = 0.05586504563689232
In grad_steps = 4027, loss = 0.03251105919480324
In grad_steps = 4028, loss = 0.7881755232810974
In grad_steps = 4029, loss = 0.7009573578834534
In grad_steps = 4030, loss = 0.20366424322128296
In grad_steps = 4031, loss = 0.051309388130903244
In grad_steps = 4032, loss = 0.7058901786804199
In grad_steps = 4033, loss = 0.08007581532001495
In grad_steps = 4034, loss = 0.29273954033851624
In grad_steps = 4035, loss = 0.2937740087509155
In grad_steps = 4036, loss = 0.15293961763381958
In grad_steps = 4037, loss = 0.5834108591079712
In grad_steps = 4038, loss = 0.05779212713241577
In grad_steps = 4039, loss = 0.4383604824542999
In grad_steps = 4040, loss = 0.30251452326774597
In grad_steps = 4041, loss = 0.07330135256052017
In grad_steps = 4042, loss = 0.1903887391090393
In grad_steps = 4043, loss = 0.1354193389415741
In grad_steps = 4044, loss = 0.08495957404375076
In grad_steps = 4045, loss = 0.3732993006706238
In grad_steps = 4046, loss = 0.31339728832244873
In grad_steps = 4047, loss = 0.12478187680244446
In grad_steps = 4048, loss = 0.10283657908439636
In grad_steps = 4049, loss = 0.2500525116920471
In grad_steps = 4050, loss = 0.1641491800546646
In grad_steps = 4051, loss = 0.044816453009843826
In grad_steps = 4052, loss = 0.24672721326351166
In grad_steps = 4053, loss = 0.1606811285018921
In grad_steps = 4054, loss = 0.6730561256408691
In grad_steps = 4055, loss = 0.014225296676158905
In grad_steps = 4056, loss = 0.5723459720611572
In grad_steps = 4057, loss = 0.5904983282089233
In grad_steps = 4058, loss = 0.07028184086084366
In grad_steps = 4059, loss = 0.2990673780441284
In grad_steps = 4060, loss = 0.2461933195590973
In grad_steps = 4061, loss = 0.09325997531414032
In grad_steps = 4062, loss = 0.07339571416378021
In grad_steps = 4063, loss = 0.14536403119564056
In grad_steps = 4064, loss = 0.032581571489572525
In grad_steps = 4065, loss = 0.0464499294757843
In grad_steps = 4066, loss = 0.9167536497116089
In grad_steps = 4067, loss = 0.019086796790361404
In grad_steps = 4068, loss = 0.08493427187204361
In grad_steps = 4069, loss = 0.38377612829208374
In grad_steps = 4070, loss = 0.7614292502403259
In grad_steps = 4071, loss = 0.013260813429951668
In grad_steps = 4072, loss = 0.053593169897794724
In grad_steps = 4073, loss = 0.04952337592840195
In grad_steps = 4074, loss = 0.11466432362794876
In grad_steps = 4075, loss = 0.7033137679100037
In grad_steps = 4076, loss = 0.031954534351825714
In grad_steps = 4077, loss = 0.18856388330459595
In grad_steps = 4078, loss = 0.11069484800100327
In grad_steps = 4079, loss = 0.33149102330207825
In grad_steps = 4080, loss = 0.08742870390415192
In grad_steps = 4081, loss = 0.037952274084091187
In grad_steps = 4082, loss = 0.3147301971912384
In grad_steps = 4083, loss = 0.07909166067838669
In grad_steps = 4084, loss = 0.03403779864311218
In grad_steps = 4085, loss = 0.050509337335824966
In grad_steps = 4086, loss = 0.025356639176607132
In grad_steps = 4087, loss = 0.3838995695114136
In grad_steps = 4088, loss = 0.05675754323601723
In grad_steps = 4089, loss = 0.24574923515319824
In grad_steps = 4090, loss = 1.0746216773986816
In grad_steps = 4091, loss = 0.04338035359978676
In grad_steps = 4092, loss = 0.03055264800786972
In grad_steps = 4093, loss = 0.016907434910535812
In grad_steps = 4094, loss = 0.19956763088703156
In grad_steps = 4095, loss = 0.17451758682727814
In grad_steps = 4096, loss = 0.018135560676455498
In grad_steps = 4097, loss = 0.735304057598114
In grad_steps = 4098, loss = 0.3414616882801056
In grad_steps = 4099, loss = 0.20916855335235596
In grad_steps = 4100, loss = 0.06642268598079681
In grad_steps = 4101, loss = 0.5423879623413086
In grad_steps = 4102, loss = 0.008403331972658634
In grad_steps = 4103, loss = 0.9631271362304688
In grad_steps = 4104, loss = 0.13196368515491486
In grad_steps = 4105, loss = 0.10425412654876709
In grad_steps = 4106, loss = 0.29010990262031555
In grad_steps = 4107, loss = 0.05058104544878006
In grad_steps = 4108, loss = 0.038346707820892334
In grad_steps = 4109, loss = 0.053631700575351715
In grad_steps = 4110, loss = 0.7264068126678467
In grad_steps = 4111, loss = 0.046250782907009125
In grad_steps = 4112, loss = 0.20764374732971191
In grad_steps = 4113, loss = 0.12334862351417542
In grad_steps = 4114, loss = 0.05188259482383728
In grad_steps = 4115, loss = 0.8239706754684448
In grad_steps = 4116, loss = 0.04326881840825081
In grad_steps = 4117, loss = 0.08631289750337601
In grad_steps = 4118, loss = 0.20441824197769165
In grad_steps = 4119, loss = 0.005292349029332399
In grad_steps = 4120, loss = 0.06314963102340698
In grad_steps = 4121, loss = 0.0859905332326889
In grad_steps = 4122, loss = 0.23769858479499817
In grad_steps = 4123, loss = 0.5277916193008423
In grad_steps = 4124, loss = 0.35739558935165405
In grad_steps = 4125, loss = 0.08903028070926666
In grad_steps = 4126, loss = 0.058102332055568695
In grad_steps = 4127, loss = 0.10790951550006866
In grad_steps = 4128, loss = 0.47448790073394775
In grad_steps = 4129, loss = 0.19046302139759064
In grad_steps = 4130, loss = 0.05142776668071747
In grad_steps = 4131, loss = 0.05582725256681442
In grad_steps = 4132, loss = 0.25826603174209595
In grad_steps = 4133, loss = 0.014802012592554092
In grad_steps = 4134, loss = 0.04998340457677841
In grad_steps = 4135, loss = 0.11201590299606323
In grad_steps = 4136, loss = 0.41256004571914673
In grad_steps = 4137, loss = 0.5558680295944214
In grad_steps = 4138, loss = 0.011460162699222565
In grad_steps = 4139, loss = 0.008056015707552433
In grad_steps = 4140, loss = 0.08000797033309937
In grad_steps = 4141, loss = 0.10115335881710052
In grad_steps = 4142, loss = 0.09781275689601898
In grad_steps = 4143, loss = 0.12249179929494858
In grad_steps = 4144, loss = 0.12716417014598846
In grad_steps = 4145, loss = 0.006974608637392521
In grad_steps = 4146, loss = 0.01634145714342594
In grad_steps = 4147, loss = 0.09052492678165436
In grad_steps = 4148, loss = 0.004927043337374926
In grad_steps = 4149, loss = 0.004274650011211634
In grad_steps = 4150, loss = 0.15008674561977386
In grad_steps = 4151, loss = 0.0147635443136096
In grad_steps = 4152, loss = 1.0358309745788574
In grad_steps = 4153, loss = 0.27343741059303284
In grad_steps = 4154, loss = 0.15918122231960297
In grad_steps = 4155, loss = 0.7265159487724304
In grad_steps = 4156, loss = 0.007368282414972782
In grad_steps = 4157, loss = 0.27411070466041565
In grad_steps = 4158, loss = 0.4637298285961151
In grad_steps = 4159, loss = 0.021514615043997765
In grad_steps = 4160, loss = 0.0385555773973465
In grad_steps = 4161, loss = 0.3606052100658417
In grad_steps = 4162, loss = 0.11194264888763428
In grad_steps = 4163, loss = 0.003788807662203908
In grad_steps = 4164, loss = 0.007795292884111404
In grad_steps = 4165, loss = 0.08307196944952011
In grad_steps = 4166, loss = 0.08741340041160583
In grad_steps = 4167, loss = 0.020554114133119583
In grad_steps = 4168, loss = 0.33942991495132446
In grad_steps = 4169, loss = 0.02845739759504795
In grad_steps = 4170, loss = 0.1345883011817932
In grad_steps = 4171, loss = 0.03571176528930664
In grad_steps = 4172, loss = 0.2033466249704361
In grad_steps = 4173, loss = 0.6745970249176025
In grad_steps = 4174, loss = 0.3232235908508301
In grad_steps = 4175, loss = 0.011522088199853897
In grad_steps = 4176, loss = 2.6245861053466797
In grad_steps = 4177, loss = 0.03679068014025688
In grad_steps = 4178, loss = 0.007695328444242477
In grad_steps = 4179, loss = 1.014498233795166
In grad_steps = 4180, loss = 0.09960702806711197
In grad_steps = 4181, loss = 1.0052237510681152
In grad_steps = 4182, loss = 0.21440258622169495
In grad_steps = 4183, loss = 0.02043481543660164
In grad_steps = 4184, loss = 0.10283572226762772
In grad_steps = 4185, loss = 0.6208146810531616
In grad_steps = 4186, loss = 0.5340248346328735
In grad_steps = 4187, loss = 0.04996895045042038
In grad_steps = 4188, loss = 0.05147796496748924
In grad_steps = 4189, loss = 0.7384629845619202
In grad_steps = 4190, loss = 0.6658275127410889
In grad_steps = 4191, loss = 0.6192454099655151
In grad_steps = 4192, loss = 0.15297245979309082
In grad_steps = 4193, loss = 0.0648794174194336
In grad_steps = 4194, loss = 0.1069219559431076
In grad_steps = 4195, loss = 0.23483815789222717
In grad_steps = 4196, loss = 0.07182323932647705
In grad_steps = 4197, loss = 0.5432286858558655
In grad_steps = 4198, loss = 0.20855480432510376
In grad_steps = 4199, loss = 0.10139187425374985
In grad_steps = 4200, loss = 0.3296273350715637
In grad_steps = 4201, loss = 0.20760250091552734
In grad_steps = 4202, loss = 0.04096333682537079
In grad_steps = 4203, loss = 0.18803274631500244
In grad_steps = 4204, loss = 0.07016976922750473
In grad_steps = 4205, loss = 0.18124672770500183
In grad_steps = 4206, loss = 0.12871290743350983
In grad_steps = 4207, loss = 0.3460375964641571
In grad_steps = 4208, loss = 0.023524967953562737
In grad_steps = 4209, loss = 0.08313920348882675
In grad_steps = 4210, loss = 0.06770002096891403
In grad_steps = 4211, loss = 0.25202080607414246
In grad_steps = 4212, loss = 0.08497162163257599
In grad_steps = 4213, loss = 0.2420654296875
In grad_steps = 4214, loss = 0.13387450575828552
In grad_steps = 4215, loss = 0.06708826124668121
In grad_steps = 4216, loss = 0.4318203032016754
In grad_steps = 4217, loss = 0.08041845262050629
In grad_steps = 4218, loss = 0.09633292257785797
In grad_steps = 4219, loss = 0.05678483098745346
In grad_steps = 4220, loss = 0.38001981377601624
In grad_steps = 4221, loss = 0.020203061401844025
In grad_steps = 4222, loss = 0.028762491419911385
In grad_steps = 4223, loss = 0.01992454007267952
In grad_steps = 4224, loss = 0.010249761864542961
In grad_steps = 4225, loss = 0.3631283640861511
In grad_steps = 4226, loss = 0.0198108721524477
In grad_steps = 4227, loss = 0.02549898996949196
In grad_steps = 4228, loss = 0.003958157729357481
In grad_steps = 4229, loss = 0.010908356867730618
In grad_steps = 4230, loss = 0.6025334000587463
In grad_steps = 4231, loss = 0.02566947415471077
In grad_steps = 4232, loss = 0.5162705779075623
In grad_steps = 4233, loss = 0.006232196465134621
In grad_steps = 4234, loss = 0.053631741553545
In grad_steps = 4235, loss = 0.06370151787996292
In grad_steps = 4236, loss = 0.1415209174156189
In grad_steps = 4237, loss = 0.018896356225013733
In grad_steps = 4238, loss = 0.1020786240696907
In grad_steps = 4239, loss = 0.49151185154914856
In grad_steps = 4240, loss = 0.00562752690166235
In grad_steps = 4241, loss = 0.13614098727703094
In grad_steps = 4242, loss = 0.0076590487733483315
In grad_steps = 4243, loss = 0.01874791458249092
In grad_steps = 4244, loss = 0.035837914794683456
In grad_steps = 4245, loss = 0.14341846108436584
In grad_steps = 4246, loss = 0.010191471315920353
In grad_steps = 4247, loss = 0.010995917953550816
In grad_steps = 4248, loss = 0.09426019340753555
In grad_steps = 4249, loss = 0.2588330805301666
In grad_steps = 4250, loss = 0.004449973814189434
In grad_steps = 4251, loss = 0.0034112101420760155
In grad_steps = 4252, loss = 0.05810335651040077
In grad_steps = 4253, loss = 0.016427544876933098
In grad_steps = 4254, loss = 0.020905347540974617
In grad_steps = 4255, loss = 0.004000756423920393
In grad_steps = 4256, loss = 0.20819474756717682
In grad_steps = 4257, loss = 0.8039334416389465
In grad_steps = 4258, loss = 0.04654667526483536
In grad_steps = 4259, loss = 0.013492606580257416
In grad_steps = 4260, loss = 0.6593871116638184
In grad_steps = 4261, loss = 0.06316334009170532
In grad_steps = 4262, loss = 0.3292519748210907
In grad_steps = 4263, loss = 0.292155921459198
In grad_steps = 4264, loss = 0.11302933096885681
In grad_steps = 4265, loss = 0.0052896952256560326
In grad_steps = 4266, loss = 0.009814801625907421
In grad_steps = 4267, loss = 0.041362207382917404
In grad_steps = 4268, loss = 1.0014482736587524
In grad_steps = 4269, loss = 0.5386853218078613
In grad_steps = 4270, loss = 0.056130941957235336
In grad_steps = 4271, loss = 0.07749956846237183
In grad_steps = 4272, loss = 0.5827447772026062
In grad_steps = 4273, loss = 0.6564421653747559
In grad_steps = 4274, loss = 0.34452152252197266
In grad_steps = 4275, loss = 0.1347292959690094
In grad_steps = 4276, loss = 0.01325109414756298
In grad_steps = 4277, loss = 0.3811325430870056
In grad_steps = 4278, loss = 0.7347732782363892
In grad_steps = 4279, loss = 0.5208247900009155
In grad_steps = 4280, loss = 0.0710453987121582
In grad_steps = 4281, loss = 0.4079506993293762
In grad_steps = 4282, loss = 0.3014148473739624
In grad_steps = 4283, loss = 0.27496325969696045
In grad_steps = 4284, loss = 0.6022459864616394
In grad_steps = 4285, loss = 0.20730020105838776
In grad_steps = 4286, loss = 0.4686541259288788
In grad_steps = 4287, loss = 0.16249462962150574
In grad_steps = 4288, loss = 0.143942192196846
In grad_steps = 4289, loss = 0.597826361656189
In grad_steps = 4290, loss = 0.047971948981285095
In grad_steps = 4291, loss = 0.11164450645446777
In grad_steps = 4292, loss = 0.46267154812812805
In grad_steps = 4293, loss = 0.1193847581744194
In grad_steps = 4294, loss = 0.11230433732271194
In grad_steps = 4295, loss = 0.2932799756526947
In grad_steps = 4296, loss = 1.5801080465316772
In grad_steps = 4297, loss = 0.07790066301822662
In grad_steps = 4298, loss = 0.0936451181769371
In grad_steps = 4299, loss = 0.1178387999534607
In grad_steps = 4300, loss = 0.18474434316158295
In grad_steps = 4301, loss = 0.05764469504356384
In grad_steps = 4302, loss = 0.21398389339447021
In grad_steps = 4303, loss = 0.2691638469696045
In grad_steps = 4304, loss = 0.4750601053237915
In grad_steps = 4305, loss = 0.09723105281591415
In grad_steps = 4306, loss = 0.1562686562538147
In grad_steps = 4307, loss = 0.37553849816322327
In grad_steps = 4308, loss = 0.031292472034692764
In grad_steps = 4309, loss = 0.047822255641222
In grad_steps = 4310, loss = 0.22448702156543732
In grad_steps = 4311, loss = 0.16673147678375244
In grad_steps = 4312, loss = 0.057940203696489334
In grad_steps = 4313, loss = 0.10345171391963959
In grad_steps = 4314, loss = 0.12776431441307068
In grad_steps = 4315, loss = 0.03825883939862251
In grad_steps = 4316, loss = 1.556943416595459
In grad_steps = 4317, loss = 0.1271367222070694
In grad_steps = 4318, loss = 0.09758405387401581
In grad_steps = 4319, loss = 1.7587746381759644
In grad_steps = 4320, loss = 0.06882515549659729
In grad_steps = 4321, loss = 0.2681567966938019
In grad_steps = 4322, loss = 0.11968960613012314
In grad_steps = 4323, loss = 0.26318445801734924
In grad_steps = 4324, loss = 0.5928800702095032
In grad_steps = 4325, loss = 0.2629143297672272
In grad_steps = 4326, loss = 0.2413022667169571
In grad_steps = 4327, loss = 0.4269038736820221
In grad_steps = 4328, loss = 0.06418143957853317
In grad_steps = 4329, loss = 0.040019456297159195
In grad_steps = 4330, loss = 0.9287711977958679
In grad_steps = 4331, loss = 0.3917994201183319
In grad_steps = 4332, loss = 0.030164547264575958
In grad_steps = 4333, loss = 0.19693218171596527
In grad_steps = 4334, loss = 1.04214608669281
In grad_steps = 4335, loss = 0.04462622106075287
In grad_steps = 4336, loss = 0.08670707792043686
In grad_steps = 4337, loss = 0.04518749564886093
In grad_steps = 4338, loss = 0.160357266664505
In grad_steps = 4339, loss = 0.24505159258842468
In grad_steps = 4340, loss = 0.06265600025653839
In grad_steps = 4341, loss = 0.6080489754676819
In grad_steps = 4342, loss = 0.2701703906059265
In grad_steps = 4343, loss = 0.4301200211048126
In grad_steps = 4344, loss = 0.05708420276641846
In grad_steps = 4345, loss = 0.43756502866744995
In grad_steps = 4346, loss = 0.11888360977172852
In grad_steps = 4347, loss = 0.5978727340698242
In grad_steps = 4348, loss = 0.37685346603393555
In grad_steps = 4349, loss = 0.05861996114253998
In grad_steps = 4350, loss = 0.09539231657981873
In grad_steps = 4351, loss = 0.19830003380775452
In grad_steps = 4352, loss = 0.21703672409057617
In grad_steps = 4353, loss = 0.1257742941379547
In grad_steps = 4354, loss = 0.1298629343509674
In grad_steps = 4355, loss = 0.5598037838935852
In grad_steps = 4356, loss = 0.08834227919578552
In grad_steps = 4357, loss = 0.042641058564186096
In grad_steps = 4358, loss = 0.015347854234278202
In grad_steps = 4359, loss = 0.07242526113986969
In grad_steps = 4360, loss = 0.07501769810914993
In grad_steps = 4361, loss = 0.10692563652992249
In grad_steps = 4362, loss = 0.42229601740837097
In grad_steps = 4363, loss = 0.11413577198982239
In grad_steps = 4364, loss = 0.12277845293283463
In grad_steps = 4365, loss = 0.06888549029827118
In grad_steps = 4366, loss = 0.07106960564851761
In grad_steps = 4367, loss = 0.04280708357691765
In grad_steps = 4368, loss = 0.005760662257671356
In grad_steps = 4369, loss = 0.026301607489585876
In grad_steps = 4370, loss = 0.13688066601753235
In grad_steps = 4371, loss = 0.4607076644897461
In grad_steps = 4372, loss = 0.47444474697113037
In grad_steps = 4373, loss = 0.10740076005458832
In grad_steps = 4374, loss = 0.041867390275001526
In grad_steps = 4375, loss = 1.0932303667068481
In grad_steps = 4376, loss = 0.08295250684022903
In grad_steps = 4377, loss = 0.023462580516934395
In grad_steps = 4378, loss = 0.024387985467910767
In grad_steps = 4379, loss = 0.007025338709354401
In grad_steps = 4380, loss = 0.02579382248222828
In grad_steps = 4381, loss = 0.017523419111967087
In grad_steps = 4382, loss = 0.2671746015548706
In grad_steps = 4383, loss = 0.04042823612689972
In grad_steps = 4384, loss = 0.01690966635942459
In grad_steps = 4385, loss = 0.007079773116856813
In grad_steps = 4386, loss = 0.024217555299401283
In grad_steps = 4387, loss = 0.007178434636443853
In grad_steps = 4388, loss = 0.11864053457975388
In grad_steps = 4389, loss = 0.007618868723511696
In grad_steps = 4390, loss = 0.02234429493546486
In grad_steps = 4391, loss = 0.7016394138336182
In grad_steps = 4392, loss = 0.8757821321487427
In grad_steps = 4393, loss = 0.306796669960022
In grad_steps = 4394, loss = 0.03687148913741112
In grad_steps = 4395, loss = 0.1540461778640747
In grad_steps = 4396, loss = 0.13093137741088867
In grad_steps = 4397, loss = 0.035726163536310196
In grad_steps = 4398, loss = 0.107852041721344
In grad_steps = 4399, loss = 0.17585329711437225
In grad_steps = 4400, loss = 0.01406337320804596
In grad_steps = 4401, loss = 0.044234808534383774
In grad_steps = 4402, loss = 2.299597978591919
In grad_steps = 4403, loss = 0.3266104757785797
In grad_steps = 4404, loss = 0.05453960597515106
In grad_steps = 4405, loss = 0.027779487892985344
In grad_steps = 4406, loss = 0.7954537868499756
In grad_steps = 4407, loss = 0.25387436151504517
In grad_steps = 4408, loss = 0.2011786848306656
In grad_steps = 4409, loss = 0.04106339439749718
In grad_steps = 4410, loss = 0.0371892936527729
In grad_steps = 4411, loss = 0.13070979714393616
In grad_steps = 4412, loss = 0.04591446369886398
In grad_steps = 4413, loss = 0.26048389077186584
In grad_steps = 4414, loss = 0.058847688138484955
In grad_steps = 4415, loss = 0.11308673769235611
In grad_steps = 4416, loss = 0.019094230607151985
In grad_steps = 4417, loss = 0.34757205843925476
In grad_steps = 4418, loss = 0.07895918935537338
In grad_steps = 4419, loss = 0.04956243932247162
In grad_steps = 4420, loss = 0.04167699068784714
In grad_steps = 4421, loss = 0.05165158584713936
In grad_steps = 4422, loss = 0.08312182128429413
In grad_steps = 4423, loss = 0.6415976285934448
In grad_steps = 4424, loss = 0.33082494139671326
In grad_steps = 4425, loss = 0.02481914684176445
In grad_steps = 4426, loss = 0.24139916896820068
In grad_steps = 4427, loss = 0.07312875986099243
In grad_steps = 4428, loss = 0.03555213660001755
In grad_steps = 4429, loss = 0.02693936601281166
In grad_steps = 4430, loss = 0.139099583029747
In grad_steps = 4431, loss = 0.1366153359413147
In grad_steps = 4432, loss = 0.5807963609695435
In grad_steps = 4433, loss = 0.21783888339996338
In grad_steps = 4434, loss = 0.009550667367875576
In grad_steps = 4435, loss = 0.017868444323539734
In grad_steps = 4436, loss = 0.02591446228325367
In grad_steps = 4437, loss = 0.075151726603508
In grad_steps = 4438, loss = 0.28632980585098267
In grad_steps = 4439, loss = 0.11323481798171997
In grad_steps = 4440, loss = 0.399330735206604
In grad_steps = 4441, loss = 0.0075842877849936485
In grad_steps = 4442, loss = 0.022120758891105652
In grad_steps = 4443, loss = 0.009868862107396126
In grad_steps = 4444, loss = 0.02508511021733284
In grad_steps = 4445, loss = 0.4247335195541382
In grad_steps = 4446, loss = 0.017661195248365402
In grad_steps = 4447, loss = 0.3540182411670685
In grad_steps = 4448, loss = 0.02238018438220024
In grad_steps = 4449, loss = 0.009009516797959805
In grad_steps = 4450, loss = 0.15380343794822693
In grad_steps = 4451, loss = 0.01643654704093933
In grad_steps = 4452, loss = 0.02442708984017372
In grad_steps = 4453, loss = 0.08244189620018005
In grad_steps = 4454, loss = 0.11586140096187592
In grad_steps = 4455, loss = 0.010449985042214394
In grad_steps = 4456, loss = 0.0054547046311199665
In grad_steps = 4457, loss = 0.3013498783111572
In grad_steps = 4458, loss = 0.033551428467035294
In grad_steps = 4459, loss = 0.09597763419151306
In grad_steps = 4460, loss = 1.261909008026123
In grad_steps = 4461, loss = 0.2013130784034729
In grad_steps = 4462, loss = 0.022520482540130615
In grad_steps = 4463, loss = 0.3409470319747925
In grad_steps = 4464, loss = 0.017551152035593987
In grad_steps = 4465, loss = 0.6557394862174988
In grad_steps = 4466, loss = 0.14325439929962158
In grad_steps = 4467, loss = 0.8442904353141785
In grad_steps = 4468, loss = 0.013242538087069988
In grad_steps = 4469, loss = 0.06590338051319122
In grad_steps = 4470, loss = 0.0833822637796402
In grad_steps = 4471, loss = 0.030574360862374306
In grad_steps = 4472, loss = 0.013386398553848267
In grad_steps = 4473, loss = 0.012822831980884075
In grad_steps = 4474, loss = 0.026349160820245743
In grad_steps = 4475, loss = 0.03259611129760742
In grad_steps = 4476, loss = 0.37792447209358215
In grad_steps = 4477, loss = 0.020115938037633896
In grad_steps = 4478, loss = 0.008755645714700222
In grad_steps = 4479, loss = 2.343085289001465
In grad_steps = 4480, loss = 0.0572550967335701
In grad_steps = 4481, loss = 0.08865796774625778
In grad_steps = 4482, loss = 0.17132426798343658
In grad_steps = 4483, loss = 0.1789836287498474
In grad_steps = 4484, loss = 0.6734543442726135
In grad_steps = 4485, loss = 0.08365839719772339
In grad_steps = 4486, loss = 0.0197820533066988
In grad_steps = 4487, loss = 0.20994994044303894
In grad_steps = 4488, loss = 0.3223356008529663
In grad_steps = 4489, loss = 0.17080020904541016
In grad_steps = 4490, loss = 0.15290232002735138
In grad_steps = 4491, loss = 0.719308078289032
In grad_steps = 4492, loss = 0.044828034937381744
In grad_steps = 4493, loss = 0.08240378648042679
In grad_steps = 4494, loss = 0.13649050891399384
In grad_steps = 4495, loss = 0.03257064148783684
In grad_steps = 4496, loss = 0.08830272406339645
In grad_steps = 4497, loss = 0.7802570462226868
In grad_steps = 4498, loss = 0.10287110507488251
In grad_steps = 4499, loss = 0.03673792630434036
In grad_steps = 4500, loss = 0.1570790559053421
In grad_steps = 4501, loss = 0.4091314673423767
In grad_steps = 4502, loss = 0.025108886882662773
In grad_steps = 4503, loss = 0.1827532947063446
In grad_steps = 4504, loss = 0.09286206215620041
In grad_steps = 4505, loss = 0.09266142547130585
In grad_steps = 4506, loss = 0.05977071821689606
In grad_steps = 4507, loss = 0.05161074548959732
In grad_steps = 4508, loss = 0.03195783495903015
In grad_steps = 4509, loss = 0.15143856406211853
In grad_steps = 4510, loss = 0.028226753696799278
In grad_steps = 4511, loss = 0.09285131841897964
In grad_steps = 4512, loss = 1.2782659530639648
In grad_steps = 4513, loss = 0.2064180225133896
In grad_steps = 4514, loss = 0.026107775047421455
In grad_steps = 4515, loss = 1.055071473121643
In grad_steps = 4516, loss = 0.015270698815584183
In grad_steps = 4517, loss = 0.024152318015694618
In grad_steps = 4518, loss = 0.17114557325839996
In grad_steps = 4519, loss = 0.11197859793901443
In grad_steps = 4520, loss = 0.018109392374753952
In grad_steps = 4521, loss = 0.09521698951721191
In grad_steps = 4522, loss = 0.3756779134273529
In grad_steps = 4523, loss = 0.0096895107999444
In grad_steps = 4524, loss = 0.7946180105209351
In grad_steps = 4525, loss = 0.8858979940414429
In grad_steps = 4526, loss = 0.011685439385473728
In grad_steps = 4527, loss = 0.015439659357070923
In grad_steps = 4528, loss = 0.27997687458992004
In grad_steps = 4529, loss = 0.05866225063800812
In grad_steps = 4530, loss = 0.1087496355175972
In grad_steps = 4531, loss = 0.045335184782743454
In grad_steps = 4532, loss = 0.3528009057044983
In grad_steps = 4533, loss = 0.04678165540099144
In grad_steps = 4534, loss = 0.04303119704127312
In grad_steps = 4535, loss = 0.04166627675294876
In grad_steps = 4536, loss = 0.2957627773284912
In grad_steps = 4537, loss = 0.08412708342075348
In grad_steps = 4538, loss = 0.12813299894332886
In grad_steps = 4539, loss = 1.1274659633636475
In grad_steps = 4540, loss = 0.10918813943862915
In grad_steps = 4541, loss = 0.027117470279335976
In grad_steps = 4542, loss = 0.6230665445327759
In grad_steps = 4543, loss = 0.03537391871213913
In grad_steps = 4544, loss = 0.036095600575208664
In grad_steps = 4545, loss = 0.5620157122612
In grad_steps = 4546, loss = 0.0636257603764534
In grad_steps = 4547, loss = 0.03716430813074112
In grad_steps = 4548, loss = 0.016385722905397415
In grad_steps = 4549, loss = 0.03343389928340912
In grad_steps = 4550, loss = 0.015853356570005417
In grad_steps = 4551, loss = 0.9488473534584045
In grad_steps = 4552, loss = 0.03897273540496826
In grad_steps = 4553, loss = 0.4593571424484253
In grad_steps = 4554, loss = 0.2019381821155548
In grad_steps = 4555, loss = 0.158742755651474
In grad_steps = 4556, loss = 0.08542051166296005
In grad_steps = 4557, loss = 0.04840197041630745
In grad_steps = 4558, loss = 0.04129614308476448
In grad_steps = 4559, loss = 0.2710321247577667
In grad_steps = 4560, loss = 0.0292240958660841
In grad_steps = 4561, loss = 0.023649200797080994
In grad_steps = 4562, loss = 0.029471129179000854
In grad_steps = 4563, loss = 0.05058562383055687
In grad_steps = 4564, loss = 0.0651196539402008
In grad_steps = 4565, loss = 1.0435515642166138
In grad_steps = 4566, loss = 0.6183839440345764
In grad_steps = 4567, loss = 0.06759350001811981
In grad_steps = 4568, loss = 0.12746953964233398
In grad_steps = 4569, loss = 0.3357153832912445
In grad_steps = 4570, loss = 1.0421253442764282
In grad_steps = 4571, loss = 0.5672082901000977
In grad_steps = 4572, loss = 0.12429599463939667
In grad_steps = 4573, loss = 0.04730040952563286
In grad_steps = 4574, loss = 0.0532938577234745
In grad_steps = 4575, loss = 0.06446949392557144
In grad_steps = 4576, loss = 0.23978449404239655
In grad_steps = 4577, loss = 0.028464099392294884
In grad_steps = 4578, loss = 0.08816665410995483
In grad_steps = 4579, loss = 0.33679112792015076
In grad_steps = 4580, loss = 0.8153077363967896
In grad_steps = 4581, loss = 0.12266416847705841
In grad_steps = 4582, loss = 0.1513533890247345
In grad_steps = 4583, loss = 0.4050888121128082
In grad_steps = 4584, loss = 0.03761739283800125
In grad_steps = 4585, loss = 0.03600955381989479
In grad_steps = 4586, loss = 0.11400824040174484
In grad_steps = 4587, loss = 0.15296390652656555
In grad_steps = 4588, loss = 0.02807704359292984
In grad_steps = 4589, loss = 0.07793648540973663
In grad_steps = 4590, loss = 0.3748263120651245
In grad_steps = 4591, loss = 0.5674331188201904
In grad_steps = 4592, loss = 0.1486874520778656
In grad_steps = 4593, loss = 0.12794655561447144
In grad_steps = 4594, loss = 0.34719279408454895
In grad_steps = 4595, loss = 0.1324455887079239
In grad_steps = 4596, loss = 0.09320858120918274
In grad_steps = 4597, loss = 0.047794267535209656
In grad_steps = 4598, loss = 1.0821826457977295
In grad_steps = 4599, loss = 0.09445735067129135
In grad_steps = 4600, loss = 0.08160343021154404
In grad_steps = 4601, loss = 1.3871910572052002
In grad_steps = 4602, loss = 0.31703120470046997
In grad_steps = 4603, loss = 0.1124253123998642
In grad_steps = 4604, loss = 0.23383845388889313
In grad_steps = 4605, loss = 0.5561277270317078
In grad_steps = 4606, loss = 0.7766460180282593
In grad_steps = 4607, loss = 0.14331261813640594
In grad_steps = 4608, loss = 0.1311943680047989
In grad_steps = 4609, loss = 0.42742273211479187
In grad_steps = 4610, loss = 0.09378302842378616
In grad_steps = 4611, loss = 0.12381486594676971
In grad_steps = 4612, loss = 0.13203008472919464
In grad_steps = 4613, loss = 0.3672342598438263
In grad_steps = 4614, loss = 0.1258191168308258
In grad_steps = 4615, loss = 0.22155854105949402
In grad_steps = 4616, loss = 0.19508954882621765
In grad_steps = 4617, loss = 0.44170063734054565
In grad_steps = 4618, loss = 0.042265549302101135
In grad_steps = 4619, loss = 0.09604816138744354
In grad_steps = 4620, loss = 0.10809735208749771
In grad_steps = 4621, loss = 0.02661784552037716
In grad_steps = 4622, loss = 0.7366081476211548
In grad_steps = 4623, loss = 0.10704582929611206
In grad_steps = 4624, loss = 0.0659293532371521
In grad_steps = 4625, loss = 0.04698308929800987
In grad_steps = 4626, loss = 0.1029917299747467
In grad_steps = 4627, loss = 0.0352175235748291
In grad_steps = 4628, loss = 0.04529394954442978
In grad_steps = 4629, loss = 0.09213826805353165
In grad_steps = 4630, loss = 0.10108830779790878
In grad_steps = 4631, loss = 0.035800132900476456
In grad_steps = 4632, loss = 0.015430119819939137
In grad_steps = 4633, loss = 0.03005204349756241
In grad_steps = 4634, loss = 0.1738204061985016
In grad_steps = 4635, loss = 0.13150745630264282
In grad_steps = 4636, loss = 0.8830277919769287
In grad_steps = 4637, loss = 0.06401723623275757
In grad_steps = 4638, loss = 0.06419084966182709
In grad_steps = 4639, loss = 0.06531283259391785
In grad_steps = 4640, loss = 0.5003266930580139
In grad_steps = 4641, loss = 0.1293763965368271
In grad_steps = 4642, loss = 0.022868823260068893
In grad_steps = 4643, loss = 0.02776632085442543
In grad_steps = 4644, loss = 0.02714650332927704
In grad_steps = 4645, loss = 0.04038329795002937
In grad_steps = 4646, loss = 0.17420727014541626
In grad_steps = 4647, loss = 0.10804963111877441
In grad_steps = 4648, loss = 0.040786296129226685
In grad_steps = 4649, loss = 0.020723145455121994
In grad_steps = 4650, loss = 0.04130695387721062
In grad_steps = 4651, loss = 0.022659895941615105
In grad_steps = 4652, loss = 0.026176108047366142
In grad_steps = 4653, loss = 0.021711010485887527
In grad_steps = 4654, loss = 0.2210465520620346
In grad_steps = 4655, loss = 0.023859476670622826
In grad_steps = 4656, loss = 0.011455416679382324
In grad_steps = 4657, loss = 0.012669957242906094
In grad_steps = 4658, loss = 0.009965157136321068
In grad_steps = 4659, loss = 0.011867733672261238
In grad_steps = 4660, loss = 0.027140341699123383
In grad_steps = 4661, loss = 0.4437452554702759
In grad_steps = 4662, loss = 0.003684941679239273
In grad_steps = 4663, loss = 1.108793020248413
In grad_steps = 4664, loss = 0.01200600154697895
In grad_steps = 4665, loss = 0.9860443472862244
In grad_steps = 4666, loss = 0.050793178379535675
In grad_steps = 4667, loss = 0.4369350075721741
In grad_steps = 4668, loss = 0.2613113224506378
In grad_steps = 4669, loss = 0.032078325748443604
In grad_steps = 4670, loss = 0.07002422213554382
In grad_steps = 4671, loss = 0.3014853000640869
In grad_steps = 4672, loss = 1.6507787704467773
In grad_steps = 4673, loss = 0.02245345152914524
In grad_steps = 4674, loss = 0.03447519615292549
In grad_steps = 4675, loss = 0.7099778056144714
In grad_steps = 4676, loss = 0.061339229345321655
In grad_steps = 4677, loss = 0.14049406349658966
In grad_steps = 4678, loss = 0.06107631325721741
In grad_steps = 4679, loss = 0.062257178127765656
In grad_steps = 4680, loss = 0.7416003346443176
In grad_steps = 4681, loss = 0.3576948344707489
In grad_steps = 4682, loss = 0.23096534609794617
In grad_steps = 4683, loss = 0.04567857086658478
In grad_steps = 4684, loss = 0.38964638113975525
In grad_steps = 4685, loss = 0.059482499957084656
In grad_steps = 4686, loss = 0.6940803527832031
In grad_steps = 4687, loss = 0.04744403436779976
In grad_steps = 4688, loss = 0.2701520025730133
In grad_steps = 4689, loss = 0.25140485167503357
In grad_steps = 4690, loss = 0.1343509703874588
In grad_steps = 4691, loss = 0.04274851456284523
In grad_steps = 4692, loss = 0.04064030200242996
In grad_steps = 4693, loss = 0.04797593504190445
In grad_steps = 4694, loss = 0.07525530457496643
In grad_steps = 4695, loss = 1.1916594505310059
In grad_steps = 4696, loss = 0.094861701130867
In grad_steps = 4697, loss = 0.8579576015472412
In grad_steps = 4698, loss = 0.13320446014404297
In grad_steps = 4699, loss = 0.16220064461231232
In grad_steps = 4700, loss = 0.03892785683274269
In grad_steps = 4701, loss = 0.04763365536928177
In grad_steps = 4702, loss = 0.07494527101516724
In grad_steps = 4703, loss = 0.21767181158065796
In grad_steps = 4704, loss = 0.26763680577278137
In grad_steps = 4705, loss = 0.011349402368068695
Elapsed time: 2675.4524142742157 seconds for ensemble 2 with 2 epochs
LoRA instance 2 evaluation complete. Data saved to /pscratch/sd/t/tianle/lucid/other_source/SURP_2024/results/tmp_lora_ensemble/Llama3-8B-set-4/test_data_instance_2_seed_20187.npz.
lora instance i = 2 Successfully finished.
Training lora instance 3
DEBUG: Total parameters: 8037076992
DEBUG: Trainable parameters: 6815744
Beginning epoch 1
In grad_steps = 0, loss = 0.5826846361160278
In grad_steps = 1, loss = 0.48004746437072754
In grad_steps = 2, loss = 1.7352862358093262
In grad_steps = 3, loss = 0.7382422089576721
In grad_steps = 4, loss = 0.9314565658569336
In grad_steps = 5, loss = 0.7385431528091431
In grad_steps = 6, loss = 1.089685082435608
In grad_steps = 7, loss = 0.8153940439224243
In grad_steps = 8, loss = 0.45539191365242004
In grad_steps = 9, loss = 0.6956820487976074
In grad_steps = 10, loss = 0.7719683051109314
In grad_steps = 11, loss = 1.023486852645874
In grad_steps = 12, loss = 1.0247793197631836
In grad_steps = 13, loss = 0.6784830093383789
In grad_steps = 14, loss = 0.6371771693229675
In grad_steps = 15, loss = 0.7619044184684753
In grad_steps = 16, loss = 0.7450134754180908
In grad_steps = 17, loss = 0.5740063786506653
In grad_steps = 18, loss = 1.1307986974716187
In grad_steps = 19, loss = 1.0392717123031616
In grad_steps = 20, loss = 1.0015743970870972
In grad_steps = 21, loss = 0.594738245010376
In grad_steps = 22, loss = 0.7009463310241699
In grad_steps = 23, loss = 0.5658884048461914
In grad_steps = 24, loss = 0.7631773948669434
In grad_steps = 25, loss = 0.6722897291183472
In grad_steps = 26, loss = 0.6556493639945984
In grad_steps = 27, loss = 0.6697934865951538
In grad_steps = 28, loss = 0.7062512636184692
In grad_steps = 29, loss = 0.7241677045822144
In grad_steps = 30, loss = 0.7222111225128174
In grad_steps = 31, loss = 0.6793127655982971
In grad_steps = 32, loss = 0.688563883304596
In grad_steps = 33, loss = 0.7298597097396851
In grad_steps = 34, loss = 0.7257407903671265
In grad_steps = 35, loss = 0.5845762491226196
In grad_steps = 36, loss = 0.7070302367210388
In grad_steps = 37, loss = 0.6767022609710693
In grad_steps = 38, loss = 0.7348161339759827
In grad_steps = 39, loss = 0.6387782096862793
In grad_steps = 40, loss = 0.6573276519775391
In grad_steps = 41, loss = 0.5641471147537231
In grad_steps = 42, loss = 0.5320113897323608
In grad_steps = 43, loss = 0.5272954702377319
In grad_steps = 44, loss = 0.7539863586425781
In grad_steps = 45, loss = 0.7518085837364197
In grad_steps = 46, loss = 0.9295746684074402
In grad_steps = 47, loss = 0.6168218851089478
In grad_steps = 48, loss = 0.9322249293327332
In grad_steps = 49, loss = 0.5227275490760803
In grad_steps = 50, loss = 0.9872715473175049
In grad_steps = 51, loss = 0.9949640035629272
In grad_steps = 52, loss = 0.7119138836860657
In grad_steps = 53, loss = 0.684897780418396
In grad_steps = 54, loss = 0.7026965618133545
In grad_steps = 55, loss = 0.6120293140411377
In grad_steps = 56, loss = 0.628248929977417
In grad_steps = 57, loss = 2.8680002689361572
In grad_steps = 58, loss = 0.6532122492790222
In grad_steps = 59, loss = 0.7038357853889465
In grad_steps = 60, loss = 0.6849954724311829
In grad_steps = 61, loss = 0.738635241985321
In grad_steps = 62, loss = 0.7999659180641174
In grad_steps = 63, loss = 0.6306809186935425
In grad_steps = 64, loss = 0.7029653191566467
In grad_steps = 65, loss = 0.7402034401893616
In grad_steps = 66, loss = 0.6883878707885742
In grad_steps = 67, loss = 0.695658802986145
In grad_steps = 68, loss = 0.6816287040710449
In grad_steps = 69, loss = 0.7843815684318542
In grad_steps = 70, loss = 0.6832833886146545
In grad_steps = 71, loss = 0.6804287433624268
In grad_steps = 72, loss = 0.6337777376174927
In grad_steps = 73, loss = 0.727657675743103
In grad_steps = 74, loss = 1.7215464115142822
In grad_steps = 75, loss = 0.5882911682128906
In grad_steps = 76, loss = 0.715979814529419
In grad_steps = 77, loss = 0.7893522381782532
In grad_steps = 78, loss = 0.6510083675384521
In grad_steps = 79, loss = 0.6096243858337402
In grad_steps = 80, loss = 0.6971960067749023
In grad_steps = 81, loss = 0.6015326976776123
In grad_steps = 82, loss = 0.6766416430473328
In grad_steps = 83, loss = 0.7079415321350098
In grad_steps = 84, loss = 0.6736067533493042
In grad_steps = 85, loss = 0.7356358766555786
In grad_steps = 86, loss = 0.6096166372299194
In grad_steps = 87, loss = 0.6066269874572754
In grad_steps = 88, loss = 0.6667135953903198
In grad_steps = 89, loss = 0.699760377407074
In grad_steps = 90, loss = 0.674107551574707
In grad_steps = 91, loss = 0.7101442813873291
In grad_steps = 92, loss = 0.7181590795516968
In grad_steps = 93, loss = 0.5944420695304871
In grad_steps = 94, loss = 0.6145980358123779
In grad_steps = 95, loss = 0.5847296118736267
In grad_steps = 96, loss = 0.7738615274429321
In grad_steps = 97, loss = 0.6578152775764465
In grad_steps = 98, loss = 0.5604531764984131
In grad_steps = 99, loss = 0.6302062273025513
In grad_steps = 100, loss = 0.5517913103103638
In grad_steps = 101, loss = 0.527727484703064
In grad_steps = 102, loss = 0.8032931089401245
In grad_steps = 103, loss = 0.6737821102142334
In grad_steps = 104, loss = 0.7084925770759583
In grad_steps = 105, loss = 0.7056829929351807
In grad_steps = 106, loss = 0.5664730072021484
In grad_steps = 107, loss = 0.825899064540863
In grad_steps = 108, loss = 0.4636564254760742
In grad_steps = 109, loss = 0.5734903812408447
In grad_steps = 110, loss = 0.5262314677238464
In grad_steps = 111, loss = 0.4978855848312378
In grad_steps = 112, loss = 0.7856467962265015
In grad_steps = 113, loss = 0.8498855829238892
In grad_steps = 114, loss = 0.5349103212356567
In grad_steps = 115, loss = 0.3855236768722534
In grad_steps = 116, loss = 0.40116825699806213
In grad_steps = 117, loss = 0.566912055015564
In grad_steps = 118, loss = 1.364372730255127
In grad_steps = 119, loss = 1.4136005640029907
In grad_steps = 120, loss = 0.879481315612793
In grad_steps = 121, loss = 0.8834661245346069
In grad_steps = 122, loss = 0.7508458495140076
In grad_steps = 123, loss = 0.6629632115364075
In grad_steps = 124, loss = 0.6324099898338318
In grad_steps = 125, loss = 0.7516587972640991
In grad_steps = 126, loss = 0.42654216289520264
In grad_steps = 127, loss = 0.7635372877120972
In grad_steps = 128, loss = 0.6921590566635132
In grad_steps = 129, loss = 0.8102434873580933
In grad_steps = 130, loss = 0.5771602988243103
In grad_steps = 131, loss = 0.8198977708816528
In grad_steps = 132, loss = 0.6359819173812866
In grad_steps = 133, loss = 0.7079301476478577
In grad_steps = 134, loss = 0.6997447609901428
In grad_steps = 135, loss = 0.6804991364479065
In grad_steps = 136, loss = 0.6999422311782837
In grad_steps = 137, loss = 0.7110295295715332
In grad_steps = 138, loss = 0.6499347686767578
In grad_steps = 139, loss = 0.5761357545852661
In grad_steps = 140, loss = 0.623279333114624
In grad_steps = 141, loss = 0.66645348072052
In grad_steps = 142, loss = 0.688396155834198
In grad_steps = 143, loss = 0.6262290477752686
In grad_steps = 144, loss = 0.6267706751823425
In grad_steps = 145, loss = 0.8050531148910522
In grad_steps = 146, loss = 0.5926995277404785
In grad_steps = 147, loss = 0.6287863254547119
In grad_steps = 148, loss = 0.6468764543533325
In grad_steps = 149, loss = 0.4559156894683838
In grad_steps = 150, loss = 0.46565255522727966
In grad_steps = 151, loss = 0.8037238121032715
In grad_steps = 152, loss = 0.1856309175491333
In grad_steps = 153, loss = 1.0814613103866577
In grad_steps = 154, loss = 1.3022921085357666
In grad_steps = 155, loss = 0.5453578233718872
In grad_steps = 156, loss = 0.6144010424613953
In grad_steps = 157, loss = 0.7384891510009766
In grad_steps = 158, loss = 0.5639493465423584
In grad_steps = 159, loss = 0.6454219818115234
In grad_steps = 160, loss = 0.6106104254722595
In grad_steps = 161, loss = 0.42320215702056885
In grad_steps = 162, loss = 0.9082444310188293
In grad_steps = 163, loss = 0.4842435419559479
In grad_steps = 164, loss = 0.9937866926193237
In grad_steps = 165, loss = 0.6070531606674194
In grad_steps = 166, loss = 0.6989361047744751
In grad_steps = 167, loss = 0.4134107530117035
In grad_steps = 168, loss = 0.45755940675735474
In grad_steps = 169, loss = 0.751968264579773
In grad_steps = 170, loss = 0.5964336395263672
In grad_steps = 171, loss = 0.6877100467681885
In grad_steps = 172, loss = 0.6309939622879028
In grad_steps = 173, loss = 0.5112572312355042
In grad_steps = 174, loss = 0.8541085720062256
In grad_steps = 175, loss = 0.5766587257385254
In grad_steps = 176, loss = 0.45580437779426575
In grad_steps = 177, loss = 0.5269311666488647
In grad_steps = 178, loss = 0.5983020663261414
In grad_steps = 179, loss = 0.544689416885376
In grad_steps = 180, loss = 0.533372700214386
In grad_steps = 181, loss = 0.5354028940200806
In grad_steps = 182, loss = 0.556578516960144
In grad_steps = 183, loss = 0.44337087869644165
In grad_steps = 184, loss = 0.4414069652557373
In grad_steps = 185, loss = 0.45516902208328247
In grad_steps = 186, loss = 1.1007583141326904
In grad_steps = 187, loss = 0.258681982755661
In grad_steps = 188, loss = 0.8150591254234314
In grad_steps = 189, loss = 0.17544028162956238
In grad_steps = 190, loss = 0.6945688724517822
In grad_steps = 191, loss = 0.676021933555603
In grad_steps = 192, loss = 0.29390954971313477
In grad_steps = 193, loss = 0.2470645308494568
In grad_steps = 194, loss = 0.7191243767738342
In grad_steps = 195, loss = 0.24232318997383118
In grad_steps = 196, loss = 0.42270228266716003
In grad_steps = 197, loss = 0.17589198052883148
In grad_steps = 198, loss = 0.7558625936508179
In grad_steps = 199, loss = 0.6569753289222717
In grad_steps = 200, loss = 0.31141582131385803
In grad_steps = 201, loss = 0.9798663854598999
In grad_steps = 202, loss = 0.2882411479949951
In grad_steps = 203, loss = 0.18934422731399536
In grad_steps = 204, loss = 0.2625845968723297
In grad_steps = 205, loss = 0.727011501789093
In grad_steps = 206, loss = 1.260872483253479
In grad_steps = 207, loss = 0.37904614210128784
In grad_steps = 208, loss = 0.22589147090911865
In grad_steps = 209, loss = 0.7482855916023254
In grad_steps = 210, loss = 0.41646116971969604
In grad_steps = 211, loss = 0.36587491631507874
In grad_steps = 212, loss = 0.3257308006286621
In grad_steps = 213, loss = 0.5766327381134033
In grad_steps = 214, loss = 0.48487043380737305
In grad_steps = 215, loss = 0.4553576111793518
In grad_steps = 216, loss = 0.488055020570755
In grad_steps = 217, loss = 0.5979282259941101
In grad_steps = 218, loss = 0.4866596460342407
In grad_steps = 219, loss = 0.39930790662765503
In grad_steps = 220, loss = 0.31777602434158325
In grad_steps = 221, loss = 0.2289421260356903
In grad_steps = 222, loss = 0.18563786149024963
In grad_steps = 223, loss = 0.4921933114528656
In grad_steps = 224, loss = 0.19037429988384247
In grad_steps = 225, loss = 1.053236484527588
In grad_steps = 226, loss = 0.6543092131614685
In grad_steps = 227, loss = 1.3716297149658203
In grad_steps = 228, loss = 0.36324167251586914
In grad_steps = 229, loss = 0.34594041109085083
In grad_steps = 230, loss = 0.48835229873657227
In grad_steps = 231, loss = 0.3218080997467041
In grad_steps = 232, loss = 0.21217571198940277
In grad_steps = 233, loss = 0.5515730381011963
In grad_steps = 234, loss = 0.5931732654571533
In grad_steps = 235, loss = 0.3836699426174164
In grad_steps = 236, loss = 0.7792104482650757
In grad_steps = 237, loss = 0.6594808101654053
In grad_steps = 238, loss = 0.2977214455604553
In grad_steps = 239, loss = 0.4563949704170227
In grad_steps = 240, loss = 0.7364857196807861
In grad_steps = 241, loss = 0.3311663866043091
In grad_steps = 242, loss = 0.6616021394729614
In grad_steps = 243, loss = 0.6183583736419678
In grad_steps = 244, loss = 0.3482893705368042
In grad_steps = 245, loss = 0.306097149848938
In grad_steps = 246, loss = 0.3297458291053772
In grad_steps = 247, loss = 0.6907670497894287
In grad_steps = 248, loss = 0.1911403387784958
In grad_steps = 249, loss = 0.2819885313510895
In grad_steps = 250, loss = 0.4032413363456726
In grad_steps = 251, loss = 0.6042962670326233
In grad_steps = 252, loss = 1.330937147140503
In grad_steps = 253, loss = 0.24441084265708923
In grad_steps = 254, loss = 0.9382758140563965
In grad_steps = 255, loss = 0.5210017561912537
In grad_steps = 256, loss = 0.36767667531967163
In grad_steps = 257, loss = 0.5812973380088806
In grad_steps = 258, loss = 0.25219249725341797
In grad_steps = 259, loss = 0.5881719589233398
In grad_steps = 260, loss = 0.8089613914489746
In grad_steps = 261, loss = 0.7093684673309326
In grad_steps = 262, loss = 0.33336907625198364
In grad_steps = 263, loss = 0.801089346408844
In grad_steps = 264, loss = 0.2855786681175232
In grad_steps = 265, loss = 0.4202529788017273
In grad_steps = 266, loss = 0.5560979247093201
In grad_steps = 267, loss = 0.4182198941707611
In grad_steps = 268, loss = 0.47845593094825745
In grad_steps = 269, loss = 0.3931545913219452
In grad_steps = 270, loss = 0.2776563763618469
In grad_steps = 271, loss = 0.3039010167121887
In grad_steps = 272, loss = 0.21257276833057404
In grad_steps = 273, loss = 0.5951867699623108
In grad_steps = 274, loss = 0.12280042469501495
In grad_steps = 275, loss = 0.2564270794391632
In grad_steps = 276, loss = 0.4168558120727539
In grad_steps = 277, loss = 0.7038334012031555
In grad_steps = 278, loss = 0.11851345747709274
In grad_steps = 279, loss = 0.8946293592453003
In grad_steps = 280, loss = 0.23448100686073303
In grad_steps = 281, loss = 0.42467066645622253
In grad_steps = 282, loss = 0.504934549331665
In grad_steps = 283, loss = 1.0069694519042969
In grad_steps = 284, loss = 0.22195430099964142
In grad_steps = 285, loss = 0.5791592001914978
In grad_steps = 286, loss = 0.7497214078903198
In grad_steps = 287, loss = 0.06407540291547775
In grad_steps = 288, loss = 0.061992719769477844
In grad_steps = 289, loss = 0.6137345433235168
In grad_steps = 290, loss = 0.07920552045106888
In grad_steps = 291, loss = 0.31737661361694336
In grad_steps = 292, loss = 0.6515553593635559
In grad_steps = 293, loss = 0.4772419333457947
In grad_steps = 294, loss = 0.3048526644706726
In grad_steps = 295, loss = 0.2305368036031723
In grad_steps = 296, loss = 0.17530806362628937
In grad_steps = 297, loss = 0.34098389744758606
In grad_steps = 298, loss = 0.4142172634601593
In grad_steps = 299, loss = 0.5380061864852905
In grad_steps = 300, loss = 0.5684068202972412
In grad_steps = 301, loss = 0.39889127016067505
In grad_steps = 302, loss = 0.44250211119651794
In grad_steps = 303, loss = 0.2637011706829071
In grad_steps = 304, loss = 0.43693113327026367
In grad_steps = 305, loss = 1.4791934490203857
In grad_steps = 306, loss = 0.11885182559490204
In grad_steps = 307, loss = 0.3453308641910553
In grad_steps = 308, loss = 0.517367959022522
In grad_steps = 309, loss = 0.7660355567932129
In grad_steps = 310, loss = 0.4624839127063751
In grad_steps = 311, loss = 0.7604231834411621
In grad_steps = 312, loss = 0.5596073865890503
In grad_steps = 313, loss = 0.6668861508369446
In grad_steps = 314, loss = 0.6373087763786316
In grad_steps = 315, loss = 0.30539923906326294
In grad_steps = 316, loss = 0.4299408197402954
In grad_steps = 317, loss = 0.4772690236568451
In grad_steps = 318, loss = 0.266772985458374
In grad_steps = 319, loss = 0.48211395740509033
In grad_steps = 320, loss = 0.24102374911308289
In grad_steps = 321, loss = 0.3873702883720398
In grad_steps = 322, loss = 0.6735000610351562
In grad_steps = 323, loss = 0.5802944302558899
In grad_steps = 324, loss = 0.2503732144832611
In grad_steps = 325, loss = 0.19266989827156067
In grad_steps = 326, loss = 0.16213591396808624
In grad_steps = 327, loss = 0.2543359100818634
In grad_steps = 328, loss = 0.17922474443912506
In grad_steps = 329, loss = 0.14742416143417358
In grad_steps = 330, loss = 0.8840478658676147
In grad_steps = 331, loss = 0.18868666887283325
In grad_steps = 332, loss = 0.12814295291900635
In grad_steps = 333, loss = 0.47711020708084106
In grad_steps = 334, loss = 0.04430136829614639
In grad_steps = 335, loss = 0.2628401815891266
In grad_steps = 336, loss = 0.5841271877288818
In grad_steps = 337, loss = 0.09503461420536041
In grad_steps = 338, loss = 0.24200215935707092
In grad_steps = 339, loss = 0.610018253326416
In grad_steps = 340, loss = 0.4519321620464325
In grad_steps = 341, loss = 0.05094793438911438
In grad_steps = 342, loss = 1.3242241144180298
In grad_steps = 343, loss = 0.7612932324409485
In grad_steps = 344, loss = 0.08606740832328796
In grad_steps = 345, loss = 0.08484888076782227
In grad_steps = 346, loss = 0.3485938608646393
In grad_steps = 347, loss = 0.754287838935852
In grad_steps = 348, loss = 0.6228821277618408
In grad_steps = 349, loss = 0.936268150806427
In grad_steps = 350, loss = 0.8103774189949036
In grad_steps = 351, loss = 0.3955378532409668
In grad_steps = 352, loss = 0.768718957901001
In grad_steps = 353, loss = 0.21889276802539825
In grad_steps = 354, loss = 0.3688957989215851
In grad_steps = 355, loss = 1.37407386302948
In grad_steps = 356, loss = 0.10980749875307083
In grad_steps = 357, loss = 0.5828086733818054
In grad_steps = 358, loss = 0.14438477158546448
In grad_steps = 359, loss = 0.6533646583557129
In grad_steps = 360, loss = 0.6972949504852295
In grad_steps = 361, loss = 0.4934305250644684
In grad_steps = 362, loss = 0.2795800566673279
In grad_steps = 363, loss = 0.3221173882484436
In grad_steps = 364, loss = 0.45084989070892334
In grad_steps = 365, loss = 0.3646547496318817
In grad_steps = 366, loss = 0.35341885685920715
In grad_steps = 367, loss = 0.3888236880302429
In grad_steps = 368, loss = 0.29087427258491516
In grad_steps = 369, loss = 0.2952430546283722
In grad_steps = 370, loss = 0.6500746011734009
In grad_steps = 371, loss = 0.3110481798648834
In grad_steps = 372, loss = 0.15580742061138153
In grad_steps = 373, loss = 0.44921553134918213
In grad_steps = 374, loss = 0.21716642379760742
In grad_steps = 375, loss = 0.4060048758983612
In grad_steps = 376, loss = 0.0844462588429451
In grad_steps = 377, loss = 0.4901580214500427
In grad_steps = 378, loss = 0.5107390284538269
In grad_steps = 379, loss = 0.2055484652519226
In grad_steps = 380, loss = 0.3703640401363373
In grad_steps = 381, loss = 0.8128606677055359
In grad_steps = 382, loss = 0.30535924434661865
In grad_steps = 383, loss = 0.27699345350265503
In grad_steps = 384, loss = 0.3001812696456909
In grad_steps = 385, loss = 0.18163836002349854
In grad_steps = 386, loss = 1.5852712392807007
In grad_steps = 387, loss = 0.4871298372745514
In grad_steps = 388, loss = 0.7570702433586121
In grad_steps = 389, loss = 0.7478557229042053
In grad_steps = 390, loss = 0.2880904972553253
In grad_steps = 391, loss = 0.17488053441047668
In grad_steps = 392, loss = 0.21567344665527344
In grad_steps = 393, loss = 0.5450441837310791
In grad_steps = 394, loss = 0.19494277238845825
In grad_steps = 395, loss = 0.4726179838180542
In grad_steps = 396, loss = 0.3882921636104584
In grad_steps = 397, loss = 0.5203282833099365
In grad_steps = 398, loss = 0.1976386308670044
In grad_steps = 399, loss = 0.36974719166755676
In grad_steps = 400, loss = 0.40152212977409363
In grad_steps = 401, loss = 0.20061597228050232
In grad_steps = 402, loss = 0.3895930051803589
In grad_steps = 403, loss = 0.8559734225273132
In grad_steps = 404, loss = 0.14696332812309265
In grad_steps = 405, loss = 0.16466960310935974
In grad_steps = 406, loss = 1.0485169887542725
In grad_steps = 407, loss = 0.33470919728279114
In grad_steps = 408, loss = 0.06706102192401886
In grad_steps = 409, loss = 0.46819496154785156
In grad_steps = 410, loss = 0.2595421373844147
In grad_steps = 411, loss = 0.853217601776123
In grad_steps = 412, loss = 0.32063353061676025
In grad_steps = 413, loss = 0.16709966957569122
In grad_steps = 414, loss = 0.67719566822052
In grad_steps = 415, loss = 0.3969937264919281
In grad_steps = 416, loss = 0.2900000214576721
In grad_steps = 417, loss = 0.2767349183559418
In grad_steps = 418, loss = 0.4830870032310486
In grad_steps = 419, loss = 0.2832317054271698
In grad_steps = 420, loss = 0.07748286426067352
In grad_steps = 421, loss = 1.0722805261611938
In grad_steps = 422, loss = 0.6716672778129578
In grad_steps = 423, loss = 0.1906927525997162
In grad_steps = 424, loss = 0.9066063761711121
In grad_steps = 425, loss = 0.1079343855381012
In grad_steps = 426, loss = 0.5449458956718445
In grad_steps = 427, loss = 0.21748843789100647
In grad_steps = 428, loss = 0.11395397037267685
In grad_steps = 429, loss = 0.5680074691772461
In grad_steps = 430, loss = 0.5220251679420471
In grad_steps = 431, loss = 0.39458155632019043
In grad_steps = 432, loss = 0.09881505370140076
In grad_steps = 433, loss = 0.3987753689289093
In grad_steps = 434, loss = 1.0121698379516602
In grad_steps = 435, loss = 0.11428339779376984
In grad_steps = 436, loss = 0.20133404433727264
In grad_steps = 437, loss = 0.2869044840335846
In grad_steps = 438, loss = 1.0976542234420776
In grad_steps = 439, loss = 0.15556734800338745
In grad_steps = 440, loss = 0.15592485666275024
In grad_steps = 441, loss = 0.6433998942375183
In grad_steps = 442, loss = 0.5753498673439026
In grad_steps = 443, loss = 0.15198278427124023
In grad_steps = 444, loss = 0.455843061208725
In grad_steps = 445, loss = 0.18122433125972748
In grad_steps = 446, loss = 1.1377530097961426
In grad_steps = 447, loss = 0.7537729740142822
In grad_steps = 448, loss = 0.8182410001754761
In grad_steps = 449, loss = 0.15717144310474396
In grad_steps = 450, loss = 0.4284629821777344
In grad_steps = 451, loss = 0.3578495681285858
In grad_steps = 452, loss = 0.3387681841850281
In grad_steps = 453, loss = 0.30752062797546387
In grad_steps = 454, loss = 0.43648967146873474
In grad_steps = 455, loss = 0.46465206146240234
In grad_steps = 456, loss = 0.6062788367271423
In grad_steps = 457, loss = 0.29682981967926025
In grad_steps = 458, loss = 0.6180199980735779
In grad_steps = 459, loss = 0.36577120423316956
In grad_steps = 460, loss = 0.4859272837638855
In grad_steps = 461, loss = 0.10910186171531677
In grad_steps = 462, loss = 0.6313072443008423
In grad_steps = 463, loss = 0.2386106550693512
In grad_steps = 464, loss = 0.5627322793006897
In grad_steps = 465, loss = 0.7053209543228149
In grad_steps = 466, loss = 0.4216199517250061
In grad_steps = 467, loss = 0.3610099256038666
In grad_steps = 468, loss = 0.857451319694519
In grad_steps = 469, loss = 0.3699829578399658
In grad_steps = 470, loss = 0.3859342634677887
In grad_steps = 471, loss = 0.1079540103673935
In grad_steps = 472, loss = 0.13302011787891388
In grad_steps = 473, loss = 0.2263687252998352
In grad_steps = 474, loss = 0.8908559679985046
In grad_steps = 475, loss = 0.6219856142997742
In grad_steps = 476, loss = 0.12189327925443649
In grad_steps = 477, loss = 0.1790495216846466
In grad_steps = 478, loss = 0.1578342616558075
In grad_steps = 479, loss = 0.6016916036605835
In grad_steps = 480, loss = 0.1270693689584732
In grad_steps = 481, loss = 0.683590292930603
In grad_steps = 482, loss = 0.08042162656784058
In grad_steps = 483, loss = 0.169429749250412
In grad_steps = 484, loss = 0.29763174057006836
In grad_steps = 485, loss = 0.3603162169456482
In grad_steps = 486, loss = 0.230682373046875
In grad_steps = 487, loss = 0.7177028656005859
In grad_steps = 488, loss = 1.1122071743011475
In grad_steps = 489, loss = 0.8791832327842712
In grad_steps = 490, loss = 0.16917112469673157
In grad_steps = 491, loss = 0.41988497972488403
In grad_steps = 492, loss = 0.22921784222126007
In grad_steps = 493, loss = 0.06118316575884819
In grad_steps = 494, loss = 1.0624603033065796
In grad_steps = 495, loss = 0.1646711379289627
In grad_steps = 496, loss = 0.7365555763244629
In grad_steps = 497, loss = 0.16108079254627228
In grad_steps = 498, loss = 0.427707701921463
In grad_steps = 499, loss = 0.15707357227802277
In grad_steps = 500, loss = 0.12844209372997284
In grad_steps = 501, loss = 0.5355994701385498
In grad_steps = 502, loss = 0.7347589731216431
In grad_steps = 503, loss = 0.5671539306640625
In grad_steps = 504, loss = 0.48678094148635864
In grad_steps = 505, loss = 0.6685857772827148
In grad_steps = 506, loss = 0.3151649236679077
In grad_steps = 507, loss = 0.599905788898468
In grad_steps = 508, loss = 0.6561948657035828
In grad_steps = 509, loss = 0.32709503173828125
In grad_steps = 510, loss = 0.1265953630208969
In grad_steps = 511, loss = 0.34345269203186035
In grad_steps = 512, loss = 0.22981588542461395
In grad_steps = 513, loss = 0.4763617515563965
In grad_steps = 514, loss = 0.3379212021827698
In grad_steps = 515, loss = 0.3624655604362488
In grad_steps = 516, loss = 0.4897449016571045
In grad_steps = 517, loss = 0.498380571603775
In grad_steps = 518, loss = 0.5688030123710632
In grad_steps = 519, loss = 0.30256593227386475
In grad_steps = 520, loss = 0.1526382863521576
In grad_steps = 521, loss = 0.37974777817726135
In grad_steps = 522, loss = 0.1270577311515808
In grad_steps = 523, loss = 0.20571884512901306
In grad_steps = 524, loss = 0.27034151554107666
In grad_steps = 525, loss = 0.3556670844554901
In grad_steps = 526, loss = 0.12332382798194885
In grad_steps = 527, loss = 0.5445418953895569
In grad_steps = 528, loss = 0.22529524564743042
In grad_steps = 529, loss = 0.05099737271666527
In grad_steps = 530, loss = 0.08381586521863937
In grad_steps = 531, loss = 0.6402444839477539
In grad_steps = 532, loss = 0.1725526750087738
In grad_steps = 533, loss = 0.2575288712978363
In grad_steps = 534, loss = 1.0226761102676392
In grad_steps = 535, loss = 0.612324059009552
In grad_steps = 536, loss = 0.21067294478416443
In grad_steps = 537, loss = 0.4217259883880615
In grad_steps = 538, loss = 1.5121331214904785
In grad_steps = 539, loss = 0.19883480668067932
In grad_steps = 540, loss = 0.10132500529289246
In grad_steps = 541, loss = 0.43633875250816345
In grad_steps = 542, loss = 0.759407639503479
In grad_steps = 543, loss = 0.079740010201931
In grad_steps = 544, loss = 1.012255311012268
In grad_steps = 545, loss = 1.1221747398376465
In grad_steps = 546, loss = 0.19951149821281433
In grad_steps = 547, loss = 0.30748024582862854
In grad_steps = 548, loss = 0.2552870512008667
In grad_steps = 549, loss = 0.4801405370235443
In grad_steps = 550, loss = 0.7311601042747498
In grad_steps = 551, loss = 0.19102871417999268
In grad_steps = 552, loss = 0.26141873002052307
In grad_steps = 553, loss = 0.3958211839199066
In grad_steps = 554, loss = 0.600734293460846
In grad_steps = 555, loss = 0.6526046991348267
In grad_steps = 556, loss = 0.6282748579978943
In grad_steps = 557, loss = 0.41046038269996643
In grad_steps = 558, loss = 0.38889867067337036
In grad_steps = 559, loss = 0.2913694977760315
In grad_steps = 560, loss = 0.8617888689041138
In grad_steps = 561, loss = 0.3563491702079773
In grad_steps = 562, loss = 0.4648124873638153
In grad_steps = 563, loss = 0.3510509729385376
In grad_steps = 564, loss = 0.9048611521720886
In grad_steps = 565, loss = 0.3983129560947418
In grad_steps = 566, loss = 0.228472501039505
In grad_steps = 567, loss = 0.7783684730529785
In grad_steps = 568, loss = 0.7250053882598877
In grad_steps = 569, loss = 0.11450859159231186
In grad_steps = 570, loss = 0.3778994083404541
In grad_steps = 571, loss = 0.14675158262252808
In grad_steps = 572, loss = 0.2033388912677765
In grad_steps = 573, loss = 0.1249731257557869
In grad_steps = 574, loss = 0.1541943997144699
In grad_steps = 575, loss = 0.20494970679283142
In grad_steps = 576, loss = 0.14824648201465607
In grad_steps = 577, loss = 0.7339215278625488
In grad_steps = 578, loss = 0.2822166681289673
In grad_steps = 579, loss = 0.32409384846687317
In grad_steps = 580, loss = 0.9560701847076416
In grad_steps = 581, loss = 0.4207882285118103
In grad_steps = 582, loss = 0.6004877686500549
In grad_steps = 583, loss = 0.21264757215976715
In grad_steps = 584, loss = 0.0737118199467659
In grad_steps = 585, loss = 0.16329367458820343
In grad_steps = 586, loss = 0.8559709787368774
In grad_steps = 587, loss = 0.3840326964855194
In grad_steps = 588, loss = 0.2979690432548523
In grad_steps = 589, loss = 1.040480613708496
In grad_steps = 590, loss = 0.36092373728752136
In grad_steps = 591, loss = 0.43255993723869324
In grad_steps = 592, loss = 1.285500168800354
In grad_steps = 593, loss = 0.136087104678154
In grad_steps = 594, loss = 0.14165087044239044
In grad_steps = 595, loss = 0.49204766750335693
In grad_steps = 596, loss = 0.5778759717941284
In grad_steps = 597, loss = 0.6946172714233398
In grad_steps = 598, loss = 0.32552045583724976
In grad_steps = 599, loss = 0.31502780318260193
In grad_steps = 600, loss = 0.5928525924682617
In grad_steps = 601, loss = 0.19994236528873444
In grad_steps = 602, loss = 0.30060645937919617
In grad_steps = 603, loss = 0.41786158084869385
In grad_steps = 604, loss = 0.13604173064231873
In grad_steps = 605, loss = 0.3299929201602936
In grad_steps = 606, loss = 0.20473383367061615
In grad_steps = 607, loss = 0.2069752812385559
In grad_steps = 608, loss = 0.3790021240711212
In grad_steps = 609, loss = 0.31283923983573914
In grad_steps = 610, loss = 0.5558687448501587
In grad_steps = 611, loss = 0.4597339630126953
In grad_steps = 612, loss = 0.5506221652030945
In grad_steps = 613, loss = 0.4240693151950836
In grad_steps = 614, loss = 0.2757878005504608
In grad_steps = 615, loss = 0.5581510066986084
In grad_steps = 616, loss = 0.1826055645942688
In grad_steps = 617, loss = 0.08319466561079025
In grad_steps = 618, loss = 0.1547621786594391
In grad_steps = 619, loss = 0.060055095702409744
In grad_steps = 620, loss = 0.18452051281929016
In grad_steps = 621, loss = 1.3422341346740723
In grad_steps = 622, loss = 0.21178984642028809
In grad_steps = 623, loss = 0.0727873146533966
In grad_steps = 624, loss = 0.05526074394583702
In grad_steps = 625, loss = 0.7336027026176453
In grad_steps = 626, loss = 0.36619865894317627
In grad_steps = 627, loss = 0.043782662600278854
In grad_steps = 628, loss = 0.15455739200115204
In grad_steps = 629, loss = 0.08350061625242233
In grad_steps = 630, loss = 0.12263959646224976
In grad_steps = 631, loss = 0.14460128545761108
In grad_steps = 632, loss = 0.07151197642087936
In grad_steps = 633, loss = 0.749671995639801
In grad_steps = 634, loss = 0.1716374307870865
In grad_steps = 635, loss = 0.4314996302127838
In grad_steps = 636, loss = 0.21150606870651245
In grad_steps = 637, loss = 0.3433575928211212
In grad_steps = 638, loss = 0.03352489322423935
In grad_steps = 639, loss = 0.32132184505462646
In grad_steps = 640, loss = 0.13405928015708923
In grad_steps = 641, loss = 0.14872799813747406
In grad_steps = 642, loss = 0.4557650089263916
In grad_steps = 643, loss = 0.572894811630249
In grad_steps = 644, loss = 1.0465410947799683
In grad_steps = 645, loss = 1.245873212814331
In grad_steps = 646, loss = 0.06327609717845917
In grad_steps = 647, loss = 0.36312875151634216
In grad_steps = 648, loss = 0.054736506193876266
In grad_steps = 649, loss = 0.48321661353111267
In grad_steps = 650, loss = 0.8682988286018372
In grad_steps = 651, loss = 0.25137659907341003
In grad_steps = 652, loss = 0.8439187407493591
In grad_steps = 653, loss = 0.4641564190387726
In grad_steps = 654, loss = 0.7231351137161255
In grad_steps = 655, loss = 0.5424948930740356
In grad_steps = 656, loss = 0.19794589281082153
In grad_steps = 657, loss = 0.22557371854782104
In grad_steps = 658, loss = 0.3264027237892151
In grad_steps = 659, loss = 0.3033762574195862
In grad_steps = 660, loss = 0.4015459716320038
In grad_steps = 661, loss = 0.22170042991638184
In grad_steps = 662, loss = 0.43776392936706543
In grad_steps = 663, loss = 0.2196677327156067
In grad_steps = 664, loss = 0.10931060463190079
In grad_steps = 665, loss = 0.2464500367641449
In grad_steps = 666, loss = 0.19551020860671997
In grad_steps = 667, loss = 0.5231408476829529
In grad_steps = 668, loss = 0.17506307363510132
In grad_steps = 669, loss = 0.30165308713912964
In grad_steps = 670, loss = 0.6787453889846802
In grad_steps = 671, loss = 0.483375608921051
In grad_steps = 672, loss = 0.09865856170654297
In grad_steps = 673, loss = 0.8206920623779297
In grad_steps = 674, loss = 0.08658858388662338
In grad_steps = 675, loss = 0.93836510181427
In grad_steps = 676, loss = 0.11067125201225281
In grad_steps = 677, loss = 0.5796232223510742
In grad_steps = 678, loss = 0.04605194926261902
In grad_steps = 679, loss = 0.10314150899648666
In grad_steps = 680, loss = 0.05686056241393089
In grad_steps = 681, loss = 1.0080398321151733
In grad_steps = 682, loss = 0.5905378460884094
In grad_steps = 683, loss = 0.24815122783184052
In grad_steps = 684, loss = 0.16533437371253967
In grad_steps = 685, loss = 0.2402121126651764
In grad_steps = 686, loss = 0.30707189440727234
In grad_steps = 687, loss = 0.3236425518989563
In grad_steps = 688, loss = 0.7073334455490112
In grad_steps = 689, loss = 0.13616864383220673
In grad_steps = 690, loss = 0.21244558691978455
In grad_steps = 691, loss = 0.1962037980556488
In grad_steps = 692, loss = 0.211874857544899
In grad_steps = 693, loss = 0.32094696164131165
In grad_steps = 694, loss = 0.3294737935066223
In grad_steps = 695, loss = 0.3319135308265686
In grad_steps = 696, loss = 0.8652504086494446
In grad_steps = 697, loss = 0.3957493305206299
In grad_steps = 698, loss = 0.22798216342926025
In grad_steps = 699, loss = 0.600591778755188
In grad_steps = 700, loss = 0.736465573310852
In grad_steps = 701, loss = 0.08732913434505463
In grad_steps = 702, loss = 0.09830643981695175
In grad_steps = 703, loss = 0.21080036461353302
In grad_steps = 704, loss = 0.2919161319732666
In grad_steps = 705, loss = 0.08872132003307343
In grad_steps = 706, loss = 0.5735937356948853
In grad_steps = 707, loss = 0.08109404891729355
In grad_steps = 708, loss = 0.03169700503349304
In grad_steps = 709, loss = 0.3695293962955475
In grad_steps = 710, loss = 0.3644956946372986
In grad_steps = 711, loss = 1.2990788221359253
In grad_steps = 712, loss = 0.7336409091949463
In grad_steps = 713, loss = 1.626935362815857
In grad_steps = 714, loss = 0.40862131118774414
In grad_steps = 715, loss = 0.976704478263855
In grad_steps = 716, loss = 0.4712817668914795
In grad_steps = 717, loss = 0.2072303593158722
In grad_steps = 718, loss = 0.4488025903701782
In grad_steps = 719, loss = 0.10357022285461426
In grad_steps = 720, loss = 0.10737891495227814
In grad_steps = 721, loss = 1.23837149143219
In grad_steps = 722, loss = 0.4403679370880127
In grad_steps = 723, loss = 0.6069721579551697
In grad_steps = 724, loss = 0.16518589854240417
In grad_steps = 725, loss = 0.5948195457458496
In grad_steps = 726, loss = 0.24339260160923004
In grad_steps = 727, loss = 0.4785428047180176
In grad_steps = 728, loss = 0.1472240388393402
In grad_steps = 729, loss = 0.22668048739433289
In grad_steps = 730, loss = 0.24306711554527283
In grad_steps = 731, loss = 0.5882406234741211
In grad_steps = 732, loss = 0.3950093388557434
In grad_steps = 733, loss = 1.0353150367736816
In grad_steps = 734, loss = 0.3495076894760132
In grad_steps = 735, loss = 0.5097862482070923
In grad_steps = 736, loss = 0.2923319935798645
In grad_steps = 737, loss = 0.15156877040863037
In grad_steps = 738, loss = 0.7996351718902588
In grad_steps = 739, loss = 0.09476887434720993
In grad_steps = 740, loss = 0.19293612241744995
In grad_steps = 741, loss = 0.44659626483917236
In grad_steps = 742, loss = 0.1415674239397049
In grad_steps = 743, loss = 0.11325427889823914
In grad_steps = 744, loss = 0.8750535249710083
In grad_steps = 745, loss = 0.7068313360214233
In grad_steps = 746, loss = 0.2466093748807907
In grad_steps = 747, loss = 0.4100208878517151
In grad_steps = 748, loss = 0.08419682085514069
In grad_steps = 749, loss = 0.8701260685920715
In grad_steps = 750, loss = 0.9212651252746582
In grad_steps = 751, loss = 0.13845111429691315
In grad_steps = 752, loss = 0.07765111327171326
In grad_steps = 753, loss = 0.18593083322048187
In grad_steps = 754, loss = 0.501413106918335
In grad_steps = 755, loss = 0.7732466459274292
In grad_steps = 756, loss = 0.1869909018278122
In grad_steps = 757, loss = 0.16206957399845123
In grad_steps = 758, loss = 0.8332136869430542
In grad_steps = 759, loss = 0.6670757532119751
In grad_steps = 760, loss = 0.3121204972267151
In grad_steps = 761, loss = 0.2700154185295105
In grad_steps = 762, loss = 0.1325632780790329
In grad_steps = 763, loss = 0.5440935492515564
In grad_steps = 764, loss = 0.823341965675354
In grad_steps = 765, loss = 0.6416603922843933
In grad_steps = 766, loss = 0.10973714292049408
In grad_steps = 767, loss = 0.272565096616745
In grad_steps = 768, loss = 0.2610425055027008
In grad_steps = 769, loss = 1.004798173904419
In grad_steps = 770, loss = 0.20426103472709656
In grad_steps = 771, loss = 0.13001300394535065
In grad_steps = 772, loss = 0.39576977491378784
In grad_steps = 773, loss = 0.9109346866607666
In grad_steps = 774, loss = 0.43174368143081665
In grad_steps = 775, loss = 0.1263149529695511
In grad_steps = 776, loss = 0.27600914239883423
In grad_steps = 777, loss = 0.675391674041748
In grad_steps = 778, loss = 0.2693477272987366
In grad_steps = 779, loss = 0.299254834651947
In grad_steps = 780, loss = 0.5845987796783447
In grad_steps = 781, loss = 0.38099485635757446
In grad_steps = 782, loss = 0.16143518686294556
In grad_steps = 783, loss = 1.1021991968154907
In grad_steps = 784, loss = 0.507000207901001
In grad_steps = 785, loss = 0.10592930763959885
In grad_steps = 786, loss = 0.23469483852386475
In grad_steps = 787, loss = 0.4262050688266754
In grad_steps = 788, loss = 0.7502846717834473
In grad_steps = 789, loss = 0.2537269592285156
In grad_steps = 790, loss = 0.4505584239959717
In grad_steps = 791, loss = 0.169653058052063
In grad_steps = 792, loss = 0.1445503830909729
In grad_steps = 793, loss = 0.5983849167823792
In grad_steps = 794, loss = 1.4597644805908203
In grad_steps = 795, loss = 0.6676708459854126
In grad_steps = 796, loss = 0.22624541819095612
In grad_steps = 797, loss = 0.5261152982711792
In grad_steps = 798, loss = 0.18714408576488495
In grad_steps = 799, loss = 0.6275753974914551
In grad_steps = 800, loss = 0.5247330069541931
In grad_steps = 801, loss = 0.39151257276535034
In grad_steps = 802, loss = 0.5412169694900513
In grad_steps = 803, loss = 0.24031192064285278
In grad_steps = 804, loss = 0.13243447244167328
In grad_steps = 805, loss = 0.4799463450908661
In grad_steps = 806, loss = 0.2094944715499878
In grad_steps = 807, loss = 0.23555758595466614
In grad_steps = 808, loss = 0.34969067573547363
In grad_steps = 809, loss = 0.13680711388587952
In grad_steps = 810, loss = 0.5944095849990845
In grad_steps = 811, loss = 0.10002332925796509
In grad_steps = 812, loss = 0.3069802522659302
In grad_steps = 813, loss = 0.33193710446357727
In grad_steps = 814, loss = 0.13750217854976654
In grad_steps = 815, loss = 0.19087857007980347
In grad_steps = 816, loss = 0.8495276570320129
In grad_steps = 817, loss = 0.06603202223777771
In grad_steps = 818, loss = 0.5127502679824829
In grad_steps = 819, loss = 0.06467948853969574
In grad_steps = 820, loss = 0.16687949001789093
In grad_steps = 821, loss = 0.15752772986888885
In grad_steps = 822, loss = 0.4625812768936157
In grad_steps = 823, loss = 0.40927615761756897
In grad_steps = 824, loss = 0.03936486691236496
In grad_steps = 825, loss = 0.039715979248285294
In grad_steps = 826, loss = 0.7989222407341003
In grad_steps = 827, loss = 0.10527299344539642
In grad_steps = 828, loss = 0.9546487927436829
In grad_steps = 829, loss = 0.1399466097354889
In grad_steps = 830, loss = 0.05058913677930832
In grad_steps = 831, loss = 0.03381715714931488
In grad_steps = 832, loss = 0.8459534049034119
In grad_steps = 833, loss = 0.5157458186149597
In grad_steps = 834, loss = 0.6335382461547852
In grad_steps = 835, loss = 1.7270188331604004
In grad_steps = 836, loss = 0.523607611656189
In grad_steps = 837, loss = 0.9366757273674011
In grad_steps = 838, loss = 1.468668818473816
In grad_steps = 839, loss = 0.735382616519928
In grad_steps = 840, loss = 0.14003704488277435
In grad_steps = 841, loss = 0.8066551089286804
In grad_steps = 842, loss = 0.16158020496368408
In grad_steps = 843, loss = 0.7022697925567627
In grad_steps = 844, loss = 0.6686203479766846
In grad_steps = 845, loss = 0.3314279615879059
In grad_steps = 846, loss = 0.9696943163871765
In grad_steps = 847, loss = 0.7239173650741577
In grad_steps = 848, loss = 0.6488157510757446
In grad_steps = 849, loss = 0.9984258413314819
In grad_steps = 850, loss = 0.7051914930343628
In grad_steps = 851, loss = 0.5887221693992615
In grad_steps = 852, loss = 0.4485715925693512
In grad_steps = 853, loss = 0.371088445186615
In grad_steps = 854, loss = 0.36922475695610046
In grad_steps = 855, loss = 0.6482598185539246
In grad_steps = 856, loss = 0.4475069046020508
In grad_steps = 857, loss = 0.3325822651386261
In grad_steps = 858, loss = 0.62519371509552
In grad_steps = 859, loss = 0.4347083270549774
In grad_steps = 860, loss = 0.40197843313217163
In grad_steps = 861, loss = 0.36030706763267517
In grad_steps = 862, loss = 0.25681596994400024
In grad_steps = 863, loss = 0.34166914224624634
In grad_steps = 864, loss = 0.3555998206138611
In grad_steps = 865, loss = 0.4608438014984131
In grad_steps = 866, loss = 0.27698835730552673
In grad_steps = 867, loss = 0.17957359552383423
In grad_steps = 868, loss = 0.11598215997219086
In grad_steps = 869, loss = 0.39351972937583923
In grad_steps = 870, loss = 0.07863681018352509
In grad_steps = 871, loss = 0.14350742101669312
In grad_steps = 872, loss = 0.2538260817527771
In grad_steps = 873, loss = 0.6821132302284241
In grad_steps = 874, loss = 0.04014511778950691
In grad_steps = 875, loss = 0.03770377114415169
In grad_steps = 876, loss = 0.07388205826282501
In grad_steps = 877, loss = 0.19784510135650635
In grad_steps = 878, loss = 0.08737757802009583
In grad_steps = 879, loss = 0.05982498079538345
In grad_steps = 880, loss = 1.5856229066848755
In grad_steps = 881, loss = 1.0116569995880127
In grad_steps = 882, loss = 0.20712250471115112
In grad_steps = 883, loss = 0.7168896198272705
In grad_steps = 884, loss = 0.2924143671989441
In grad_steps = 885, loss = 0.10972996056079865
In grad_steps = 886, loss = 0.6436996459960938
In grad_steps = 887, loss = 0.1113944798707962
In grad_steps = 888, loss = 0.1945675164461136
In grad_steps = 889, loss = 0.739176869392395
In grad_steps = 890, loss = 0.28617948293685913
In grad_steps = 891, loss = 0.281625360250473
In grad_steps = 892, loss = 0.9932476878166199
In grad_steps = 893, loss = 0.4263654947280884
In grad_steps = 894, loss = 0.2766915261745453
In grad_steps = 895, loss = 0.2837800979614258
In grad_steps = 896, loss = 0.3880993127822876
In grad_steps = 897, loss = 0.5011323690414429
In grad_steps = 898, loss = 0.895026445388794
In grad_steps = 899, loss = 0.23967991769313812
In grad_steps = 900, loss = 0.2983549237251282
In grad_steps = 901, loss = 0.6275287866592407
In grad_steps = 902, loss = 0.28146085143089294
In grad_steps = 903, loss = 0.1401771754026413
In grad_steps = 904, loss = 0.32380467653274536
In grad_steps = 905, loss = 0.49000775814056396
In grad_steps = 906, loss = 0.22069130837917328
In grad_steps = 907, loss = 0.17270727455615997
In grad_steps = 908, loss = 0.1717151552438736
In grad_steps = 909, loss = 0.6722763180732727
In grad_steps = 910, loss = 0.0776720643043518
In grad_steps = 911, loss = 0.14736774563789368
In grad_steps = 912, loss = 0.11444266885519028
In grad_steps = 913, loss = 0.6675201654434204
In grad_steps = 914, loss = 0.36644870042800903
In grad_steps = 915, loss = 0.17718733847141266
In grad_steps = 916, loss = 0.09279932081699371
In grad_steps = 917, loss = 0.0761917233467102
In grad_steps = 918, loss = 1.180508017539978
In grad_steps = 919, loss = 1.7396972179412842
In grad_steps = 920, loss = 0.9720624089241028
In grad_steps = 921, loss = 0.5498180389404297
In grad_steps = 922, loss = 0.04086188226938248
In grad_steps = 923, loss = 1.1564443111419678
In grad_steps = 924, loss = 1.007338523864746
In grad_steps = 925, loss = 0.1690191626548767
In grad_steps = 926, loss = 0.193634033203125
In grad_steps = 927, loss = 0.3654235899448395
In grad_steps = 928, loss = 0.8356110453605652
In grad_steps = 929, loss = 0.1745998114347458
In grad_steps = 930, loss = 0.3516457676887512
In grad_steps = 931, loss = 0.355299174785614
In grad_steps = 932, loss = 0.4389913082122803
In grad_steps = 933, loss = 0.5163079500198364
In grad_steps = 934, loss = 0.17021961510181427
In grad_steps = 935, loss = 0.2518634796142578
In grad_steps = 936, loss = 0.22765307128429413
In grad_steps = 937, loss = 0.5533945560455322
In grad_steps = 938, loss = 0.3022959530353546
In grad_steps = 939, loss = 0.541386604309082
In grad_steps = 940, loss = 0.5984941124916077
In grad_steps = 941, loss = 0.30662453174591064
In grad_steps = 942, loss = 0.1889500916004181
In grad_steps = 943, loss = 0.8448339104652405
In grad_steps = 944, loss = 0.2136182188987732
In grad_steps = 945, loss = 0.16792993247509003
In grad_steps = 946, loss = 0.22790823876857758
In grad_steps = 947, loss = 0.8647741079330444
In grad_steps = 948, loss = 0.5419069528579712
In grad_steps = 949, loss = 0.7177623510360718
In grad_steps = 950, loss = 0.22790604829788208
In grad_steps = 951, loss = 0.8054460287094116
In grad_steps = 952, loss = 0.19206522405147552
In grad_steps = 953, loss = 0.6074862480163574
In grad_steps = 954, loss = 0.5060499310493469
In grad_steps = 955, loss = 0.43613845109939575
In grad_steps = 956, loss = 0.8995120525360107
In grad_steps = 957, loss = 0.5965033769607544
In grad_steps = 958, loss = 0.3545575439929962
In grad_steps = 959, loss = 0.3248427212238312
In grad_steps = 960, loss = 0.21561262011528015
In grad_steps = 961, loss = 0.38511380553245544
In grad_steps = 962, loss = 0.6296451091766357
In grad_steps = 963, loss = 1.007505178451538
In grad_steps = 964, loss = 0.3317088484764099
In grad_steps = 965, loss = 0.6073108911514282
In grad_steps = 966, loss = 0.4601949155330658
In grad_steps = 967, loss = 0.6244828104972839
In grad_steps = 968, loss = 0.2879721522331238
In grad_steps = 969, loss = 0.2925870716571808
In grad_steps = 970, loss = 0.26694217324256897
In grad_steps = 971, loss = 0.2452578842639923
In grad_steps = 972, loss = 0.2449944019317627
In grad_steps = 973, loss = 0.3498688042163849
In grad_steps = 974, loss = 0.11066154390573502
In grad_steps = 975, loss = 0.2663021981716156
In grad_steps = 976, loss = 0.21856217086315155
In grad_steps = 977, loss = 0.17958518862724304
In grad_steps = 978, loss = 0.13329298794269562
In grad_steps = 979, loss = 0.384337842464447
In grad_steps = 980, loss = 0.23761960864067078
In grad_steps = 981, loss = 0.5771092772483826
In grad_steps = 982, loss = 0.10447519272565842
In grad_steps = 983, loss = 0.222695454955101
In grad_steps = 984, loss = 0.028709715232253075
In grad_steps = 985, loss = 0.16996964812278748
In grad_steps = 986, loss = 0.7845645546913147
In grad_steps = 987, loss = 0.5963270664215088
In grad_steps = 988, loss = 0.024202583357691765
In grad_steps = 989, loss = 0.01800691895186901
In grad_steps = 990, loss = 0.34959253668785095
In grad_steps = 991, loss = 0.8151070475578308
In grad_steps = 992, loss = 1.287583589553833
In grad_steps = 993, loss = 0.555450975894928
In grad_steps = 994, loss = 0.4496944844722748
In grad_steps = 995, loss = 0.051244981586933136
In grad_steps = 996, loss = 0.22559793293476105
In grad_steps = 997, loss = 0.12812042236328125
In grad_steps = 998, loss = 0.0745038092136383
In grad_steps = 999, loss = 0.4445383846759796
In grad_steps = 1000, loss = 0.442023903131485
In grad_steps = 1001, loss = 0.26766228675842285
In grad_steps = 1002, loss = 0.15116263926029205
In grad_steps = 1003, loss = 0.6152232885360718
In grad_steps = 1004, loss = 0.39043667912483215
In grad_steps = 1005, loss = 0.22630096971988678
In grad_steps = 1006, loss = 0.24735429883003235
In grad_steps = 1007, loss = 0.20267744362354279
In grad_steps = 1008, loss = 0.11916292458772659
In grad_steps = 1009, loss = 0.0717795267701149
In grad_steps = 1010, loss = 0.0639089047908783
In grad_steps = 1011, loss = 0.37085604667663574
In grad_steps = 1012, loss = 0.9715327620506287
In grad_steps = 1013, loss = 0.06520143896341324
In grad_steps = 1014, loss = 0.6039295196533203
In grad_steps = 1015, loss = 0.4581179618835449
In grad_steps = 1016, loss = 0.11855584383010864
In grad_steps = 1017, loss = 1.2085005044937134
In grad_steps = 1018, loss = 0.2585902810096741
In grad_steps = 1019, loss = 0.8067268133163452
In grad_steps = 1020, loss = 0.3970864713191986
In grad_steps = 1021, loss = 0.28357580304145813
In grad_steps = 1022, loss = 0.7689510583877563
In grad_steps = 1023, loss = 0.1769256889820099
In grad_steps = 1024, loss = 0.72255939245224
In grad_steps = 1025, loss = 0.6716897487640381
In grad_steps = 1026, loss = 0.8340522050857544
In grad_steps = 1027, loss = 0.22402766346931458
In grad_steps = 1028, loss = 0.26051390171051025
In grad_steps = 1029, loss = 0.6196213960647583
In grad_steps = 1030, loss = 0.4693887233734131
In grad_steps = 1031, loss = 0.43751928210258484
In grad_steps = 1032, loss = 0.108909472823143
In grad_steps = 1033, loss = 0.6760225296020508
In grad_steps = 1034, loss = 0.2441948652267456
In grad_steps = 1035, loss = 0.5706541538238525
In grad_steps = 1036, loss = 0.17198802530765533
In grad_steps = 1037, loss = 0.3504303991794586
In grad_steps = 1038, loss = 0.3354471027851105
In grad_steps = 1039, loss = 0.14646072685718536
In grad_steps = 1040, loss = 0.5823730230331421
In grad_steps = 1041, loss = 0.1227990984916687
In grad_steps = 1042, loss = 0.18133682012557983
In grad_steps = 1043, loss = 0.1666848361492157
In grad_steps = 1044, loss = 0.8035303354263306
In grad_steps = 1045, loss = 0.05106351524591446
In grad_steps = 1046, loss = 0.3892323076725006
In grad_steps = 1047, loss = 0.04677040874958038
In grad_steps = 1048, loss = 0.250661700963974
In grad_steps = 1049, loss = 0.17332150042057037
In grad_steps = 1050, loss = 0.764014482498169
In grad_steps = 1051, loss = 0.3040574789047241
In grad_steps = 1052, loss = 0.8390446305274963
In grad_steps = 1053, loss = 0.053450167179107666
In grad_steps = 1054, loss = 0.5137876868247986
In grad_steps = 1055, loss = 0.7075373530387878
In grad_steps = 1056, loss = 0.3002324402332306
In grad_steps = 1057, loss = 0.18465828895568848
In grad_steps = 1058, loss = 0.2959301471710205
In grad_steps = 1059, loss = 0.5350391268730164
In grad_steps = 1060, loss = 0.0842248871922493
In grad_steps = 1061, loss = 0.10255638509988785
In grad_steps = 1062, loss = 0.052244335412979126
In grad_steps = 1063, loss = 0.6713245511054993
In grad_steps = 1064, loss = 0.16702768206596375
In grad_steps = 1065, loss = 0.27347907423973083
In grad_steps = 1066, loss = 0.8509347438812256
In grad_steps = 1067, loss = 0.4626021385192871
In grad_steps = 1068, loss = 0.030833568423986435
In grad_steps = 1069, loss = 0.9250972867012024
In grad_steps = 1070, loss = 0.43937453627586365
In grad_steps = 1071, loss = 0.10714735090732574
In grad_steps = 1072, loss = 0.14173178374767303
In grad_steps = 1073, loss = 1.0868617296218872
In grad_steps = 1074, loss = 0.24575704336166382
In grad_steps = 1075, loss = 0.05858498066663742
In grad_steps = 1076, loss = 0.3437618613243103
In grad_steps = 1077, loss = 0.8668237924575806
In grad_steps = 1078, loss = 0.33326947689056396
In grad_steps = 1079, loss = 0.7943418025970459
In grad_steps = 1080, loss = 0.1257128268480301
In grad_steps = 1081, loss = 0.19486373662948608
In grad_steps = 1082, loss = 0.6050060987472534
In grad_steps = 1083, loss = 0.22444497048854828
In grad_steps = 1084, loss = 0.2175460308790207
In grad_steps = 1085, loss = 0.3689410984516144
In grad_steps = 1086, loss = 0.2070208042860031
In grad_steps = 1087, loss = 0.3495914936065674
In grad_steps = 1088, loss = 0.518804669380188
In grad_steps = 1089, loss = 0.19227468967437744
In grad_steps = 1090, loss = 0.372201532125473
In grad_steps = 1091, loss = 0.5172713994979858
In grad_steps = 1092, loss = 0.40575337409973145
In grad_steps = 1093, loss = 0.30195677280426025
In grad_steps = 1094, loss = 0.2642992436885834
In grad_steps = 1095, loss = 0.25322189927101135
In grad_steps = 1096, loss = 0.17170128226280212
In grad_steps = 1097, loss = 0.10022760927677155
In grad_steps = 1098, loss = 0.21645620465278625
In grad_steps = 1099, loss = 0.7988511919975281
In grad_steps = 1100, loss = 0.37544724345207214
In grad_steps = 1101, loss = 0.11448835581541061
In grad_steps = 1102, loss = 0.4154207706451416
In grad_steps = 1103, loss = 0.5755131244659424
In grad_steps = 1104, loss = 0.10023541003465652
In grad_steps = 1105, loss = 0.05146126449108124
In grad_steps = 1106, loss = 0.3987120985984802
In grad_steps = 1107, loss = 0.49251604080200195
In grad_steps = 1108, loss = 0.5069584846496582
In grad_steps = 1109, loss = 0.07522653043270111
In grad_steps = 1110, loss = 0.4113340973854065
In grad_steps = 1111, loss = 0.18333512544631958
In grad_steps = 1112, loss = 1.0740405321121216
In grad_steps = 1113, loss = 0.8543472290039062
In grad_steps = 1114, loss = 1.0633444786071777
In grad_steps = 1115, loss = 0.11230960488319397
In grad_steps = 1116, loss = 0.1636890172958374
In grad_steps = 1117, loss = 0.3863091766834259
In grad_steps = 1118, loss = 0.20597244799137115
In grad_steps = 1119, loss = 0.23165984451770782
In grad_steps = 1120, loss = 0.0848255529999733
In grad_steps = 1121, loss = 0.09176434576511383
In grad_steps = 1122, loss = 0.22381316125392914
In grad_steps = 1123, loss = 0.42161497473716736
In grad_steps = 1124, loss = 0.64560866355896
In grad_steps = 1125, loss = 0.1642540842294693
In grad_steps = 1126, loss = 0.16776025295257568
In grad_steps = 1127, loss = 0.24097192287445068
In grad_steps = 1128, loss = 0.06395208835601807
In grad_steps = 1129, loss = 0.4498094916343689
In grad_steps = 1130, loss = 0.7865695357322693
In grad_steps = 1131, loss = 0.6184114813804626
In grad_steps = 1132, loss = 0.24569284915924072
In grad_steps = 1133, loss = 0.06561879068613052
In grad_steps = 1134, loss = 0.5970576405525208
In grad_steps = 1135, loss = 0.8814111948013306
In grad_steps = 1136, loss = 0.10309003293514252
In grad_steps = 1137, loss = 0.38850221037864685
In grad_steps = 1138, loss = 0.4258732795715332
In grad_steps = 1139, loss = 0.15739522874355316
In grad_steps = 1140, loss = 0.08983577787876129
In grad_steps = 1141, loss = 0.3339110016822815
In grad_steps = 1142, loss = 0.5207138061523438
In grad_steps = 1143, loss = 0.4242362082004547
In grad_steps = 1144, loss = 0.2800130248069763
In grad_steps = 1145, loss = 0.19171030819416046
In grad_steps = 1146, loss = 0.6005484461784363
In grad_steps = 1147, loss = 0.25911131501197815
In grad_steps = 1148, loss = 0.5462831258773804
In grad_steps = 1149, loss = 0.17413952946662903
In grad_steps = 1150, loss = 0.1038823202252388
In grad_steps = 1151, loss = 0.7625709772109985
In grad_steps = 1152, loss = 0.09728358685970306
In grad_steps = 1153, loss = 0.39880478382110596
In grad_steps = 1154, loss = 0.06938225775957108
In grad_steps = 1155, loss = 0.46245554089546204
In grad_steps = 1156, loss = 0.6138780117034912
In grad_steps = 1157, loss = 0.04874716326594353
In grad_steps = 1158, loss = 0.1287437081336975
In grad_steps = 1159, loss = 0.06256930530071259
In grad_steps = 1160, loss = 0.19941511750221252
In grad_steps = 1161, loss = 0.43776512145996094
In grad_steps = 1162, loss = 0.07965941727161407
In grad_steps = 1163, loss = 0.1174522191286087
In grad_steps = 1164, loss = 0.08741720020771027
In grad_steps = 1165, loss = 0.6170250177383423
In grad_steps = 1166, loss = 0.5629587173461914
In grad_steps = 1167, loss = 0.14392703771591187
In grad_steps = 1168, loss = 0.559611976146698
In grad_steps = 1169, loss = 0.8683922290802002
In grad_steps = 1170, loss = 0.21148841083049774
In grad_steps = 1171, loss = 0.38849082589149475
In grad_steps = 1172, loss = 0.05250683054327965
In grad_steps = 1173, loss = 0.2051835060119629
In grad_steps = 1174, loss = 1.1030380725860596
In grad_steps = 1175, loss = 1.087077260017395
In grad_steps = 1176, loss = 0.8102046251296997
In grad_steps = 1177, loss = 0.23586660623550415
In grad_steps = 1178, loss = 0.07135099172592163
In grad_steps = 1179, loss = 0.2833019495010376
In grad_steps = 1180, loss = 0.3391339182853699
In grad_steps = 1181, loss = 0.7232739925384521
In grad_steps = 1182, loss = 0.4957749843597412
In grad_steps = 1183, loss = 0.3807108998298645
In grad_steps = 1184, loss = 0.17080901563167572
In grad_steps = 1185, loss = 0.7163580656051636
In grad_steps = 1186, loss = 0.5468621850013733
In grad_steps = 1187, loss = 0.7389854788780212
In grad_steps = 1188, loss = 0.101294606924057
In grad_steps = 1189, loss = 0.8483474254608154
In grad_steps = 1190, loss = 0.38291481137275696
In grad_steps = 1191, loss = 0.28139621019363403
In grad_steps = 1192, loss = 0.2396492213010788
In grad_steps = 1193, loss = 0.2444382905960083
In grad_steps = 1194, loss = 0.17446060478687286
In grad_steps = 1195, loss = 0.6960428357124329
In grad_steps = 1196, loss = 0.7440000772476196
In grad_steps = 1197, loss = 0.2716650366783142
In grad_steps = 1198, loss = 0.18921694159507751
In grad_steps = 1199, loss = 0.5902202129364014
In grad_steps = 1200, loss = 0.524196207523346
In grad_steps = 1201, loss = 0.40796029567718506
In grad_steps = 1202, loss = 0.19906345009803772
In grad_steps = 1203, loss = 0.37604978680610657
In grad_steps = 1204, loss = 0.11586324870586395
In grad_steps = 1205, loss = 0.16074854135513306
In grad_steps = 1206, loss = 0.6634045243263245
In grad_steps = 1207, loss = 0.19253778457641602
In grad_steps = 1208, loss = 0.10726477950811386
In grad_steps = 1209, loss = 0.5629664659500122
In grad_steps = 1210, loss = 0.06289587914943695
In grad_steps = 1211, loss = 0.2888936698436737
In grad_steps = 1212, loss = 0.0371391698718071
In grad_steps = 1213, loss = 0.4370548129081726
In grad_steps = 1214, loss = 0.46474382281303406
In grad_steps = 1215, loss = 0.2211020439863205
In grad_steps = 1216, loss = 0.05624260753393173
In grad_steps = 1217, loss = 0.05546676367521286
In grad_steps = 1218, loss = 0.040820956230163574
In grad_steps = 1219, loss = 0.033642105758190155
In grad_steps = 1220, loss = 0.9583337903022766
In grad_steps = 1221, loss = 0.5972776412963867
In grad_steps = 1222, loss = 0.11848179996013641
In grad_steps = 1223, loss = 0.900506854057312
In grad_steps = 1224, loss = 0.020717207342386246
In grad_steps = 1225, loss = 0.4266393780708313
In grad_steps = 1226, loss = 0.18051306903362274
In grad_steps = 1227, loss = 0.03272700682282448
In grad_steps = 1228, loss = 0.3765358328819275
In grad_steps = 1229, loss = 0.13671165704727173
In grad_steps = 1230, loss = 0.057422563433647156
In grad_steps = 1231, loss = 0.6267999410629272
In grad_steps = 1232, loss = 0.621176540851593
In grad_steps = 1233, loss = 0.03508785367012024
In grad_steps = 1234, loss = 0.3476867377758026
In grad_steps = 1235, loss = 0.6575619578361511
In grad_steps = 1236, loss = 0.28515270352363586
In grad_steps = 1237, loss = 0.5377532839775085
In grad_steps = 1238, loss = 0.48687979578971863
In grad_steps = 1239, loss = 0.3575400710105896
In grad_steps = 1240, loss = 0.7311735153198242
In grad_steps = 1241, loss = 0.5945172905921936
In grad_steps = 1242, loss = 0.17888328433036804
In grad_steps = 1243, loss = 0.9905804395675659
In grad_steps = 1244, loss = 0.43515247106552124
In grad_steps = 1245, loss = 0.20332548022270203
In grad_steps = 1246, loss = 0.4231918156147003
In grad_steps = 1247, loss = 0.35477596521377563
In grad_steps = 1248, loss = 0.737941563129425
In grad_steps = 1249, loss = 0.27572962641716003
In grad_steps = 1250, loss = 0.2044455111026764
In grad_steps = 1251, loss = 0.24941204488277435
In grad_steps = 1252, loss = 0.33021241426467896
In grad_steps = 1253, loss = 0.6377456188201904
In grad_steps = 1254, loss = 0.1285438984632492
In grad_steps = 1255, loss = 0.2108772099018097
In grad_steps = 1256, loss = 0.6376746296882629
In grad_steps = 1257, loss = 0.036194346845149994
In grad_steps = 1258, loss = 1.0544663667678833
In grad_steps = 1259, loss = 0.24358463287353516
In grad_steps = 1260, loss = 0.575161337852478
In grad_steps = 1261, loss = 0.2476804554462433
In grad_steps = 1262, loss = 0.05967511236667633
In grad_steps = 1263, loss = 0.1846884787082672
In grad_steps = 1264, loss = 0.7038501501083374
In grad_steps = 1265, loss = 0.2403770238161087
In grad_steps = 1266, loss = 0.693192183971405
In grad_steps = 1267, loss = 0.20597289502620697
In grad_steps = 1268, loss = 0.20592577755451202
In grad_steps = 1269, loss = 0.3088720440864563
In grad_steps = 1270, loss = 0.47602322697639465
In grad_steps = 1271, loss = 0.5711514949798584
In grad_steps = 1272, loss = 0.031941525638103485
In grad_steps = 1273, loss = 0.14363867044448853
In grad_steps = 1274, loss = 0.16042804718017578
In grad_steps = 1275, loss = 0.05281541496515274
In grad_steps = 1276, loss = 0.37782469391822815
In grad_steps = 1277, loss = 0.15564019978046417
In grad_steps = 1278, loss = 0.03940918296575546
In grad_steps = 1279, loss = 0.023444825783371925
In grad_steps = 1280, loss = 0.02807740680873394
In grad_steps = 1281, loss = 0.03171133995056152
In grad_steps = 1282, loss = 0.2931240200996399
In grad_steps = 1283, loss = 0.05248936265707016
In grad_steps = 1284, loss = 0.015926390886306763
In grad_steps = 1285, loss = 0.685142993927002
In grad_steps = 1286, loss = 0.4405122995376587
In grad_steps = 1287, loss = 0.6807618737220764
In grad_steps = 1288, loss = 0.19321373105049133
In grad_steps = 1289, loss = 0.1006968766450882
In grad_steps = 1290, loss = 0.6693764328956604
In grad_steps = 1291, loss = 0.060127172619104385
In grad_steps = 1292, loss = 0.3613514304161072
In grad_steps = 1293, loss = 0.0608731247484684
In grad_steps = 1294, loss = 0.6271873712539673
In grad_steps = 1295, loss = 0.11161482334136963
In grad_steps = 1296, loss = 0.4453202784061432
In grad_steps = 1297, loss = 0.3039173185825348
In grad_steps = 1298, loss = 0.13289470970630646
In grad_steps = 1299, loss = 0.3483564555644989
In grad_steps = 1300, loss = 0.6122480630874634
In grad_steps = 1301, loss = 0.01793493516743183
In grad_steps = 1302, loss = 0.814853310585022
In grad_steps = 1303, loss = 0.09267707914113998
In grad_steps = 1304, loss = 0.9020681977272034
In grad_steps = 1305, loss = 0.06329978257417679
In grad_steps = 1306, loss = 0.35112839937210083
In grad_steps = 1307, loss = 0.12310503423213959
In grad_steps = 1308, loss = 0.24675774574279785
In grad_steps = 1309, loss = 0.3185063600540161
In grad_steps = 1310, loss = 0.3122037649154663
In grad_steps = 1311, loss = 0.03664299100637436
In grad_steps = 1312, loss = 0.6762030720710754
In grad_steps = 1313, loss = 0.036306530237197876
In grad_steps = 1314, loss = 0.39860978722572327
In grad_steps = 1315, loss = 0.07649998366832733
In grad_steps = 1316, loss = 1.1627546548843384
In grad_steps = 1317, loss = 0.3461489677429199
In grad_steps = 1318, loss = 0.5211408138275146
In grad_steps = 1319, loss = 0.42530542612075806
In grad_steps = 1320, loss = 0.2761569023132324
In grad_steps = 1321, loss = 0.22650396823883057
In grad_steps = 1322, loss = 0.5861616134643555
In grad_steps = 1323, loss = 0.057758890092372894
In grad_steps = 1324, loss = 0.6895725131034851
In grad_steps = 1325, loss = 0.29840287566185
In grad_steps = 1326, loss = 0.3522683382034302
In grad_steps = 1327, loss = 0.28270551562309265
In grad_steps = 1328, loss = 0.9143429398536682
In grad_steps = 1329, loss = 0.1049913614988327
In grad_steps = 1330, loss = 0.32932600378990173
In grad_steps = 1331, loss = 0.1282666176557541
In grad_steps = 1332, loss = 0.5055472254753113
In grad_steps = 1333, loss = 0.06114620715379715
In grad_steps = 1334, loss = 0.1841013878583908
In grad_steps = 1335, loss = 0.1283515840768814
In grad_steps = 1336, loss = 0.10377977043390274
In grad_steps = 1337, loss = 0.28094854950904846
In grad_steps = 1338, loss = 0.8430342078208923
In grad_steps = 1339, loss = 0.858103334903717
In grad_steps = 1340, loss = 0.5765990614891052
In grad_steps = 1341, loss = 0.31057804822921753
In grad_steps = 1342, loss = 0.11361926794052124
In grad_steps = 1343, loss = 0.13840176165103912
In grad_steps = 1344, loss = 0.2485838234424591
In grad_steps = 1345, loss = 0.32015737891197205
In grad_steps = 1346, loss = 0.25586503744125366
In grad_steps = 1347, loss = 0.14813219010829926
In grad_steps = 1348, loss = 0.45219334959983826
In grad_steps = 1349, loss = 0.0315113291144371
In grad_steps = 1350, loss = 0.06125209480524063
In grad_steps = 1351, loss = 0.15440398454666138
In grad_steps = 1352, loss = 0.09515054523944855
In grad_steps = 1353, loss = 0.07143626362085342
In grad_steps = 1354, loss = 0.39731600880622864
In grad_steps = 1355, loss = 0.17183932662010193
In grad_steps = 1356, loss = 0.01524401269853115
In grad_steps = 1357, loss = 0.09861576557159424
In grad_steps = 1358, loss = 1.0258607864379883
In grad_steps = 1359, loss = 0.06583236157894135
In grad_steps = 1360, loss = 0.5358492136001587
In grad_steps = 1361, loss = 0.015517886728048325
In grad_steps = 1362, loss = 0.02129189297556877
In grad_steps = 1363, loss = 0.05834623798727989
In grad_steps = 1364, loss = 0.3139752745628357
In grad_steps = 1365, loss = 0.3746255934238434
In grad_steps = 1366, loss = 0.7172189950942993
In grad_steps = 1367, loss = 1.3965541124343872
In grad_steps = 1368, loss = 0.40564242005348206
In grad_steps = 1369, loss = 0.3503246307373047
In grad_steps = 1370, loss = 0.7308067083358765
In grad_steps = 1371, loss = 0.2263329029083252
In grad_steps = 1372, loss = 0.9961845278739929
In grad_steps = 1373, loss = 0.4396706521511078
In grad_steps = 1374, loss = 0.07701684534549713
In grad_steps = 1375, loss = 0.28168320655822754
In grad_steps = 1376, loss = 0.1202559620141983
In grad_steps = 1377, loss = 0.5969523191452026
In grad_steps = 1378, loss = 0.15909665822982788
In grad_steps = 1379, loss = 0.61005038022995
In grad_steps = 1380, loss = 0.33268308639526367
In grad_steps = 1381, loss = 0.3968600928783417
In grad_steps = 1382, loss = 0.9317190647125244
In grad_steps = 1383, loss = 0.2681618332862854
In grad_steps = 1384, loss = 0.04553861543536186
In grad_steps = 1385, loss = 0.41964951157569885
In grad_steps = 1386, loss = 0.1429091989994049
In grad_steps = 1387, loss = 0.2809164524078369
In grad_steps = 1388, loss = 0.4799578785896301
In grad_steps = 1389, loss = 0.06427964568138123
In grad_steps = 1390, loss = 0.1166524589061737
In grad_steps = 1391, loss = 0.1107557862997055
In grad_steps = 1392, loss = 0.07171805202960968
In grad_steps = 1393, loss = 0.6224862933158875
In grad_steps = 1394, loss = 0.27057501673698425
In grad_steps = 1395, loss = 0.1900213211774826
In grad_steps = 1396, loss = 0.1034492701292038
In grad_steps = 1397, loss = 1.0261530876159668
In grad_steps = 1398, loss = 0.21309342980384827
In grad_steps = 1399, loss = 0.6933848261833191
In grad_steps = 1400, loss = 0.3423302173614502
In grad_steps = 1401, loss = 0.48721441626548767
In grad_steps = 1402, loss = 0.05875019729137421
In grad_steps = 1403, loss = 0.28790947794914246
In grad_steps = 1404, loss = 0.12253028899431229
In grad_steps = 1405, loss = 0.05119888857007027
In grad_steps = 1406, loss = 0.3585882782936096
In grad_steps = 1407, loss = 0.26299431920051575
In grad_steps = 1408, loss = 0.5463364124298096
In grad_steps = 1409, loss = 0.4339181184768677
In grad_steps = 1410, loss = 0.4470474123954773
In grad_steps = 1411, loss = 0.05048691853880882
In grad_steps = 1412, loss = 0.2649705111980438
In grad_steps = 1413, loss = 0.2661152184009552
In grad_steps = 1414, loss = 0.08711269497871399
In grad_steps = 1415, loss = 0.09437749534845352
In grad_steps = 1416, loss = 0.12423132359981537
In grad_steps = 1417, loss = 0.6251824498176575
In grad_steps = 1418, loss = 0.04125126451253891
In grad_steps = 1419, loss = 0.14500746130943298
In grad_steps = 1420, loss = 0.09782475233078003
In grad_steps = 1421, loss = 0.6795607209205627
In grad_steps = 1422, loss = 0.2509913444519043
In grad_steps = 1423, loss = 0.02721305750310421
In grad_steps = 1424, loss = 0.6055583357810974
In grad_steps = 1425, loss = 0.07901302725076675
In grad_steps = 1426, loss = 0.05328407138586044
In grad_steps = 1427, loss = 0.026041531935334206
In grad_steps = 1428, loss = 0.008982992731034756
In grad_steps = 1429, loss = 1.041856288909912
In grad_steps = 1430, loss = 1.4326304197311401
In grad_steps = 1431, loss = 0.07629536092281342
In grad_steps = 1432, loss = 0.3267296850681305
In grad_steps = 1433, loss = 0.6880876421928406
In grad_steps = 1434, loss = 0.41586199402809143
In grad_steps = 1435, loss = 0.06960000842809677
In grad_steps = 1436, loss = 1.1023858785629272
In grad_steps = 1437, loss = 0.7206020951271057
In grad_steps = 1438, loss = 0.07972254604101181
In grad_steps = 1439, loss = 0.13052815198898315
In grad_steps = 1440, loss = 0.5731040239334106
In grad_steps = 1441, loss = 0.17079637944698334
In grad_steps = 1442, loss = 0.0631662905216217
In grad_steps = 1443, loss = 0.2776493430137634
In grad_steps = 1444, loss = 0.6251646280288696
In grad_steps = 1445, loss = 0.46719101071357727
In grad_steps = 1446, loss = 0.19455817341804504
In grad_steps = 1447, loss = 0.2624989449977875
In grad_steps = 1448, loss = 0.3086606562137604
In grad_steps = 1449, loss = 0.3462175130844116
In grad_steps = 1450, loss = 0.17867712676525116
In grad_steps = 1451, loss = 0.42660290002822876
In grad_steps = 1452, loss = 0.3769665062427521
In grad_steps = 1453, loss = 0.09439785778522491
In grad_steps = 1454, loss = 0.38860827684402466
In grad_steps = 1455, loss = 0.3954755365848541
In grad_steps = 1456, loss = 0.5351475477218628
In grad_steps = 1457, loss = 0.633995532989502
In grad_steps = 1458, loss = 0.21484258770942688
In grad_steps = 1459, loss = 0.3188689351081848
In grad_steps = 1460, loss = 0.04125812277197838
In grad_steps = 1461, loss = 0.7262711524963379
In grad_steps = 1462, loss = 0.24225544929504395
In grad_steps = 1463, loss = 0.6504275798797607
In grad_steps = 1464, loss = 0.20242920517921448
In grad_steps = 1465, loss = 1.1071723699569702
In grad_steps = 1466, loss = 0.2518245279788971
In grad_steps = 1467, loss = 0.05391091853380203
In grad_steps = 1468, loss = 0.10282456874847412
In grad_steps = 1469, loss = 0.5119379162788391
In grad_steps = 1470, loss = 0.048229362815618515
In grad_steps = 1471, loss = 0.3128701448440552
In grad_steps = 1472, loss = 0.9263834357261658
In grad_steps = 1473, loss = 0.11129964143037796
In grad_steps = 1474, loss = 0.4241470992565155
In grad_steps = 1475, loss = 1.1488531827926636
In grad_steps = 1476, loss = 0.21568714082241058
In grad_steps = 1477, loss = 0.1576041579246521
In grad_steps = 1478, loss = 0.7663158774375916
In grad_steps = 1479, loss = 0.29928791522979736
In grad_steps = 1480, loss = 0.4522896111011505
In grad_steps = 1481, loss = 0.21441234648227692
In grad_steps = 1482, loss = 0.07108767330646515
In grad_steps = 1483, loss = 0.28953683376312256
In grad_steps = 1484, loss = 0.10226860642433167
In grad_steps = 1485, loss = 0.09139293432235718
In grad_steps = 1486, loss = 0.555716872215271
In grad_steps = 1487, loss = 0.35765618085861206
In grad_steps = 1488, loss = 0.11051937937736511
In grad_steps = 1489, loss = 0.361444354057312
In grad_steps = 1490, loss = 0.1883155256509781
In grad_steps = 1491, loss = 0.2591332197189331
In grad_steps = 1492, loss = 0.1521187573671341
In grad_steps = 1493, loss = 0.38624903559684753
In grad_steps = 1494, loss = 0.6157952547073364
In grad_steps = 1495, loss = 0.1735895574092865
In grad_steps = 1496, loss = 0.5458608865737915
In grad_steps = 1497, loss = 0.5041908621788025
In grad_steps = 1498, loss = 0.30792051553726196
In grad_steps = 1499, loss = 0.10543785989284515
In grad_steps = 1500, loss = 0.42334964871406555
In grad_steps = 1501, loss = 0.662080705165863
In grad_steps = 1502, loss = 0.04273298382759094
In grad_steps = 1503, loss = 0.5909461379051208
In grad_steps = 1504, loss = 0.22520068287849426
In grad_steps = 1505, loss = 0.20422734320163727
In grad_steps = 1506, loss = 0.09544211626052856
In grad_steps = 1507, loss = 0.11613304913043976
In grad_steps = 1508, loss = 0.5244619250297546
In grad_steps = 1509, loss = 0.7801436185836792
In grad_steps = 1510, loss = 0.5773907899856567
In grad_steps = 1511, loss = 0.2874991297721863
In grad_steps = 1512, loss = 0.044385988265275955
In grad_steps = 1513, loss = 1.0837986469268799
In grad_steps = 1514, loss = 0.4331604838371277
In grad_steps = 1515, loss = 0.11311158537864685
In grad_steps = 1516, loss = 0.07559042423963547
In grad_steps = 1517, loss = 0.3598969280719757
In grad_steps = 1518, loss = 0.35612380504608154
In grad_steps = 1519, loss = 0.7509872913360596
In grad_steps = 1520, loss = 0.09216468781232834
In grad_steps = 1521, loss = 0.4292360544204712
In grad_steps = 1522, loss = 0.2509094774723053
In grad_steps = 1523, loss = 0.1823561191558838
In grad_steps = 1524, loss = 0.06191273033618927
In grad_steps = 1525, loss = 0.6505545973777771
In grad_steps = 1526, loss = 0.07128195464611053
In grad_steps = 1527, loss = 0.06389249861240387
In grad_steps = 1528, loss = 0.1732257604598999
In grad_steps = 1529, loss = 0.12609322369098663
In grad_steps = 1530, loss = 0.08711123466491699
In grad_steps = 1531, loss = 0.06670834124088287
In grad_steps = 1532, loss = 0.531700074672699
In grad_steps = 1533, loss = 0.786860466003418
In grad_steps = 1534, loss = 0.21131184697151184
In grad_steps = 1535, loss = 0.03571005538105965
In grad_steps = 1536, loss = 0.16411061584949493
In grad_steps = 1537, loss = 0.2963307201862335
In grad_steps = 1538, loss = 0.4034382998943329
In grad_steps = 1539, loss = 0.03780336678028107
In grad_steps = 1540, loss = 0.056561216711997986
In grad_steps = 1541, loss = 0.186643585562706
In grad_steps = 1542, loss = 0.22864152491092682
In grad_steps = 1543, loss = 0.33466652035713196
In grad_steps = 1544, loss = 0.431365042924881
In grad_steps = 1545, loss = 0.014669293537735939
In grad_steps = 1546, loss = 0.978852391242981
In grad_steps = 1547, loss = 0.05075985938310623
In grad_steps = 1548, loss = 0.12332899123430252
In grad_steps = 1549, loss = 0.041461411863565445
In grad_steps = 1550, loss = 0.06717191636562347
In grad_steps = 1551, loss = 0.1859775185585022
In grad_steps = 1552, loss = 0.5925253033638
In grad_steps = 1553, loss = 0.09221202880144119
In grad_steps = 1554, loss = 0.8775045871734619
In grad_steps = 1555, loss = 0.026142077520489693
In grad_steps = 1556, loss = 0.02479596994817257
In grad_steps = 1557, loss = 0.9199055433273315
In grad_steps = 1558, loss = 0.26909783482551575
In grad_steps = 1559, loss = 0.5223777294158936
In grad_steps = 1560, loss = 0.8137344717979431
In grad_steps = 1561, loss = 0.3182887136936188
In grad_steps = 1562, loss = 0.05607645958662033
In grad_steps = 1563, loss = 0.26630643010139465
In grad_steps = 1564, loss = 0.09521599113941193
In grad_steps = 1565, loss = 0.22083623707294464
In grad_steps = 1566, loss = 0.7862831950187683
In grad_steps = 1567, loss = 0.17471691966056824
In grad_steps = 1568, loss = 0.469221830368042
In grad_steps = 1569, loss = 0.1897173672914505
In grad_steps = 1570, loss = 0.9710496664047241
In grad_steps = 1571, loss = 0.2485904097557068
In grad_steps = 1572, loss = 0.42359837889671326
In grad_steps = 1573, loss = 0.2295699417591095
In grad_steps = 1574, loss = 0.2319389134645462
In grad_steps = 1575, loss = 0.09257937967777252
In grad_steps = 1576, loss = 0.11541979014873505
In grad_steps = 1577, loss = 0.4444528818130493
In grad_steps = 1578, loss = 0.8290045857429504
In grad_steps = 1579, loss = 0.08066939562559128
In grad_steps = 1580, loss = 0.1148354560136795
In grad_steps = 1581, loss = 0.2484608143568039
In grad_steps = 1582, loss = 0.10013965517282486
In grad_steps = 1583, loss = 0.5779384970664978
In grad_steps = 1584, loss = 0.4994980990886688
In grad_steps = 1585, loss = 1.2683045864105225
In grad_steps = 1586, loss = 0.7325925230979919
In grad_steps = 1587, loss = 0.42130666971206665
In grad_steps = 1588, loss = 0.22548043727874756
In grad_steps = 1589, loss = 0.11790969967842102
In grad_steps = 1590, loss = 0.34311410784721375
In grad_steps = 1591, loss = 0.03972231596708298
In grad_steps = 1592, loss = 0.5993970036506653
In grad_steps = 1593, loss = 0.10235263407230377
In grad_steps = 1594, loss = 0.17773115634918213
In grad_steps = 1595, loss = 0.30756622552871704
In grad_steps = 1596, loss = 0.5466190576553345
In grad_steps = 1597, loss = 0.2539370059967041
In grad_steps = 1598, loss = 0.09557931870222092
In grad_steps = 1599, loss = 0.5594799518585205
In grad_steps = 1600, loss = 0.495948851108551
In grad_steps = 1601, loss = 0.2662312984466553
In grad_steps = 1602, loss = 0.4868592917919159
In grad_steps = 1603, loss = 0.08527146279811859
In grad_steps = 1604, loss = 0.20079295337200165
In grad_steps = 1605, loss = 0.10639041662216187
In grad_steps = 1606, loss = 0.6862385272979736
In grad_steps = 1607, loss = 0.16628751158714294
In grad_steps = 1608, loss = 0.15560440719127655
In grad_steps = 1609, loss = 0.37454158067703247
In grad_steps = 1610, loss = 0.21925026178359985
In grad_steps = 1611, loss = 0.09709413349628448
In grad_steps = 1612, loss = 0.9497275948524475
In grad_steps = 1613, loss = 0.2400098592042923
In grad_steps = 1614, loss = 0.11493401229381561
In grad_steps = 1615, loss = 0.13276657462120056
In grad_steps = 1616, loss = 1.2256441116333008
In grad_steps = 1617, loss = 0.02560317888855934
In grad_steps = 1618, loss = 0.552661120891571
In grad_steps = 1619, loss = 0.31716489791870117
In grad_steps = 1620, loss = 0.03453783690929413
In grad_steps = 1621, loss = 0.09191645681858063
In grad_steps = 1622, loss = 0.7777352929115295
In grad_steps = 1623, loss = 0.30740001797676086
In grad_steps = 1624, loss = 0.22703693807125092
In grad_steps = 1625, loss = 0.21348348259925842
In grad_steps = 1626, loss = 0.03883852809667587
In grad_steps = 1627, loss = 0.08569635450839996
In grad_steps = 1628, loss = 0.33259713649749756
In grad_steps = 1629, loss = 0.0664631724357605
In grad_steps = 1630, loss = 0.74898362159729
In grad_steps = 1631, loss = 0.17114929854869843
In grad_steps = 1632, loss = 0.5616339445114136
In grad_steps = 1633, loss = 0.043889421969652176
In grad_steps = 1634, loss = 0.08581769466400146
In grad_steps = 1635, loss = 0.2295503318309784
In grad_steps = 1636, loss = 0.5981920957565308
In grad_steps = 1637, loss = 0.02903839573264122
In grad_steps = 1638, loss = 0.10450876504182816
In grad_steps = 1639, loss = 1.4414641857147217
In grad_steps = 1640, loss = 0.3476189374923706
In grad_steps = 1641, loss = 0.10379062592983246
In grad_steps = 1642, loss = 0.8338690996170044
In grad_steps = 1643, loss = 0.13424378633499146
In grad_steps = 1644, loss = 0.7188718318939209
In grad_steps = 1645, loss = 0.1620466113090515
In grad_steps = 1646, loss = 0.29708781838417053
In grad_steps = 1647, loss = 0.39737990498542786
In grad_steps = 1648, loss = 0.08913572132587433
In grad_steps = 1649, loss = 0.0627773329615593
In grad_steps = 1650, loss = 0.7858136892318726
In grad_steps = 1651, loss = 0.31793656945228577
In grad_steps = 1652, loss = 0.4299502670764923
In grad_steps = 1653, loss = 0.42677998542785645
In grad_steps = 1654, loss = 0.3378974497318268
In grad_steps = 1655, loss = 0.4439859688282013
In grad_steps = 1656, loss = 0.535039484500885
In grad_steps = 1657, loss = 0.10244166851043701
In grad_steps = 1658, loss = 0.09672054648399353
In grad_steps = 1659, loss = 0.9268636703491211
In grad_steps = 1660, loss = 0.5502011179924011
In grad_steps = 1661, loss = 0.3979979157447815
In grad_steps = 1662, loss = 0.9090005159378052
In grad_steps = 1663, loss = 0.21623282134532928
In grad_steps = 1664, loss = 0.1689653992652893
In grad_steps = 1665, loss = 0.21103011071681976
In grad_steps = 1666, loss = 0.05144423246383667
In grad_steps = 1667, loss = 0.09364822506904602
In grad_steps = 1668, loss = 0.8446340560913086
In grad_steps = 1669, loss = 0.3857784867286682
In grad_steps = 1670, loss = 0.11291281878948212
In grad_steps = 1671, loss = 0.2341657280921936
In grad_steps = 1672, loss = 0.6546072363853455
In grad_steps = 1673, loss = 0.21246778964996338
In grad_steps = 1674, loss = 0.11123126745223999
In grad_steps = 1675, loss = 0.6652674078941345
In grad_steps = 1676, loss = 0.16840894520282745
In grad_steps = 1677, loss = 0.2628755569458008
In grad_steps = 1678, loss = 0.21882425248622894
In grad_steps = 1679, loss = 0.6355997323989868
In grad_steps = 1680, loss = 0.2621261477470398
In grad_steps = 1681, loss = 0.20414575934410095
In grad_steps = 1682, loss = 0.04748693108558655
In grad_steps = 1683, loss = 0.20533093810081482
In grad_steps = 1684, loss = 0.4773436188697815
In grad_steps = 1685, loss = 0.13460904359817505
In grad_steps = 1686, loss = 0.48828721046447754
In grad_steps = 1687, loss = 0.2648177146911621
In grad_steps = 1688, loss = 0.18505996465682983
In grad_steps = 1689, loss = 0.19397446513175964
In grad_steps = 1690, loss = 0.09544924646615982
In grad_steps = 1691, loss = 0.22121241688728333
In grad_steps = 1692, loss = 0.1324537992477417
In grad_steps = 1693, loss = 0.3506869971752167
In grad_steps = 1694, loss = 0.09691257774829865
In grad_steps = 1695, loss = 0.050346363335847855
In grad_steps = 1696, loss = 0.7832191586494446
In grad_steps = 1697, loss = 0.37059906125068665
In grad_steps = 1698, loss = 0.025506265461444855
In grad_steps = 1699, loss = 0.08754851669073105
In grad_steps = 1700, loss = 0.05236116051673889
In grad_steps = 1701, loss = 1.0728082656860352
In grad_steps = 1702, loss = 0.016614550724625587
In grad_steps = 1703, loss = 0.24587392807006836
In grad_steps = 1704, loss = 0.059344902634620667
In grad_steps = 1705, loss = 0.5623970031738281
In grad_steps = 1706, loss = 0.3054560720920563
In grad_steps = 1707, loss = 0.332547664642334
In grad_steps = 1708, loss = 0.6256530284881592
In grad_steps = 1709, loss = 0.059926874935626984
In grad_steps = 1710, loss = 0.08917127549648285
In grad_steps = 1711, loss = 0.04790836200118065
In grad_steps = 1712, loss = 0.35533055663108826
In grad_steps = 1713, loss = 0.8715443015098572
In grad_steps = 1714, loss = 0.03859160095453262
In grad_steps = 1715, loss = 0.5887689590454102
In grad_steps = 1716, loss = 0.7272027730941772
In grad_steps = 1717, loss = 0.8101183772087097
In grad_steps = 1718, loss = 0.051665984094142914
In grad_steps = 1719, loss = 0.5617392063140869
In grad_steps = 1720, loss = 0.2846813201904297
In grad_steps = 1721, loss = 0.06519278883934021
In grad_steps = 1722, loss = 0.5856932997703552
In grad_steps = 1723, loss = 0.14473555982112885
In grad_steps = 1724, loss = 1.0858768224716187
In grad_steps = 1725, loss = 0.23808571696281433
In grad_steps = 1726, loss = 0.33061355352401733
In grad_steps = 1727, loss = 0.4169817566871643
In grad_steps = 1728, loss = 0.15877503156661987
In grad_steps = 1729, loss = 0.5258336067199707
In grad_steps = 1730, loss = 0.14254990220069885
In grad_steps = 1731, loss = 0.3444598913192749
In grad_steps = 1732, loss = 0.10127829760313034
In grad_steps = 1733, loss = 0.05972719192504883
In grad_steps = 1734, loss = 0.16493457555770874
In grad_steps = 1735, loss = 0.25907212495803833
In grad_steps = 1736, loss = 0.26112860441207886
In grad_steps = 1737, loss = 0.32412219047546387
In grad_steps = 1738, loss = 0.10966704785823822
In grad_steps = 1739, loss = 0.04304977506399155
In grad_steps = 1740, loss = 0.031553965061903
In grad_steps = 1741, loss = 0.4990273416042328
In grad_steps = 1742, loss = 0.40638023614883423
In grad_steps = 1743, loss = 0.038243282586336136
In grad_steps = 1744, loss = 0.8321869969367981
In grad_steps = 1745, loss = 0.2824789583683014
In grad_steps = 1746, loss = 0.3409312069416046
In grad_steps = 1747, loss = 0.03198239952325821
In grad_steps = 1748, loss = 0.18909886479377747
In grad_steps = 1749, loss = 0.012387489899992943
In grad_steps = 1750, loss = 0.7169977426528931
In grad_steps = 1751, loss = 0.12986628711223602
In grad_steps = 1752, loss = 0.10037985444068909
In grad_steps = 1753, loss = 0.0929839164018631
In grad_steps = 1754, loss = 0.0703534483909607
In grad_steps = 1755, loss = 0.03136247396469116
In grad_steps = 1756, loss = 0.24206775426864624
In grad_steps = 1757, loss = 1.0440995693206787
In grad_steps = 1758, loss = 0.07662687450647354
In grad_steps = 1759, loss = 0.057331107556819916
In grad_steps = 1760, loss = 0.3125647008419037
In grad_steps = 1761, loss = 0.18595410883426666
In grad_steps = 1762, loss = 0.5246185064315796
In grad_steps = 1763, loss = 0.17461800575256348
In grad_steps = 1764, loss = 0.09910745918750763
In grad_steps = 1765, loss = 0.31099236011505127
In grad_steps = 1766, loss = 0.010635536164045334
In grad_steps = 1767, loss = 0.05333959683775902
In grad_steps = 1768, loss = 0.5288659334182739
In grad_steps = 1769, loss = 0.14047423005104065
In grad_steps = 1770, loss = 1.311461329460144
In grad_steps = 1771, loss = 0.7328788638114929
In grad_steps = 1772, loss = 0.04026332497596741
In grad_steps = 1773, loss = 0.029415976256132126
In grad_steps = 1774, loss = 0.03002883866429329
In grad_steps = 1775, loss = 0.5211743116378784
In grad_steps = 1776, loss = 0.9102756381034851
In grad_steps = 1777, loss = 0.17936097085475922
In grad_steps = 1778, loss = 0.33291101455688477
In grad_steps = 1779, loss = 0.601879894733429
In grad_steps = 1780, loss = 0.028005152940750122
In grad_steps = 1781, loss = 0.09350588172674179
In grad_steps = 1782, loss = 0.31004971265792847
In grad_steps = 1783, loss = 0.26164647936820984
In grad_steps = 1784, loss = 0.9895828366279602
In grad_steps = 1785, loss = 0.04604308679699898
In grad_steps = 1786, loss = 0.1944989413022995
In grad_steps = 1787, loss = 0.19256235659122467
In grad_steps = 1788, loss = 0.22454652190208435
In grad_steps = 1789, loss = 0.2995145916938782
In grad_steps = 1790, loss = 0.6608576774597168
In grad_steps = 1791, loss = 0.3696134090423584
In grad_steps = 1792, loss = 0.062329649925231934
In grad_steps = 1793, loss = 0.08880339562892914
In grad_steps = 1794, loss = 0.1394696980714798
In grad_steps = 1795, loss = 0.10285106301307678
In grad_steps = 1796, loss = 0.06347820907831192
In grad_steps = 1797, loss = 0.3468568027019501
In grad_steps = 1798, loss = 0.09851229190826416
In grad_steps = 1799, loss = 0.6812713742256165
In grad_steps = 1800, loss = 0.6498372554779053
In grad_steps = 1801, loss = 0.34072715044021606
In grad_steps = 1802, loss = 0.7264716625213623
In grad_steps = 1803, loss = 0.05962391570210457
In grad_steps = 1804, loss = 0.6161984205245972
In grad_steps = 1805, loss = 0.34854400157928467
In grad_steps = 1806, loss = 0.12079163640737534
In grad_steps = 1807, loss = 0.14789994060993195
In grad_steps = 1808, loss = 0.5090148448944092
In grad_steps = 1809, loss = 0.31859296560287476
In grad_steps = 1810, loss = 0.03540049493312836
In grad_steps = 1811, loss = 0.03574327006936073
In grad_steps = 1812, loss = 0.28515517711639404
In grad_steps = 1813, loss = 0.2394508421421051
In grad_steps = 1814, loss = 0.10475674271583557
In grad_steps = 1815, loss = 0.5056529641151428
In grad_steps = 1816, loss = 0.1640956848859787
In grad_steps = 1817, loss = 0.32055234909057617
In grad_steps = 1818, loss = 0.289530485868454
In grad_steps = 1819, loss = 0.36998263001441956
In grad_steps = 1820, loss = 1.4767630100250244
In grad_steps = 1821, loss = 0.22387641668319702
In grad_steps = 1822, loss = 0.04882202297449112
In grad_steps = 1823, loss = 1.4269930124282837
In grad_steps = 1824, loss = 0.0676010400056839
In grad_steps = 1825, loss = 0.06833203136920929
In grad_steps = 1826, loss = 0.8596727848052979
In grad_steps = 1827, loss = 1.222823143005371
In grad_steps = 1828, loss = 0.9072080850601196
In grad_steps = 1829, loss = 0.660465657711029
In grad_steps = 1830, loss = 0.42850446701049805
In grad_steps = 1831, loss = 0.18985116481781006
In grad_steps = 1832, loss = 0.7026888132095337
In grad_steps = 1833, loss = 1.1970850229263306
In grad_steps = 1834, loss = 0.11363225430250168
In grad_steps = 1835, loss = 0.2861289381980896
In grad_steps = 1836, loss = 0.3892103433609009
In grad_steps = 1837, loss = 0.6678698062896729
In grad_steps = 1838, loss = 0.5436177253723145
In grad_steps = 1839, loss = 0.3239896297454834
In grad_steps = 1840, loss = 0.13428284227848053
In grad_steps = 1841, loss = 0.4320835769176483
In grad_steps = 1842, loss = 0.4166925847530365
In grad_steps = 1843, loss = 0.316611647605896
In grad_steps = 1844, loss = 0.9016923904418945
In grad_steps = 1845, loss = 0.33129099011421204
In grad_steps = 1846, loss = 0.4164024591445923
In grad_steps = 1847, loss = 0.5987618565559387
In grad_steps = 1848, loss = 0.11551330983638763
In grad_steps = 1849, loss = 0.1271675080060959
In grad_steps = 1850, loss = 0.151116281747818
In grad_steps = 1851, loss = 0.30103397369384766
In grad_steps = 1852, loss = 0.20642931759357452
In grad_steps = 1853, loss = 0.6450991630554199
In grad_steps = 1854, loss = 0.2209266573190689
In grad_steps = 1855, loss = 0.09824631363153458
In grad_steps = 1856, loss = 0.047993555665016174
In grad_steps = 1857, loss = 0.159096360206604
In grad_steps = 1858, loss = 0.0903613343834877
In grad_steps = 1859, loss = 0.8547043204307556
In grad_steps = 1860, loss = 0.627888023853302
In grad_steps = 1861, loss = 0.028661414980888367
In grad_steps = 1862, loss = 0.5484315156936646
In grad_steps = 1863, loss = 0.25879257917404175
In grad_steps = 1864, loss = 0.10991674661636353
In grad_steps = 1865, loss = 0.11077487468719482
In grad_steps = 1866, loss = 0.17254823446273804
In grad_steps = 1867, loss = 1.0473508834838867
In grad_steps = 1868, loss = 0.048804640769958496
In grad_steps = 1869, loss = 0.10157957673072815
In grad_steps = 1870, loss = 0.06540828943252563
In grad_steps = 1871, loss = 0.2474723756313324
In grad_steps = 1872, loss = 0.19884845614433289
In grad_steps = 1873, loss = 0.03439111262559891
In grad_steps = 1874, loss = 0.20609858632087708
In grad_steps = 1875, loss = 0.30461347103118896
In grad_steps = 1876, loss = 0.0457226037979126
In grad_steps = 1877, loss = 0.5260880589485168
In grad_steps = 1878, loss = 0.1170947328209877
In grad_steps = 1879, loss = 0.642228364944458
In grad_steps = 1880, loss = 0.05734918639063835
In grad_steps = 1881, loss = 0.5477724671363831
In grad_steps = 1882, loss = 0.1595849245786667
In grad_steps = 1883, loss = 0.4887382388114929
In grad_steps = 1884, loss = 0.17679893970489502
In grad_steps = 1885, loss = 0.1469293236732483
In grad_steps = 1886, loss = 0.7601392865180969
In grad_steps = 1887, loss = 0.05982924997806549
In grad_steps = 1888, loss = 0.33508381247520447
In grad_steps = 1889, loss = 0.07903726398944855
In grad_steps = 1890, loss = 0.04921416938304901
In grad_steps = 1891, loss = 0.20505164563655853
In grad_steps = 1892, loss = 0.4220930337905884
In grad_steps = 1893, loss = 0.04830339550971985
In grad_steps = 1894, loss = 0.04941558837890625
In grad_steps = 1895, loss = 0.35954105854034424
In grad_steps = 1896, loss = 0.20653077960014343
In grad_steps = 1897, loss = 0.034237101674079895
In grad_steps = 1898, loss = 0.03055419772863388
In grad_steps = 1899, loss = 0.06786426901817322
In grad_steps = 1900, loss = 0.07827770709991455
In grad_steps = 1901, loss = 0.5948348641395569
In grad_steps = 1902, loss = 0.3153710961341858
In grad_steps = 1903, loss = 0.2618917226791382
In grad_steps = 1904, loss = 0.2841789126396179
In grad_steps = 1905, loss = 0.06756756454706192
In grad_steps = 1906, loss = 0.23551678657531738
In grad_steps = 1907, loss = 1.6113790273666382
In grad_steps = 1908, loss = 0.30131417512893677
In grad_steps = 1909, loss = 0.0947379618883133
In grad_steps = 1910, loss = 0.6533383131027222
In grad_steps = 1911, loss = 0.2793518900871277
In grad_steps = 1912, loss = 0.23600280284881592
In grad_steps = 1913, loss = 0.15514494478702545
In grad_steps = 1914, loss = 0.05767274647951126
In grad_steps = 1915, loss = 0.4314391016960144
In grad_steps = 1916, loss = 0.5466641187667847
In grad_steps = 1917, loss = 0.17164471745491028
In grad_steps = 1918, loss = 0.2797168791294098
In grad_steps = 1919, loss = 0.32662004232406616
In grad_steps = 1920, loss = 0.2659420967102051
In grad_steps = 1921, loss = 0.3585842251777649
In grad_steps = 1922, loss = 0.1349434107542038
In grad_steps = 1923, loss = 0.014597872272133827
In grad_steps = 1924, loss = 0.4457238018512726
In grad_steps = 1925, loss = 0.9484208822250366
In grad_steps = 1926, loss = 0.7183197140693665
In grad_steps = 1927, loss = 0.17085731029510498
In grad_steps = 1928, loss = 1.2339404821395874
In grad_steps = 1929, loss = 0.3431110382080078
In grad_steps = 1930, loss = 0.3877071738243103
In grad_steps = 1931, loss = 1.0765174627304077
In grad_steps = 1932, loss = 0.17138773202896118
In grad_steps = 1933, loss = 0.6833755970001221
In grad_steps = 1934, loss = 0.22638049721717834
In grad_steps = 1935, loss = 0.23914140462875366
In grad_steps = 1936, loss = 0.11847665160894394
In grad_steps = 1937, loss = 0.08462044596672058
In grad_steps = 1938, loss = 0.3027685284614563
In grad_steps = 1939, loss = 0.4112789034843445
In grad_steps = 1940, loss = 0.15093518793582916
In grad_steps = 1941, loss = 0.2617020905017853
In grad_steps = 1942, loss = 0.28099799156188965
In grad_steps = 1943, loss = 0.9708532691001892
In grad_steps = 1944, loss = 0.15772603452205658
In grad_steps = 1945, loss = 0.2040790617465973
In grad_steps = 1946, loss = 0.14836783707141876
In grad_steps = 1947, loss = 0.3238120675086975
In grad_steps = 1948, loss = 0.08811811357736588
In grad_steps = 1949, loss = 0.18214327096939087
In grad_steps = 1950, loss = 0.22934986650943756
In grad_steps = 1951, loss = 0.5367398262023926
In grad_steps = 1952, loss = 0.10861579328775406
In grad_steps = 1953, loss = 0.13947302103042603
In grad_steps = 1954, loss = 0.681848406791687
In grad_steps = 1955, loss = 0.014680860564112663
In grad_steps = 1956, loss = 0.1031949445605278
In grad_steps = 1957, loss = 0.0337403304874897
In grad_steps = 1958, loss = 0.07142458111047745
In grad_steps = 1959, loss = 0.05343639850616455
In grad_steps = 1960, loss = 0.06260646879673004
In grad_steps = 1961, loss = 0.13919907808303833
In grad_steps = 1962, loss = 0.05212939530611038
In grad_steps = 1963, loss = 0.8694493770599365
In grad_steps = 1964, loss = 0.6245236992835999
In grad_steps = 1965, loss = 0.1829819530248642
In grad_steps = 1966, loss = 1.9212032556533813
In grad_steps = 1967, loss = 0.266048401594162
In grad_steps = 1968, loss = 0.25942662358283997
In grad_steps = 1969, loss = 0.09010021388530731
In grad_steps = 1970, loss = 0.23712070286273956
In grad_steps = 1971, loss = 0.7098001837730408
In grad_steps = 1972, loss = 0.720469057559967
In grad_steps = 1973, loss = 0.29628294706344604
In grad_steps = 1974, loss = 0.49011707305908203
In grad_steps = 1975, loss = 0.33870065212249756
In grad_steps = 1976, loss = 0.16489365696907043
In grad_steps = 1977, loss = 0.6177674531936646
In grad_steps = 1978, loss = 0.29159003496170044
In grad_steps = 1979, loss = 0.07147382944822311
In grad_steps = 1980, loss = 0.2610149681568146
In grad_steps = 1981, loss = 0.5868433713912964
In grad_steps = 1982, loss = 0.05598573014140129
In grad_steps = 1983, loss = 0.1046857237815857
In grad_steps = 1984, loss = 0.19667376577854156
In grad_steps = 1985, loss = 0.4048348665237427
In grad_steps = 1986, loss = 0.5002241730690002
In grad_steps = 1987, loss = 0.14035120606422424
In grad_steps = 1988, loss = 0.7946244478225708
In grad_steps = 1989, loss = 0.3719286322593689
In grad_steps = 1990, loss = 0.2745509743690491
In grad_steps = 1991, loss = 0.12199554592370987
In grad_steps = 1992, loss = 0.40236788988113403
In grad_steps = 1993, loss = 0.23347759246826172
In grad_steps = 1994, loss = 0.2552657723426819
In grad_steps = 1995, loss = 0.7339245080947876
In grad_steps = 1996, loss = 0.09551379084587097
In grad_steps = 1997, loss = 0.10336065292358398
In grad_steps = 1998, loss = 0.3219539523124695
In grad_steps = 1999, loss = 0.13804073631763458
In grad_steps = 2000, loss = 0.10991998016834259
In grad_steps = 2001, loss = 0.23297449946403503
In grad_steps = 2002, loss = 0.5457214713096619
In grad_steps = 2003, loss = 0.14840003848075867
In grad_steps = 2004, loss = 0.10613773763179779
In grad_steps = 2005, loss = 0.039286594837903976
In grad_steps = 2006, loss = 0.21622967720031738
In grad_steps = 2007, loss = 0.2578697204589844
In grad_steps = 2008, loss = 0.11819322407245636
In grad_steps = 2009, loss = 0.4494919180870056
In grad_steps = 2010, loss = 0.14163272082805634
In grad_steps = 2011, loss = 0.44990384578704834
In grad_steps = 2012, loss = 0.3767282962799072
In grad_steps = 2013, loss = 0.0405319482088089
In grad_steps = 2014, loss = 0.16669802367687225
In grad_steps = 2015, loss = 0.008724247105419636
In grad_steps = 2016, loss = 0.02595626190304756
In grad_steps = 2017, loss = 0.048421088606119156
In grad_steps = 2018, loss = 0.08699636906385422
In grad_steps = 2019, loss = 0.8222912549972534
In grad_steps = 2020, loss = 0.12330937385559082
In grad_steps = 2021, loss = 0.12327412515878677
In grad_steps = 2022, loss = 0.9517000913619995
In grad_steps = 2023, loss = 0.45821353793144226
In grad_steps = 2024, loss = 0.07007939368486404
In grad_steps = 2025, loss = 0.24565495550632477
In grad_steps = 2026, loss = 0.1532706767320633
In grad_steps = 2027, loss = 0.4216598868370056
In grad_steps = 2028, loss = 0.3377557694911957
In grad_steps = 2029, loss = 0.3512725234031677
In grad_steps = 2030, loss = 0.24643386900424957
In grad_steps = 2031, loss = 0.032089754939079285
In grad_steps = 2032, loss = 0.014123791828751564
In grad_steps = 2033, loss = 0.02950756438076496
In grad_steps = 2034, loss = 0.05342823266983032
In grad_steps = 2035, loss = 0.11095242947340012
In grad_steps = 2036, loss = 0.08874384313821793
In grad_steps = 2037, loss = 0.2647099494934082
In grad_steps = 2038, loss = 0.3216170072555542
In grad_steps = 2039, loss = 0.3912499248981476
In grad_steps = 2040, loss = 0.5998955965042114
In grad_steps = 2041, loss = 0.23595868051052094
In grad_steps = 2042, loss = 0.3587728440761566
In grad_steps = 2043, loss = 0.22674480080604553
In grad_steps = 2044, loss = 0.0645248293876648
In grad_steps = 2045, loss = 0.5662841200828552
In grad_steps = 2046, loss = 0.4843480885028839
In grad_steps = 2047, loss = 0.012414194643497467
In grad_steps = 2048, loss = 0.1600063592195511
In grad_steps = 2049, loss = 2.530475616455078
In grad_steps = 2050, loss = 0.3259742259979248
In grad_steps = 2051, loss = 0.36194249987602234
In grad_steps = 2052, loss = 0.03811121731996536
In grad_steps = 2053, loss = 1.3013123273849487
In grad_steps = 2054, loss = 0.5111303329467773
In grad_steps = 2055, loss = 0.12309500575065613
In grad_steps = 2056, loss = 0.17909562587738037
In grad_steps = 2057, loss = 0.10873180627822876
In grad_steps = 2058, loss = 0.2474300116300583
In grad_steps = 2059, loss = 0.08831679075956345
In grad_steps = 2060, loss = 0.5296539068222046
In grad_steps = 2061, loss = 0.2914413511753082
In grad_steps = 2062, loss = 0.1786576509475708
In grad_steps = 2063, loss = 0.10886431485414505
In grad_steps = 2064, loss = 0.4750431478023529
In grad_steps = 2065, loss = 0.07745982706546783
In grad_steps = 2066, loss = 0.11010220646858215
In grad_steps = 2067, loss = 0.1200375109910965
In grad_steps = 2068, loss = 0.24331121146678925
In grad_steps = 2069, loss = 0.13852597773075104
In grad_steps = 2070, loss = 0.6596771478652954
In grad_steps = 2071, loss = 0.23714208602905273
In grad_steps = 2072, loss = 0.04193909093737602
In grad_steps = 2073, loss = 0.5653467774391174
In grad_steps = 2074, loss = 0.2170787751674652
In grad_steps = 2075, loss = 0.06220749765634537
In grad_steps = 2076, loss = 0.0955054759979248
In grad_steps = 2077, loss = 0.07929011434316635
In grad_steps = 2078, loss = 0.9601100087165833
In grad_steps = 2079, loss = 1.1471748352050781
In grad_steps = 2080, loss = 0.35829582810401917
In grad_steps = 2081, loss = 0.015417961403727531
In grad_steps = 2082, loss = 0.046793632209300995
In grad_steps = 2083, loss = 0.13495850563049316
In grad_steps = 2084, loss = 0.12129238247871399
In grad_steps = 2085, loss = 0.18303412199020386
In grad_steps = 2086, loss = 0.18626785278320312
In grad_steps = 2087, loss = 0.31344395875930786
In grad_steps = 2088, loss = 0.025804553180933
In grad_steps = 2089, loss = 0.03659098967909813
In grad_steps = 2090, loss = 0.06662856787443161
In grad_steps = 2091, loss = 0.03246103972196579
In grad_steps = 2092, loss = 0.24378602206707
In grad_steps = 2093, loss = 0.04085884988307953
In grad_steps = 2094, loss = 0.17443381249904633
In grad_steps = 2095, loss = 0.06612128764390945
In grad_steps = 2096, loss = 0.024422716349363327
In grad_steps = 2097, loss = 1.6895966529846191
In grad_steps = 2098, loss = 0.034392330795526505
In grad_steps = 2099, loss = 0.02658483013510704
In grad_steps = 2100, loss = 0.03449723869562149
In grad_steps = 2101, loss = 0.09811630845069885
In grad_steps = 2102, loss = 0.023715941235423088
In grad_steps = 2103, loss = 0.2764691114425659
In grad_steps = 2104, loss = 0.8250240087509155
In grad_steps = 2105, loss = 1.1258288621902466
In grad_steps = 2106, loss = 0.05161450058221817
In grad_steps = 2107, loss = 1.2803199291229248
In grad_steps = 2108, loss = 0.17682534456253052
In grad_steps = 2109, loss = 0.07154390215873718
In grad_steps = 2110, loss = 0.6278635263442993
In grad_steps = 2111, loss = 0.10883194208145142
In grad_steps = 2112, loss = 0.6747204661369324
In grad_steps = 2113, loss = 0.21934056282043457
In grad_steps = 2114, loss = 0.17799532413482666
In grad_steps = 2115, loss = 0.0871187150478363
In grad_steps = 2116, loss = 0.13919883966445923
In grad_steps = 2117, loss = 0.460847944021225
In grad_steps = 2118, loss = 0.280227392911911
In grad_steps = 2119, loss = 0.06086603179574013
In grad_steps = 2120, loss = 0.0353122353553772
In grad_steps = 2121, loss = 0.15001749992370605
In grad_steps = 2122, loss = 0.14232324063777924
In grad_steps = 2123, loss = 0.5633940696716309
In grad_steps = 2124, loss = 0.12685172259807587
In grad_steps = 2125, loss = 0.07625596225261688
In grad_steps = 2126, loss = 0.9681262969970703
In grad_steps = 2127, loss = 0.17437011003494263
In grad_steps = 2128, loss = 0.17729513347148895
In grad_steps = 2129, loss = 0.3323277235031128
In grad_steps = 2130, loss = 0.5051754713058472
In grad_steps = 2131, loss = 0.5738335847854614
In grad_steps = 2132, loss = 0.5463805198669434
In grad_steps = 2133, loss = 0.04018345847725868
In grad_steps = 2134, loss = 0.19535507261753082
In grad_steps = 2135, loss = 0.28525927662849426
In grad_steps = 2136, loss = 0.43960797786712646
In grad_steps = 2137, loss = 0.2814917266368866
In grad_steps = 2138, loss = 0.6279627084732056
In grad_steps = 2139, loss = 0.15047895908355713
In grad_steps = 2140, loss = 0.1473812460899353
In grad_steps = 2141, loss = 0.19257062673568726
In grad_steps = 2142, loss = 0.04388314485549927
In grad_steps = 2143, loss = 0.10597055405378342
In grad_steps = 2144, loss = 0.9135118722915649
In grad_steps = 2145, loss = 0.3072206974029541
In grad_steps = 2146, loss = 0.19107471406459808
In grad_steps = 2147, loss = 0.6294885873794556
In grad_steps = 2148, loss = 0.8457579612731934
In grad_steps = 2149, loss = 0.05087009444832802
In grad_steps = 2150, loss = 0.27781885862350464
In grad_steps = 2151, loss = 0.4483737051486969
In grad_steps = 2152, loss = 0.3454788625240326
In grad_steps = 2153, loss = 0.2342255711555481
In grad_steps = 2154, loss = 0.05408170074224472
In grad_steps = 2155, loss = 0.26483386754989624
In grad_steps = 2156, loss = 0.2841205298900604
In grad_steps = 2157, loss = 0.04810098186135292
In grad_steps = 2158, loss = 0.11708377301692963
In grad_steps = 2159, loss = 0.8323660492897034
In grad_steps = 2160, loss = 0.1023225486278534
In grad_steps = 2161, loss = 0.04996141791343689
In grad_steps = 2162, loss = 0.8993302583694458
In grad_steps = 2163, loss = 0.08984090387821198
In grad_steps = 2164, loss = 0.0691775381565094
In grad_steps = 2165, loss = 0.4858933389186859
In grad_steps = 2166, loss = 0.5516627430915833
In grad_steps = 2167, loss = 0.056020110845565796
In grad_steps = 2168, loss = 0.08139205724000931
In grad_steps = 2169, loss = 0.3603300154209137
In grad_steps = 2170, loss = 0.015996824949979782
In grad_steps = 2171, loss = 0.7475744485855103
In grad_steps = 2172, loss = 0.873193085193634
In grad_steps = 2173, loss = 0.009797130711376667
In grad_steps = 2174, loss = 0.07172504812479019
In grad_steps = 2175, loss = 0.35857248306274414
In grad_steps = 2176, loss = 0.460710346698761
In grad_steps = 2177, loss = 0.06370575726032257
In grad_steps = 2178, loss = 0.06772033870220184
In grad_steps = 2179, loss = 0.4167858958244324
In grad_steps = 2180, loss = 0.05071541666984558
In grad_steps = 2181, loss = 0.03976041078567505
In grad_steps = 2182, loss = 0.30529648065567017
In grad_steps = 2183, loss = 0.28425735235214233
In grad_steps = 2184, loss = 0.10323367267847061
In grad_steps = 2185, loss = 0.11507768929004669
In grad_steps = 2186, loss = 0.6466497778892517
In grad_steps = 2187, loss = 0.1066170260310173
In grad_steps = 2188, loss = 0.040468744933605194
In grad_steps = 2189, loss = 0.2881445288658142
In grad_steps = 2190, loss = 0.04610805958509445
In grad_steps = 2191, loss = 0.15351159870624542
In grad_steps = 2192, loss = 0.5882522463798523
In grad_steps = 2193, loss = 0.07621419429779053
In grad_steps = 2194, loss = 0.04703290015459061
In grad_steps = 2195, loss = 0.06138147413730621
In grad_steps = 2196, loss = 0.04739223048090935
In grad_steps = 2197, loss = 0.025381749495863914
In grad_steps = 2198, loss = 1.5603986978530884
In grad_steps = 2199, loss = 0.1347961127758026
In grad_steps = 2200, loss = 0.30696457624435425
In grad_steps = 2201, loss = 0.6525475382804871
In grad_steps = 2202, loss = 0.5838989615440369
In grad_steps = 2203, loss = 0.22857144474983215
In grad_steps = 2204, loss = 0.051659464836120605
In grad_steps = 2205, loss = 0.017722785472869873
In grad_steps = 2206, loss = 0.26006805896759033
In grad_steps = 2207, loss = 0.06722903996706009
In grad_steps = 2208, loss = 0.504209041595459
In grad_steps = 2209, loss = 0.11401745676994324
In grad_steps = 2210, loss = 0.3748997747898102
In grad_steps = 2211, loss = 0.14236252009868622
In grad_steps = 2212, loss = 1.5304975509643555
In grad_steps = 2213, loss = 1.148497223854065
In grad_steps = 2214, loss = 0.21583177149295807
In grad_steps = 2215, loss = 0.11084453761577606
In grad_steps = 2216, loss = 0.5270425081253052
In grad_steps = 2217, loss = 1.3487499952316284
In grad_steps = 2218, loss = 0.42283710837364197
In grad_steps = 2219, loss = 0.43401700258255005
In grad_steps = 2220, loss = 0.1232098639011383
In grad_steps = 2221, loss = 0.16884827613830566
In grad_steps = 2222, loss = 0.4266330897808075
In grad_steps = 2223, loss = 0.4663948118686676
In grad_steps = 2224, loss = 0.13767948746681213
In grad_steps = 2225, loss = 0.21730394661426544
In grad_steps = 2226, loss = 0.2641717195510864
In grad_steps = 2227, loss = 0.47997933626174927
In grad_steps = 2228, loss = 0.3620917499065399
In grad_steps = 2229, loss = 0.21179625391960144
In grad_steps = 2230, loss = 0.1852550208568573
In grad_steps = 2231, loss = 0.07223472744226456
In grad_steps = 2232, loss = 0.1202496662735939
In grad_steps = 2233, loss = 0.1429024636745453
In grad_steps = 2234, loss = 0.2342354953289032
In grad_steps = 2235, loss = 0.028276877477765083
In grad_steps = 2236, loss = 0.12886247038841248
In grad_steps = 2237, loss = 0.8884496092796326
In grad_steps = 2238, loss = 0.26811522245407104
In grad_steps = 2239, loss = 0.5427606701850891
In grad_steps = 2240, loss = 0.14390325546264648
In grad_steps = 2241, loss = 0.23662123084068298
In grad_steps = 2242, loss = 0.5482535362243652
In grad_steps = 2243, loss = 0.07151879370212555
In grad_steps = 2244, loss = 0.25699037313461304
In grad_steps = 2245, loss = 1.1430717706680298
In grad_steps = 2246, loss = 0.5751163959503174
In grad_steps = 2247, loss = 0.08784659951925278
In grad_steps = 2248, loss = 0.8363046646118164
In grad_steps = 2249, loss = 0.6036564707756042
In grad_steps = 2250, loss = 0.11615166813135147
In grad_steps = 2251, loss = 0.15906625986099243
In grad_steps = 2252, loss = 0.7029517292976379
In grad_steps = 2253, loss = 0.5917391777038574
In grad_steps = 2254, loss = 0.24297232925891876
In grad_steps = 2255, loss = 0.2785854935646057
In grad_steps = 2256, loss = 0.38059958815574646
In grad_steps = 2257, loss = 0.15226325392723083
In grad_steps = 2258, loss = 0.18340341746807098
In grad_steps = 2259, loss = 0.4044716954231262
In grad_steps = 2260, loss = 0.14208485186100006
In grad_steps = 2261, loss = 0.23799313604831696
In grad_steps = 2262, loss = 0.1318524032831192
In grad_steps = 2263, loss = 0.2914312183856964
In grad_steps = 2264, loss = 0.30518025159835815
In grad_steps = 2265, loss = 0.08863510191440582
In grad_steps = 2266, loss = 0.4436834454536438
In grad_steps = 2267, loss = 0.3876659870147705
In grad_steps = 2268, loss = 0.03685975819826126
In grad_steps = 2269, loss = 0.6512565612792969
In grad_steps = 2270, loss = 0.10754451900720596
In grad_steps = 2271, loss = 0.195027157664299
In grad_steps = 2272, loss = 0.04570924863219261
In grad_steps = 2273, loss = 0.28149548172950745
In grad_steps = 2274, loss = 0.037308599799871445
In grad_steps = 2275, loss = 0.03304912894964218
In grad_steps = 2276, loss = 0.33300939202308655
In grad_steps = 2277, loss = 0.24527621269226074
In grad_steps = 2278, loss = 0.15155941247940063
In grad_steps = 2279, loss = 0.012819986790418625
In grad_steps = 2280, loss = 0.02275386080145836
In grad_steps = 2281, loss = 0.17501553893089294
In grad_steps = 2282, loss = 0.10806786268949509
In grad_steps = 2283, loss = 0.891910195350647
In grad_steps = 2284, loss = 0.3114040195941925
In grad_steps = 2285, loss = 0.024978704750537872
In grad_steps = 2286, loss = 0.24706248939037323
In grad_steps = 2287, loss = 0.8333988785743713
In grad_steps = 2288, loss = 0.11965037882328033
In grad_steps = 2289, loss = 0.018248002976179123
In grad_steps = 2290, loss = 0.3068240284919739
In grad_steps = 2291, loss = 0.024904735386371613
In grad_steps = 2292, loss = 0.4598868489265442
In grad_steps = 2293, loss = 0.26761138439178467
In grad_steps = 2294, loss = 0.14173661172389984
In grad_steps = 2295, loss = 0.2727716863155365
In grad_steps = 2296, loss = 0.04851619899272919
In grad_steps = 2297, loss = 0.32699257135391235
In grad_steps = 2298, loss = 0.11452747136354446
In grad_steps = 2299, loss = 0.126650869846344
In grad_steps = 2300, loss = 0.044309601187705994
In grad_steps = 2301, loss = 0.395478755235672
In grad_steps = 2302, loss = 0.13559338450431824
In grad_steps = 2303, loss = 0.010781567543745041
In grad_steps = 2304, loss = 0.01511755958199501
In grad_steps = 2305, loss = 0.4779517352581024
In grad_steps = 2306, loss = 0.03095366433262825
In grad_steps = 2307, loss = 0.23008525371551514
In grad_steps = 2308, loss = 0.07184234261512756
In grad_steps = 2309, loss = 0.005844191648066044
In grad_steps = 2310, loss = 0.9278959035873413
In grad_steps = 2311, loss = 0.03317786753177643
In grad_steps = 2312, loss = 0.9936368465423584
In grad_steps = 2313, loss = 0.9435598850250244
In grad_steps = 2314, loss = 0.034850798547267914
In grad_steps = 2315, loss = 0.35609251260757446
In grad_steps = 2316, loss = 0.1279638409614563
In grad_steps = 2317, loss = 0.10719918459653854
In grad_steps = 2318, loss = 0.4026497006416321
In grad_steps = 2319, loss = 1.9683971405029297
In grad_steps = 2320, loss = 0.07839415222406387
In grad_steps = 2321, loss = 0.11167026311159134
In grad_steps = 2322, loss = 0.3728824853897095
In grad_steps = 2323, loss = 0.05136270448565483
In grad_steps = 2324, loss = 0.25088930130004883
In grad_steps = 2325, loss = 0.08731678128242493
In grad_steps = 2326, loss = 0.03319789096713066
In grad_steps = 2327, loss = 0.8429841995239258
In grad_steps = 2328, loss = 0.5996416807174683
In grad_steps = 2329, loss = 0.1770680546760559
In grad_steps = 2330, loss = 0.1875096708536148
In grad_steps = 2331, loss = 0.510491132736206
In grad_steps = 2332, loss = 0.1178952157497406
In grad_steps = 2333, loss = 0.6478014588356018
In grad_steps = 2334, loss = 0.07100728154182434
In grad_steps = 2335, loss = 0.41609257459640503
In grad_steps = 2336, loss = 0.3501274585723877
In grad_steps = 2337, loss = 0.13253970444202423
In grad_steps = 2338, loss = 0.14581313729286194
In grad_steps = 2339, loss = 0.1917715072631836
In grad_steps = 2340, loss = 0.34435033798217773
In grad_steps = 2341, loss = 0.23951372504234314
In grad_steps = 2342, loss = 1.0211076736450195
In grad_steps = 2343, loss = 0.14491330087184906
In grad_steps = 2344, loss = 0.6584057211875916
In grad_steps = 2345, loss = 0.24038612842559814
In grad_steps = 2346, loss = 0.18307073414325714
In grad_steps = 2347, loss = 0.27433887124061584
In grad_steps = 2348, loss = 0.09519192576408386
In grad_steps = 2349, loss = 0.11175423115491867
In grad_steps = 2350, loss = 0.20433878898620605
In grad_steps = 2351, loss = 0.4776606261730194
In grad_steps = 2352, loss = 0.01639830321073532
Beginning epoch 2
In grad_steps = 2353, loss = 0.1366526037454605
In grad_steps = 2354, loss = 1.216463327407837
In grad_steps = 2355, loss = 0.3735561966896057
In grad_steps = 2356, loss = 0.33887213468551636
In grad_steps = 2357, loss = 0.1827079951763153
In grad_steps = 2358, loss = 0.30496907234191895
In grad_steps = 2359, loss = 0.22439196705818176
In grad_steps = 2360, loss = 0.09278777241706848
In grad_steps = 2361, loss = 0.06028255075216293
In grad_steps = 2362, loss = 0.8752346634864807
In grad_steps = 2363, loss = 0.2734474539756775
In grad_steps = 2364, loss = 0.756962776184082
In grad_steps = 2365, loss = 0.2601105570793152
In grad_steps = 2366, loss = 0.2166522592306137
In grad_steps = 2367, loss = 0.16876071691513062
In grad_steps = 2368, loss = 0.047120995819568634
In grad_steps = 2369, loss = 0.11844803392887115
In grad_steps = 2370, loss = 0.37563371658325195
In grad_steps = 2371, loss = 0.4692706763744354
In grad_steps = 2372, loss = 0.393716961145401
In grad_steps = 2373, loss = 0.060890380293130875
In grad_steps = 2374, loss = 0.22631505131721497
In grad_steps = 2375, loss = 0.2815244197845459
In grad_steps = 2376, loss = 0.7884883284568787
In grad_steps = 2377, loss = 0.5061790943145752
In grad_steps = 2378, loss = 0.19039344787597656
In grad_steps = 2379, loss = 0.03706040233373642
In grad_steps = 2380, loss = 0.0120127247646451
In grad_steps = 2381, loss = 0.10193344950675964
In grad_steps = 2382, loss = 0.055325765162706375
In grad_steps = 2383, loss = 0.09843578189611435
In grad_steps = 2384, loss = 0.3760205805301666
In grad_steps = 2385, loss = 0.1934557855129242
In grad_steps = 2386, loss = 0.1122494712471962
In grad_steps = 2387, loss = 0.2794857323169708
In grad_steps = 2388, loss = 0.35120999813079834
In grad_steps = 2389, loss = 0.02520555816590786
In grad_steps = 2390, loss = 0.022934958338737488
In grad_steps = 2391, loss = 0.3422574996948242
In grad_steps = 2392, loss = 0.41960081458091736
In grad_steps = 2393, loss = 0.038113050162792206
In grad_steps = 2394, loss = 0.3765926957130432
In grad_steps = 2395, loss = 0.15994882583618164
In grad_steps = 2396, loss = 0.018406258895993233
In grad_steps = 2397, loss = 0.35051339864730835
In grad_steps = 2398, loss = 0.03741646930575371
In grad_steps = 2399, loss = 0.12240403145551682
In grad_steps = 2400, loss = 0.9715384840965271
In grad_steps = 2401, loss = 0.23393476009368896
In grad_steps = 2402, loss = 0.4591333270072937
In grad_steps = 2403, loss = 0.36942315101623535
In grad_steps = 2404, loss = 1.8602739572525024
In grad_steps = 2405, loss = 0.04179355874657631
In grad_steps = 2406, loss = 0.22986918687820435
In grad_steps = 2407, loss = 0.022264238446950912
In grad_steps = 2408, loss = 0.03667602688074112
In grad_steps = 2409, loss = 0.3761104345321655
In grad_steps = 2410, loss = 0.06206633150577545
In grad_steps = 2411, loss = 0.1954064965248108
In grad_steps = 2412, loss = 0.035106271505355835
In grad_steps = 2413, loss = 0.18800511956214905
In grad_steps = 2414, loss = 0.10109306126832962
In grad_steps = 2415, loss = 1.0408529043197632
In grad_steps = 2416, loss = 0.6059179306030273
In grad_steps = 2417, loss = 0.20873528718948364
In grad_steps = 2418, loss = 0.04982757940888405
In grad_steps = 2419, loss = 0.06719840317964554
In grad_steps = 2420, loss = 0.06645870208740234
In grad_steps = 2421, loss = 0.03970633074641228
In grad_steps = 2422, loss = 0.9282070994377136
In grad_steps = 2423, loss = 0.6771261692047119
In grad_steps = 2424, loss = 0.05376441404223442
In grad_steps = 2425, loss = 1.2419617176055908
In grad_steps = 2426, loss = 0.09572333097457886
In grad_steps = 2427, loss = 0.18339233100414276
In grad_steps = 2428, loss = 0.0799112394452095
In grad_steps = 2429, loss = 0.24192744493484497
In grad_steps = 2430, loss = 0.6080058217048645
In grad_steps = 2431, loss = 0.2318388968706131
In grad_steps = 2432, loss = 0.13606509566307068
In grad_steps = 2433, loss = 0.24713633954524994
In grad_steps = 2434, loss = 0.05396232381463051
In grad_steps = 2435, loss = 0.1331651657819748
In grad_steps = 2436, loss = 0.2373856157064438
In grad_steps = 2437, loss = 0.07300729304552078
In grad_steps = 2438, loss = 0.3649355471134186
In grad_steps = 2439, loss = 0.9745176434516907
In grad_steps = 2440, loss = 0.11628332734107971
In grad_steps = 2441, loss = 0.1399194300174713
In grad_steps = 2442, loss = 0.126238614320755
In grad_steps = 2443, loss = 1.3836073875427246
In grad_steps = 2444, loss = 0.7193002700805664
In grad_steps = 2445, loss = 0.2322973757982254
In grad_steps = 2446, loss = 0.21512024104595184
In grad_steps = 2447, loss = 0.04574672132730484
In grad_steps = 2448, loss = 0.5306575894355774
In grad_steps = 2449, loss = 0.12003366649150848
In grad_steps = 2450, loss = 0.06343819200992584
In grad_steps = 2451, loss = 0.18401584029197693
In grad_steps = 2452, loss = 0.39777690172195435
In grad_steps = 2453, loss = 0.4585656523704529
In grad_steps = 2454, loss = 0.15755891799926758
In grad_steps = 2455, loss = 0.48736006021499634
In grad_steps = 2456, loss = 0.735709547996521
In grad_steps = 2457, loss = 0.5194924473762512
In grad_steps = 2458, loss = 0.5391314029693604
In grad_steps = 2459, loss = 0.05328415334224701
In grad_steps = 2460, loss = 0.04396699368953705
In grad_steps = 2461, loss = 0.31023937463760376
In grad_steps = 2462, loss = 0.28479060530662537
In grad_steps = 2463, loss = 0.20884321630001068
In grad_steps = 2464, loss = 0.10055229812860489
In grad_steps = 2465, loss = 0.35287946462631226
In grad_steps = 2466, loss = 0.15803812444210052
In grad_steps = 2467, loss = 0.6662284731864929
In grad_steps = 2468, loss = 0.21351805329322815
In grad_steps = 2469, loss = 0.6816777586936951
In grad_steps = 2470, loss = 0.2817177474498749
In grad_steps = 2471, loss = 0.14254504442214966
In grad_steps = 2472, loss = 0.5835320353507996
In grad_steps = 2473, loss = 0.6636142730712891
In grad_steps = 2474, loss = 0.09092491865158081
In grad_steps = 2475, loss = 0.33317628502845764
In grad_steps = 2476, loss = 0.13896818459033966
In grad_steps = 2477, loss = 0.3649846911430359
In grad_steps = 2478, loss = 0.9886208772659302
In grad_steps = 2479, loss = 0.4339989721775055
In grad_steps = 2480, loss = 0.09681341052055359
In grad_steps = 2481, loss = 0.4980665445327759
In grad_steps = 2482, loss = 0.10668160021305084
In grad_steps = 2483, loss = 0.28585904836654663
In grad_steps = 2484, loss = 0.13630613684654236
In grad_steps = 2485, loss = 0.23882879316806793
In grad_steps = 2486, loss = 0.07693204283714294
In grad_steps = 2487, loss = 0.5080440640449524
In grad_steps = 2488, loss = 0.08419672399759293
In grad_steps = 2489, loss = 0.10757523775100708
In grad_steps = 2490, loss = 0.1332346349954605
In grad_steps = 2491, loss = 0.4752923846244812
In grad_steps = 2492, loss = 0.0804789736866951
In grad_steps = 2493, loss = 0.026799503713846207
In grad_steps = 2494, loss = 0.25648605823516846
In grad_steps = 2495, loss = 0.38144949078559875
In grad_steps = 2496, loss = 0.28009891510009766
In grad_steps = 2497, loss = 0.15512646734714508
In grad_steps = 2498, loss = 0.263887494802475
In grad_steps = 2499, loss = 0.1851465106010437
In grad_steps = 2500, loss = 0.7172271609306335
In grad_steps = 2501, loss = 1.1649848222732544
In grad_steps = 2502, loss = 1.2460025548934937
In grad_steps = 2503, loss = 0.11144871264696121
In grad_steps = 2504, loss = 0.21560433506965637
In grad_steps = 2505, loss = 0.14010441303253174
In grad_steps = 2506, loss = 0.6007618308067322
In grad_steps = 2507, loss = 0.7363424301147461
In grad_steps = 2508, loss = 0.2902859151363373
In grad_steps = 2509, loss = 0.2126007229089737
In grad_steps = 2510, loss = 0.3617081642150879
In grad_steps = 2511, loss = 0.11891813576221466
In grad_steps = 2512, loss = 0.1871977150440216
In grad_steps = 2513, loss = 0.46377480030059814
In grad_steps = 2514, loss = 0.06242308020591736
In grad_steps = 2515, loss = 0.3608251214027405
In grad_steps = 2516, loss = 0.14078815281391144
In grad_steps = 2517, loss = 0.15481816232204437
In grad_steps = 2518, loss = 0.14449645578861237
In grad_steps = 2519, loss = 0.4541880786418915
In grad_steps = 2520, loss = 0.05898188427090645
In grad_steps = 2521, loss = 0.053115975111722946
In grad_steps = 2522, loss = 0.6117603778839111
In grad_steps = 2523, loss = 0.0670756846666336
In grad_steps = 2524, loss = 0.24191980063915253
In grad_steps = 2525, loss = 0.08204258978366852
In grad_steps = 2526, loss = 0.11183115839958191
In grad_steps = 2527, loss = 0.5081227421760559
In grad_steps = 2528, loss = 0.17851251363754272
In grad_steps = 2529, loss = 0.06334956735372543
In grad_steps = 2530, loss = 0.04750955104827881
In grad_steps = 2531, loss = 0.6459011435508728
In grad_steps = 2532, loss = 0.18031224608421326
In grad_steps = 2533, loss = 0.11565835773944855
In grad_steps = 2534, loss = 0.036654453724622726
In grad_steps = 2535, loss = 0.03449114412069321
In grad_steps = 2536, loss = 0.04094948619604111
In grad_steps = 2537, loss = 0.06307315081357956
In grad_steps = 2538, loss = 0.3507910370826721
In grad_steps = 2539, loss = 0.12260974198579788
In grad_steps = 2540, loss = 0.015310949645936489
In grad_steps = 2541, loss = 0.19213755428791046
In grad_steps = 2542, loss = 0.011723512783646584
In grad_steps = 2543, loss = 0.0834161639213562
In grad_steps = 2544, loss = 0.5066573023796082
In grad_steps = 2545, loss = 0.27417975664138794
In grad_steps = 2546, loss = 0.020573947578668594
In grad_steps = 2547, loss = 0.2970661222934723
In grad_steps = 2548, loss = 0.010961460880935192
In grad_steps = 2549, loss = 0.6761727929115295
In grad_steps = 2550, loss = 0.005361900664865971
In grad_steps = 2551, loss = 0.07438748329877853
In grad_steps = 2552, loss = 0.061220433562994
In grad_steps = 2553, loss = 0.044866759330034256
In grad_steps = 2554, loss = 1.430372714996338
In grad_steps = 2555, loss = 0.6145125031471252
In grad_steps = 2556, loss = 0.030697554349899292
In grad_steps = 2557, loss = 0.049332261085510254
In grad_steps = 2558, loss = 0.22247286140918732
In grad_steps = 2559, loss = 0.7835080027580261
In grad_steps = 2560, loss = 0.12242041528224945
In grad_steps = 2561, loss = 0.08769820630550385
In grad_steps = 2562, loss = 0.7197399139404297
In grad_steps = 2563, loss = 0.07689247280359268
In grad_steps = 2564, loss = 0.1661357432603836
In grad_steps = 2565, loss = 0.10274491459131241
In grad_steps = 2566, loss = 0.07328712940216064
In grad_steps = 2567, loss = 0.4272575080394745
In grad_steps = 2568, loss = 0.2534615695476532
In grad_steps = 2569, loss = 0.04429522901773453
In grad_steps = 2570, loss = 0.5318436026573181
In grad_steps = 2571, loss = 0.47518518567085266
In grad_steps = 2572, loss = 0.08481502532958984
In grad_steps = 2573, loss = 0.06944321095943451
In grad_steps = 2574, loss = 0.3471495509147644
In grad_steps = 2575, loss = 0.017332585528492928
In grad_steps = 2576, loss = 0.11608115583658218
In grad_steps = 2577, loss = 0.20960594713687897
In grad_steps = 2578, loss = 0.8355224132537842
In grad_steps = 2579, loss = 0.23789480328559875
In grad_steps = 2580, loss = 0.4496532082557678
In grad_steps = 2581, loss = 0.2847879230976105
In grad_steps = 2582, loss = 0.6474722027778625
In grad_steps = 2583, loss = 0.21793070435523987
In grad_steps = 2584, loss = 0.07595840096473694
In grad_steps = 2585, loss = 0.0756072849035263
In grad_steps = 2586, loss = 0.9513529539108276
In grad_steps = 2587, loss = 0.37368524074554443
In grad_steps = 2588, loss = 0.144504114985466
In grad_steps = 2589, loss = 0.1989486813545227
In grad_steps = 2590, loss = 0.33029210567474365
In grad_steps = 2591, loss = 0.09245625883340836
In grad_steps = 2592, loss = 0.2593965232372284
In grad_steps = 2593, loss = 0.2962583005428314
In grad_steps = 2594, loss = 0.08517751097679138
In grad_steps = 2595, loss = 0.04470086470246315
In grad_steps = 2596, loss = 0.1443898230791092
In grad_steps = 2597, loss = 0.0786716416478157
In grad_steps = 2598, loss = 0.08179955184459686
In grad_steps = 2599, loss = 0.023557551205158234
In grad_steps = 2600, loss = 0.4619258642196655
In grad_steps = 2601, loss = 0.01629350334405899
In grad_steps = 2602, loss = 0.04907568544149399
In grad_steps = 2603, loss = 0.24148811399936676
In grad_steps = 2604, loss = 0.38240519165992737
In grad_steps = 2605, loss = 1.6856964826583862
In grad_steps = 2606, loss = 0.0532635897397995
In grad_steps = 2607, loss = 0.2739088833332062
In grad_steps = 2608, loss = 0.9246318936347961
In grad_steps = 2609, loss = 0.11828358471393585
In grad_steps = 2610, loss = 0.5812197327613831
In grad_steps = 2611, loss = 0.12965939939022064
In grad_steps = 2612, loss = 0.091911680996418
In grad_steps = 2613, loss = 0.8164618611335754
In grad_steps = 2614, loss = 0.705909788608551
In grad_steps = 2615, loss = 0.12616890668869019
In grad_steps = 2616, loss = 0.538877546787262
In grad_steps = 2617, loss = 0.10081036388874054
In grad_steps = 2618, loss = 0.12940312922000885
In grad_steps = 2619, loss = 0.18732601404190063
In grad_steps = 2620, loss = 0.1915527880191803
In grad_steps = 2621, loss = 0.3500925302505493
In grad_steps = 2622, loss = 0.11901581287384033
In grad_steps = 2623, loss = 0.11146056652069092
In grad_steps = 2624, loss = 0.2364736795425415
In grad_steps = 2625, loss = 0.1435091644525528
In grad_steps = 2626, loss = 0.42120012640953064
In grad_steps = 2627, loss = 0.039099954068660736
In grad_steps = 2628, loss = 0.04611219838261604
In grad_steps = 2629, loss = 0.389810174703598
In grad_steps = 2630, loss = 0.6247299909591675
In grad_steps = 2631, loss = 0.019955500960350037
In grad_steps = 2632, loss = 1.0461684465408325
In grad_steps = 2633, loss = 0.40091750025749207
In grad_steps = 2634, loss = 0.5811153054237366
In grad_steps = 2635, loss = 0.24940069019794464
In grad_steps = 2636, loss = 0.6540163159370422
In grad_steps = 2637, loss = 0.13237528502941132
In grad_steps = 2638, loss = 0.6382759809494019
In grad_steps = 2639, loss = 0.4815482497215271
In grad_steps = 2640, loss = 0.054464131593704224
In grad_steps = 2641, loss = 0.032610390335321426
In grad_steps = 2642, loss = 0.48147696256637573
In grad_steps = 2643, loss = 0.053147464990615845
In grad_steps = 2644, loss = 0.24054083228111267
In grad_steps = 2645, loss = 0.2635135352611542
In grad_steps = 2646, loss = 0.3194447159767151
In grad_steps = 2647, loss = 0.15278495848178864
In grad_steps = 2648, loss = 0.24809926748275757
In grad_steps = 2649, loss = 0.05025676265358925
In grad_steps = 2650, loss = 0.2194863259792328
In grad_steps = 2651, loss = 0.2524529993534088
In grad_steps = 2652, loss = 0.05805103853344917
In grad_steps = 2653, loss = 0.11345390230417252
In grad_steps = 2654, loss = 0.2039826661348343
In grad_steps = 2655, loss = 0.018868515267968178
In grad_steps = 2656, loss = 0.37081804871559143
In grad_steps = 2657, loss = 0.27982667088508606
In grad_steps = 2658, loss = 1.0665329694747925
In grad_steps = 2659, loss = 0.08815157413482666
In grad_steps = 2660, loss = 0.2458377182483673
In grad_steps = 2661, loss = 0.21271003782749176
In grad_steps = 2662, loss = 0.29678845405578613
In grad_steps = 2663, loss = 0.11396054178476334
In grad_steps = 2664, loss = 0.3407042324542999
In grad_steps = 2665, loss = 0.22549915313720703
In grad_steps = 2666, loss = 0.2691830098628998
In grad_steps = 2667, loss = 0.892525315284729
In grad_steps = 2668, loss = 0.48132526874542236
In grad_steps = 2669, loss = 0.047203972935676575
In grad_steps = 2670, loss = 0.214723140001297
In grad_steps = 2671, loss = 0.3242315351963043
In grad_steps = 2672, loss = 0.15341448783874512
In grad_steps = 2673, loss = 0.07862361520528793
In grad_steps = 2674, loss = 0.16081602871418
In grad_steps = 2675, loss = 0.07987019419670105
In grad_steps = 2676, loss = 0.33415549993515015
In grad_steps = 2677, loss = 0.07692894339561462
In grad_steps = 2678, loss = 0.08149069547653198
In grad_steps = 2679, loss = 0.03479507938027382
In grad_steps = 2680, loss = 0.02368147112429142
In grad_steps = 2681, loss = 0.05816372483968735
In grad_steps = 2682, loss = 0.008619257248938084
In grad_steps = 2683, loss = 0.10222934186458588
In grad_steps = 2684, loss = 0.14821821451187134
In grad_steps = 2685, loss = 0.10110607743263245
In grad_steps = 2686, loss = 1.2865617275238037
In grad_steps = 2687, loss = 0.011838003061711788
In grad_steps = 2688, loss = 0.03192730247974396
In grad_steps = 2689, loss = 0.10537659376859665
In grad_steps = 2690, loss = 0.10887405276298523
In grad_steps = 2691, loss = 0.049616724252700806
In grad_steps = 2692, loss = 0.07835738360881805
In grad_steps = 2693, loss = 0.5426543951034546
In grad_steps = 2694, loss = 0.03810620307922363
In grad_steps = 2695, loss = 1.0925817489624023
In grad_steps = 2696, loss = 0.2304660677909851
In grad_steps = 2697, loss = 0.19497661292552948
In grad_steps = 2698, loss = 0.041107602417469025
In grad_steps = 2699, loss = 0.09086853265762329
In grad_steps = 2700, loss = 0.06351561099290848
In grad_steps = 2701, loss = 0.8026685118675232
In grad_steps = 2702, loss = 0.7441710233688354
In grad_steps = 2703, loss = 0.7082069516181946
In grad_steps = 2704, loss = 0.17367218434810638
In grad_steps = 2705, loss = 0.32587701082229614
In grad_steps = 2706, loss = 0.062535360455513
In grad_steps = 2707, loss = 0.14104993641376495
In grad_steps = 2708, loss = 0.7048060297966003
In grad_steps = 2709, loss = 0.48624396324157715
In grad_steps = 2710, loss = 0.1514624059200287
In grad_steps = 2711, loss = 0.10744474083185196
In grad_steps = 2712, loss = 0.314756840467453
In grad_steps = 2713, loss = 0.20267966389656067
In grad_steps = 2714, loss = 0.08001694083213806
In grad_steps = 2715, loss = 0.16259725391864777
In grad_steps = 2716, loss = 0.036291081458330154
In grad_steps = 2717, loss = 0.052738018333911896
In grad_steps = 2718, loss = 0.08434826880693436
In grad_steps = 2719, loss = 0.01942792534828186
In grad_steps = 2720, loss = 0.2336379438638687
In grad_steps = 2721, loss = 0.2916422188282013
In grad_steps = 2722, loss = 0.09071285277605057
In grad_steps = 2723, loss = 1.1048974990844727
In grad_steps = 2724, loss = 0.019515952095389366
In grad_steps = 2725, loss = 0.04110398888587952
In grad_steps = 2726, loss = 0.14839565753936768
In grad_steps = 2727, loss = 0.06774148344993591
In grad_steps = 2728, loss = 0.02903727814555168
In grad_steps = 2729, loss = 0.022067293524742126
In grad_steps = 2730, loss = 0.17205572128295898
In grad_steps = 2731, loss = 0.31472498178482056
In grad_steps = 2732, loss = 0.144757479429245
In grad_steps = 2733, loss = 0.09409545361995697
In grad_steps = 2734, loss = 0.4228915870189667
In grad_steps = 2735, loss = 0.2537463903427124
In grad_steps = 2736, loss = 0.02051507867872715
In grad_steps = 2737, loss = 0.13790692389011383
In grad_steps = 2738, loss = 0.1552201807498932
In grad_steps = 2739, loss = 1.2840323448181152
In grad_steps = 2740, loss = 0.05876031145453453
In grad_steps = 2741, loss = 0.35089993476867676
In grad_steps = 2742, loss = 0.4370013177394867
In grad_steps = 2743, loss = 0.04953661933541298
In grad_steps = 2744, loss = 0.051217950880527496
In grad_steps = 2745, loss = 0.027855947613716125
In grad_steps = 2746, loss = 0.29437175393104553
In grad_steps = 2747, loss = 0.0177803635597229
In grad_steps = 2748, loss = 0.12289241701364517
In grad_steps = 2749, loss = 0.03911036625504494
In grad_steps = 2750, loss = 0.09595867246389389
In grad_steps = 2751, loss = 0.08662634342908859
In grad_steps = 2752, loss = 0.3140556812286377
In grad_steps = 2753, loss = 0.5065958499908447
In grad_steps = 2754, loss = 0.11546574532985687
In grad_steps = 2755, loss = 0.1105688288807869
In grad_steps = 2756, loss = 0.6554296612739563
In grad_steps = 2757, loss = 0.24276578426361084
In grad_steps = 2758, loss = 0.020759763196110725
In grad_steps = 2759, loss = 1.0589969158172607
In grad_steps = 2760, loss = 0.17639769613742828
In grad_steps = 2761, loss = 0.07175330072641373
In grad_steps = 2762, loss = 0.49904748797416687
In grad_steps = 2763, loss = 0.022878799587488174
In grad_steps = 2764, loss = 0.40016159415245056
In grad_steps = 2765, loss = 0.15732473134994507
In grad_steps = 2766, loss = 0.17058978974819183
In grad_steps = 2767, loss = 0.3026939332485199
In grad_steps = 2768, loss = 0.20074641704559326
In grad_steps = 2769, loss = 0.11924275010824203
In grad_steps = 2770, loss = 0.16817952692508698
In grad_steps = 2771, loss = 0.22860783338546753
In grad_steps = 2772, loss = 0.09648942202329636
In grad_steps = 2773, loss = 0.005265696905553341
In grad_steps = 2774, loss = 0.3315027952194214
In grad_steps = 2775, loss = 0.7570140361785889
In grad_steps = 2776, loss = 0.08322779834270477
In grad_steps = 2777, loss = 0.2950752377510071
In grad_steps = 2778, loss = 0.016648657619953156
In grad_steps = 2779, loss = 0.18886937201023102
In grad_steps = 2780, loss = 0.07850311696529388
In grad_steps = 2781, loss = 0.05164385959506035
In grad_steps = 2782, loss = 0.7467785477638245
In grad_steps = 2783, loss = 0.26002490520477295
In grad_steps = 2784, loss = 0.2426028549671173
In grad_steps = 2785, loss = 0.033966079354286194
In grad_steps = 2786, loss = 0.33040907979011536
In grad_steps = 2787, loss = 0.859907865524292
In grad_steps = 2788, loss = 0.043707944452762604
In grad_steps = 2789, loss = 0.2767921984195709
In grad_steps = 2790, loss = 0.30317553877830505
In grad_steps = 2791, loss = 0.7786433100700378
In grad_steps = 2792, loss = 0.08488503098487854
In grad_steps = 2793, loss = 0.020615823566913605
In grad_steps = 2794, loss = 0.12382908165454865
In grad_steps = 2795, loss = 0.5900723934173584
In grad_steps = 2796, loss = 0.14703473448753357
In grad_steps = 2797, loss = 0.165385439991951
In grad_steps = 2798, loss = 0.01999676413834095
In grad_steps = 2799, loss = 0.5003771185874939
In grad_steps = 2800, loss = 0.07130453735589981
In grad_steps = 2801, loss = 0.13020262122154236
In grad_steps = 2802, loss = 0.07128768414258957
In grad_steps = 2803, loss = 0.05875629186630249
In grad_steps = 2804, loss = 0.08866634964942932
In grad_steps = 2805, loss = 0.2712295949459076
In grad_steps = 2806, loss = 0.06215941160917282
In grad_steps = 2807, loss = 0.6642678380012512
In grad_steps = 2808, loss = 0.023250427097082138
In grad_steps = 2809, loss = 0.3512074053287506
In grad_steps = 2810, loss = 0.041648317128419876
In grad_steps = 2811, loss = 0.4298463463783264
In grad_steps = 2812, loss = 0.03862324357032776
In grad_steps = 2813, loss = 0.031647611409425735
In grad_steps = 2814, loss = 0.005365983583033085
In grad_steps = 2815, loss = 0.9295657277107239
In grad_steps = 2816, loss = 0.04148854315280914
In grad_steps = 2817, loss = 0.323100745677948
In grad_steps = 2818, loss = 0.7381614446640015
In grad_steps = 2819, loss = 0.09642644226551056
In grad_steps = 2820, loss = 0.1326177418231964
In grad_steps = 2821, loss = 0.275709867477417
In grad_steps = 2822, loss = 0.15282846987247467
In grad_steps = 2823, loss = 0.146410271525383
In grad_steps = 2824, loss = 0.016470208764076233
In grad_steps = 2825, loss = 0.25727805495262146
In grad_steps = 2826, loss = 0.037683628499507904
In grad_steps = 2827, loss = 0.41474613547325134
In grad_steps = 2828, loss = 0.08715777099132538
In grad_steps = 2829, loss = 0.010815836489200592
In grad_steps = 2830, loss = 0.11099081486463547
In grad_steps = 2831, loss = 0.03545738011598587
In grad_steps = 2832, loss = 0.574351966381073
In grad_steps = 2833, loss = 0.02534114569425583
In grad_steps = 2834, loss = 0.09272847324609756
In grad_steps = 2835, loss = 0.019428370520472527
In grad_steps = 2836, loss = 0.06281296163797379
In grad_steps = 2837, loss = 0.2068241536617279
In grad_steps = 2838, loss = 0.3147399127483368
In grad_steps = 2839, loss = 0.01388823427259922
In grad_steps = 2840, loss = 0.7400392293930054
In grad_steps = 2841, loss = 1.6974362134933472
In grad_steps = 2842, loss = 0.5978668332099915
In grad_steps = 2843, loss = 0.3528115153312683
In grad_steps = 2844, loss = 0.4091401696205139
In grad_steps = 2845, loss = 0.06404495239257812
In grad_steps = 2846, loss = 0.03331316262483597
In grad_steps = 2847, loss = 0.6568055152893066
In grad_steps = 2848, loss = 0.031376127153635025
In grad_steps = 2849, loss = 0.15068446099758148
In grad_steps = 2850, loss = 0.0682658702135086
In grad_steps = 2851, loss = 0.18796679377555847
In grad_steps = 2852, loss = 0.11798468232154846
In grad_steps = 2853, loss = 0.05528616905212402
In grad_steps = 2854, loss = 0.23234082758426666
In grad_steps = 2855, loss = 0.7432802319526672
In grad_steps = 2856, loss = 0.3847874402999878
In grad_steps = 2857, loss = 0.1616860032081604
In grad_steps = 2858, loss = 0.765663743019104
In grad_steps = 2859, loss = 0.1178223192691803
In grad_steps = 2860, loss = 0.19547857344150543
In grad_steps = 2861, loss = 0.865598201751709
In grad_steps = 2862, loss = 0.3485831022262573
In grad_steps = 2863, loss = 0.029600515961647034
In grad_steps = 2864, loss = 0.4728434085845947
In grad_steps = 2865, loss = 0.11508288979530334
In grad_steps = 2866, loss = 0.12401215732097626
In grad_steps = 2867, loss = 0.19375668466091156
In grad_steps = 2868, loss = 0.19866463541984558
In grad_steps = 2869, loss = 0.4120645225048065
In grad_steps = 2870, loss = 0.31958627700805664
In grad_steps = 2871, loss = 0.23953132331371307
In grad_steps = 2872, loss = 0.49934151768684387
In grad_steps = 2873, loss = 0.11973055452108383
In grad_steps = 2874, loss = 0.5616459846496582
In grad_steps = 2875, loss = 0.05797364562749863
In grad_steps = 2876, loss = 0.03682619333267212
In grad_steps = 2877, loss = 0.05073946714401245
In grad_steps = 2878, loss = 0.15455415844917297
In grad_steps = 2879, loss = 0.06989370286464691
In grad_steps = 2880, loss = 0.38409772515296936
In grad_steps = 2881, loss = 0.11569265276193619
In grad_steps = 2882, loss = 0.027308154851198196
In grad_steps = 2883, loss = 0.02986394241452217
In grad_steps = 2884, loss = 0.3146732747554779
In grad_steps = 2885, loss = 0.19664064049720764
In grad_steps = 2886, loss = 0.05174344778060913
In grad_steps = 2887, loss = 1.188314437866211
In grad_steps = 2888, loss = 0.7982625961303711
In grad_steps = 2889, loss = 0.4171135723590851
In grad_steps = 2890, loss = 0.10725001990795135
In grad_steps = 2891, loss = 0.27278953790664673
In grad_steps = 2892, loss = 0.07892948389053345
In grad_steps = 2893, loss = 0.1342315822839737
In grad_steps = 2894, loss = 0.03409188240766525
In grad_steps = 2895, loss = 0.22711190581321716
In grad_steps = 2896, loss = 0.018585657700896263
In grad_steps = 2897, loss = 0.6722363233566284
In grad_steps = 2898, loss = 0.6753771305084229
In grad_steps = 2899, loss = 0.17317764461040497
In grad_steps = 2900, loss = 0.4156220257282257
In grad_steps = 2901, loss = 0.095115065574646
In grad_steps = 2902, loss = 0.24138827621936798
In grad_steps = 2903, loss = 0.7618820071220398
In grad_steps = 2904, loss = 0.2029532492160797
In grad_steps = 2905, loss = 0.6668312549591064
In grad_steps = 2906, loss = 0.05200948566198349
In grad_steps = 2907, loss = 0.36157867312431335
In grad_steps = 2908, loss = 0.7249892950057983
In grad_steps = 2909, loss = 0.8232167959213257
In grad_steps = 2910, loss = 0.20915107429027557
In grad_steps = 2911, loss = 0.4814126491546631
In grad_steps = 2912, loss = 0.3803124725818634
In grad_steps = 2913, loss = 0.7731522917747498
In grad_steps = 2914, loss = 0.1556725800037384
In grad_steps = 2915, loss = 0.1666199266910553
In grad_steps = 2916, loss = 0.2808260917663574
In grad_steps = 2917, loss = 0.9057491421699524
In grad_steps = 2918, loss = 0.32511478662490845
In grad_steps = 2919, loss = 0.35483577847480774
In grad_steps = 2920, loss = 0.8447654247283936
In grad_steps = 2921, loss = 0.24841515719890594
In grad_steps = 2922, loss = 0.08230063319206238
In grad_steps = 2923, loss = 0.16208592057228088
In grad_steps = 2924, loss = 0.06810218840837479
In grad_steps = 2925, loss = 0.2252998948097229
In grad_steps = 2926, loss = 0.0896746888756752
In grad_steps = 2927, loss = 0.12995855510234833
In grad_steps = 2928, loss = 0.11902160942554474
In grad_steps = 2929, loss = 0.27297210693359375
In grad_steps = 2930, loss = 0.05043327808380127
In grad_steps = 2931, loss = 0.17134645581245422
In grad_steps = 2932, loss = 0.15970352292060852
In grad_steps = 2933, loss = 0.3542777895927429
In grad_steps = 2934, loss = 0.24561363458633423
In grad_steps = 2935, loss = 0.03784593939781189
In grad_steps = 2936, loss = 0.25557103753089905
In grad_steps = 2937, loss = 0.031836412847042084
In grad_steps = 2938, loss = 0.03241867199540138
In grad_steps = 2939, loss = 0.050768062472343445
In grad_steps = 2940, loss = 0.1192920058965683
In grad_steps = 2941, loss = 0.31289491057395935
In grad_steps = 2942, loss = 1.190082311630249
In grad_steps = 2943, loss = 0.07230675965547562
In grad_steps = 2944, loss = 0.09491263329982758
In grad_steps = 2945, loss = 0.8045276403427124
In grad_steps = 2946, loss = 0.08273756504058838
In grad_steps = 2947, loss = 0.05097536742687225
In grad_steps = 2948, loss = 0.2970065772533417
In grad_steps = 2949, loss = 1.1615302562713623
In grad_steps = 2950, loss = 0.5729767680168152
In grad_steps = 2951, loss = 0.15622207522392273
In grad_steps = 2952, loss = 0.5585353374481201
In grad_steps = 2953, loss = 0.45057472586631775
In grad_steps = 2954, loss = 0.03327783942222595
In grad_steps = 2955, loss = 0.04023275896906853
In grad_steps = 2956, loss = 0.14209476113319397
In grad_steps = 2957, loss = 0.08807379007339478
In grad_steps = 2958, loss = 0.06076507270336151
In grad_steps = 2959, loss = 0.06900765746831894
In grad_steps = 2960, loss = 0.06858038157224655
In grad_steps = 2961, loss = 0.2964019775390625
In grad_steps = 2962, loss = 0.09581266343593597
In grad_steps = 2963, loss = 0.3229134678840637
In grad_steps = 2964, loss = 0.39248770475387573
In grad_steps = 2965, loss = 0.1093272715806961
In grad_steps = 2966, loss = 0.7736961245536804
In grad_steps = 2967, loss = 0.1121300607919693
In grad_steps = 2968, loss = 0.2704390287399292
In grad_steps = 2969, loss = 0.26479941606521606
In grad_steps = 2970, loss = 0.02607349306344986
In grad_steps = 2971, loss = 0.22550749778747559
In grad_steps = 2972, loss = 0.016663050279021263
In grad_steps = 2973, loss = 0.07270555943250656
In grad_steps = 2974, loss = 0.949147641658783
In grad_steps = 2975, loss = 0.1774771809577942
In grad_steps = 2976, loss = 0.025700442492961884
In grad_steps = 2977, loss = 0.029819291085004807
In grad_steps = 2978, loss = 0.30900681018829346
In grad_steps = 2979, loss = 0.7589790225028992
In grad_steps = 2980, loss = 0.011846652254462242
In grad_steps = 2981, loss = 0.22560766339302063
In grad_steps = 2982, loss = 0.044293347746133804
In grad_steps = 2983, loss = 0.027723871171474457
In grad_steps = 2984, loss = 0.046759508550167084
In grad_steps = 2985, loss = 0.1836215704679489
In grad_steps = 2986, loss = 0.26946812868118286
In grad_steps = 2987, loss = 0.021830670535564423
In grad_steps = 2988, loss = 0.05559242144227028
In grad_steps = 2989, loss = 0.03475349768996239
In grad_steps = 2990, loss = 0.12256835401058197
In grad_steps = 2991, loss = 0.01191119384020567
In grad_steps = 2992, loss = 0.7249256372451782
In grad_steps = 2993, loss = 0.11488588154315948
In grad_steps = 2994, loss = 0.01983635500073433
In grad_steps = 2995, loss = 0.035749804228544235
In grad_steps = 2996, loss = 0.14374388754367828
In grad_steps = 2997, loss = 0.3114851713180542
In grad_steps = 2998, loss = 1.297279715538025
In grad_steps = 2999, loss = 0.09328056871891022
In grad_steps = 3000, loss = 0.09587303549051285
In grad_steps = 3001, loss = 0.42642441391944885
In grad_steps = 3002, loss = 0.046936068683862686
In grad_steps = 3003, loss = 0.27925750613212585
In grad_steps = 3004, loss = 0.052357543259859085
In grad_steps = 3005, loss = 1.2886133193969727
In grad_steps = 3006, loss = 0.5474680066108704
In grad_steps = 3007, loss = 0.8070852160453796
In grad_steps = 3008, loss = 1.0303722620010376
In grad_steps = 3009, loss = 0.07529119402170181
In grad_steps = 3010, loss = 0.050325166434049606
In grad_steps = 3011, loss = 0.09108719229698181
In grad_steps = 3012, loss = 0.029164206236600876
In grad_steps = 3013, loss = 0.0803285464644432
In grad_steps = 3014, loss = 0.11319060623645782
In grad_steps = 3015, loss = 0.6417007446289062
In grad_steps = 3016, loss = 0.04153084382414818
In grad_steps = 3017, loss = 0.03527631238102913
In grad_steps = 3018, loss = 0.07789944112300873
In grad_steps = 3019, loss = 0.05017421394586563
In grad_steps = 3020, loss = 0.15978215634822845
In grad_steps = 3021, loss = 0.07241319864988327
In grad_steps = 3022, loss = 0.32023346424102783
In grad_steps = 3023, loss = 0.22418810427188873
In grad_steps = 3024, loss = 0.9369022250175476
In grad_steps = 3025, loss = 0.09475329518318176
In grad_steps = 3026, loss = 1.1447726488113403
In grad_steps = 3027, loss = 0.05462972819805145
In grad_steps = 3028, loss = 0.3911362290382385
In grad_steps = 3029, loss = 0.10428677499294281
In grad_steps = 3030, loss = 0.09287671744823456
In grad_steps = 3031, loss = 0.08501453697681427
In grad_steps = 3032, loss = 0.1479398012161255
In grad_steps = 3033, loss = 0.027047716081142426
In grad_steps = 3034, loss = 0.7001015543937683
In grad_steps = 3035, loss = 0.21015068888664246
In grad_steps = 3036, loss = 0.2325136959552765
In grad_steps = 3037, loss = 0.12692151963710785
In grad_steps = 3038, loss = 0.10041733086109161
In grad_steps = 3039, loss = 0.08854015171527863
In grad_steps = 3040, loss = 0.2954632639884949
In grad_steps = 3041, loss = 0.372413694858551
In grad_steps = 3042, loss = 0.027023132890462875
In grad_steps = 3043, loss = 0.46150633692741394
In grad_steps = 3044, loss = 0.41764986515045166
In grad_steps = 3045, loss = 0.15322253108024597
In grad_steps = 3046, loss = 0.057093098759651184
In grad_steps = 3047, loss = 0.04443873092532158
In grad_steps = 3048, loss = 0.43225857615470886
In grad_steps = 3049, loss = 0.14706754684448242
In grad_steps = 3050, loss = 0.1972755491733551
In grad_steps = 3051, loss = 0.05380051210522652
In grad_steps = 3052, loss = 0.3064132034778595
In grad_steps = 3053, loss = 0.5253190994262695
In grad_steps = 3054, loss = 0.031511060893535614
In grad_steps = 3055, loss = 0.09214578568935394
In grad_steps = 3056, loss = 0.042815931141376495
In grad_steps = 3057, loss = 0.04813374951481819
In grad_steps = 3058, loss = 0.03537099063396454
In grad_steps = 3059, loss = 0.22579406201839447
In grad_steps = 3060, loss = 0.02017812989652157
In grad_steps = 3061, loss = 0.018952053040266037
In grad_steps = 3062, loss = 0.053711555898189545
In grad_steps = 3063, loss = 0.027126889675855637
In grad_steps = 3064, loss = 0.10795392841100693
In grad_steps = 3065, loss = 0.7198960185050964
In grad_steps = 3066, loss = 1.0748889446258545
In grad_steps = 3067, loss = 1.004143238067627
In grad_steps = 3068, loss = 1.6142562627792358
In grad_steps = 3069, loss = 0.044184762984514236
In grad_steps = 3070, loss = 0.02281491830945015
In grad_steps = 3071, loss = 0.039918769150972366
In grad_steps = 3072, loss = 0.02257624641060829
In grad_steps = 3073, loss = 0.023129770532250404
In grad_steps = 3074, loss = 1.1028410196304321
In grad_steps = 3075, loss = 0.22567936778068542
In grad_steps = 3076, loss = 0.4155902564525604
In grad_steps = 3077, loss = 0.056599609553813934
In grad_steps = 3078, loss = 0.08880611509084702
In grad_steps = 3079, loss = 0.058933109045028687
In grad_steps = 3080, loss = 0.17346346378326416
In grad_steps = 3081, loss = 0.06138874217867851
In grad_steps = 3082, loss = 0.026918020099401474
In grad_steps = 3083, loss = 0.3939376771450043
In grad_steps = 3084, loss = 0.3431665599346161
In grad_steps = 3085, loss = 0.511221170425415
In grad_steps = 3086, loss = 0.7403333783149719
In grad_steps = 3087, loss = 0.06652811169624329
In grad_steps = 3088, loss = 0.35363221168518066
In grad_steps = 3089, loss = 0.0779644027352333
In grad_steps = 3090, loss = 0.05842713639140129
In grad_steps = 3091, loss = 0.3742123246192932
In grad_steps = 3092, loss = 0.1314062923192978
In grad_steps = 3093, loss = 0.42035138607025146
In grad_steps = 3094, loss = 0.1206110343337059
In grad_steps = 3095, loss = 0.07563425600528717
In grad_steps = 3096, loss = 0.06093382090330124
In grad_steps = 3097, loss = 0.5787163376808167
In grad_steps = 3098, loss = 0.6647010445594788
In grad_steps = 3099, loss = 0.23456338047981262
In grad_steps = 3100, loss = 0.2246742695569992
In grad_steps = 3101, loss = 0.10180608928203583
In grad_steps = 3102, loss = 0.36715957522392273
In grad_steps = 3103, loss = 0.2844933271408081
In grad_steps = 3104, loss = 0.05964353680610657
In grad_steps = 3105, loss = 0.08054014295339584
In grad_steps = 3106, loss = 0.044940728694200516
In grad_steps = 3107, loss = 0.3072541654109955
In grad_steps = 3108, loss = 0.3538980782032013
In grad_steps = 3109, loss = 0.05395185947418213
In grad_steps = 3110, loss = 0.10801156610250473
In grad_steps = 3111, loss = 0.756169855594635
In grad_steps = 3112, loss = 0.26376572251319885
In grad_steps = 3113, loss = 0.14270152151584625
In grad_steps = 3114, loss = 0.31081724166870117
In grad_steps = 3115, loss = 0.16782255470752716
In grad_steps = 3116, loss = 0.047903791069984436
In grad_steps = 3117, loss = 0.3164159953594208
In grad_steps = 3118, loss = 0.7770933508872986
In grad_steps = 3119, loss = 0.018663322553038597
In grad_steps = 3120, loss = 0.05514248460531235
In grad_steps = 3121, loss = 0.08973775058984756
In grad_steps = 3122, loss = 0.7306197881698608
In grad_steps = 3123, loss = 0.07096219062805176
In grad_steps = 3124, loss = 0.05843197926878929
In grad_steps = 3125, loss = 0.08767643570899963
In grad_steps = 3126, loss = 1.3891299962997437
In grad_steps = 3127, loss = 0.017287567257881165
In grad_steps = 3128, loss = 0.11349022388458252
In grad_steps = 3129, loss = 0.0927349105477333
In grad_steps = 3130, loss = 0.020144769921898842
In grad_steps = 3131, loss = 0.10929291695356369
In grad_steps = 3132, loss = 0.18306425213813782
In grad_steps = 3133, loss = 0.04736271873116493
In grad_steps = 3134, loss = 0.03499585762619972
In grad_steps = 3135, loss = 0.04179682955145836
In grad_steps = 3136, loss = 0.49108392000198364
In grad_steps = 3137, loss = 0.2975507974624634
In grad_steps = 3138, loss = 0.02035512588918209
In grad_steps = 3139, loss = 0.065347820520401
In grad_steps = 3140, loss = 0.23852914571762085
In grad_steps = 3141, loss = 0.2826281487941742
In grad_steps = 3142, loss = 0.1547672152519226
In grad_steps = 3143, loss = 0.052851736545562744
In grad_steps = 3144, loss = 0.06780340522527695
In grad_steps = 3145, loss = 0.023098580539226532
In grad_steps = 3146, loss = 0.5433817505836487
In grad_steps = 3147, loss = 1.5993740558624268
In grad_steps = 3148, loss = 0.5687635540962219
In grad_steps = 3149, loss = 0.25958800315856934
In grad_steps = 3150, loss = 0.6913630366325378
In grad_steps = 3151, loss = 0.04512423276901245
In grad_steps = 3152, loss = 0.3470022976398468
In grad_steps = 3153, loss = 0.28413188457489014
In grad_steps = 3154, loss = 0.2506970167160034
In grad_steps = 3155, loss = 0.13336686789989471
In grad_steps = 3156, loss = 0.05003921687602997
In grad_steps = 3157, loss = 0.03262947499752045
In grad_steps = 3158, loss = 0.18190738558769226
In grad_steps = 3159, loss = 0.05824851989746094
In grad_steps = 3160, loss = 0.06487429887056351
In grad_steps = 3161, loss = 0.13682644069194794
In grad_steps = 3162, loss = 0.042617958039045334
In grad_steps = 3163, loss = 0.840660810470581
In grad_steps = 3164, loss = 0.03154522180557251
In grad_steps = 3165, loss = 0.4427623450756073
In grad_steps = 3166, loss = 0.09629879891872406
In grad_steps = 3167, loss = 0.05606071278452873
In grad_steps = 3168, loss = 0.13309216499328613
In grad_steps = 3169, loss = 0.5644606351852417
In grad_steps = 3170, loss = 0.032888688147068024
In grad_steps = 3171, loss = 0.1462874412536621
In grad_steps = 3172, loss = 0.04862910509109497
In grad_steps = 3173, loss = 0.027856115251779556
In grad_steps = 3174, loss = 0.0812022015452385
In grad_steps = 3175, loss = 0.21257290244102478
In grad_steps = 3176, loss = 0.14370988309383392
In grad_steps = 3177, loss = 0.023260455578565598
In grad_steps = 3178, loss = 0.02832518145442009
In grad_steps = 3179, loss = 0.6819177269935608
In grad_steps = 3180, loss = 0.5903135538101196
In grad_steps = 3181, loss = 0.39710533618927
In grad_steps = 3182, loss = 0.3315081000328064
In grad_steps = 3183, loss = 0.6401517391204834
In grad_steps = 3184, loss = 0.043351296335458755
In grad_steps = 3185, loss = 0.81600022315979
In grad_steps = 3186, loss = 0.7552490830421448
In grad_steps = 3187, loss = 0.07752454280853271
In grad_steps = 3188, loss = 1.4573556184768677
In grad_steps = 3189, loss = 0.12734520435333252
In grad_steps = 3190, loss = 0.06455209106206894
In grad_steps = 3191, loss = 0.14954569935798645
In grad_steps = 3192, loss = 0.15066362917423248
In grad_steps = 3193, loss = 0.0590120330452919
In grad_steps = 3194, loss = 0.7003726363182068
In grad_steps = 3195, loss = 0.06289557367563248
In grad_steps = 3196, loss = 1.2546007633209229
In grad_steps = 3197, loss = 0.6314606070518494
In grad_steps = 3198, loss = 0.04506738483905792
In grad_steps = 3199, loss = 0.5776739120483398
In grad_steps = 3200, loss = 0.22541311383247375
In grad_steps = 3201, loss = 0.8469880819320679
In grad_steps = 3202, loss = 0.6555483937263489
In grad_steps = 3203, loss = 0.5736083984375
In grad_steps = 3204, loss = 0.5645508766174316
In grad_steps = 3205, loss = 0.42961961030960083
In grad_steps = 3206, loss = 0.27037620544433594
In grad_steps = 3207, loss = 0.19244828820228577
In grad_steps = 3208, loss = 0.6359772682189941
In grad_steps = 3209, loss = 0.13918466866016388
In grad_steps = 3210, loss = 0.3385297656059265
In grad_steps = 3211, loss = 0.6453690528869629
In grad_steps = 3212, loss = 0.2656034529209137
In grad_steps = 3213, loss = 0.25217923521995544
In grad_steps = 3214, loss = 0.2950683534145355
In grad_steps = 3215, loss = 0.12705352902412415
In grad_steps = 3216, loss = 0.2353385090827942
In grad_steps = 3217, loss = 0.24591350555419922
In grad_steps = 3218, loss = 0.2541636824607849
In grad_steps = 3219, loss = 0.1390553116798401
In grad_steps = 3220, loss = 0.1730184406042099
In grad_steps = 3221, loss = 0.2496582269668579
In grad_steps = 3222, loss = 0.23547101020812988
In grad_steps = 3223, loss = 0.17704719305038452
In grad_steps = 3224, loss = 0.16928890347480774
In grad_steps = 3225, loss = 0.2735072374343872
In grad_steps = 3226, loss = 0.2227005958557129
In grad_steps = 3227, loss = 0.09359695762395859
In grad_steps = 3228, loss = 0.27193254232406616
In grad_steps = 3229, loss = 0.05149839445948601
In grad_steps = 3230, loss = 0.014581768773496151
In grad_steps = 3231, loss = 0.02978399395942688
In grad_steps = 3232, loss = 0.24368293583393097
In grad_steps = 3233, loss = 0.023167923092842102
In grad_steps = 3234, loss = 0.6000494360923767
In grad_steps = 3235, loss = 0.02863158844411373
In grad_steps = 3236, loss = 0.05201893672347069
In grad_steps = 3237, loss = 0.007651288062334061
In grad_steps = 3238, loss = 0.018732532858848572
In grad_steps = 3239, loss = 0.12198875844478607
In grad_steps = 3240, loss = 0.0069006518460810184
In grad_steps = 3241, loss = 0.2957705557346344
In grad_steps = 3242, loss = 1.1953786611557007
In grad_steps = 3243, loss = 0.1494426280260086
In grad_steps = 3244, loss = 0.0056716641411185265
In grad_steps = 3245, loss = 1.6528644561767578
In grad_steps = 3246, loss = 0.03370951861143112
In grad_steps = 3247, loss = 0.03699305281043053
In grad_steps = 3248, loss = 0.06705082207918167
In grad_steps = 3249, loss = 0.1775553673505783
In grad_steps = 3250, loss = 0.30433571338653564
In grad_steps = 3251, loss = 0.8861263990402222
In grad_steps = 3252, loss = 0.09163369983434677
In grad_steps = 3253, loss = 0.1597577929496765
In grad_steps = 3254, loss = 0.13797511160373688
In grad_steps = 3255, loss = 0.11406926810741425
In grad_steps = 3256, loss = 0.8878661394119263
In grad_steps = 3257, loss = 0.25785601139068604
In grad_steps = 3258, loss = 0.175853431224823
In grad_steps = 3259, loss = 0.16362597048282623
In grad_steps = 3260, loss = 0.05845595896244049
In grad_steps = 3261, loss = 0.0738295465707779
In grad_steps = 3262, loss = 0.33220720291137695
In grad_steps = 3263, loss = 0.04052317887544632
In grad_steps = 3264, loss = 0.06540337204933167
In grad_steps = 3265, loss = 0.159719318151474
In grad_steps = 3266, loss = 0.5623552203178406
In grad_steps = 3267, loss = 0.6279038190841675
In grad_steps = 3268, loss = 0.1626349836587906
In grad_steps = 3269, loss = 0.2528223991394043
In grad_steps = 3270, loss = 0.05428935959935188
In grad_steps = 3271, loss = 0.76173335313797
In grad_steps = 3272, loss = 1.2118879556655884
In grad_steps = 3273, loss = 0.9315319061279297
In grad_steps = 3274, loss = 0.4455433785915375
In grad_steps = 3275, loss = 0.027813000604510307
In grad_steps = 3276, loss = 1.1654715538024902
In grad_steps = 3277, loss = 0.9378601312637329
In grad_steps = 3278, loss = 0.09423251450061798
In grad_steps = 3279, loss = 0.24157994985580444
In grad_steps = 3280, loss = 0.44999897480010986
In grad_steps = 3281, loss = 0.8668652176856995
In grad_steps = 3282, loss = 0.12668734788894653
In grad_steps = 3283, loss = 0.26563888788223267
In grad_steps = 3284, loss = 0.26717042922973633
In grad_steps = 3285, loss = 0.5425282716751099
In grad_steps = 3286, loss = 0.5970804691314697
In grad_steps = 3287, loss = 0.09990929067134857
In grad_steps = 3288, loss = 0.24712686240673065
In grad_steps = 3289, loss = 0.19767434895038605
In grad_steps = 3290, loss = 0.4952731430530548
In grad_steps = 3291, loss = 0.23726902902126312
In grad_steps = 3292, loss = 0.32351094484329224
In grad_steps = 3293, loss = 0.29769662022590637
In grad_steps = 3294, loss = 0.21541865170001984
In grad_steps = 3295, loss = 0.13905900716781616
In grad_steps = 3296, loss = 0.36979812383651733
In grad_steps = 3297, loss = 0.22026276588439941
In grad_steps = 3298, loss = 0.08230666071176529
In grad_steps = 3299, loss = 0.43412142992019653
In grad_steps = 3300, loss = 0.4456391930580139
In grad_steps = 3301, loss = 0.44769757986068726
In grad_steps = 3302, loss = 0.5740818977355957
In grad_steps = 3303, loss = 0.05392918363213539
In grad_steps = 3304, loss = 0.5888190865516663
In grad_steps = 3305, loss = 0.17983411252498627
In grad_steps = 3306, loss = 0.352301687002182
In grad_steps = 3307, loss = 0.2392491102218628
In grad_steps = 3308, loss = 0.35712820291519165
In grad_steps = 3309, loss = 0.3247283399105072
In grad_steps = 3310, loss = 0.23426470160484314
In grad_steps = 3311, loss = 0.10605302453041077
In grad_steps = 3312, loss = 0.07620023190975189
In grad_steps = 3313, loss = 0.08736201375722885
In grad_steps = 3314, loss = 0.0362333208322525
In grad_steps = 3315, loss = 0.08175930380821228
In grad_steps = 3316, loss = 0.39274540543556213
In grad_steps = 3317, loss = 0.03379339352250099
In grad_steps = 3318, loss = 0.5658138394355774
In grad_steps = 3319, loss = 0.0482640378177166
In grad_steps = 3320, loss = 0.3489753007888794
In grad_steps = 3321, loss = 0.0221671424806118
In grad_steps = 3322, loss = 0.07292104512453079
In grad_steps = 3323, loss = 0.1751764416694641
In grad_steps = 3324, loss = 0.03197458013892174
In grad_steps = 3325, loss = 0.015113646164536476
In grad_steps = 3326, loss = 0.3555239737033844
In grad_steps = 3327, loss = 0.007399789057672024
In grad_steps = 3328, loss = 0.275748074054718
In grad_steps = 3329, loss = 0.03175480291247368
In grad_steps = 3330, loss = 0.024421323090791702
In grad_steps = 3331, loss = 0.013606658205389977
In grad_steps = 3332, loss = 0.06685971468687057
In grad_steps = 3333, loss = 0.2048364132642746
In grad_steps = 3334, loss = 0.035806410014629364
In grad_steps = 3335, loss = 0.0222398079931736
In grad_steps = 3336, loss = 0.004154594149440527
In grad_steps = 3337, loss = 0.002158096991479397
In grad_steps = 3338, loss = 0.01135426014661789
In grad_steps = 3339, loss = 0.12448915839195251
In grad_steps = 3340, loss = 0.05779311805963516
In grad_steps = 3341, loss = 0.08901750296354294
In grad_steps = 3342, loss = 0.01190456748008728
In grad_steps = 3343, loss = 0.004694617819041014
In grad_steps = 3344, loss = 0.585395872592926
In grad_steps = 3345, loss = 0.0714988261461258
In grad_steps = 3346, loss = 0.022215869277715683
In grad_steps = 3347, loss = 1.394407868385315
In grad_steps = 3348, loss = 0.0017387601546943188
In grad_steps = 3349, loss = 0.04617531597614288
In grad_steps = 3350, loss = 0.072930246591568
In grad_steps = 3351, loss = 0.009307368658483028
In grad_steps = 3352, loss = 0.02682504616677761
In grad_steps = 3353, loss = 0.05864748731255531
In grad_steps = 3354, loss = 0.16291259229183197
In grad_steps = 3355, loss = 0.02411622926592827
In grad_steps = 3356, loss = 0.19689805805683136
In grad_steps = 3357, loss = 0.02898206003010273
In grad_steps = 3358, loss = 0.02793557569384575
In grad_steps = 3359, loss = 0.3826685845851898
In grad_steps = 3360, loss = 0.019626904278993607
In grad_steps = 3361, loss = 0.007945103570818901
In grad_steps = 3362, loss = 0.01714550144970417
In grad_steps = 3363, loss = 0.0032973478082567453
In grad_steps = 3364, loss = 0.07217651605606079
In grad_steps = 3365, loss = 0.6564353704452515
In grad_steps = 3366, loss = 0.16963841021060944
In grad_steps = 3367, loss = 0.058075614273548126
In grad_steps = 3368, loss = 0.19292211532592773
In grad_steps = 3369, loss = 0.012348812073469162
In grad_steps = 3370, loss = 0.28073763847351074
In grad_steps = 3371, loss = 0.24652411043643951
In grad_steps = 3372, loss = 0.6509087681770325
In grad_steps = 3373, loss = 0.8054639101028442
In grad_steps = 3374, loss = 0.188423752784729
In grad_steps = 3375, loss = 0.3892818093299866
In grad_steps = 3376, loss = 0.19391612708568573
In grad_steps = 3377, loss = 0.7419638633728027
In grad_steps = 3378, loss = 0.6517311334609985
In grad_steps = 3379, loss = 0.4981094300746918
In grad_steps = 3380, loss = 0.1690162718296051
In grad_steps = 3381, loss = 0.1452159434556961
In grad_steps = 3382, loss = 0.06635479629039764
In grad_steps = 3383, loss = 0.8944928050041199
In grad_steps = 3384, loss = 0.6862501502037048
In grad_steps = 3385, loss = 0.01585409790277481
In grad_steps = 3386, loss = 0.04686643183231354
In grad_steps = 3387, loss = 0.07813999801874161
In grad_steps = 3388, loss = 0.1400279402732849
In grad_steps = 3389, loss = 0.036936771124601364
In grad_steps = 3390, loss = 0.7645289897918701
In grad_steps = 3391, loss = 0.09274880588054657
In grad_steps = 3392, loss = 0.268647164106369
In grad_steps = 3393, loss = 1.0149773359298706
In grad_steps = 3394, loss = 0.014237891882658005
In grad_steps = 3395, loss = 0.08761874586343765
In grad_steps = 3396, loss = 0.061844781041145325
In grad_steps = 3397, loss = 0.3984690308570862
In grad_steps = 3398, loss = 0.09252378344535828
In grad_steps = 3399, loss = 0.22970299422740936
In grad_steps = 3400, loss = 0.06891956180334091
In grad_steps = 3401, loss = 0.08212690055370331
In grad_steps = 3402, loss = 0.20228323340415955
In grad_steps = 3403, loss = 0.886268675327301
In grad_steps = 3404, loss = 0.18543213605880737
In grad_steps = 3405, loss = 0.8471378087997437
In grad_steps = 3406, loss = 0.07233662903308868
In grad_steps = 3407, loss = 0.18178145587444305
In grad_steps = 3408, loss = 0.5375176072120667
In grad_steps = 3409, loss = 0.30318549275398254
In grad_steps = 3410, loss = 0.10963498055934906
In grad_steps = 3411, loss = 0.24106618762016296
In grad_steps = 3412, loss = 0.039981089532375336
In grad_steps = 3413, loss = 0.08410963416099548
In grad_steps = 3414, loss = 0.07033119350671768
In grad_steps = 3415, loss = 0.028987005352973938
In grad_steps = 3416, loss = 0.8220065236091614
In grad_steps = 3417, loss = 0.032651811838150024
In grad_steps = 3418, loss = 0.09678738564252853
In grad_steps = 3419, loss = 0.2337203323841095
In grad_steps = 3420, loss = 0.09238338470458984
In grad_steps = 3421, loss = 0.01688709296286106
In grad_steps = 3422, loss = 0.249868243932724
In grad_steps = 3423, loss = 0.4943300485610962
In grad_steps = 3424, loss = 0.5071578621864319
In grad_steps = 3425, loss = 0.15351389348506927
In grad_steps = 3426, loss = 0.9333102703094482
In grad_steps = 3427, loss = 0.1461016833782196
In grad_steps = 3428, loss = 0.06300606578588486
In grad_steps = 3429, loss = 0.2427084892988205
In grad_steps = 3430, loss = 0.1665547639131546
In grad_steps = 3431, loss = 0.4752011299133301
In grad_steps = 3432, loss = 0.5584812760353088
In grad_steps = 3433, loss = 0.10240726172924042
In grad_steps = 3434, loss = 0.1836472451686859
In grad_steps = 3435, loss = 0.9602039456367493
In grad_steps = 3436, loss = 0.08874013274908066
In grad_steps = 3437, loss = 0.08490897715091705
In grad_steps = 3438, loss = 0.09598775207996368
In grad_steps = 3439, loss = 0.0676247775554657
In grad_steps = 3440, loss = 0.11042675375938416
In grad_steps = 3441, loss = 0.9656481146812439
In grad_steps = 3442, loss = 0.3788273334503174
In grad_steps = 3443, loss = 0.07752034813165665
In grad_steps = 3444, loss = 0.24061697721481323
In grad_steps = 3445, loss = 0.32443758845329285
In grad_steps = 3446, loss = 0.22922922670841217
In grad_steps = 3447, loss = 0.07407567650079727
In grad_steps = 3448, loss = 0.07254936546087265
In grad_steps = 3449, loss = 0.07432986795902252
In grad_steps = 3450, loss = 0.10936035215854645
In grad_steps = 3451, loss = 0.14172106981277466
In grad_steps = 3452, loss = 0.2323400378227234
In grad_steps = 3453, loss = 0.498879998922348
In grad_steps = 3454, loss = 0.019243741407990456
In grad_steps = 3455, loss = 0.07273135334253311
In grad_steps = 3456, loss = 0.6735969185829163
In grad_steps = 3457, loss = 0.03436047211289406
In grad_steps = 3458, loss = 0.025692783296108246
In grad_steps = 3459, loss = 0.5094760656356812
In grad_steps = 3460, loss = 0.47442805767059326
In grad_steps = 3461, loss = 0.12035272270441055
In grad_steps = 3462, loss = 0.2187730371952057
In grad_steps = 3463, loss = 0.06461229175329208
In grad_steps = 3464, loss = 0.04332974925637245
In grad_steps = 3465, loss = 0.18154694139957428
In grad_steps = 3466, loss = 0.09984169900417328
In grad_steps = 3467, loss = 0.7517346143722534
In grad_steps = 3468, loss = 0.03303902968764305
In grad_steps = 3469, loss = 0.0938364565372467
In grad_steps = 3470, loss = 0.37794730067253113
In grad_steps = 3471, loss = 0.12903569638729095
In grad_steps = 3472, loss = 0.0917338952422142
In grad_steps = 3473, loss = 0.02494077757000923
In grad_steps = 3474, loss = 0.03546496108174324
In grad_steps = 3475, loss = 0.7335245609283447
In grad_steps = 3476, loss = 0.05043433606624603
In grad_steps = 3477, loss = 0.29370254278182983
In grad_steps = 3478, loss = 0.026172732934355736
In grad_steps = 3479, loss = 0.0224543958902359
In grad_steps = 3480, loss = 0.1221516951918602
In grad_steps = 3481, loss = 0.050749264657497406
In grad_steps = 3482, loss = 0.10457673668861389
In grad_steps = 3483, loss = 0.731698215007782
In grad_steps = 3484, loss = 0.14669281244277954
In grad_steps = 3485, loss = 0.0509660430252552
In grad_steps = 3486, loss = 0.088981993496418
In grad_steps = 3487, loss = 0.95528644323349
In grad_steps = 3488, loss = 0.05119554325938225
In grad_steps = 3489, loss = 0.025769559666514397
In grad_steps = 3490, loss = 0.9166691303253174
In grad_steps = 3491, loss = 0.5682714581489563
In grad_steps = 3492, loss = 0.020178133621811867
In grad_steps = 3493, loss = 0.340416818857193
In grad_steps = 3494, loss = 0.11047561466693878
In grad_steps = 3495, loss = 0.7174323797225952
In grad_steps = 3496, loss = 0.11958572268486023
In grad_steps = 3497, loss = 0.0689728856086731
In grad_steps = 3498, loss = 0.17876695096492767
In grad_steps = 3499, loss = 0.1324118673801422
In grad_steps = 3500, loss = 0.1649429053068161
In grad_steps = 3501, loss = 0.6385412216186523
In grad_steps = 3502, loss = 0.2582174241542816
In grad_steps = 3503, loss = 0.0795869529247284
In grad_steps = 3504, loss = 0.4791652262210846
In grad_steps = 3505, loss = 0.715727686882019
In grad_steps = 3506, loss = 0.42778074741363525
In grad_steps = 3507, loss = 0.053428683429956436
In grad_steps = 3508, loss = 0.2700512707233429
In grad_steps = 3509, loss = 0.24177035689353943
In grad_steps = 3510, loss = 0.024732815101742744
In grad_steps = 3511, loss = 0.30398550629615784
In grad_steps = 3512, loss = 0.023190714418888092
In grad_steps = 3513, loss = 0.2537567913532257
In grad_steps = 3514, loss = 0.224425807595253
In grad_steps = 3515, loss = 0.07387538254261017
In grad_steps = 3516, loss = 0.03421228379011154
In grad_steps = 3517, loss = 0.01725338026881218
In grad_steps = 3518, loss = 0.046918101608753204
In grad_steps = 3519, loss = 0.1424916833639145
In grad_steps = 3520, loss = 0.03301733732223511
In grad_steps = 3521, loss = 0.32648801803588867
In grad_steps = 3522, loss = 0.8585407733917236
In grad_steps = 3523, loss = 0.5920670032501221
In grad_steps = 3524, loss = 0.045217081904411316
In grad_steps = 3525, loss = 0.011149037629365921
In grad_steps = 3526, loss = 0.12897749245166779
In grad_steps = 3527, loss = 0.2105831354856491
In grad_steps = 3528, loss = 0.11185643076896667
In grad_steps = 3529, loss = 0.18985676765441895
In grad_steps = 3530, loss = 0.13899259269237518
In grad_steps = 3531, loss = 0.02143010124564171
In grad_steps = 3532, loss = 0.02246376872062683
In grad_steps = 3533, loss = 0.016353990882635117
In grad_steps = 3534, loss = 0.6190625429153442
In grad_steps = 3535, loss = 0.16477090120315552
In grad_steps = 3536, loss = 0.000652775343041867
In grad_steps = 3537, loss = 0.03503371775150299
In grad_steps = 3538, loss = 0.39542895555496216
In grad_steps = 3539, loss = 0.03073893114924431
In grad_steps = 3540, loss = 0.12039920687675476
In grad_steps = 3541, loss = 0.024314619600772858
In grad_steps = 3542, loss = 0.250561386346817
In grad_steps = 3543, loss = 0.09186342358589172
In grad_steps = 3544, loss = 0.05381381884217262
In grad_steps = 3545, loss = 0.018193084746599197
In grad_steps = 3546, loss = 0.19417430460453033
In grad_steps = 3547, loss = 0.06227288767695427
In grad_steps = 3548, loss = 0.5140084624290466
In grad_steps = 3549, loss = 0.25441691279411316
In grad_steps = 3550, loss = 0.05804768204689026
In grad_steps = 3551, loss = 0.009366106241941452
In grad_steps = 3552, loss = 0.07687133550643921
In grad_steps = 3553, loss = 0.21061132848262787
In grad_steps = 3554, loss = 0.18539635837078094
In grad_steps = 3555, loss = 0.5252583622932434
In grad_steps = 3556, loss = 0.2193358689546585
In grad_steps = 3557, loss = 0.006344168446958065
In grad_steps = 3558, loss = 0.0056986380368471146
In grad_steps = 3559, loss = 0.9606009125709534
In grad_steps = 3560, loss = 0.015900615602731705
In grad_steps = 3561, loss = 0.2150937020778656
In grad_steps = 3562, loss = 0.3278401792049408
In grad_steps = 3563, loss = 0.008995460346341133
In grad_steps = 3564, loss = 0.0034656377974897623
In grad_steps = 3565, loss = 0.004345989786088467
In grad_steps = 3566, loss = 0.5124384164810181
In grad_steps = 3567, loss = 0.2005147933959961
In grad_steps = 3568, loss = 0.5151820778846741
In grad_steps = 3569, loss = 0.160706028342247
In grad_steps = 3570, loss = 0.074064239859581
In grad_steps = 3571, loss = 0.2568005323410034
In grad_steps = 3572, loss = 0.2744811177253723
In grad_steps = 3573, loss = 0.3775559365749359
In grad_steps = 3574, loss = 0.1210336983203888
In grad_steps = 3575, loss = 0.04427460953593254
In grad_steps = 3576, loss = 0.4674272835254669
In grad_steps = 3577, loss = 0.008870507590472698
In grad_steps = 3578, loss = 0.1479882448911667
In grad_steps = 3579, loss = 0.181134432554245
In grad_steps = 3580, loss = 0.05572940409183502
In grad_steps = 3581, loss = 0.08872527629137039
In grad_steps = 3582, loss = 0.05210019275546074
In grad_steps = 3583, loss = 0.0666334480047226
In grad_steps = 3584, loss = 0.4218333661556244
In grad_steps = 3585, loss = 0.10702449083328247
In grad_steps = 3586, loss = 0.04495811089873314
In grad_steps = 3587, loss = 0.23736263811588287
In grad_steps = 3588, loss = 0.25897228717803955
In grad_steps = 3589, loss = 0.0935409665107727
In grad_steps = 3590, loss = 0.4688892960548401
In grad_steps = 3591, loss = 0.5600356459617615
In grad_steps = 3592, loss = 0.32154515385627747
In grad_steps = 3593, loss = 0.9349204301834106
In grad_steps = 3594, loss = 0.8370519876480103
In grad_steps = 3595, loss = 0.021140094846487045
In grad_steps = 3596, loss = 1.0709304809570312
In grad_steps = 3597, loss = 0.16831240057945251
In grad_steps = 3598, loss = 0.044611405581235886
In grad_steps = 3599, loss = 0.5089757442474365
In grad_steps = 3600, loss = 0.1160607635974884
In grad_steps = 3601, loss = 0.12545306980609894
In grad_steps = 3602, loss = 0.1639057844877243
In grad_steps = 3603, loss = 0.20799045264720917
In grad_steps = 3604, loss = 0.24412447214126587
In grad_steps = 3605, loss = 0.1401187628507614
In grad_steps = 3606, loss = 0.2486787885427475
In grad_steps = 3607, loss = 0.08391043543815613
In grad_steps = 3608, loss = 0.06391947716474533
In grad_steps = 3609, loss = 0.2477501779794693
In grad_steps = 3610, loss = 0.009808281436562538
In grad_steps = 3611, loss = 0.17522290349006653
In grad_steps = 3612, loss = 0.1783359795808792
In grad_steps = 3613, loss = 0.12789028882980347
In grad_steps = 3614, loss = 0.09940986335277557
In grad_steps = 3615, loss = 0.0161786749958992
In grad_steps = 3616, loss = 0.0831771194934845
In grad_steps = 3617, loss = 0.3468337059020996
In grad_steps = 3618, loss = 0.11071894317865372
In grad_steps = 3619, loss = 0.28875353932380676
In grad_steps = 3620, loss = 0.0646684542298317
In grad_steps = 3621, loss = 0.13866108655929565
In grad_steps = 3622, loss = 0.04238820821046829
In grad_steps = 3623, loss = 0.0709003284573555
In grad_steps = 3624, loss = 0.07918713241815567
In grad_steps = 3625, loss = 0.004052804782986641
In grad_steps = 3626, loss = 0.006482102908194065
In grad_steps = 3627, loss = 0.035711370408535004
In grad_steps = 3628, loss = 0.0031320250127464533
In grad_steps = 3629, loss = 0.4633476436138153
In grad_steps = 3630, loss = 0.6855559349060059
In grad_steps = 3631, loss = 0.005393310450017452
In grad_steps = 3632, loss = 0.0024913428351283073
In grad_steps = 3633, loss = 0.0035270554944872856
In grad_steps = 3634, loss = 0.003572197165340185
In grad_steps = 3635, loss = 0.014275777153670788
In grad_steps = 3636, loss = 0.012716138735413551
In grad_steps = 3637, loss = 0.0048624491319060326
In grad_steps = 3638, loss = 1.097357153892517
In grad_steps = 3639, loss = 0.46126651763916016
In grad_steps = 3640, loss = 0.4391768276691437
In grad_steps = 3641, loss = 0.12661708891391754
In grad_steps = 3642, loss = 0.033594440668821335
In grad_steps = 3643, loss = 0.0360739603638649
In grad_steps = 3644, loss = 0.021329181268811226
In grad_steps = 3645, loss = 0.27354925870895386
In grad_steps = 3646, loss = 0.017518920823931694
In grad_steps = 3647, loss = 0.24358151853084564
In grad_steps = 3648, loss = 0.06125912442803383
In grad_steps = 3649, loss = 0.3340163230895996
In grad_steps = 3650, loss = 0.3864211142063141
In grad_steps = 3651, loss = 0.025262722745537758
In grad_steps = 3652, loss = 0.036423955112695694
In grad_steps = 3653, loss = 0.07065349817276001
In grad_steps = 3654, loss = 0.004932643845677376
In grad_steps = 3655, loss = 0.1037038043141365
In grad_steps = 3656, loss = 0.05225814878940582
In grad_steps = 3657, loss = 0.37962013483047485
In grad_steps = 3658, loss = 0.009745772927999496
In grad_steps = 3659, loss = 0.08566203713417053
In grad_steps = 3660, loss = 0.022002199664711952
In grad_steps = 3661, loss = 0.09488874673843384
In grad_steps = 3662, loss = 0.42060723900794983
In grad_steps = 3663, loss = 0.08715419471263885
In grad_steps = 3664, loss = 0.005733382422477007
In grad_steps = 3665, loss = 0.21129944920539856
In grad_steps = 3666, loss = 0.01531166024506092
In grad_steps = 3667, loss = 0.13634973764419556
In grad_steps = 3668, loss = 0.12646114826202393
In grad_steps = 3669, loss = 0.6064214706420898
In grad_steps = 3670, loss = 0.722934901714325
In grad_steps = 3671, loss = 0.59840989112854
In grad_steps = 3672, loss = 0.014252888970077038
In grad_steps = 3673, loss = 0.9657003879547119
In grad_steps = 3674, loss = 0.09226725995540619
In grad_steps = 3675, loss = 0.7638945579528809
In grad_steps = 3676, loss = 0.19065147638320923
In grad_steps = 3677, loss = 0.06752157211303711
In grad_steps = 3678, loss = 0.43362540006637573
In grad_steps = 3679, loss = 0.04000337794423103
In grad_steps = 3680, loss = 0.02756914682686329
In grad_steps = 3681, loss = 1.0621534585952759
In grad_steps = 3682, loss = 0.02242756262421608
In grad_steps = 3683, loss = 0.10237760841846466
In grad_steps = 3684, loss = 0.11456452310085297
In grad_steps = 3685, loss = 0.32397937774658203
In grad_steps = 3686, loss = 0.043873779475688934
In grad_steps = 3687, loss = 0.096612848341465
In grad_steps = 3688, loss = 0.05725008621811867
In grad_steps = 3689, loss = 0.029789913445711136
In grad_steps = 3690, loss = 0.2976694405078888
In grad_steps = 3691, loss = 0.8254716992378235
In grad_steps = 3692, loss = 0.4846680462360382
In grad_steps = 3693, loss = 0.792499303817749
In grad_steps = 3694, loss = 0.12066945433616638
In grad_steps = 3695, loss = 0.09951262176036835
In grad_steps = 3696, loss = 0.07672204822301865
In grad_steps = 3697, loss = 0.1970083862543106
In grad_steps = 3698, loss = 0.0794883593916893
In grad_steps = 3699, loss = 0.06179768219590187
In grad_steps = 3700, loss = 0.19336479902267456
In grad_steps = 3701, loss = 0.09039687365293503
In grad_steps = 3702, loss = 0.033235128968954086
In grad_steps = 3703, loss = 0.07840602844953537
In grad_steps = 3704, loss = 0.12329661101102829
In grad_steps = 3705, loss = 0.22591647505760193
In grad_steps = 3706, loss = 0.05072402581572533
In grad_steps = 3707, loss = 0.3702530264854431
In grad_steps = 3708, loss = 0.3622501492500305
In grad_steps = 3709, loss = 0.02485007792711258
In grad_steps = 3710, loss = 0.13485421240329742
In grad_steps = 3711, loss = 0.20809808373451233
In grad_steps = 3712, loss = 0.11170168966054916
In grad_steps = 3713, loss = 0.7599021196365356
In grad_steps = 3714, loss = 0.01645389199256897
In grad_steps = 3715, loss = 0.010715223848819733
In grad_steps = 3716, loss = 0.00986404623836279
In grad_steps = 3717, loss = 0.0244151558727026
In grad_steps = 3718, loss = 0.016063231974840164
In grad_steps = 3719, loss = 0.15931758284568787
In grad_steps = 3720, loss = 1.3463102579116821
In grad_steps = 3721, loss = 0.9484847187995911
In grad_steps = 3722, loss = 0.0986267700791359
In grad_steps = 3723, loss = 0.1184241771697998
In grad_steps = 3724, loss = 0.2276906669139862
In grad_steps = 3725, loss = 1.2547519207000732
In grad_steps = 3726, loss = 0.04638677090406418
In grad_steps = 3727, loss = 0.012084392830729485
In grad_steps = 3728, loss = 0.07979950308799744
In grad_steps = 3729, loss = 0.013014137744903564
In grad_steps = 3730, loss = 0.4744206666946411
In grad_steps = 3731, loss = 0.17334794998168945
In grad_steps = 3732, loss = 0.4318270981311798
In grad_steps = 3733, loss = 0.03117471933364868
In grad_steps = 3734, loss = 0.03492880240082741
In grad_steps = 3735, loss = 0.4111942946910858
In grad_steps = 3736, loss = 0.36968472599983215
In grad_steps = 3737, loss = 0.015863047912716866
In grad_steps = 3738, loss = 0.05707813799381256
In grad_steps = 3739, loss = 0.07438907772302628
In grad_steps = 3740, loss = 0.14210303127765656
In grad_steps = 3741, loss = 0.38753360509872437
In grad_steps = 3742, loss = 0.0582776702940464
In grad_steps = 3743, loss = 0.044504206627607346
In grad_steps = 3744, loss = 0.052476733922958374
In grad_steps = 3745, loss = 0.012285190634429455
In grad_steps = 3746, loss = 0.27276352047920227
In grad_steps = 3747, loss = 0.07342138886451721
In grad_steps = 3748, loss = 0.16710726916790009
In grad_steps = 3749, loss = 0.31723639369010925
In grad_steps = 3750, loss = 0.8929873108863831
In grad_steps = 3751, loss = 0.12026357650756836
In grad_steps = 3752, loss = 0.5454225540161133
In grad_steps = 3753, loss = 0.13990193605422974
In grad_steps = 3754, loss = 0.3892285227775574
In grad_steps = 3755, loss = 0.05236782133579254
In grad_steps = 3756, loss = 0.073001429438591
In grad_steps = 3757, loss = 0.14621859788894653
In grad_steps = 3758, loss = 0.01697448454797268
In grad_steps = 3759, loss = 0.24479471147060394
In grad_steps = 3760, loss = 0.09876728057861328
In grad_steps = 3761, loss = 0.3289281725883484
In grad_steps = 3762, loss = 0.05437525361776352
In grad_steps = 3763, loss = 0.07448403537273407
In grad_steps = 3764, loss = 0.016782110556960106
In grad_steps = 3765, loss = 0.026893669739365578
In grad_steps = 3766, loss = 0.03581168130040169
In grad_steps = 3767, loss = 0.018107261508703232
In grad_steps = 3768, loss = 0.12863963842391968
In grad_steps = 3769, loss = 0.11838392913341522
In grad_steps = 3770, loss = 0.28878968954086304
In grad_steps = 3771, loss = 0.031275004148483276
In grad_steps = 3772, loss = 0.03405028209090233
In grad_steps = 3773, loss = 0.012943606823682785
In grad_steps = 3774, loss = 0.4717109799385071
In grad_steps = 3775, loss = 0.08594534546136856
In grad_steps = 3776, loss = 0.013218523934483528
In grad_steps = 3777, loss = 0.2168971449136734
In grad_steps = 3778, loss = 0.06950554251670837
In grad_steps = 3779, loss = 0.026728155091404915
In grad_steps = 3780, loss = 0.025818945840001106
In grad_steps = 3781, loss = 0.0025423942133784294
In grad_steps = 3782, loss = 1.0202784538269043
In grad_steps = 3783, loss = 1.5326815843582153
In grad_steps = 3784, loss = 0.09849947690963745
In grad_steps = 3785, loss = 0.09219856560230255
In grad_steps = 3786, loss = 0.2054065614938736
In grad_steps = 3787, loss = 0.3437880575656891
In grad_steps = 3788, loss = 0.0316990427672863
In grad_steps = 3789, loss = 1.0395326614379883
In grad_steps = 3790, loss = 0.44548845291137695
In grad_steps = 3791, loss = 0.01532044168561697
In grad_steps = 3792, loss = 0.03135409578680992
In grad_steps = 3793, loss = 0.10043513774871826
In grad_steps = 3794, loss = 0.07830437272787094
In grad_steps = 3795, loss = 0.016225138679146767
In grad_steps = 3796, loss = 0.6529124975204468
In grad_steps = 3797, loss = 0.09013549983501434
In grad_steps = 3798, loss = 0.07060200721025467
In grad_steps = 3799, loss = 0.028840143233537674
In grad_steps = 3800, loss = 0.06443658471107483
In grad_steps = 3801, loss = 0.21876971423625946
In grad_steps = 3802, loss = 0.1955883502960205
In grad_steps = 3803, loss = 0.03328137472271919
In grad_steps = 3804, loss = 0.2435222864151001
In grad_steps = 3805, loss = 0.32806047797203064
In grad_steps = 3806, loss = 0.0059536597691476345
In grad_steps = 3807, loss = 0.06425638496875763
In grad_steps = 3808, loss = 0.847850501537323
In grad_steps = 3809, loss = 0.16677960753440857
In grad_steps = 3810, loss = 1.2158124446868896
In grad_steps = 3811, loss = 0.08663643151521683
In grad_steps = 3812, loss = 0.061275094747543335
In grad_steps = 3813, loss = 0.017586465924978256
In grad_steps = 3814, loss = 0.3186929523944855
In grad_steps = 3815, loss = 0.10188461095094681
In grad_steps = 3816, loss = 0.5203048586845398
In grad_steps = 3817, loss = 0.06406901031732559
In grad_steps = 3818, loss = 0.967677116394043
In grad_steps = 3819, loss = 0.26858198642730713
In grad_steps = 3820, loss = 0.06744558364152908
In grad_steps = 3821, loss = 0.025699827820062637
In grad_steps = 3822, loss = 0.7741086483001709
In grad_steps = 3823, loss = 0.05476272851228714
In grad_steps = 3824, loss = 0.09065014868974686
In grad_steps = 3825, loss = 0.5746868848800659
In grad_steps = 3826, loss = 0.05747382342815399
In grad_steps = 3827, loss = 0.4650404155254364
In grad_steps = 3828, loss = 0.293679803609848
In grad_steps = 3829, loss = 0.2423545867204666
In grad_steps = 3830, loss = 0.09413798153400421
In grad_steps = 3831, loss = 1.1002223491668701
In grad_steps = 3832, loss = 0.1620832234621048
In grad_steps = 3833, loss = 0.4674333333969116
In grad_steps = 3834, loss = 0.21319614350795746
In grad_steps = 3835, loss = 0.046242523938417435
In grad_steps = 3836, loss = 0.172636479139328
In grad_steps = 3837, loss = 0.09951289743185043
In grad_steps = 3838, loss = 0.04182679206132889
In grad_steps = 3839, loss = 0.24215292930603027
In grad_steps = 3840, loss = 0.43657198548316956
In grad_steps = 3841, loss = 0.16435600817203522
In grad_steps = 3842, loss = 0.009903981350362301
In grad_steps = 3843, loss = 0.3204479217529297
In grad_steps = 3844, loss = 0.07032503187656403
In grad_steps = 3845, loss = 0.09297304600477219
In grad_steps = 3846, loss = 0.20926915109157562
In grad_steps = 3847, loss = 0.22804170846939087
In grad_steps = 3848, loss = 0.036402300000190735
In grad_steps = 3849, loss = 0.25517717003822327
In grad_steps = 3850, loss = 0.07214639335870743
In grad_steps = 3851, loss = 0.04223678261041641
In grad_steps = 3852, loss = 0.048476412892341614
In grad_steps = 3853, loss = 0.03907474875450134
In grad_steps = 3854, loss = 0.6509467959403992
In grad_steps = 3855, loss = 0.015083330683410168
In grad_steps = 3856, loss = 0.576521635055542
In grad_steps = 3857, loss = 0.19731000065803528
In grad_steps = 3858, loss = 0.013331141322851181
In grad_steps = 3859, loss = 0.008346600458025932
In grad_steps = 3860, loss = 0.1447249948978424
In grad_steps = 3861, loss = 0.07683534175157547
In grad_steps = 3862, loss = 0.17386963963508606
In grad_steps = 3863, loss = 0.2943013906478882
In grad_steps = 3864, loss = 0.3149661421775818
In grad_steps = 3865, loss = 0.1576819121837616
In grad_steps = 3866, loss = 1.6986397504806519
In grad_steps = 3867, loss = 0.138646200299263
In grad_steps = 3868, loss = 0.12657180428504944
In grad_steps = 3869, loss = 0.02757362462580204
In grad_steps = 3870, loss = 0.07672584056854248
In grad_steps = 3871, loss = 0.47141802310943604
In grad_steps = 3872, loss = 0.870993971824646
In grad_steps = 3873, loss = 0.019050560891628265
In grad_steps = 3874, loss = 0.20344193279743195
In grad_steps = 3875, loss = 0.07291269302368164
In grad_steps = 3876, loss = 0.06860581040382385
In grad_steps = 3877, loss = 0.09418027102947235
In grad_steps = 3878, loss = 0.3850436806678772
In grad_steps = 3879, loss = 0.035737294703722
In grad_steps = 3880, loss = 0.029518794268369675
In grad_steps = 3881, loss = 0.06743284314870834
In grad_steps = 3882, loss = 0.05280907452106476
In grad_steps = 3883, loss = 0.0403924360871315
In grad_steps = 3884, loss = 0.031486622989177704
In grad_steps = 3885, loss = 0.15016454458236694
In grad_steps = 3886, loss = 0.04654964804649353
In grad_steps = 3887, loss = 0.030104855075478554
In grad_steps = 3888, loss = 0.01679389178752899
In grad_steps = 3889, loss = 0.0408349372446537
In grad_steps = 3890, loss = 0.4757276475429535
In grad_steps = 3891, loss = 0.41454583406448364
In grad_steps = 3892, loss = 0.022563550621271133
In grad_steps = 3893, loss = 0.013204189017415047
In grad_steps = 3894, loss = 0.016548914834856987
In grad_steps = 3895, loss = 0.018128305673599243
In grad_steps = 3896, loss = 0.27625221014022827
In grad_steps = 3897, loss = 0.03953951224684715
In grad_steps = 3898, loss = 0.016741866245865822
In grad_steps = 3899, loss = 0.5594243407249451
In grad_steps = 3900, loss = 0.010901141911745071
In grad_steps = 3901, loss = 0.22094836831092834
In grad_steps = 3902, loss = 0.013001689687371254
In grad_steps = 3903, loss = 0.033215686678886414
In grad_steps = 3904, loss = 0.042883262038230896
In grad_steps = 3905, loss = 0.11339325457811356
In grad_steps = 3906, loss = 0.033433280885219574
In grad_steps = 3907, loss = 0.2411901205778122
In grad_steps = 3908, loss = 0.010960767976939678
In grad_steps = 3909, loss = 0.005096284672617912
In grad_steps = 3910, loss = 1.1234030723571777
In grad_steps = 3911, loss = 0.2569083869457245
In grad_steps = 3912, loss = 1.1304410696029663
In grad_steps = 3913, loss = 0.7125784754753113
In grad_steps = 3914, loss = 0.0601634681224823
In grad_steps = 3915, loss = 0.050594765692949295
In grad_steps = 3916, loss = 0.07864976674318314
In grad_steps = 3917, loss = 0.053810182958841324
In grad_steps = 3918, loss = 0.053860634565353394
In grad_steps = 3919, loss = 0.8976850509643555
In grad_steps = 3920, loss = 0.07847902178764343
In grad_steps = 3921, loss = 0.35404935479164124
In grad_steps = 3922, loss = 0.08858086168766022
In grad_steps = 3923, loss = 0.3522006571292877
In grad_steps = 3924, loss = 0.3423236310482025
In grad_steps = 3925, loss = 0.4233345091342926
In grad_steps = 3926, loss = 0.14745652675628662
In grad_steps = 3927, loss = 0.1538562923669815
In grad_steps = 3928, loss = 0.037298839539289474
In grad_steps = 3929, loss = 0.10153262317180634
In grad_steps = 3930, loss = 0.22285926342010498
In grad_steps = 3931, loss = 0.3206751346588135
In grad_steps = 3932, loss = 0.06804191321134567
In grad_steps = 3933, loss = 0.06023717671632767
In grad_steps = 3934, loss = 0.18504688143730164
In grad_steps = 3935, loss = 0.03746335208415985
In grad_steps = 3936, loss = 0.16663074493408203
In grad_steps = 3937, loss = 0.3026372492313385
In grad_steps = 3938, loss = 0.9223801493644714
In grad_steps = 3939, loss = 0.056377384811639786
In grad_steps = 3940, loss = 0.20720580220222473
In grad_steps = 3941, loss = 0.48974841833114624
In grad_steps = 3942, loss = 0.02840835601091385
In grad_steps = 3943, loss = 0.16977013647556305
In grad_steps = 3944, loss = 0.011349418200552464
In grad_steps = 3945, loss = 0.5815090537071228
In grad_steps = 3946, loss = 0.0396357923746109
In grad_steps = 3947, loss = 0.011321182362735271
In grad_steps = 3948, loss = 0.2222973108291626
In grad_steps = 3949, loss = 0.20517954230308533
In grad_steps = 3950, loss = 0.07218202948570251
In grad_steps = 3951, loss = 0.02535688318312168
In grad_steps = 3952, loss = 0.4734412729740143
In grad_steps = 3953, loss = 0.19779375195503235
In grad_steps = 3954, loss = 0.12425363063812256
In grad_steps = 3955, loss = 0.3417358100414276
In grad_steps = 3956, loss = 0.028632601723074913
In grad_steps = 3957, loss = 0.038047268986701965
In grad_steps = 3958, loss = 0.034456923604011536
In grad_steps = 3959, loss = 0.15655556321144104
In grad_steps = 3960, loss = 0.06443265825510025
In grad_steps = 3961, loss = 0.050264619290828705
In grad_steps = 3962, loss = 0.04106021672487259
In grad_steps = 3963, loss = 0.11851827800273895
In grad_steps = 3964, loss = 0.02420848235487938
In grad_steps = 3965, loss = 0.570933997631073
In grad_steps = 3966, loss = 0.058845438063144684
In grad_steps = 3967, loss = 0.12544122338294983
In grad_steps = 3968, loss = 0.003544890321791172
In grad_steps = 3969, loss = 0.2751880884170532
In grad_steps = 3970, loss = 0.008038113825023174
In grad_steps = 3971, loss = 0.6467180848121643
In grad_steps = 3972, loss = 0.28559085726737976
In grad_steps = 3973, loss = 0.011466268450021744
In grad_steps = 3974, loss = 0.18167096376419067
In grad_steps = 3975, loss = 0.3858993947505951
In grad_steps = 3976, loss = 0.0488826148211956
In grad_steps = 3977, loss = 0.17356549203395844
In grad_steps = 3978, loss = 0.1226748377084732
In grad_steps = 3979, loss = 0.009669763036072254
In grad_steps = 3980, loss = 0.025823470205068588
In grad_steps = 3981, loss = 0.051709361374378204
In grad_steps = 3982, loss = 0.007270470727235079
In grad_steps = 3983, loss = 0.8877107501029968
In grad_steps = 3984, loss = 0.012903748080134392
In grad_steps = 3985, loss = 0.15406975150108337
In grad_steps = 3986, loss = 0.006336756516247988
In grad_steps = 3987, loss = 0.028795646503567696
In grad_steps = 3988, loss = 0.0296140369027853
In grad_steps = 3989, loss = 0.4936979115009308
In grad_steps = 3990, loss = 0.0033045047894120216
In grad_steps = 3991, loss = 0.09112049639225006
In grad_steps = 3992, loss = 0.3030952513217926
In grad_steps = 3993, loss = 0.09861941635608673
In grad_steps = 3994, loss = 0.0365893617272377
In grad_steps = 3995, loss = 0.9811310768127441
In grad_steps = 3996, loss = 0.010960653424263
In grad_steps = 3997, loss = 0.9800201654434204
In grad_steps = 3998, loss = 0.02880580723285675
In grad_steps = 3999, loss = 0.0704837217926979
In grad_steps = 4000, loss = 0.09126266092061996
In grad_steps = 4001, loss = 0.045913174748420715
In grad_steps = 4002, loss = 0.008902398869395256
In grad_steps = 4003, loss = 0.5223222970962524
In grad_steps = 4004, loss = 0.07033178210258484
In grad_steps = 4005, loss = 0.0418885201215744
In grad_steps = 4006, loss = 0.23222118616104126
In grad_steps = 4007, loss = 0.14858099818229675
In grad_steps = 4008, loss = 0.31461483240127563
In grad_steps = 4009, loss = 0.6335530877113342
In grad_steps = 4010, loss = 0.042657699435949326
In grad_steps = 4011, loss = 0.23301920294761658
In grad_steps = 4012, loss = 0.3289196491241455
In grad_steps = 4013, loss = 0.5413051247596741
In grad_steps = 4014, loss = 0.24587775766849518
In grad_steps = 4015, loss = 0.4229915142059326
In grad_steps = 4016, loss = 0.23160973191261292
In grad_steps = 4017, loss = 0.10406753420829773
In grad_steps = 4018, loss = 0.38996341824531555
In grad_steps = 4019, loss = 0.0174611434340477
In grad_steps = 4020, loss = 0.146510511636734
In grad_steps = 4021, loss = 0.7914168834686279
In grad_steps = 4022, loss = 0.265427827835083
In grad_steps = 4023, loss = 0.10778714716434479
In grad_steps = 4024, loss = 0.12745973467826843
In grad_steps = 4025, loss = 0.7518479228019714
In grad_steps = 4026, loss = 0.1131553128361702
In grad_steps = 4027, loss = 0.047152839601039886
In grad_steps = 4028, loss = 0.7008951902389526
In grad_steps = 4029, loss = 0.054454781115055084
In grad_steps = 4030, loss = 0.17331944406032562
In grad_steps = 4031, loss = 0.08343075215816498
In grad_steps = 4032, loss = 0.8140676021575928
In grad_steps = 4033, loss = 0.15488135814666748
In grad_steps = 4034, loss = 0.15678487718105316
In grad_steps = 4035, loss = 0.14237633347511292
In grad_steps = 4036, loss = 0.22055886685848236
In grad_steps = 4037, loss = 0.10640615969896317
In grad_steps = 4038, loss = 0.07739675045013428
In grad_steps = 4039, loss = 0.4738435745239258
In grad_steps = 4040, loss = 0.09574587643146515
In grad_steps = 4041, loss = 0.07479938864707947
In grad_steps = 4042, loss = 0.10603523254394531
In grad_steps = 4043, loss = 0.0904807299375534
In grad_steps = 4044, loss = 0.07355369627475739
In grad_steps = 4045, loss = 0.05143622308969498
In grad_steps = 4046, loss = 0.02551216445863247
In grad_steps = 4047, loss = 0.10878439992666245
In grad_steps = 4048, loss = 0.3664422333240509
In grad_steps = 4049, loss = 0.2942003607749939
In grad_steps = 4050, loss = 0.11080470681190491
In grad_steps = 4051, loss = 0.02186412923038006
In grad_steps = 4052, loss = 0.36405855417251587
In grad_steps = 4053, loss = 0.07952401041984558
In grad_steps = 4054, loss = 0.8323978781700134
In grad_steps = 4055, loss = 0.004629733972251415
In grad_steps = 4056, loss = 0.13098667562007904
In grad_steps = 4057, loss = 0.03550843521952629
In grad_steps = 4058, loss = 0.22413180768489838
In grad_steps = 4059, loss = 0.6597440242767334
In grad_steps = 4060, loss = 0.08305822312831879
In grad_steps = 4061, loss = 0.05989683419466019
In grad_steps = 4062, loss = 0.027619147673249245
In grad_steps = 4063, loss = 0.3008609414100647
In grad_steps = 4064, loss = 0.015482768416404724
In grad_steps = 4065, loss = 0.03128453344106674
In grad_steps = 4066, loss = 0.9921035766601562
In grad_steps = 4067, loss = 0.012622939422726631
In grad_steps = 4068, loss = 0.3590973913669586
In grad_steps = 4069, loss = 0.16860496997833252
In grad_steps = 4070, loss = 0.7624596357345581
In grad_steps = 4071, loss = 0.011053928174078465
In grad_steps = 4072, loss = 0.022828012704849243
In grad_steps = 4073, loss = 0.06346118450164795
In grad_steps = 4074, loss = 0.03439720347523689
In grad_steps = 4075, loss = 0.5550397634506226
In grad_steps = 4076, loss = 0.04652005434036255
In grad_steps = 4077, loss = 0.9905018210411072
In grad_steps = 4078, loss = 0.1516283005475998
In grad_steps = 4079, loss = 0.12867912650108337
In grad_steps = 4080, loss = 0.0827346071600914
In grad_steps = 4081, loss = 0.18939988315105438
In grad_steps = 4082, loss = 0.23961083590984344
In grad_steps = 4083, loss = 0.067347951233387
In grad_steps = 4084, loss = 0.04429632052779198
In grad_steps = 4085, loss = 0.049578957259655
In grad_steps = 4086, loss = 0.04353621229529381
In grad_steps = 4087, loss = 0.13011722266674042
In grad_steps = 4088, loss = 0.051763370633125305
In grad_steps = 4089, loss = 0.40120550990104675
In grad_steps = 4090, loss = 0.9015867114067078
In grad_steps = 4091, loss = 0.06653574854135513
In grad_steps = 4092, loss = 0.02957325428724289
In grad_steps = 4093, loss = 0.024628091603517532
In grad_steps = 4094, loss = 0.1389705091714859
In grad_steps = 4095, loss = 0.12498898804187775
In grad_steps = 4096, loss = 0.02872461825609207
In grad_steps = 4097, loss = 0.6350751519203186
In grad_steps = 4098, loss = 0.044404275715351105
In grad_steps = 4099, loss = 0.11647693812847137
In grad_steps = 4100, loss = 0.03178964927792549
In grad_steps = 4101, loss = 0.3541656732559204
In grad_steps = 4102, loss = 0.005862584803253412
In grad_steps = 4103, loss = 0.7723078727722168
In grad_steps = 4104, loss = 0.07269386202096939
In grad_steps = 4105, loss = 0.07930658757686615
In grad_steps = 4106, loss = 0.3313104808330536
In grad_steps = 4107, loss = 0.04820515960454941
In grad_steps = 4108, loss = 0.003272492438554764
In grad_steps = 4109, loss = 0.013685891404747963
In grad_steps = 4110, loss = 0.8799221515655518
In grad_steps = 4111, loss = 0.04127492755651474
In grad_steps = 4112, loss = 0.26694580912590027
In grad_steps = 4113, loss = 0.12546968460083008
In grad_steps = 4114, loss = 0.04161043092608452
In grad_steps = 4115, loss = 0.6381514072418213
In grad_steps = 4116, loss = 0.02954040840268135
In grad_steps = 4117, loss = 0.057670898735523224
In grad_steps = 4118, loss = 0.1389150619506836
In grad_steps = 4119, loss = 0.0016068570548668504
In grad_steps = 4120, loss = 0.019304852932691574
In grad_steps = 4121, loss = 0.03070664405822754
In grad_steps = 4122, loss = 0.4006192684173584
In grad_steps = 4123, loss = 0.496724933385849
In grad_steps = 4124, loss = 0.32962697744369507
In grad_steps = 4125, loss = 0.056274645030498505
In grad_steps = 4126, loss = 0.030749129131436348
In grad_steps = 4127, loss = 0.052625495940446854
In grad_steps = 4128, loss = 0.19587308168411255
In grad_steps = 4129, loss = 0.17201215028762817
In grad_steps = 4130, loss = 0.023225493729114532
In grad_steps = 4131, loss = 0.2254718393087387
In grad_steps = 4132, loss = 0.04429519176483154
In grad_steps = 4133, loss = 0.012935597449541092
In grad_steps = 4134, loss = 0.056549616158008575
In grad_steps = 4135, loss = 0.20306144654750824
In grad_steps = 4136, loss = 0.07670515775680542
In grad_steps = 4137, loss = 0.6358453631401062
In grad_steps = 4138, loss = 0.009877082891762257
In grad_steps = 4139, loss = 0.005228125490248203
In grad_steps = 4140, loss = 0.09713546931743622
In grad_steps = 4141, loss = 0.04034428298473358
In grad_steps = 4142, loss = 0.03435266762971878
In grad_steps = 4143, loss = 0.04287489503622055
In grad_steps = 4144, loss = 0.5172619223594666
In grad_steps = 4145, loss = 0.00640721432864666
In grad_steps = 4146, loss = 0.03768356516957283
In grad_steps = 4147, loss = 0.036869656294584274
In grad_steps = 4148, loss = 0.019079161807894707
In grad_steps = 4149, loss = 0.0021900504361838102
In grad_steps = 4150, loss = 0.38795700669288635
In grad_steps = 4151, loss = 0.034759521484375
In grad_steps = 4152, loss = 0.8426018357276917
In grad_steps = 4153, loss = 0.1747864931821823
In grad_steps = 4154, loss = 0.04323301464319229
In grad_steps = 4155, loss = 0.3633817136287689
In grad_steps = 4156, loss = 0.009129229933023453
In grad_steps = 4157, loss = 0.09447470307350159
In grad_steps = 4158, loss = 0.051141828298568726
In grad_steps = 4159, loss = 0.04914955049753189
In grad_steps = 4160, loss = 0.01649470441043377
In grad_steps = 4161, loss = 0.4702199101448059
In grad_steps = 4162, loss = 0.06609036773443222
In grad_steps = 4163, loss = 0.0003868727944791317
In grad_steps = 4164, loss = 0.0029855668544769287
In grad_steps = 4165, loss = 0.2926439642906189
In grad_steps = 4166, loss = 0.05112897977232933
In grad_steps = 4167, loss = 0.013650985434651375
In grad_steps = 4168, loss = 0.11895803362131119
In grad_steps = 4169, loss = 0.011184551753103733
In grad_steps = 4170, loss = 0.13539385795593262
In grad_steps = 4171, loss = 0.011901960708200932
In grad_steps = 4172, loss = 0.1454852968454361
In grad_steps = 4173, loss = 1.0766164064407349
In grad_steps = 4174, loss = 0.1800062358379364
In grad_steps = 4175, loss = 0.10757026821374893
In grad_steps = 4176, loss = 2.7790770530700684
In grad_steps = 4177, loss = 0.006319402251392603
In grad_steps = 4178, loss = 0.005635275971144438
In grad_steps = 4179, loss = 0.831595242023468
In grad_steps = 4180, loss = 0.1436249166727066
In grad_steps = 4181, loss = 1.0752880573272705
In grad_steps = 4182, loss = 0.11423540115356445
In grad_steps = 4183, loss = 0.02765742875635624
In grad_steps = 4184, loss = 0.05334540456533432
In grad_steps = 4185, loss = 0.8196921348571777
In grad_steps = 4186, loss = 0.6357876658439636
In grad_steps = 4187, loss = 0.08475034683942795
In grad_steps = 4188, loss = 0.04159318655729294
In grad_steps = 4189, loss = 0.13126416504383087
In grad_steps = 4190, loss = 0.6905547380447388
In grad_steps = 4191, loss = 0.31607553362846375
In grad_steps = 4192, loss = 0.1898358166217804
In grad_steps = 4193, loss = 0.07053069770336151
In grad_steps = 4194, loss = 0.19687674939632416
In grad_steps = 4195, loss = 0.13067413866519928
In grad_steps = 4196, loss = 0.10988453775644302
In grad_steps = 4197, loss = 0.4589281678199768
In grad_steps = 4198, loss = 0.29642021656036377
In grad_steps = 4199, loss = 0.18058040738105774
In grad_steps = 4200, loss = 0.4823518395423889
In grad_steps = 4201, loss = 0.06267013400793076
In grad_steps = 4202, loss = 0.021682340651750565
In grad_steps = 4203, loss = 0.17568618059158325
In grad_steps = 4204, loss = 0.09563025832176208
In grad_steps = 4205, loss = 0.35854220390319824
In grad_steps = 4206, loss = 0.16473257541656494
In grad_steps = 4207, loss = 0.06999880820512772
In grad_steps = 4208, loss = 0.009567906148731709
In grad_steps = 4209, loss = 0.19587507843971252
In grad_steps = 4210, loss = 0.13503585755825043
In grad_steps = 4211, loss = 0.05487314611673355
In grad_steps = 4212, loss = 0.02485281229019165
In grad_steps = 4213, loss = 0.012107116170227528
In grad_steps = 4214, loss = 0.01021852158010006
In grad_steps = 4215, loss = 0.01733430288732052
In grad_steps = 4216, loss = 0.5412203669548035
In grad_steps = 4217, loss = 0.16491180658340454
In grad_steps = 4218, loss = 0.12612570822238922
In grad_steps = 4219, loss = 0.004379496444016695
In grad_steps = 4220, loss = 0.1406295895576477
In grad_steps = 4221, loss = 0.0031560249626636505
In grad_steps = 4222, loss = 0.009619674645364285
In grad_steps = 4223, loss = 0.25121888518333435
In grad_steps = 4224, loss = 0.01404003519564867
In grad_steps = 4225, loss = 1.026267170906067
In grad_steps = 4226, loss = 0.00822847057133913
In grad_steps = 4227, loss = 0.14170363545417786
In grad_steps = 4228, loss = 0.6671000123023987
In grad_steps = 4229, loss = 0.010316608473658562
In grad_steps = 4230, loss = 0.7524038553237915
In grad_steps = 4231, loss = 0.023304548114538193
In grad_steps = 4232, loss = 0.39024558663368225
In grad_steps = 4233, loss = 0.022943327203392982
In grad_steps = 4234, loss = 0.16984224319458008
In grad_steps = 4235, loss = 0.11372799426317215
In grad_steps = 4236, loss = 0.1058950200676918
In grad_steps = 4237, loss = 0.05881665647029877
In grad_steps = 4238, loss = 0.03963306173682213
In grad_steps = 4239, loss = 1.1807399988174438
In grad_steps = 4240, loss = 0.03632336109876633
In grad_steps = 4241, loss = 0.026033876463770866
In grad_steps = 4242, loss = 0.07142971456050873
In grad_steps = 4243, loss = 0.03949747234582901
In grad_steps = 4244, loss = 0.03617294132709503
In grad_steps = 4245, loss = 0.07432138919830322
In grad_steps = 4246, loss = 0.06805066019296646
In grad_steps = 4247, loss = 0.03819927200675011
In grad_steps = 4248, loss = 0.3097998797893524
In grad_steps = 4249, loss = 0.33034294843673706
In grad_steps = 4250, loss = 0.012008346617221832
In grad_steps = 4251, loss = 0.007799230050295591
In grad_steps = 4252, loss = 0.1697598099708557
In grad_steps = 4253, loss = 0.029853317886590958
In grad_steps = 4254, loss = 0.39929747581481934
In grad_steps = 4255, loss = 0.019510788843035698
In grad_steps = 4256, loss = 0.12496397644281387
In grad_steps = 4257, loss = 0.11390155553817749
In grad_steps = 4258, loss = 0.02176470123231411
In grad_steps = 4259, loss = 0.3533575236797333
In grad_steps = 4260, loss = 0.867090106010437
In grad_steps = 4261, loss = 0.12177810072898865
In grad_steps = 4262, loss = 0.04299908131361008
In grad_steps = 4263, loss = 0.3793685734272003
In grad_steps = 4264, loss = 0.03990419954061508
In grad_steps = 4265, loss = 0.022379586473107338
In grad_steps = 4266, loss = 0.04540092870593071
In grad_steps = 4267, loss = 0.00803784467279911
In grad_steps = 4268, loss = 1.1339943408966064
In grad_steps = 4269, loss = 0.020502537488937378
In grad_steps = 4270, loss = 0.024426981806755066
In grad_steps = 4271, loss = 0.2837350368499756
In grad_steps = 4272, loss = 0.15842297673225403
In grad_steps = 4273, loss = 0.052864525467157364
In grad_steps = 4274, loss = 0.19320040941238403
In grad_steps = 4275, loss = 0.27600058913230896
In grad_steps = 4276, loss = 0.010858475230634212
In grad_steps = 4277, loss = 0.927803099155426
In grad_steps = 4278, loss = 1.1602741479873657
In grad_steps = 4279, loss = 0.17655788362026215
In grad_steps = 4280, loss = 0.029681406915187836
In grad_steps = 4281, loss = 1.1417295932769775
In grad_steps = 4282, loss = 0.10739552974700928
In grad_steps = 4283, loss = 0.09751875698566437
In grad_steps = 4284, loss = 0.9069470167160034
In grad_steps = 4285, loss = 0.145534947514534
In grad_steps = 4286, loss = 0.18306735157966614
In grad_steps = 4287, loss = 0.06929168850183487
In grad_steps = 4288, loss = 0.2084309160709381
In grad_steps = 4289, loss = 0.0710250586271286
In grad_steps = 4290, loss = 0.048835426568984985
In grad_steps = 4291, loss = 0.19719570875167847
In grad_steps = 4292, loss = 0.35333165526390076
In grad_steps = 4293, loss = 0.08240359276533127
In grad_steps = 4294, loss = 0.050224419683218
In grad_steps = 4295, loss = 0.12547819316387177
In grad_steps = 4296, loss = 1.0630154609680176
In grad_steps = 4297, loss = 0.02928844839334488
In grad_steps = 4298, loss = 0.043994300067424774
In grad_steps = 4299, loss = 0.07293244451284409
In grad_steps = 4300, loss = 0.14451760053634644
In grad_steps = 4301, loss = 0.07402558624744415
In grad_steps = 4302, loss = 0.21285861730575562
In grad_steps = 4303, loss = 0.07922444492578506
In grad_steps = 4304, loss = 0.5425262451171875
In grad_steps = 4305, loss = 0.1009519100189209
In grad_steps = 4306, loss = 0.08186705410480499
In grad_steps = 4307, loss = 0.5409021377563477
In grad_steps = 4308, loss = 0.018508393317461014
In grad_steps = 4309, loss = 0.029960347339510918
In grad_steps = 4310, loss = 0.049670301377773285
In grad_steps = 4311, loss = 0.08580921590328217
In grad_steps = 4312, loss = 0.02690856158733368
In grad_steps = 4313, loss = 0.05674531310796738
In grad_steps = 4314, loss = 0.2779768407344818
In grad_steps = 4315, loss = 0.267324298620224
In grad_steps = 4316, loss = 0.9386658668518066
In grad_steps = 4317, loss = 0.13705581426620483
In grad_steps = 4318, loss = 0.18333129584789276
In grad_steps = 4319, loss = 1.399369239807129
In grad_steps = 4320, loss = 0.25790494680404663
In grad_steps = 4321, loss = 0.3493006229400635
In grad_steps = 4322, loss = 0.1473250836133957
In grad_steps = 4323, loss = 0.06015048176050186
In grad_steps = 4324, loss = 0.34078261256217957
In grad_steps = 4325, loss = 0.42121759057044983
In grad_steps = 4326, loss = 0.06608898937702179
In grad_steps = 4327, loss = 0.3184455335140228
In grad_steps = 4328, loss = 0.045777931809425354
In grad_steps = 4329, loss = 0.04008321836590767
In grad_steps = 4330, loss = 0.6671063899993896
In grad_steps = 4331, loss = 0.13409772515296936
In grad_steps = 4332, loss = 0.029072437435388565
In grad_steps = 4333, loss = 0.03825207054615021
In grad_steps = 4334, loss = 0.8757221698760986
In grad_steps = 4335, loss = 0.05011362209916115
In grad_steps = 4336, loss = 0.042656175792217255
In grad_steps = 4337, loss = 0.038197603076696396
In grad_steps = 4338, loss = 0.12048853933811188
In grad_steps = 4339, loss = 0.5424633622169495
In grad_steps = 4340, loss = 0.07080025970935822
In grad_steps = 4341, loss = 0.10361789166927338
In grad_steps = 4342, loss = 0.08200230449438095
In grad_steps = 4343, loss = 0.5338640213012695
In grad_steps = 4344, loss = 0.05739029496908188
In grad_steps = 4345, loss = 0.07693882286548615
In grad_steps = 4346, loss = 0.08494671434164047
In grad_steps = 4347, loss = 0.1847461760044098
In grad_steps = 4348, loss = 0.33587709069252014
In grad_steps = 4349, loss = 0.021727411076426506
In grad_steps = 4350, loss = 0.017300717532634735
In grad_steps = 4351, loss = 0.2689027190208435
In grad_steps = 4352, loss = 0.07542483508586884
In grad_steps = 4353, loss = 0.02638532593846321
In grad_steps = 4354, loss = 0.35181042551994324
In grad_steps = 4355, loss = 0.3133828341960907
In grad_steps = 4356, loss = 0.08902589976787567
In grad_steps = 4357, loss = 0.028722994029521942
In grad_steps = 4358, loss = 0.009913661517202854
In grad_steps = 4359, loss = 0.03411642834544182
In grad_steps = 4360, loss = 0.017560631036758423
In grad_steps = 4361, loss = 0.10432812571525574
In grad_steps = 4362, loss = 0.2954545319080353
In grad_steps = 4363, loss = 0.1159333661198616
In grad_steps = 4364, loss = 0.05179815739393234
In grad_steps = 4365, loss = 0.1785583794116974
In grad_steps = 4366, loss = 0.015455435030162334
In grad_steps = 4367, loss = 0.08720065653324127
In grad_steps = 4368, loss = 0.006423867307603359
In grad_steps = 4369, loss = 0.008516963571310043
In grad_steps = 4370, loss = 0.06617570668458939
In grad_steps = 4371, loss = 0.032723747193813324
In grad_steps = 4372, loss = 0.7955031991004944
In grad_steps = 4373, loss = 0.020110679790377617
In grad_steps = 4374, loss = 0.01892475038766861
In grad_steps = 4375, loss = 0.853543221950531
In grad_steps = 4376, loss = 0.021178562194108963
In grad_steps = 4377, loss = 0.010024398565292358
In grad_steps = 4378, loss = 0.10190065950155258
In grad_steps = 4379, loss = 0.007226560730487108
In grad_steps = 4380, loss = 0.10546594858169556
In grad_steps = 4381, loss = 0.0201574694365263
In grad_steps = 4382, loss = 0.3331611752510071
In grad_steps = 4383, loss = 0.012352758087217808
In grad_steps = 4384, loss = 0.011597449891269207
In grad_steps = 4385, loss = 0.003433027770370245
In grad_steps = 4386, loss = 0.010576502420008183
In grad_steps = 4387, loss = 0.010265198536217213
In grad_steps = 4388, loss = 0.4366375505924225
In grad_steps = 4389, loss = 0.015497738495469093
In grad_steps = 4390, loss = 0.019834326580166817
In grad_steps = 4391, loss = 0.45265883207321167
In grad_steps = 4392, loss = 0.4250096380710602
In grad_steps = 4393, loss = 0.1265924572944641
In grad_steps = 4394, loss = 0.016968723386526108
In grad_steps = 4395, loss = 0.0953507125377655
In grad_steps = 4396, loss = 0.23528197407722473
In grad_steps = 4397, loss = 0.019089244306087494
In grad_steps = 4398, loss = 0.2691470682621002
In grad_steps = 4399, loss = 0.5737655758857727
In grad_steps = 4400, loss = 0.012483680620789528
In grad_steps = 4401, loss = 0.061468616127967834
In grad_steps = 4402, loss = 2.5894522666931152
In grad_steps = 4403, loss = 0.08471516519784927
In grad_steps = 4404, loss = 0.19541417062282562
In grad_steps = 4405, loss = 0.07791759073734283
In grad_steps = 4406, loss = 1.028992772102356
In grad_steps = 4407, loss = 0.13303044438362122
In grad_steps = 4408, loss = 0.1513361781835556
In grad_steps = 4409, loss = 0.02178538590669632
In grad_steps = 4410, loss = 0.32241129875183105
In grad_steps = 4411, loss = 0.17434953153133392
In grad_steps = 4412, loss = 0.021974097937345505
In grad_steps = 4413, loss = 0.14022111892700195
In grad_steps = 4414, loss = 0.15511226654052734
In grad_steps = 4415, loss = 0.01982768438756466
In grad_steps = 4416, loss = 0.02122591808438301
In grad_steps = 4417, loss = 0.47869673371315
In grad_steps = 4418, loss = 0.0494849756360054
In grad_steps = 4419, loss = 0.04780173674225807
In grad_steps = 4420, loss = 0.01730409264564514
In grad_steps = 4421, loss = 0.01521215308457613
In grad_steps = 4422, loss = 0.02920389734208584
In grad_steps = 4423, loss = 0.07388599961996078
In grad_steps = 4424, loss = 0.045236460864543915
In grad_steps = 4425, loss = 0.015074940398335457
In grad_steps = 4426, loss = 0.1997462809085846
In grad_steps = 4427, loss = 0.41352832317352295
In grad_steps = 4428, loss = 0.03004978597164154
In grad_steps = 4429, loss = 0.021327659487724304
In grad_steps = 4430, loss = 0.5135708451271057
In grad_steps = 4431, loss = 1.3574695587158203
In grad_steps = 4432, loss = 0.5632770657539368
In grad_steps = 4433, loss = 0.2403743416070938
In grad_steps = 4434, loss = 0.008294600993394852
In grad_steps = 4435, loss = 0.04413512349128723
In grad_steps = 4436, loss = 0.023770995438098907
In grad_steps = 4437, loss = 0.07947423309087753
In grad_steps = 4438, loss = 0.13571883738040924
In grad_steps = 4439, loss = 0.08631524443626404
In grad_steps = 4440, loss = 0.05309078469872475
In grad_steps = 4441, loss = 0.0399683378636837
In grad_steps = 4442, loss = 0.03083990328013897
In grad_steps = 4443, loss = 0.044013217091560364
In grad_steps = 4444, loss = 0.022849898785352707
In grad_steps = 4445, loss = 0.3290804922580719
In grad_steps = 4446, loss = 0.015468482859432697
In grad_steps = 4447, loss = 0.5562401413917542
In grad_steps = 4448, loss = 0.048012591898441315
In grad_steps = 4449, loss = 0.02225334197282791
In grad_steps = 4450, loss = 0.1623215675354004
In grad_steps = 4451, loss = 0.05005893483757973
In grad_steps = 4452, loss = 0.026885725557804108
In grad_steps = 4453, loss = 0.03919419273734093
In grad_steps = 4454, loss = 0.24601303040981293
In grad_steps = 4455, loss = 0.010217457078397274
In grad_steps = 4456, loss = 0.018013259395956993
In grad_steps = 4457, loss = 0.5107572078704834
In grad_steps = 4458, loss = 0.19089673459529877
In grad_steps = 4459, loss = 0.09882159531116486
In grad_steps = 4460, loss = 0.92684406042099
In grad_steps = 4461, loss = 0.08885089308023453
In grad_steps = 4462, loss = 0.03181586414575577
In grad_steps = 4463, loss = 0.5129424929618835
In grad_steps = 4464, loss = 0.024865981191396713
In grad_steps = 4465, loss = 0.7518841028213501
In grad_steps = 4466, loss = 0.1295880824327469
In grad_steps = 4467, loss = 0.34296560287475586
In grad_steps = 4468, loss = 0.03170653060078621
In grad_steps = 4469, loss = 0.018774650990962982
In grad_steps = 4470, loss = 0.05468461290001869
In grad_steps = 4471, loss = 0.48597532510757446
In grad_steps = 4472, loss = 0.010305405594408512
In grad_steps = 4473, loss = 0.010382553562521935
In grad_steps = 4474, loss = 0.026455556973814964
In grad_steps = 4475, loss = 0.04261165112257004
In grad_steps = 4476, loss = 0.21772579848766327
In grad_steps = 4477, loss = 0.03343828022480011
In grad_steps = 4478, loss = 0.017205756157636642
In grad_steps = 4479, loss = 1.087646722793579
In grad_steps = 4480, loss = 0.037138618528842926
In grad_steps = 4481, loss = 0.039891064167022705
In grad_steps = 4482, loss = 0.1538548469543457
In grad_steps = 4483, loss = 0.19129791855812073
In grad_steps = 4484, loss = 0.17902763187885284
In grad_steps = 4485, loss = 0.03215590491890907
In grad_steps = 4486, loss = 0.01097348053008318
In grad_steps = 4487, loss = 0.10152004659175873
In grad_steps = 4488, loss = 0.16598382592201233
In grad_steps = 4489, loss = 0.28725141286849976
In grad_steps = 4490, loss = 0.026783021166920662
In grad_steps = 4491, loss = 1.248064637184143
In grad_steps = 4492, loss = 0.012566971592605114
In grad_steps = 4493, loss = 0.04519324004650116
In grad_steps = 4494, loss = 0.24725870788097382
In grad_steps = 4495, loss = 0.024249469861388206
In grad_steps = 4496, loss = 0.044242486357688904
In grad_steps = 4497, loss = 0.8822291493415833
In grad_steps = 4498, loss = 0.05262179672718048
In grad_steps = 4499, loss = 0.2595689594745636
In grad_steps = 4500, loss = 0.5429145097732544
In grad_steps = 4501, loss = 0.3298424482345581
In grad_steps = 4502, loss = 0.02442796714603901
In grad_steps = 4503, loss = 0.14080214500427246
In grad_steps = 4504, loss = 0.17796528339385986
In grad_steps = 4505, loss = 0.09895707666873932
In grad_steps = 4506, loss = 0.06654264777898788
In grad_steps = 4507, loss = 0.1016053706407547
In grad_steps = 4508, loss = 0.041491422802209854
In grad_steps = 4509, loss = 0.0751977264881134
In grad_steps = 4510, loss = 0.01350775733590126
In grad_steps = 4511, loss = 0.06660432368516922
In grad_steps = 4512, loss = 1.0350642204284668
In grad_steps = 4513, loss = 0.4010336399078369
In grad_steps = 4514, loss = 0.017600279301404953
In grad_steps = 4515, loss = 1.032027006149292
In grad_steps = 4516, loss = 0.020785408094525337
In grad_steps = 4517, loss = 0.035178571939468384
In grad_steps = 4518, loss = 0.2544528841972351
In grad_steps = 4519, loss = 0.30445414781570435
In grad_steps = 4520, loss = 0.020773015916347504
In grad_steps = 4521, loss = 0.07936195284128189
In grad_steps = 4522, loss = 0.06632000207901001
In grad_steps = 4523, loss = 0.0087886992841959
In grad_steps = 4524, loss = 0.7942653298377991
In grad_steps = 4525, loss = 0.7996477484703064
In grad_steps = 4526, loss = 0.0054711755365133286
In grad_steps = 4527, loss = 0.01955491490662098
In grad_steps = 4528, loss = 0.11528443545103073
In grad_steps = 4529, loss = 0.02460569515824318
In grad_steps = 4530, loss = 0.08565352112054825
In grad_steps = 4531, loss = 0.04574216529726982
In grad_steps = 4532, loss = 0.14384150505065918
In grad_steps = 4533, loss = 0.036708567291498184
In grad_steps = 4534, loss = 0.03346413001418114
In grad_steps = 4535, loss = 0.19629937410354614
In grad_steps = 4536, loss = 0.3531414270401001
In grad_steps = 4537, loss = 0.16914993524551392
In grad_steps = 4538, loss = 0.08166740089654922
In grad_steps = 4539, loss = 0.05574096739292145
In grad_steps = 4540, loss = 0.03647700697183609
In grad_steps = 4541, loss = 0.0272836871445179
In grad_steps = 4542, loss = 0.2712079584598541
In grad_steps = 4543, loss = 0.03927447646856308
In grad_steps = 4544, loss = 0.027675485238432884
In grad_steps = 4545, loss = 0.09793668240308762
In grad_steps = 4546, loss = 0.05848752334713936
In grad_steps = 4547, loss = 0.03559103235602379
In grad_steps = 4548, loss = 0.011903658509254456
In grad_steps = 4549, loss = 0.04460151121020317
In grad_steps = 4550, loss = 0.016907356679439545
In grad_steps = 4551, loss = 1.619773030281067
In grad_steps = 4552, loss = 0.036159537732601166
In grad_steps = 4553, loss = 0.05971881374716759
In grad_steps = 4554, loss = 0.11121129989624023
In grad_steps = 4555, loss = 0.1552606225013733
In grad_steps = 4556, loss = 0.3021083474159241
In grad_steps = 4557, loss = 0.03335988521575928
In grad_steps = 4558, loss = 0.01586509309709072
In grad_steps = 4559, loss = 0.3968179523944855
In grad_steps = 4560, loss = 0.02518443949520588
In grad_steps = 4561, loss = 1.0188533067703247
In grad_steps = 4562, loss = 0.4908810555934906
In grad_steps = 4563, loss = 0.03442941978573799
In grad_steps = 4564, loss = 0.0685325562953949
In grad_steps = 4565, loss = 1.5821126699447632
In grad_steps = 4566, loss = 0.6363981366157532
In grad_steps = 4567, loss = 0.059690531343221664
In grad_steps = 4568, loss = 0.04608946293592453
In grad_steps = 4569, loss = 0.08647209405899048
In grad_steps = 4570, loss = 0.9676132798194885
In grad_steps = 4571, loss = 0.2749718129634857
In grad_steps = 4572, loss = 0.4181346595287323
In grad_steps = 4573, loss = 0.07213860750198364
In grad_steps = 4574, loss = 0.135487899184227
In grad_steps = 4575, loss = 0.07641158998012543
In grad_steps = 4576, loss = 0.6788956522941589
In grad_steps = 4577, loss = 0.050976552069187164
In grad_steps = 4578, loss = 0.26729243993759155
In grad_steps = 4579, loss = 0.20013409852981567
In grad_steps = 4580, loss = 0.7604509592056274
In grad_steps = 4581, loss = 0.06829506158828735
In grad_steps = 4582, loss = 0.3366851210594177
In grad_steps = 4583, loss = 0.05646546557545662
In grad_steps = 4584, loss = 0.045711807906627655
In grad_steps = 4585, loss = 0.04419296979904175
In grad_steps = 4586, loss = 0.04595257341861725
In grad_steps = 4587, loss = 0.46360650658607483
In grad_steps = 4588, loss = 0.021158747375011444
In grad_steps = 4589, loss = 0.06514552235603333
In grad_steps = 4590, loss = 0.3687669634819031
In grad_steps = 4591, loss = 0.5332528352737427
In grad_steps = 4592, loss = 0.2381775826215744
In grad_steps = 4593, loss = 0.08786729723215103
In grad_steps = 4594, loss = 0.06065003201365471
In grad_steps = 4595, loss = 0.9504910111427307
In grad_steps = 4596, loss = 0.033398039638996124
In grad_steps = 4597, loss = 0.1358504593372345
In grad_steps = 4598, loss = 0.7428539991378784
In grad_steps = 4599, loss = 0.09260004758834839
In grad_steps = 4600, loss = 0.08607442677021027
In grad_steps = 4601, loss = 0.31902194023132324
In grad_steps = 4602, loss = 0.26422202587127686
In grad_steps = 4603, loss = 0.23048678040504456
In grad_steps = 4604, loss = 0.11062649637460709
In grad_steps = 4605, loss = 0.3044925630092621
In grad_steps = 4606, loss = 0.4716565012931824
In grad_steps = 4607, loss = 0.37110185623168945
In grad_steps = 4608, loss = 0.37000611424446106
In grad_steps = 4609, loss = 0.6116598844528198
In grad_steps = 4610, loss = 0.0959383025765419
In grad_steps = 4611, loss = 0.09450597316026688
In grad_steps = 4612, loss = 0.254655659198761
In grad_steps = 4613, loss = 0.4452936351299286
In grad_steps = 4614, loss = 0.09789465367794037
In grad_steps = 4615, loss = 0.040179185569286346
In grad_steps = 4616, loss = 0.23180988430976868
In grad_steps = 4617, loss = 0.6896989345550537
In grad_steps = 4618, loss = 0.03978797420859337
In grad_steps = 4619, loss = 0.06833088397979736
In grad_steps = 4620, loss = 0.06147637590765953
In grad_steps = 4621, loss = 0.013565979897975922
In grad_steps = 4622, loss = 0.3621850907802582
In grad_steps = 4623, loss = 0.04869941249489784
In grad_steps = 4624, loss = 0.03259195387363434
In grad_steps = 4625, loss = 0.043137326836586
In grad_steps = 4626, loss = 0.41527777910232544
In grad_steps = 4627, loss = 0.06519156694412231
In grad_steps = 4628, loss = 0.01719735935330391
In grad_steps = 4629, loss = 0.01164331380277872
In grad_steps = 4630, loss = 0.03433273732662201
In grad_steps = 4631, loss = 0.028824783861637115
In grad_steps = 4632, loss = 0.006173328962177038
In grad_steps = 4633, loss = 0.015114489011466503
In grad_steps = 4634, loss = 0.4247443974018097
In grad_steps = 4635, loss = 0.06775525212287903
In grad_steps = 4636, loss = 0.4026057720184326
In grad_steps = 4637, loss = 0.029983684420585632
In grad_steps = 4638, loss = 0.014963382855057716
In grad_steps = 4639, loss = 0.12261608988046646
In grad_steps = 4640, loss = 0.23547595739364624
In grad_steps = 4641, loss = 0.05285671725869179
In grad_steps = 4642, loss = 0.011505583301186562
In grad_steps = 4643, loss = 0.017659220844507217
In grad_steps = 4644, loss = 0.05381431803107262
In grad_steps = 4645, loss = 0.045860785990953445
In grad_steps = 4646, loss = 0.015507083386182785
In grad_steps = 4647, loss = 0.008504677563905716
In grad_steps = 4648, loss = 0.025397896766662598
In grad_steps = 4649, loss = 0.022259710356593132
In grad_steps = 4650, loss = 0.13896925747394562
In grad_steps = 4651, loss = 1.2459391355514526
In grad_steps = 4652, loss = 0.019003858789801598
In grad_steps = 4653, loss = 0.010563632473349571
In grad_steps = 4654, loss = 0.04684542492032051
In grad_steps = 4655, loss = 0.2449192851781845
In grad_steps = 4656, loss = 0.004246779717504978
In grad_steps = 4657, loss = 0.005097608081996441
In grad_steps = 4658, loss = 0.1264912635087967
In grad_steps = 4659, loss = 0.05132278427481651
In grad_steps = 4660, loss = 0.03108656220138073
In grad_steps = 4661, loss = 0.026556208729743958
In grad_steps = 4662, loss = 0.014480233192443848
In grad_steps = 4663, loss = 1.577504277229309
In grad_steps = 4664, loss = 0.05064721032977104
In grad_steps = 4665, loss = 0.9836561679840088
In grad_steps = 4666, loss = 0.09539730846881866
In grad_steps = 4667, loss = 0.19528760015964508
In grad_steps = 4668, loss = 0.1192178726196289
In grad_steps = 4669, loss = 0.03975936770439148
In grad_steps = 4670, loss = 0.07327041774988174
In grad_steps = 4671, loss = 0.6191950440406799
In grad_steps = 4672, loss = 1.1450785398483276
In grad_steps = 4673, loss = 0.07327781617641449
In grad_steps = 4674, loss = 0.10205356031656265
In grad_steps = 4675, loss = 0.5746777653694153
In grad_steps = 4676, loss = 0.13468515872955322
In grad_steps = 4677, loss = 0.2976129353046417
In grad_steps = 4678, loss = 0.18019983172416687
In grad_steps = 4679, loss = 0.051011715084314346
In grad_steps = 4680, loss = 1.3483991622924805
In grad_steps = 4681, loss = 0.45702409744262695
In grad_steps = 4682, loss = 0.26430290937423706
In grad_steps = 4683, loss = 0.1472291201353073
In grad_steps = 4684, loss = 0.3283417820930481
In grad_steps = 4685, loss = 0.047277629375457764
In grad_steps = 4686, loss = 0.3229016065597534
In grad_steps = 4687, loss = 0.04757251590490341
In grad_steps = 4688, loss = 0.33609458804130554
In grad_steps = 4689, loss = 0.16299061477184296
In grad_steps = 4690, loss = 0.03278365358710289
In grad_steps = 4691, loss = 0.025965619832277298
In grad_steps = 4692, loss = 0.0865318551659584
In grad_steps = 4693, loss = 0.02062799036502838
In grad_steps = 4694, loss = 0.06639893352985382
In grad_steps = 4695, loss = 0.80936199426651
In grad_steps = 4696, loss = 0.044028617441654205
In grad_steps = 4697, loss = 0.6468532681465149
In grad_steps = 4698, loss = 0.31704044342041016
In grad_steps = 4699, loss = 0.047458890825510025
In grad_steps = 4700, loss = 0.07846008986234665
In grad_steps = 4701, loss = 0.02302112989127636
In grad_steps = 4702, loss = 0.05938134342432022
In grad_steps = 4703, loss = 0.07802171260118484
In grad_steps = 4704, loss = 0.29349061846733093
In grad_steps = 4705, loss = 0.0016412371769547462
Elapsed time: 2682.518373966217 seconds for ensemble 3 with 2 epochs
LoRA instance 3 evaluation complete. Data saved to /pscratch/sd/t/tianle/lucid/other_source/SURP_2024/results/tmp_lora_ensemble/Llama3-8B-set-4/test_data_instance_3_seed_30280.npz.
lora instance i = 3 Successfully finished.
Training lora instance 4
DEBUG: Total parameters: 8037076992
DEBUG: Trainable parameters: 6815744
Beginning epoch 1
In grad_steps = 0, loss = 0.5826846361160278
In grad_steps = 1, loss = 0.4809085726737976
In grad_steps = 2, loss = 1.7502833604812622
In grad_steps = 3, loss = 0.7461727857589722
In grad_steps = 4, loss = 0.9387269616127014
In grad_steps = 5, loss = 0.7286399602890015
In grad_steps = 6, loss = 1.0866509675979614
In grad_steps = 7, loss = 0.8292027115821838
In grad_steps = 8, loss = 0.4516308903694153
In grad_steps = 9, loss = 0.7025350332260132
In grad_steps = 10, loss = 0.7711173295974731
In grad_steps = 11, loss = 1.0281484127044678
In grad_steps = 12, loss = 1.015870213508606
In grad_steps = 13, loss = 0.6927973031997681
In grad_steps = 14, loss = 0.639200747013092
In grad_steps = 15, loss = 0.7647101283073425
In grad_steps = 16, loss = 0.7436262965202332
In grad_steps = 17, loss = 0.5806880593299866
In grad_steps = 18, loss = 1.1312270164489746
In grad_steps = 19, loss = 1.0311037302017212
In grad_steps = 20, loss = 0.9987585544586182
In grad_steps = 21, loss = 0.5921640396118164
In grad_steps = 22, loss = 0.700548529624939
In grad_steps = 23, loss = 0.5771132707595825
In grad_steps = 24, loss = 0.7571515440940857
In grad_steps = 25, loss = 0.6746160984039307
In grad_steps = 26, loss = 0.6614707112312317
In grad_steps = 27, loss = 0.6687288880348206
In grad_steps = 28, loss = 0.7064177989959717
In grad_steps = 29, loss = 0.7284961938858032
In grad_steps = 30, loss = 0.7218494415283203
In grad_steps = 31, loss = 0.6806161999702454
In grad_steps = 32, loss = 0.6941540241241455
In grad_steps = 33, loss = 0.7344957590103149
In grad_steps = 34, loss = 0.72331702709198
In grad_steps = 35, loss = 0.585227370262146
In grad_steps = 36, loss = 0.712412416934967
In grad_steps = 37, loss = 0.6798394918441772
In grad_steps = 38, loss = 0.7348141670227051
In grad_steps = 39, loss = 0.6446029543876648
In grad_steps = 40, loss = 0.6615093946456909
In grad_steps = 41, loss = 0.5626736879348755
In grad_steps = 42, loss = 0.5313071012496948
In grad_steps = 43, loss = 0.5319185256958008
In grad_steps = 44, loss = 0.7532044649124146
In grad_steps = 45, loss = 0.7521182298660278
In grad_steps = 46, loss = 0.923569917678833
In grad_steps = 47, loss = 0.6105771660804749
In grad_steps = 48, loss = 0.9281039834022522
In grad_steps = 49, loss = 0.5234009027481079
In grad_steps = 50, loss = 0.9999074339866638
In grad_steps = 51, loss = 1.006523609161377
In grad_steps = 52, loss = 0.7192862629890442
In grad_steps = 53, loss = 0.681684136390686
In grad_steps = 54, loss = 0.7053287625312805
In grad_steps = 55, loss = 0.617051899433136
In grad_steps = 56, loss = 0.6368454694747925
In grad_steps = 57, loss = 2.888437271118164
In grad_steps = 58, loss = 0.6550217866897583
In grad_steps = 59, loss = 0.701384425163269
In grad_steps = 60, loss = 0.6868582367897034
In grad_steps = 61, loss = 0.7361600995063782
In grad_steps = 62, loss = 0.7945423722267151
In grad_steps = 63, loss = 0.6378143429756165
In grad_steps = 64, loss = 0.6977701187133789
In grad_steps = 65, loss = 0.7402262091636658
In grad_steps = 66, loss = 0.6950922608375549
In grad_steps = 67, loss = 0.7078245878219604
In grad_steps = 68, loss = 0.6853739023208618
In grad_steps = 69, loss = 0.7878665924072266
In grad_steps = 70, loss = 0.6819958090782166
In grad_steps = 71, loss = 0.6826133728027344
In grad_steps = 72, loss = 0.6263800859451294
In grad_steps = 73, loss = 0.7448098659515381
In grad_steps = 74, loss = 1.7981982231140137
In grad_steps = 75, loss = 0.583030104637146
In grad_steps = 76, loss = 0.7189093232154846
In grad_steps = 77, loss = 0.8060646057128906
In grad_steps = 78, loss = 0.6497891545295715
In grad_steps = 79, loss = 0.6157962083816528
In grad_steps = 80, loss = 0.7039856314659119
In grad_steps = 81, loss = 0.60371994972229
In grad_steps = 82, loss = 0.6768749952316284
In grad_steps = 83, loss = 0.7023024559020996
In grad_steps = 84, loss = 0.6756103038787842
In grad_steps = 85, loss = 0.7364724278450012
In grad_steps = 86, loss = 0.6227906346321106
In grad_steps = 87, loss = 0.6141074895858765
In grad_steps = 88, loss = 0.6715406775474548
In grad_steps = 89, loss = 0.7051340341567993
In grad_steps = 90, loss = 0.667252242565155
In grad_steps = 91, loss = 0.7110538482666016
In grad_steps = 92, loss = 0.7137960195541382
In grad_steps = 93, loss = 0.6030111312866211
In grad_steps = 94, loss = 0.6303483247756958
In grad_steps = 95, loss = 0.5974618196487427
In grad_steps = 96, loss = 0.7753631472587585
In grad_steps = 97, loss = 0.6633068919181824
In grad_steps = 98, loss = 0.5675160884857178
In grad_steps = 99, loss = 0.6453383564949036
In grad_steps = 100, loss = 0.5564165115356445
In grad_steps = 101, loss = 0.5530411601066589
In grad_steps = 102, loss = 0.8230046033859253
In grad_steps = 103, loss = 0.6527481079101562
In grad_steps = 104, loss = 0.6733027100563049
In grad_steps = 105, loss = 0.7092986106872559
In grad_steps = 106, loss = 0.6020435690879822
In grad_steps = 107, loss = 0.8543270826339722
In grad_steps = 108, loss = 0.4774966239929199
In grad_steps = 109, loss = 0.6273173689842224
In grad_steps = 110, loss = 0.5464707016944885
In grad_steps = 111, loss = 0.52530837059021
In grad_steps = 112, loss = 0.7488654851913452
In grad_steps = 113, loss = 0.7635098099708557
In grad_steps = 114, loss = 0.5505517721176147
In grad_steps = 115, loss = 0.43944311141967773
In grad_steps = 116, loss = 0.5113173723220825
In grad_steps = 117, loss = 0.5543313026428223
In grad_steps = 118, loss = 1.1250755786895752
In grad_steps = 119, loss = 1.2218984365463257
In grad_steps = 120, loss = 0.8888571858406067
In grad_steps = 121, loss = 0.8681321740150452
In grad_steps = 122, loss = 0.7689940333366394
In grad_steps = 123, loss = 0.6538470387458801
In grad_steps = 124, loss = 0.62300705909729
In grad_steps = 125, loss = 0.7570546269416809
In grad_steps = 126, loss = 0.4319770336151123
In grad_steps = 127, loss = 0.7643252611160278
In grad_steps = 128, loss = 0.6865215301513672
In grad_steps = 129, loss = 0.7788277864456177
In grad_steps = 130, loss = 0.5740479230880737
In grad_steps = 131, loss = 0.820452094078064
In grad_steps = 132, loss = 0.6232414841651917
In grad_steps = 133, loss = 0.7222126126289368
In grad_steps = 134, loss = 0.7236379981040955
In grad_steps = 135, loss = 0.6114157438278198
In grad_steps = 136, loss = 0.6557732224464417
In grad_steps = 137, loss = 0.6913631558418274
In grad_steps = 138, loss = 0.6452562212944031
In grad_steps = 139, loss = 0.5460253357887268
In grad_steps = 140, loss = 0.5993718504905701
In grad_steps = 141, loss = 0.6642353534698486
In grad_steps = 142, loss = 0.6880229711532593
In grad_steps = 143, loss = 0.6107949614524841
In grad_steps = 144, loss = 0.60561603307724
In grad_steps = 145, loss = 0.7868672609329224
In grad_steps = 146, loss = 0.5760901570320129
In grad_steps = 147, loss = 0.6284939050674438
In grad_steps = 148, loss = 0.6416573524475098
In grad_steps = 149, loss = 0.4334738850593567
In grad_steps = 150, loss = 0.4530182182788849
In grad_steps = 151, loss = 0.847354531288147
In grad_steps = 152, loss = 0.18413904309272766
In grad_steps = 153, loss = 1.0628153085708618
In grad_steps = 154, loss = 1.2946380376815796
In grad_steps = 155, loss = 0.5357890725135803
In grad_steps = 156, loss = 0.6030151844024658
In grad_steps = 157, loss = 0.7148543000221252
In grad_steps = 158, loss = 0.5356377363204956
In grad_steps = 159, loss = 0.6719681024551392
In grad_steps = 160, loss = 0.6102104187011719
In grad_steps = 161, loss = 0.4278687834739685
In grad_steps = 162, loss = 0.9083983898162842
In grad_steps = 163, loss = 0.46728840470314026
In grad_steps = 164, loss = 0.9904854893684387
In grad_steps = 165, loss = 0.5953060388565063
In grad_steps = 166, loss = 0.709989070892334
In grad_steps = 167, loss = 0.44279494881629944
In grad_steps = 168, loss = 0.4918671250343323
In grad_steps = 169, loss = 0.6935975551605225
In grad_steps = 170, loss = 0.6643704175949097
In grad_steps = 171, loss = 0.6787271499633789
In grad_steps = 172, loss = 0.6349355578422546
In grad_steps = 173, loss = 0.5315653681755066
In grad_steps = 174, loss = 0.8427141904830933
In grad_steps = 175, loss = 0.5518285036087036
In grad_steps = 176, loss = 0.4399279057979584
In grad_steps = 177, loss = 0.5449993014335632
In grad_steps = 178, loss = 0.5460671782493591
In grad_steps = 179, loss = 0.5424505472183228
In grad_steps = 180, loss = 0.5767643451690674
In grad_steps = 181, loss = 0.5850262641906738
In grad_steps = 182, loss = 0.5569955110549927
In grad_steps = 183, loss = 0.5163452625274658
In grad_steps = 184, loss = 0.5162945985794067
In grad_steps = 185, loss = 0.5428467988967896
In grad_steps = 186, loss = 1.0384488105773926
In grad_steps = 187, loss = 0.22806847095489502
In grad_steps = 188, loss = 0.9194974899291992
In grad_steps = 189, loss = 0.21084916591644287
In grad_steps = 190, loss = 0.6626455783843994
In grad_steps = 191, loss = 0.5868716239929199
In grad_steps = 192, loss = 0.25075066089630127
In grad_steps = 193, loss = 0.29019373655319214
In grad_steps = 194, loss = 0.8639228940010071
In grad_steps = 195, loss = 0.2763158977031708
In grad_steps = 196, loss = 0.48881465196609497
In grad_steps = 197, loss = 0.22289201617240906
In grad_steps = 198, loss = 0.6590598821640015
In grad_steps = 199, loss = 0.5602796077728271
In grad_steps = 200, loss = 0.29222017526626587
In grad_steps = 201, loss = 0.8721559047698975
In grad_steps = 202, loss = 0.3470432758331299
In grad_steps = 203, loss = 0.21080420911312103
In grad_steps = 204, loss = 0.2762500047683716
In grad_steps = 205, loss = 0.7311049699783325
In grad_steps = 206, loss = 1.0491855144500732
In grad_steps = 207, loss = 0.4257356524467468
In grad_steps = 208, loss = 0.21599224209785461
In grad_steps = 209, loss = 0.8212664723396301
In grad_steps = 210, loss = 0.39597445726394653
In grad_steps = 211, loss = 0.31087526679039
In grad_steps = 212, loss = 0.2462717592716217
In grad_steps = 213, loss = 0.5514688491821289
In grad_steps = 214, loss = 0.48304349184036255
In grad_steps = 215, loss = 0.3686320185661316
In grad_steps = 216, loss = 0.5625760555267334
In grad_steps = 217, loss = 0.6900649070739746
In grad_steps = 218, loss = 0.5949624180793762
In grad_steps = 219, loss = 0.3419242799282074
In grad_steps = 220, loss = 0.31309831142425537
In grad_steps = 221, loss = 0.26747462153434753
In grad_steps = 222, loss = 0.18774358928203583
In grad_steps = 223, loss = 0.46078500151634216
In grad_steps = 224, loss = 0.19636574387550354
In grad_steps = 225, loss = 1.0686118602752686
In grad_steps = 226, loss = 0.5745586156845093
In grad_steps = 227, loss = 1.059636116027832
In grad_steps = 228, loss = 0.342742383480072
In grad_steps = 229, loss = 0.4300459027290344
In grad_steps = 230, loss = 0.5213045477867126
In grad_steps = 231, loss = 0.3566894829273224
In grad_steps = 232, loss = 0.2290332168340683
In grad_steps = 233, loss = 0.5377060174942017
In grad_steps = 234, loss = 0.525951623916626
In grad_steps = 235, loss = 0.37631237506866455
In grad_steps = 236, loss = 0.7075861096382141
In grad_steps = 237, loss = 0.5938757658004761
In grad_steps = 238, loss = 0.2699465751647949
In grad_steps = 239, loss = 0.39404991269111633
In grad_steps = 240, loss = 0.7753898501396179
In grad_steps = 241, loss = 0.31733137369155884
In grad_steps = 242, loss = 0.587828516960144
In grad_steps = 243, loss = 0.5847580432891846
In grad_steps = 244, loss = 0.28146785497665405
In grad_steps = 245, loss = 0.20617111027240753
In grad_steps = 246, loss = 0.18175162374973297
In grad_steps = 247, loss = 0.5936999917030334
In grad_steps = 248, loss = 0.09152797609567642
In grad_steps = 249, loss = 0.40276846289634705
In grad_steps = 250, loss = 0.3141440451145172
In grad_steps = 251, loss = 0.6399741768836975
In grad_steps = 252, loss = 1.453342080116272
In grad_steps = 253, loss = 0.2025102823972702
In grad_steps = 254, loss = 0.6264423131942749
In grad_steps = 255, loss = 0.636884331703186
In grad_steps = 256, loss = 0.5380572080612183
In grad_steps = 257, loss = 0.5819992423057556
In grad_steps = 258, loss = 0.19523683190345764
In grad_steps = 259, loss = 0.7620538473129272
In grad_steps = 260, loss = 0.7808270454406738
In grad_steps = 261, loss = 0.6648455858230591
In grad_steps = 262, loss = 0.36760008335113525
In grad_steps = 263, loss = 0.7425169348716736
In grad_steps = 264, loss = 0.3306286036968231
In grad_steps = 265, loss = 0.430534303188324
In grad_steps = 266, loss = 0.515659511089325
In grad_steps = 267, loss = 0.3962845802307129
In grad_steps = 268, loss = 0.49984610080718994
In grad_steps = 269, loss = 0.36883053183555603
In grad_steps = 270, loss = 0.28439855575561523
In grad_steps = 271, loss = 0.37500452995300293
In grad_steps = 272, loss = 0.22692669928073883
In grad_steps = 273, loss = 0.6268776655197144
In grad_steps = 274, loss = 0.10771861672401428
In grad_steps = 275, loss = 0.263571560382843
In grad_steps = 276, loss = 0.42580512166023254
In grad_steps = 277, loss = 0.7871853709220886
In grad_steps = 278, loss = 0.10720322281122208
In grad_steps = 279, loss = 1.164400339126587
In grad_steps = 280, loss = 0.3397520184516907
In grad_steps = 281, loss = 0.4830304980278015
In grad_steps = 282, loss = 0.6156670451164246
In grad_steps = 283, loss = 0.9677813649177551
In grad_steps = 284, loss = 0.257942259311676
In grad_steps = 285, loss = 0.5994371175765991
In grad_steps = 286, loss = 0.594552755355835
In grad_steps = 287, loss = 0.07645483314990997
In grad_steps = 288, loss = 0.07986319065093994
In grad_steps = 289, loss = 0.5926705002784729
In grad_steps = 290, loss = 0.09228929132223129
In grad_steps = 291, loss = 0.4400971531867981
In grad_steps = 292, loss = 0.69970703125
In grad_steps = 293, loss = 0.5792583227157593
In grad_steps = 294, loss = 0.31641021370887756
In grad_steps = 295, loss = 0.25110557675361633
In grad_steps = 296, loss = 0.21342453360557556
In grad_steps = 297, loss = 0.31897926330566406
In grad_steps = 298, loss = 0.36134234070777893
In grad_steps = 299, loss = 0.4467446506023407
In grad_steps = 300, loss = 0.5351065397262573
In grad_steps = 301, loss = 0.37079787254333496
In grad_steps = 302, loss = 0.4225923717021942
In grad_steps = 303, loss = 0.21565979719161987
In grad_steps = 304, loss = 0.41955870389938354
In grad_steps = 305, loss = 1.7697503566741943
In grad_steps = 306, loss = 0.09824922680854797
In grad_steps = 307, loss = 0.4769693613052368
In grad_steps = 308, loss = 0.4988633990287781
In grad_steps = 309, loss = 0.8435723185539246
In grad_steps = 310, loss = 0.4190478026866913
In grad_steps = 311, loss = 0.7846810817718506
In grad_steps = 312, loss = 0.6088578701019287
In grad_steps = 313, loss = 0.6865954399108887
In grad_steps = 314, loss = 0.6646482944488525
In grad_steps = 315, loss = 0.29799360036849976
In grad_steps = 316, loss = 0.41147199273109436
In grad_steps = 317, loss = 0.4242542088031769
In grad_steps = 318, loss = 0.2927652895450592
In grad_steps = 319, loss = 0.43655073642730713
In grad_steps = 320, loss = 0.25920361280441284
In grad_steps = 321, loss = 0.3728702664375305
In grad_steps = 322, loss = 0.6876690983772278
In grad_steps = 323, loss = 0.559611976146698
In grad_steps = 324, loss = 0.270035982131958
In grad_steps = 325, loss = 0.190320685505867
In grad_steps = 326, loss = 0.18859142065048218
In grad_steps = 327, loss = 0.28768596053123474
In grad_steps = 328, loss = 0.21114377677440643
In grad_steps = 329, loss = 0.15502533316612244
In grad_steps = 330, loss = 0.6871802806854248
In grad_steps = 331, loss = 0.21138536930084229
In grad_steps = 332, loss = 0.13369916379451752
In grad_steps = 333, loss = 0.4308469295501709
In grad_steps = 334, loss = 0.04252616688609123
In grad_steps = 335, loss = 0.34554123878479004
In grad_steps = 336, loss = 0.2950199544429779
In grad_steps = 337, loss = 0.11115005612373352
In grad_steps = 338, loss = 0.2722555100917816
In grad_steps = 339, loss = 0.6995158195495605
In grad_steps = 340, loss = 0.6851793527603149
In grad_steps = 341, loss = 0.037098463624715805
In grad_steps = 342, loss = 1.3484768867492676
In grad_steps = 343, loss = 0.9839890599250793
In grad_steps = 344, loss = 0.05282043293118477
In grad_steps = 345, loss = 0.05891767144203186
In grad_steps = 346, loss = 0.4077123999595642
In grad_steps = 347, loss = 0.686195969581604
In grad_steps = 348, loss = 0.5609432458877563
In grad_steps = 349, loss = 0.9539620280265808
In grad_steps = 350, loss = 0.7355906367301941
In grad_steps = 351, loss = 0.5387241244316101
In grad_steps = 352, loss = 0.7987780570983887
In grad_steps = 353, loss = 0.2504951059818268
In grad_steps = 354, loss = 0.37564077973365784
In grad_steps = 355, loss = 1.051621437072754
In grad_steps = 356, loss = 0.16123145818710327
In grad_steps = 357, loss = 0.5717896819114685
In grad_steps = 358, loss = 0.16888131201267242
In grad_steps = 359, loss = 0.6548972129821777
In grad_steps = 360, loss = 0.6653329133987427
In grad_steps = 361, loss = 0.48323899507522583
In grad_steps = 362, loss = 0.25397446751594543
In grad_steps = 363, loss = 0.2969842255115509
In grad_steps = 364, loss = 0.44865378737449646
In grad_steps = 365, loss = 0.30174508690834045
In grad_steps = 366, loss = 0.28798288106918335
In grad_steps = 367, loss = 0.431447297334671
In grad_steps = 368, loss = 0.2809317409992218
In grad_steps = 369, loss = 0.275526762008667
In grad_steps = 370, loss = 0.5831714868545532
In grad_steps = 371, loss = 0.2932989299297333
In grad_steps = 372, loss = 0.12884359061717987
In grad_steps = 373, loss = 0.46367934346199036
In grad_steps = 374, loss = 0.17486952245235443
In grad_steps = 375, loss = 0.24032211303710938
In grad_steps = 376, loss = 0.050574563443660736
In grad_steps = 377, loss = 0.7947180271148682
In grad_steps = 378, loss = 0.5150521993637085
In grad_steps = 379, loss = 0.1330719292163849
In grad_steps = 380, loss = 0.3038700819015503
In grad_steps = 381, loss = 0.695167064666748
In grad_steps = 382, loss = 0.2249673306941986
In grad_steps = 383, loss = 0.2545056641101837
In grad_steps = 384, loss = 0.21257109940052032
In grad_steps = 385, loss = 0.1873510330915451
In grad_steps = 386, loss = 1.6956965923309326
In grad_steps = 387, loss = 0.45888862013816833
In grad_steps = 388, loss = 0.7104323506355286
In grad_steps = 389, loss = 0.7302172780036926
In grad_steps = 390, loss = 0.29978078603744507
In grad_steps = 391, loss = 0.3159654140472412
In grad_steps = 392, loss = 0.17259304225444794
In grad_steps = 393, loss = 0.4860757887363434
In grad_steps = 394, loss = 0.2358420342206955
In grad_steps = 395, loss = 0.4643493592739105
In grad_steps = 396, loss = 0.3056589365005493
In grad_steps = 397, loss = 0.4521211087703705
In grad_steps = 398, loss = 0.15598291158676147
In grad_steps = 399, loss = 0.29529714584350586
In grad_steps = 400, loss = 0.43137744069099426
In grad_steps = 401, loss = 0.18548700213432312
In grad_steps = 402, loss = 0.4534260928630829
In grad_steps = 403, loss = 0.8105310201644897
In grad_steps = 404, loss = 0.12493354827165604
In grad_steps = 405, loss = 0.14752653241157532
In grad_steps = 406, loss = 1.0465331077575684
In grad_steps = 407, loss = 0.36213067173957825
In grad_steps = 408, loss = 0.09443672746419907
In grad_steps = 409, loss = 0.49026045203208923
In grad_steps = 410, loss = 0.22260504961013794
In grad_steps = 411, loss = 0.7204618453979492
In grad_steps = 412, loss = 0.2127714306116104
In grad_steps = 413, loss = 0.25383061170578003
In grad_steps = 414, loss = 0.528161346912384
In grad_steps = 415, loss = 0.38829025626182556
In grad_steps = 416, loss = 0.20129507780075073
In grad_steps = 417, loss = 0.24824394285678864
In grad_steps = 418, loss = 0.4551694691181183
In grad_steps = 419, loss = 0.23614782094955444
In grad_steps = 420, loss = 0.048721808940172195
In grad_steps = 421, loss = 1.0844002962112427
In grad_steps = 422, loss = 0.6869264841079712
In grad_steps = 423, loss = 0.18803419172763824
In grad_steps = 424, loss = 0.6346805691719055
In grad_steps = 425, loss = 0.09112010896205902
In grad_steps = 426, loss = 0.5920524597167969
In grad_steps = 427, loss = 0.27182745933532715
In grad_steps = 428, loss = 0.08680793642997742
In grad_steps = 429, loss = 0.6326028108596802
In grad_steps = 430, loss = 0.5760546922683716
In grad_steps = 431, loss = 0.4086637496948242
In grad_steps = 432, loss = 0.10421088337898254
In grad_steps = 433, loss = 0.3979836106300354
In grad_steps = 434, loss = 1.056433081626892
In grad_steps = 435, loss = 0.09243740141391754
In grad_steps = 436, loss = 0.23733612895011902
In grad_steps = 437, loss = 0.2823072075843811
In grad_steps = 438, loss = 1.009060263633728
In grad_steps = 439, loss = 0.1842833161354065
In grad_steps = 440, loss = 0.12748005986213684
In grad_steps = 441, loss = 0.5492601990699768
In grad_steps = 442, loss = 0.558615505695343
In grad_steps = 443, loss = 0.11785794049501419
In grad_steps = 444, loss = 0.5048155784606934
In grad_steps = 445, loss = 0.157440647482872
In grad_steps = 446, loss = 1.112206220626831
In grad_steps = 447, loss = 0.7596322298049927
In grad_steps = 448, loss = 0.6540648937225342
In grad_steps = 449, loss = 0.1610434651374817
In grad_steps = 450, loss = 0.45862358808517456
In grad_steps = 451, loss = 0.32149940729141235
In grad_steps = 452, loss = 0.36827409267425537
In grad_steps = 453, loss = 0.3158888518810272
In grad_steps = 454, loss = 0.3732183277606964
In grad_steps = 455, loss = 0.3623723089694977
In grad_steps = 456, loss = 0.6878705620765686
In grad_steps = 457, loss = 0.26009488105773926
In grad_steps = 458, loss = 0.7457886934280396
In grad_steps = 459, loss = 0.3703661859035492
In grad_steps = 460, loss = 0.4593747854232788
In grad_steps = 461, loss = 0.09449731558561325
In grad_steps = 462, loss = 0.6981989741325378
In grad_steps = 463, loss = 0.22652006149291992
In grad_steps = 464, loss = 0.6621175408363342
In grad_steps = 465, loss = 0.6532598733901978
In grad_steps = 466, loss = 0.5116280913352966
In grad_steps = 467, loss = 0.502822756767273
In grad_steps = 468, loss = 0.9355826377868652
In grad_steps = 469, loss = 0.4073829650878906
In grad_steps = 470, loss = 0.47633135318756104
In grad_steps = 471, loss = 0.13286538422107697
In grad_steps = 472, loss = 0.1329822540283203
In grad_steps = 473, loss = 0.2979332208633423
In grad_steps = 474, loss = 0.8375663161277771
In grad_steps = 475, loss = 0.5843711495399475
In grad_steps = 476, loss = 0.14867591857910156
In grad_steps = 477, loss = 0.24143290519714355
In grad_steps = 478, loss = 0.17512109875679016
In grad_steps = 479, loss = 0.5550386905670166
In grad_steps = 480, loss = 0.19570107758045197
In grad_steps = 481, loss = 0.8473917841911316
In grad_steps = 482, loss = 0.11021105200052261
In grad_steps = 483, loss = 0.1953345388174057
In grad_steps = 484, loss = 0.2826812267303467
In grad_steps = 485, loss = 0.4770084023475647
In grad_steps = 486, loss = 0.306622177362442
In grad_steps = 487, loss = 0.5741279721260071
In grad_steps = 488, loss = 0.900373101234436
In grad_steps = 489, loss = 0.9370648264884949
In grad_steps = 490, loss = 0.16167287528514862
In grad_steps = 491, loss = 0.4675491750240326
In grad_steps = 492, loss = 0.198842853307724
In grad_steps = 493, loss = 0.04430581256747246
In grad_steps = 494, loss = 1.0545454025268555
In grad_steps = 495, loss = 0.12784142792224884
In grad_steps = 496, loss = 0.6837382316589355
In grad_steps = 497, loss = 0.13701491057872772
In grad_steps = 498, loss = 0.487179696559906
In grad_steps = 499, loss = 0.23698890209197998
In grad_steps = 500, loss = 0.12747330963611603
In grad_steps = 501, loss = 0.5040051937103271
In grad_steps = 502, loss = 0.7110735177993774
In grad_steps = 503, loss = 0.544951319694519
In grad_steps = 504, loss = 0.45999786257743835
In grad_steps = 505, loss = 0.6361193060874939
In grad_steps = 506, loss = 0.32155507802963257
In grad_steps = 507, loss = 0.6545180082321167
In grad_steps = 508, loss = 0.7643823027610779
In grad_steps = 509, loss = 0.4938350021839142
In grad_steps = 510, loss = 0.11433733999729156
In grad_steps = 511, loss = 0.3545391261577606
In grad_steps = 512, loss = 0.24261625111103058
In grad_steps = 513, loss = 0.4925546646118164
In grad_steps = 514, loss = 0.39224711060523987
In grad_steps = 515, loss = 0.4841645061969757
In grad_steps = 516, loss = 0.5511732697486877
In grad_steps = 517, loss = 0.5229706168174744
In grad_steps = 518, loss = 0.4575127363204956
In grad_steps = 519, loss = 0.33988508582115173
In grad_steps = 520, loss = 0.17031076550483704
In grad_steps = 521, loss = 0.4939245879650116
In grad_steps = 522, loss = 0.1449316442012787
In grad_steps = 523, loss = 0.248879075050354
In grad_steps = 524, loss = 0.3144078552722931
In grad_steps = 525, loss = 0.4069555103778839
In grad_steps = 526, loss = 0.19477525353431702
In grad_steps = 527, loss = 0.5021659731864929
In grad_steps = 528, loss = 0.22777001559734344
In grad_steps = 529, loss = 0.06116756796836853
In grad_steps = 530, loss = 0.11976193636655807
In grad_steps = 531, loss = 0.5902163982391357
In grad_steps = 532, loss = 0.23309262096881866
In grad_steps = 533, loss = 0.2961999773979187
In grad_steps = 534, loss = 1.0226794481277466
In grad_steps = 535, loss = 0.5128668546676636
In grad_steps = 536, loss = 0.23924823105335236
In grad_steps = 537, loss = 0.4390258193016052
In grad_steps = 538, loss = 1.38773775100708
In grad_steps = 539, loss = 0.26026904582977295
In grad_steps = 540, loss = 0.10188110172748566
In grad_steps = 541, loss = 0.5000990629196167
In grad_steps = 542, loss = 0.7693820595741272
In grad_steps = 543, loss = 0.09870808571577072
In grad_steps = 544, loss = 0.6883740425109863
In grad_steps = 545, loss = 1.0089051723480225
In grad_steps = 546, loss = 0.2541678249835968
In grad_steps = 547, loss = 0.30949854850769043
In grad_steps = 548, loss = 0.30317816138267517
In grad_steps = 549, loss = 0.5423118472099304
In grad_steps = 550, loss = 0.6523889303207397
In grad_steps = 551, loss = 0.20151376724243164
In grad_steps = 552, loss = 0.26733505725860596
In grad_steps = 553, loss = 0.43069955706596375
In grad_steps = 554, loss = 0.6228183507919312
In grad_steps = 555, loss = 0.6411021947860718
In grad_steps = 556, loss = 0.6461428999900818
In grad_steps = 557, loss = 0.4009372889995575
In grad_steps = 558, loss = 0.43767988681793213
In grad_steps = 559, loss = 0.2828230559825897
In grad_steps = 560, loss = 0.8844119906425476
In grad_steps = 561, loss = 0.4040430188179016
In grad_steps = 562, loss = 0.4759524464607239
In grad_steps = 563, loss = 0.33762601017951965
In grad_steps = 564, loss = 0.869910478591919
In grad_steps = 565, loss = 0.44670289754867554
In grad_steps = 566, loss = 0.23877555131912231
In grad_steps = 567, loss = 0.8110784292221069
In grad_steps = 568, loss = 0.888451337814331
In grad_steps = 569, loss = 0.1829306185245514
In grad_steps = 570, loss = 0.44313761591911316
In grad_steps = 571, loss = 0.15160296857357025
In grad_steps = 572, loss = 0.28501152992248535
In grad_steps = 573, loss = 0.1810220330953598
In grad_steps = 574, loss = 0.1686909794807434
In grad_steps = 575, loss = 0.19752925634384155
In grad_steps = 576, loss = 0.15458336472511292
In grad_steps = 577, loss = 0.5459235310554504
In grad_steps = 578, loss = 0.36656874418258667
In grad_steps = 579, loss = 0.2600889503955841
In grad_steps = 580, loss = 0.8562671542167664
In grad_steps = 581, loss = 0.2769363522529602
In grad_steps = 582, loss = 0.6751438975334167
In grad_steps = 583, loss = 0.1016477644443512
In grad_steps = 584, loss = 0.04931023716926575
In grad_steps = 585, loss = 0.14734533429145813
In grad_steps = 586, loss = 0.8402606248855591
In grad_steps = 587, loss = 0.28757697343826294
In grad_steps = 588, loss = 0.3292211592197418
In grad_steps = 589, loss = 0.8773109912872314
In grad_steps = 590, loss = 0.3297068476676941
In grad_steps = 591, loss = 0.3040085732936859
In grad_steps = 592, loss = 1.470533847808838
In grad_steps = 593, loss = 0.08981579542160034
In grad_steps = 594, loss = 0.10837088525295258
In grad_steps = 595, loss = 0.3943323493003845
In grad_steps = 596, loss = 0.5394034385681152
In grad_steps = 597, loss = 0.5546653270721436
In grad_steps = 598, loss = 0.2607755661010742
In grad_steps = 599, loss = 0.25398552417755127
In grad_steps = 600, loss = 0.6629016399383545
In grad_steps = 601, loss = 0.13660800457000732
In grad_steps = 602, loss = 0.25479036569595337
In grad_steps = 603, loss = 0.3548738956451416
In grad_steps = 604, loss = 0.10301321744918823
In grad_steps = 605, loss = 0.41062670946121216
In grad_steps = 606, loss = 0.143451526761055
In grad_steps = 607, loss = 0.24624963104724884
In grad_steps = 608, loss = 0.35628312826156616
In grad_steps = 609, loss = 0.29382404685020447
In grad_steps = 610, loss = 0.506589412689209
In grad_steps = 611, loss = 0.4095478057861328
In grad_steps = 612, loss = 0.5623911619186401
In grad_steps = 613, loss = 0.5606265664100647
In grad_steps = 614, loss = 0.2534220516681671
In grad_steps = 615, loss = 0.5732353925704956
In grad_steps = 616, loss = 0.19785796105861664
In grad_steps = 617, loss = 0.07813063263893127
In grad_steps = 618, loss = 0.1786687970161438
In grad_steps = 619, loss = 0.0644192174077034
In grad_steps = 620, loss = 0.23540477454662323
In grad_steps = 621, loss = 1.2363423109054565
In grad_steps = 622, loss = 0.24828502535820007
In grad_steps = 623, loss = 0.07396085560321808
In grad_steps = 624, loss = 0.0803288072347641
In grad_steps = 625, loss = 0.6280530095100403
In grad_steps = 626, loss = 0.46651026606559753
In grad_steps = 627, loss = 0.04181790351867676
In grad_steps = 628, loss = 0.1133037582039833
In grad_steps = 629, loss = 0.09488740563392639
In grad_steps = 630, loss = 0.13426268100738525
In grad_steps = 631, loss = 0.15993113815784454
In grad_steps = 632, loss = 0.13560034334659576
In grad_steps = 633, loss = 0.8480185866355896
In grad_steps = 634, loss = 0.28136444091796875
In grad_steps = 635, loss = 0.3256775736808777
In grad_steps = 636, loss = 0.21470633149147034
In grad_steps = 637, loss = 0.3223661780357361
In grad_steps = 638, loss = 0.03700520843267441
In grad_steps = 639, loss = 0.3230116069316864
In grad_steps = 640, loss = 0.1336958110332489
In grad_steps = 641, loss = 0.16232538223266602
In grad_steps = 642, loss = 0.3764953017234802
In grad_steps = 643, loss = 0.6749615669250488
In grad_steps = 644, loss = 1.072511911392212
In grad_steps = 645, loss = 1.4053235054016113
In grad_steps = 646, loss = 0.10846748948097229
In grad_steps = 647, loss = 0.34379830956459045
In grad_steps = 648, loss = 0.04876766726374626
In grad_steps = 649, loss = 0.616112232208252
In grad_steps = 650, loss = 0.9868648648262024
In grad_steps = 651, loss = 0.257589727640152
In grad_steps = 652, loss = 0.9524163007736206
In grad_steps = 653, loss = 0.5365915298461914
In grad_steps = 654, loss = 0.667342483997345
In grad_steps = 655, loss = 0.6114382743835449
In grad_steps = 656, loss = 0.18738538026809692
In grad_steps = 657, loss = 0.21183082461357117
In grad_steps = 658, loss = 0.34165775775909424
In grad_steps = 659, loss = 0.314399778842926
In grad_steps = 660, loss = 0.4208398163318634
In grad_steps = 661, loss = 0.21969956159591675
In grad_steps = 662, loss = 0.4459800124168396
In grad_steps = 663, loss = 0.2683922052383423
In grad_steps = 664, loss = 0.11756382882595062
In grad_steps = 665, loss = 0.37090831995010376
In grad_steps = 666, loss = 0.2018287181854248
In grad_steps = 667, loss = 0.5320652723312378
In grad_steps = 668, loss = 0.12409045547246933
In grad_steps = 669, loss = 0.4026326537132263
In grad_steps = 670, loss = 0.6984097361564636
In grad_steps = 671, loss = 0.5658692121505737
In grad_steps = 672, loss = 0.11703911423683167
In grad_steps = 673, loss = 0.6865556836128235
In grad_steps = 674, loss = 0.10943224281072617
In grad_steps = 675, loss = 0.8483988642692566
In grad_steps = 676, loss = 0.14579147100448608
In grad_steps = 677, loss = 0.5328676700592041
In grad_steps = 678, loss = 0.046023063361644745
In grad_steps = 679, loss = 0.14132310450077057
In grad_steps = 680, loss = 0.04532709717750549
In grad_steps = 681, loss = 1.020506501197815
In grad_steps = 682, loss = 0.6878873705863953
In grad_steps = 683, loss = 0.2859607934951782
In grad_steps = 684, loss = 0.18661540746688843
In grad_steps = 685, loss = 0.3266408443450928
In grad_steps = 686, loss = 0.21870338916778564
In grad_steps = 687, loss = 0.37034550309181213
In grad_steps = 688, loss = 0.6744075417518616
In grad_steps = 689, loss = 0.1474585235118866
In grad_steps = 690, loss = 0.2762526273727417
In grad_steps = 691, loss = 0.22548380494117737
In grad_steps = 692, loss = 0.35043856501579285
In grad_steps = 693, loss = 0.36414244771003723
In grad_steps = 694, loss = 0.3553423285484314
In grad_steps = 695, loss = 0.4349813461303711
In grad_steps = 696, loss = 0.7611854672431946
In grad_steps = 697, loss = 0.3920210003852844
In grad_steps = 698, loss = 0.2593171000480652
In grad_steps = 699, loss = 0.686076819896698
In grad_steps = 700, loss = 0.5531669855117798
In grad_steps = 701, loss = 0.09378954023122787
In grad_steps = 702, loss = 0.17117594182491302
In grad_steps = 703, loss = 0.28122684359550476
In grad_steps = 704, loss = 0.37318217754364014
In grad_steps = 705, loss = 0.09392812848091125
In grad_steps = 706, loss = 0.6440432071685791
In grad_steps = 707, loss = 0.08517983555793762
In grad_steps = 708, loss = 0.034575000405311584
In grad_steps = 709, loss = 0.24460145831108093
In grad_steps = 710, loss = 0.37382107973098755
In grad_steps = 711, loss = 1.105558156967163
In grad_steps = 712, loss = 0.5005394220352173
In grad_steps = 713, loss = 1.674917221069336
In grad_steps = 714, loss = 0.3590220510959625
In grad_steps = 715, loss = 1.0109316110610962
In grad_steps = 716, loss = 0.39705222845077515
In grad_steps = 717, loss = 0.1382806897163391
In grad_steps = 718, loss = 0.23531793057918549
In grad_steps = 719, loss = 0.07934091240167618
In grad_steps = 720, loss = 0.052383292466402054
In grad_steps = 721, loss = 1.3535020351409912
In grad_steps = 722, loss = 0.26831185817718506
In grad_steps = 723, loss = 0.5455799102783203
In grad_steps = 724, loss = 0.09184626489877701
In grad_steps = 725, loss = 0.5641047358512878
In grad_steps = 726, loss = 0.17956183850765228
In grad_steps = 727, loss = 0.5299673080444336
In grad_steps = 728, loss = 0.10689469426870346
In grad_steps = 729, loss = 0.09378489851951599
In grad_steps = 730, loss = 0.26724445819854736
In grad_steps = 731, loss = 0.593630850315094
In grad_steps = 732, loss = 0.3787147104740143
In grad_steps = 733, loss = 0.9998701810836792
In grad_steps = 734, loss = 0.41661781072616577
In grad_steps = 735, loss = 0.3642770051956177
In grad_steps = 736, loss = 0.28807687759399414
In grad_steps = 737, loss = 0.14071175456047058
In grad_steps = 738, loss = 0.7101430892944336
In grad_steps = 739, loss = 0.10458925366401672
In grad_steps = 740, loss = 0.20808784663677216
In grad_steps = 741, loss = 0.549476683139801
In grad_steps = 742, loss = 0.13963457942008972
In grad_steps = 743, loss = 0.1582505702972412
In grad_steps = 744, loss = 0.8203607201576233
In grad_steps = 745, loss = 0.6819567680358887
In grad_steps = 746, loss = 0.25135302543640137
In grad_steps = 747, loss = 0.414734423160553
In grad_steps = 748, loss = 0.10314498841762543
In grad_steps = 749, loss = 0.8146089911460876
In grad_steps = 750, loss = 0.9397322535514832
In grad_steps = 751, loss = 0.12631776928901672
In grad_steps = 752, loss = 0.08776336163282394
In grad_steps = 753, loss = 0.162063866853714
In grad_steps = 754, loss = 0.6531206965446472
In grad_steps = 755, loss = 0.7713163495063782
In grad_steps = 756, loss = 0.14087344706058502
In grad_steps = 757, loss = 0.15328064560890198
In grad_steps = 758, loss = 0.868741512298584
In grad_steps = 759, loss = 0.5796250104904175
In grad_steps = 760, loss = 0.3172075152397156
In grad_steps = 761, loss = 0.3875746726989746
In grad_steps = 762, loss = 0.25914716720581055
In grad_steps = 763, loss = 0.4709770381450653
In grad_steps = 764, loss = 0.7525832653045654
In grad_steps = 765, loss = 0.5027880668640137
In grad_steps = 766, loss = 0.14559341967105865
In grad_steps = 767, loss = 0.4566851854324341
In grad_steps = 768, loss = 0.3179418742656708
In grad_steps = 769, loss = 0.977808952331543
In grad_steps = 770, loss = 0.2374092936515808
In grad_steps = 771, loss = 0.2082851380109787
In grad_steps = 772, loss = 0.43119314312934875
In grad_steps = 773, loss = 0.9766918420791626
In grad_steps = 774, loss = 0.4003647565841675
In grad_steps = 775, loss = 0.1454494595527649
In grad_steps = 776, loss = 0.3072206377983093
In grad_steps = 777, loss = 0.6115564107894897
In grad_steps = 778, loss = 0.1901940554380417
In grad_steps = 779, loss = 0.16300973296165466
In grad_steps = 780, loss = 0.6018625497817993
In grad_steps = 781, loss = 0.5287663340568542
In grad_steps = 782, loss = 0.17528754472732544
In grad_steps = 783, loss = 0.8589751720428467
In grad_steps = 784, loss = 0.4648289084434509
In grad_steps = 785, loss = 0.09485231339931488
In grad_steps = 786, loss = 0.15913686156272888
In grad_steps = 787, loss = 0.3428305387496948
In grad_steps = 788, loss = 0.5165706872940063
In grad_steps = 789, loss = 0.29004400968551636
In grad_steps = 790, loss = 0.4872954487800598
In grad_steps = 791, loss = 0.1207159161567688
In grad_steps = 792, loss = 0.12883546948432922
In grad_steps = 793, loss = 0.5218804478645325
In grad_steps = 794, loss = 1.7022606134414673
In grad_steps = 795, loss = 0.7507666349411011
In grad_steps = 796, loss = 0.17797093093395233
In grad_steps = 797, loss = 0.56263267993927
In grad_steps = 798, loss = 0.21808698773384094
In grad_steps = 799, loss = 0.5022015571594238
In grad_steps = 800, loss = 0.4880269467830658
In grad_steps = 801, loss = 0.32028284668922424
In grad_steps = 802, loss = 0.37402671575546265
In grad_steps = 803, loss = 0.257590115070343
In grad_steps = 804, loss = 0.08190666139125824
In grad_steps = 805, loss = 0.5901006460189819
In grad_steps = 806, loss = 0.15301421284675598
In grad_steps = 807, loss = 0.2304554283618927
In grad_steps = 808, loss = 0.29695579409599304
In grad_steps = 809, loss = 0.12888379395008087
In grad_steps = 810, loss = 0.6958653926849365
In grad_steps = 811, loss = 0.11099226772785187
In grad_steps = 812, loss = 0.48654723167419434
In grad_steps = 813, loss = 0.27399659156799316
In grad_steps = 814, loss = 0.11990751326084137
In grad_steps = 815, loss = 0.1104666218161583
In grad_steps = 816, loss = 0.9724420309066772
In grad_steps = 817, loss = 0.05652381107211113
In grad_steps = 818, loss = 0.393371045589447
In grad_steps = 819, loss = 0.058002930134534836
In grad_steps = 820, loss = 0.20665614306926727
In grad_steps = 821, loss = 0.2317342609167099
In grad_steps = 822, loss = 0.7032753825187683
In grad_steps = 823, loss = 0.25734516978263855
In grad_steps = 824, loss = 0.045981526374816895
In grad_steps = 825, loss = 0.053517136722803116
In grad_steps = 826, loss = 0.8295606970787048
In grad_steps = 827, loss = 0.17096339166164398
In grad_steps = 828, loss = 0.8218498229980469
In grad_steps = 829, loss = 0.2073928564786911
In grad_steps = 830, loss = 0.12036658823490143
In grad_steps = 831, loss = 0.039098337292671204
In grad_steps = 832, loss = 0.8659912943840027
In grad_steps = 833, loss = 0.5595884919166565
In grad_steps = 834, loss = 0.6584932804107666
In grad_steps = 835, loss = 1.5322428941726685
In grad_steps = 836, loss = 0.672580897808075
In grad_steps = 837, loss = 0.8406059145927429
In grad_steps = 838, loss = 1.2411363124847412
In grad_steps = 839, loss = 0.6706225872039795
In grad_steps = 840, loss = 0.16856065392494202
In grad_steps = 841, loss = 0.6604658961296082
In grad_steps = 842, loss = 0.20495006442070007
In grad_steps = 843, loss = 0.541744589805603
In grad_steps = 844, loss = 0.6490528583526611
In grad_steps = 845, loss = 0.44474953413009644
In grad_steps = 846, loss = 0.8575633764266968
In grad_steps = 847, loss = 0.7256646156311035
In grad_steps = 848, loss = 0.6084880828857422
In grad_steps = 849, loss = 0.891926109790802
In grad_steps = 850, loss = 0.6633710861206055
In grad_steps = 851, loss = 0.5046917200088501
In grad_steps = 852, loss = 0.46270984411239624
In grad_steps = 853, loss = 0.354145884513855
In grad_steps = 854, loss = 0.3800320625305176
In grad_steps = 855, loss = 0.5422466993331909
In grad_steps = 856, loss = 0.3761180341243744
In grad_steps = 857, loss = 0.3036149740219116
In grad_steps = 858, loss = 0.5669294595718384
In grad_steps = 859, loss = 0.3561957776546478
In grad_steps = 860, loss = 0.33178022503852844
In grad_steps = 861, loss = 0.37419891357421875
In grad_steps = 862, loss = 0.17279919981956482
In grad_steps = 863, loss = 0.27550554275512695
In grad_steps = 864, loss = 0.2510787844657898
In grad_steps = 865, loss = 0.4436936676502228
In grad_steps = 866, loss = 0.14948762953281403
In grad_steps = 867, loss = 0.21985943615436554
In grad_steps = 868, loss = 0.07066483050584793
In grad_steps = 869, loss = 0.6136046051979065
In grad_steps = 870, loss = 0.037338919937610626
In grad_steps = 871, loss = 0.1736045479774475
In grad_steps = 872, loss = 0.2298424243927002
In grad_steps = 873, loss = 0.6526328921318054
In grad_steps = 874, loss = 0.05725473165512085
In grad_steps = 875, loss = 0.010692628100514412
In grad_steps = 876, loss = 0.1353171467781067
In grad_steps = 877, loss = 0.14643250405788422
In grad_steps = 878, loss = 0.2930432856082916
In grad_steps = 879, loss = 0.03627216815948486
In grad_steps = 880, loss = 0.8848690390586853
In grad_steps = 881, loss = 0.7859014868736267
In grad_steps = 882, loss = 0.4897499084472656
In grad_steps = 883, loss = 0.7481001019477844
In grad_steps = 884, loss = 0.1621636152267456
In grad_steps = 885, loss = 0.05841470882296562
In grad_steps = 886, loss = 0.5502128601074219
In grad_steps = 887, loss = 0.1292359083890915
In grad_steps = 888, loss = 0.22073665261268616
In grad_steps = 889, loss = 0.794391393661499
In grad_steps = 890, loss = 0.44407394528388977
In grad_steps = 891, loss = 0.19737088680267334
In grad_steps = 892, loss = 0.9422749280929565
In grad_steps = 893, loss = 0.2661369740962982
In grad_steps = 894, loss = 0.25963088870048523
In grad_steps = 895, loss = 0.33323705196380615
In grad_steps = 896, loss = 0.30643466114997864
In grad_steps = 897, loss = 0.5281788110733032
In grad_steps = 898, loss = 0.9632574319839478
In grad_steps = 899, loss = 0.22580713033676147
In grad_steps = 900, loss = 0.3837606608867645
In grad_steps = 901, loss = 0.5029719471931458
In grad_steps = 902, loss = 0.2805525064468384
In grad_steps = 903, loss = 0.08017565310001373
In grad_steps = 904, loss = 0.2240009754896164
In grad_steps = 905, loss = 0.24185806512832642
In grad_steps = 906, loss = 0.2286832183599472
In grad_steps = 907, loss = 0.19396957755088806
In grad_steps = 908, loss = 0.17518968880176544
In grad_steps = 909, loss = 0.6410019993782043
In grad_steps = 910, loss = 0.05390129238367081
In grad_steps = 911, loss = 0.13116665184497833
In grad_steps = 912, loss = 0.09135924279689789
In grad_steps = 913, loss = 0.7468904852867126
In grad_steps = 914, loss = 0.50697261095047
In grad_steps = 915, loss = 0.21156084537506104
In grad_steps = 916, loss = 0.08819671720266342
In grad_steps = 917, loss = 0.1648714691400528
In grad_steps = 918, loss = 1.0237452983856201
In grad_steps = 919, loss = 2.159771203994751
In grad_steps = 920, loss = 0.8124620914459229
In grad_steps = 921, loss = 0.6446446180343628
In grad_steps = 922, loss = 0.039240188896656036
In grad_steps = 923, loss = 1.1342787742614746
In grad_steps = 924, loss = 0.9595382213592529
In grad_steps = 925, loss = 0.13109290599822998
In grad_steps = 926, loss = 0.23236531019210815
In grad_steps = 927, loss = 0.5038037896156311
In grad_steps = 928, loss = 0.7635312080383301
In grad_steps = 929, loss = 0.22234392166137695
In grad_steps = 930, loss = 0.443511962890625
In grad_steps = 931, loss = 0.3723704516887665
In grad_steps = 932, loss = 0.4549563527107239
In grad_steps = 933, loss = 0.4509463310241699
In grad_steps = 934, loss = 0.3018454313278198
In grad_steps = 935, loss = 0.32617390155792236
In grad_steps = 936, loss = 0.28632843494415283
In grad_steps = 937, loss = 0.5466452240943909
In grad_steps = 938, loss = 0.3802007734775543
In grad_steps = 939, loss = 0.4973992109298706
In grad_steps = 940, loss = 0.5489811301231384
In grad_steps = 941, loss = 0.18910378217697144
In grad_steps = 942, loss = 0.2836577594280243
In grad_steps = 943, loss = 0.872221827507019
In grad_steps = 944, loss = 0.3134697377681732
In grad_steps = 945, loss = 0.13733993470668793
In grad_steps = 946, loss = 0.1847536861896515
In grad_steps = 947, loss = 0.7171040773391724
In grad_steps = 948, loss = 0.447454035282135
In grad_steps = 949, loss = 0.7374260425567627
In grad_steps = 950, loss = 0.1828777939081192
In grad_steps = 951, loss = 0.6219693422317505
In grad_steps = 952, loss = 0.23434293270111084
In grad_steps = 953, loss = 0.6174445152282715
In grad_steps = 954, loss = 0.6008050441741943
In grad_steps = 955, loss = 0.4990154504776001
In grad_steps = 956, loss = 0.6396269202232361
In grad_steps = 957, loss = 0.6215159893035889
In grad_steps = 958, loss = 0.4101892113685608
In grad_steps = 959, loss = 0.2552527189254761
In grad_steps = 960, loss = 0.28916269540786743
In grad_steps = 961, loss = 0.4258255958557129
In grad_steps = 962, loss = 0.5884015560150146
In grad_steps = 963, loss = 0.9561660289764404
In grad_steps = 964, loss = 0.3022409677505493
In grad_steps = 965, loss = 0.5253981351852417
In grad_steps = 966, loss = 0.6158667206764221
In grad_steps = 967, loss = 1.087283730506897
In grad_steps = 968, loss = 0.21107488870620728
In grad_steps = 969, loss = 0.46157899498939514
In grad_steps = 970, loss = 0.445165753364563
In grad_steps = 971, loss = 0.19654303789138794
In grad_steps = 972, loss = 0.24405236542224884
In grad_steps = 973, loss = 0.4347717761993408
In grad_steps = 974, loss = 0.12807591259479523
In grad_steps = 975, loss = 0.3579341471195221
In grad_steps = 976, loss = 0.19902902841567993
In grad_steps = 977, loss = 0.2515915632247925
In grad_steps = 978, loss = 0.13347427546977997
In grad_steps = 979, loss = 0.5826384425163269
In grad_steps = 980, loss = 0.3058713972568512
In grad_steps = 981, loss = 0.5226512551307678
In grad_steps = 982, loss = 0.2986701726913452
In grad_steps = 983, loss = 0.47692370414733887
In grad_steps = 984, loss = 0.05507233366370201
In grad_steps = 985, loss = 0.3618394434452057
In grad_steps = 986, loss = 1.3218648433685303
In grad_steps = 987, loss = 0.19768239557743073
In grad_steps = 988, loss = 0.06281913816928864
In grad_steps = 989, loss = 0.07621228694915771
In grad_steps = 990, loss = 0.42508405447006226
In grad_steps = 991, loss = 0.38491857051849365
In grad_steps = 992, loss = 0.9506874084472656
In grad_steps = 993, loss = 0.39134010672569275
In grad_steps = 994, loss = 0.38243788480758667
In grad_steps = 995, loss = 0.09456788748502731
In grad_steps = 996, loss = 0.18616712093353271
In grad_steps = 997, loss = 0.12016613036394119
In grad_steps = 998, loss = 0.073068767786026
In grad_steps = 999, loss = 0.6935144066810608
In grad_steps = 1000, loss = 0.36843520402908325
In grad_steps = 1001, loss = 0.24119606614112854
In grad_steps = 1002, loss = 0.08795147389173508
In grad_steps = 1003, loss = 0.4685463309288025
In grad_steps = 1004, loss = 0.2973446846008301
In grad_steps = 1005, loss = 0.2554017901420593
In grad_steps = 1006, loss = 0.20912256836891174
In grad_steps = 1007, loss = 0.2674282491207123
In grad_steps = 1008, loss = 0.0786026194691658
In grad_steps = 1009, loss = 0.0376930758357048
In grad_steps = 1010, loss = 0.019871052354574203
In grad_steps = 1011, loss = 0.3611816167831421
In grad_steps = 1012, loss = 0.9380836486816406
In grad_steps = 1013, loss = 0.1267191618680954
In grad_steps = 1014, loss = 0.7104862928390503
In grad_steps = 1015, loss = 0.7750439643859863
In grad_steps = 1016, loss = 0.23655299842357635
In grad_steps = 1017, loss = 0.9876934885978699
In grad_steps = 1018, loss = 0.16223175823688507
In grad_steps = 1019, loss = 1.1876554489135742
In grad_steps = 1020, loss = 0.30474716424942017
In grad_steps = 1021, loss = 0.12507712841033936
In grad_steps = 1022, loss = 0.7230842709541321
In grad_steps = 1023, loss = 0.17716576159000397
In grad_steps = 1024, loss = 0.7528123259544373
In grad_steps = 1025, loss = 0.6562850475311279
In grad_steps = 1026, loss = 0.9839141964912415
In grad_steps = 1027, loss = 0.2636660635471344
In grad_steps = 1028, loss = 0.23762747645378113
In grad_steps = 1029, loss = 0.6155946850776672
In grad_steps = 1030, loss = 0.5261621475219727
In grad_steps = 1031, loss = 0.6020968556404114
In grad_steps = 1032, loss = 0.22319334745407104
In grad_steps = 1033, loss = 0.6743959784507751
In grad_steps = 1034, loss = 0.33927521109580994
In grad_steps = 1035, loss = 0.5733827352523804
In grad_steps = 1036, loss = 0.23183801770210266
In grad_steps = 1037, loss = 0.39806467294692993
In grad_steps = 1038, loss = 0.33095747232437134
In grad_steps = 1039, loss = 0.22371327877044678
In grad_steps = 1040, loss = 0.5755325555801392
In grad_steps = 1041, loss = 0.12622229754924774
In grad_steps = 1042, loss = 0.18347573280334473
In grad_steps = 1043, loss = 0.17774061858654022
In grad_steps = 1044, loss = 0.8872383832931519
In grad_steps = 1045, loss = 0.05516490340232849
In grad_steps = 1046, loss = 0.3342526853084564
In grad_steps = 1047, loss = 0.0615743063390255
In grad_steps = 1048, loss = 0.16301585733890533
In grad_steps = 1049, loss = 0.20911787450313568
In grad_steps = 1050, loss = 0.7764532566070557
In grad_steps = 1051, loss = 0.5147603750228882
In grad_steps = 1052, loss = 0.7053214311599731
In grad_steps = 1053, loss = 0.05026968568563461
In grad_steps = 1054, loss = 0.5150054097175598
In grad_steps = 1055, loss = 0.7815505862236023
In grad_steps = 1056, loss = 0.09842047095298767
In grad_steps = 1057, loss = 0.19635076820850372
In grad_steps = 1058, loss = 0.20205682516098022
In grad_steps = 1059, loss = 0.5940743684768677
In grad_steps = 1060, loss = 0.08078932762145996
In grad_steps = 1061, loss = 0.17738032341003418
In grad_steps = 1062, loss = 0.04510970413684845
In grad_steps = 1063, loss = 0.6756042242050171
In grad_steps = 1064, loss = 0.14546184241771698
In grad_steps = 1065, loss = 0.30295658111572266
In grad_steps = 1066, loss = 0.9386983513832092
In grad_steps = 1067, loss = 0.4463118612766266
In grad_steps = 1068, loss = 0.030171062797307968
In grad_steps = 1069, loss = 0.5088738203048706
In grad_steps = 1070, loss = 0.5001093745231628
In grad_steps = 1071, loss = 0.28524965047836304
In grad_steps = 1072, loss = 0.18248479068279266
In grad_steps = 1073, loss = 1.0365345478057861
In grad_steps = 1074, loss = 0.26738929748535156
In grad_steps = 1075, loss = 0.0689406543970108
In grad_steps = 1076, loss = 0.18889492750167847
In grad_steps = 1077, loss = 0.7056627869606018
In grad_steps = 1078, loss = 0.4122789204120636
In grad_steps = 1079, loss = 0.8431190252304077
In grad_steps = 1080, loss = 0.12549498677253723
In grad_steps = 1081, loss = 0.1828313171863556
In grad_steps = 1082, loss = 0.5628082156181335
In grad_steps = 1083, loss = 0.22428210079669952
In grad_steps = 1084, loss = 0.24247388541698456
In grad_steps = 1085, loss = 0.3214842677116394
In grad_steps = 1086, loss = 0.20261719822883606
In grad_steps = 1087, loss = 0.3981574773788452
In grad_steps = 1088, loss = 0.43794381618499756
In grad_steps = 1089, loss = 0.23174136877059937
In grad_steps = 1090, loss = 0.42527517676353455
In grad_steps = 1091, loss = 0.5153665542602539
In grad_steps = 1092, loss = 0.30504271388053894
In grad_steps = 1093, loss = 0.2464759200811386
In grad_steps = 1094, loss = 0.2130071222782135
In grad_steps = 1095, loss = 0.219757080078125
In grad_steps = 1096, loss = 0.16948920488357544
In grad_steps = 1097, loss = 0.11840172111988068
In grad_steps = 1098, loss = 0.1983589082956314
In grad_steps = 1099, loss = 1.0848520994186401
In grad_steps = 1100, loss = 0.3109408915042877
In grad_steps = 1101, loss = 0.10009834170341492
In grad_steps = 1102, loss = 0.3106193244457245
In grad_steps = 1103, loss = 0.5076886415481567
In grad_steps = 1104, loss = 0.14204706251621246
In grad_steps = 1105, loss = 0.03910130634903908
In grad_steps = 1106, loss = 0.3560692369937897
In grad_steps = 1107, loss = 0.37421637773513794
In grad_steps = 1108, loss = 0.5322555899620056
In grad_steps = 1109, loss = 0.09320087730884552
In grad_steps = 1110, loss = 0.42236554622650146
In grad_steps = 1111, loss = 0.24800123274326324
In grad_steps = 1112, loss = 1.0292181968688965
In grad_steps = 1113, loss = 0.784288763999939
In grad_steps = 1114, loss = 1.056032419204712
In grad_steps = 1115, loss = 0.07431289553642273
In grad_steps = 1116, loss = 0.10018349438905716
In grad_steps = 1117, loss = 0.2995479106903076
In grad_steps = 1118, loss = 0.22059185802936554
In grad_steps = 1119, loss = 0.18050995469093323
In grad_steps = 1120, loss = 0.07572411745786667
In grad_steps = 1121, loss = 0.10837427526712418
In grad_steps = 1122, loss = 0.14593222737312317
In grad_steps = 1123, loss = 0.6679392457008362
In grad_steps = 1124, loss = 0.7607111930847168
In grad_steps = 1125, loss = 0.14660939574241638
In grad_steps = 1126, loss = 0.21192318201065063
In grad_steps = 1127, loss = 0.2097851186990738
In grad_steps = 1128, loss = 0.07337646186351776
In grad_steps = 1129, loss = 0.42822661995887756
In grad_steps = 1130, loss = 0.9380154609680176
In grad_steps = 1131, loss = 0.7264652252197266
In grad_steps = 1132, loss = 0.15008896589279175
In grad_steps = 1133, loss = 0.06069786474108696
In grad_steps = 1134, loss = 0.5963370203971863
In grad_steps = 1135, loss = 0.7800764441490173
In grad_steps = 1136, loss = 0.1271502524614334
In grad_steps = 1137, loss = 0.4028555452823639
In grad_steps = 1138, loss = 0.3671298921108246
In grad_steps = 1139, loss = 0.3709576427936554
In grad_steps = 1140, loss = 0.13516521453857422
In grad_steps = 1141, loss = 0.4119279980659485
In grad_steps = 1142, loss = 0.43986329436302185
In grad_steps = 1143, loss = 0.5674266815185547
In grad_steps = 1144, loss = 0.16943886876106262
In grad_steps = 1145, loss = 0.11890504509210587
In grad_steps = 1146, loss = 0.3815779685974121
In grad_steps = 1147, loss = 0.37192776799201965
In grad_steps = 1148, loss = 0.3989562392234802
In grad_steps = 1149, loss = 0.25983086228370667
In grad_steps = 1150, loss = 0.14702078700065613
In grad_steps = 1151, loss = 0.7785471677780151
In grad_steps = 1152, loss = 0.05065227672457695
In grad_steps = 1153, loss = 0.3350941836833954
In grad_steps = 1154, loss = 0.07887889444828033
In grad_steps = 1155, loss = 0.5847023725509644
In grad_steps = 1156, loss = 0.6800588965415955
In grad_steps = 1157, loss = 0.045081935822963715
In grad_steps = 1158, loss = 0.11570575088262558
In grad_steps = 1159, loss = 0.06540025770664215
In grad_steps = 1160, loss = 0.2067885398864746
In grad_steps = 1161, loss = 0.394952654838562
In grad_steps = 1162, loss = 0.07787838578224182
In grad_steps = 1163, loss = 0.10312144458293915
In grad_steps = 1164, loss = 0.09507505595684052
In grad_steps = 1165, loss = 0.5784841775894165
In grad_steps = 1166, loss = 0.36864370107650757
In grad_steps = 1167, loss = 0.16420766711235046
In grad_steps = 1168, loss = 0.6182460784912109
In grad_steps = 1169, loss = 1.1945111751556396
In grad_steps = 1170, loss = 0.3560447692871094
In grad_steps = 1171, loss = 0.251885324716568
In grad_steps = 1172, loss = 0.07475980371236801
In grad_steps = 1173, loss = 0.27559053897857666
In grad_steps = 1174, loss = 1.3792394399642944
In grad_steps = 1175, loss = 1.0202537775039673
In grad_steps = 1176, loss = 0.795903205871582
In grad_steps = 1177, loss = 0.40192294120788574
In grad_steps = 1178, loss = 0.07839666306972504
In grad_steps = 1179, loss = 0.2558118402957916
In grad_steps = 1180, loss = 0.3093104064464569
In grad_steps = 1181, loss = 0.6085776686668396
In grad_steps = 1182, loss = 0.6540170907974243
In grad_steps = 1183, loss = 0.3216670751571655
In grad_steps = 1184, loss = 0.11165309697389603
In grad_steps = 1185, loss = 0.6952145099639893
In grad_steps = 1186, loss = 0.525235652923584
In grad_steps = 1187, loss = 0.7688860297203064
In grad_steps = 1188, loss = 0.12315361946821213
In grad_steps = 1189, loss = 0.6400041580200195
In grad_steps = 1190, loss = 0.3685472011566162
In grad_steps = 1191, loss = 0.26782718300819397
In grad_steps = 1192, loss = 0.22923871874809265
In grad_steps = 1193, loss = 0.16889449954032898
In grad_steps = 1194, loss = 0.14434577524662018
In grad_steps = 1195, loss = 0.7063736915588379
In grad_steps = 1196, loss = 0.8374263644218445
In grad_steps = 1197, loss = 0.17313294112682343
In grad_steps = 1198, loss = 0.12903426587581635
In grad_steps = 1199, loss = 0.6266707181930542
In grad_steps = 1200, loss = 0.39183247089385986
In grad_steps = 1201, loss = 0.25643080472946167
In grad_steps = 1202, loss = 0.12313072383403778
In grad_steps = 1203, loss = 0.25769123435020447
In grad_steps = 1204, loss = 0.13881637156009674
In grad_steps = 1205, loss = 0.10411141067743301
In grad_steps = 1206, loss = 0.813037097454071
In grad_steps = 1207, loss = 0.288841187953949
In grad_steps = 1208, loss = 0.09558984637260437
In grad_steps = 1209, loss = 0.4978649616241455
In grad_steps = 1210, loss = 0.040817052125930786
In grad_steps = 1211, loss = 0.429461270570755
In grad_steps = 1212, loss = 0.029977358877658844
In grad_steps = 1213, loss = 0.5985338687896729
In grad_steps = 1214, loss = 0.24812349677085876
In grad_steps = 1215, loss = 0.2536969780921936
In grad_steps = 1216, loss = 0.10518579185009003
In grad_steps = 1217, loss = 0.03421463444828987
In grad_steps = 1218, loss = 0.023162953555583954
In grad_steps = 1219, loss = 0.029772544279694557
In grad_steps = 1220, loss = 1.0180102586746216
In grad_steps = 1221, loss = 0.9719364047050476
In grad_steps = 1222, loss = 0.36063212156295776
In grad_steps = 1223, loss = 0.8274340629577637
In grad_steps = 1224, loss = 0.025450000539422035
In grad_steps = 1225, loss = 0.4644574522972107
In grad_steps = 1226, loss = 0.10749339312314987
In grad_steps = 1227, loss = 0.08183412253856659
In grad_steps = 1228, loss = 0.4819061756134033
In grad_steps = 1229, loss = 0.0938740000128746
In grad_steps = 1230, loss = 0.09664460271596909
In grad_steps = 1231, loss = 0.7255368232727051
In grad_steps = 1232, loss = 1.0333367586135864
In grad_steps = 1233, loss = 0.18684878945350647
In grad_steps = 1234, loss = 0.46653419733047485
In grad_steps = 1235, loss = 1.1362279653549194
In grad_steps = 1236, loss = 0.22641980648040771
In grad_steps = 1237, loss = 0.5638861656188965
In grad_steps = 1238, loss = 0.47716662287712097
In grad_steps = 1239, loss = 0.5944435596466064
In grad_steps = 1240, loss = 0.7685821652412415
In grad_steps = 1241, loss = 0.5227555632591248
In grad_steps = 1242, loss = 0.2907295227050781
In grad_steps = 1243, loss = 0.7157561182975769
In grad_steps = 1244, loss = 0.48535382747650146
In grad_steps = 1245, loss = 0.34733718633651733
In grad_steps = 1246, loss = 0.32180070877075195
In grad_steps = 1247, loss = 0.6360515356063843
In grad_steps = 1248, loss = 0.9680322408676147
In grad_steps = 1249, loss = 0.23861053586006165
In grad_steps = 1250, loss = 0.26892706751823425
In grad_steps = 1251, loss = 0.3266085386276245
In grad_steps = 1252, loss = 0.44710782170295715
In grad_steps = 1253, loss = 0.9034180641174316
In grad_steps = 1254, loss = 0.1542486697435379
In grad_steps = 1255, loss = 0.2637958228588104
In grad_steps = 1256, loss = 0.6754963994026184
In grad_steps = 1257, loss = 0.04352130740880966
In grad_steps = 1258, loss = 0.8593703508377075
In grad_steps = 1259, loss = 0.18151134252548218
In grad_steps = 1260, loss = 0.46881577372550964
In grad_steps = 1261, loss = 0.20831990242004395
In grad_steps = 1262, loss = 0.0508008673787117
In grad_steps = 1263, loss = 0.12356407940387726
In grad_steps = 1264, loss = 1.0023603439331055
In grad_steps = 1265, loss = 0.24688656628131866
In grad_steps = 1266, loss = 0.8375032544136047
In grad_steps = 1267, loss = 0.19926996529102325
In grad_steps = 1268, loss = 0.22954508662223816
In grad_steps = 1269, loss = 0.24304687976837158
In grad_steps = 1270, loss = 0.3207074701786041
In grad_steps = 1271, loss = 0.6527001857757568
In grad_steps = 1272, loss = 0.02511986531317234
In grad_steps = 1273, loss = 0.12235856056213379
In grad_steps = 1274, loss = 0.17210134863853455
In grad_steps = 1275, loss = 0.04155641049146652
In grad_steps = 1276, loss = 0.3276675343513489
In grad_steps = 1277, loss = 0.08733156323432922
In grad_steps = 1278, loss = 0.029247567057609558
In grad_steps = 1279, loss = 0.014924317598342896
In grad_steps = 1280, loss = 0.03901761770248413
In grad_steps = 1281, loss = 0.023652303963899612
In grad_steps = 1282, loss = 0.2210434526205063
In grad_steps = 1283, loss = 0.028152896091341972
In grad_steps = 1284, loss = 0.016059236600995064
In grad_steps = 1285, loss = 0.7790005207061768
In grad_steps = 1286, loss = 0.6452597379684448
In grad_steps = 1287, loss = 0.6303025484085083
In grad_steps = 1288, loss = 0.23133337497711182
In grad_steps = 1289, loss = 0.07611031830310822
In grad_steps = 1290, loss = 0.5817490816116333
In grad_steps = 1291, loss = 0.03432624414563179
In grad_steps = 1292, loss = 0.3213788568973541
In grad_steps = 1293, loss = 0.08145027607679367
In grad_steps = 1294, loss = 0.8336033821105957
In grad_steps = 1295, loss = 0.3296058475971222
In grad_steps = 1296, loss = 0.5517141819000244
In grad_steps = 1297, loss = 0.1807868480682373
In grad_steps = 1298, loss = 0.1967417299747467
In grad_steps = 1299, loss = 0.6193562150001526
In grad_steps = 1300, loss = 0.4040099084377289
In grad_steps = 1301, loss = 0.027133679017424583
In grad_steps = 1302, loss = 0.5179682970046997
In grad_steps = 1303, loss = 0.2095733880996704
In grad_steps = 1304, loss = 0.7586750388145447
In grad_steps = 1305, loss = 0.06660780310630798
In grad_steps = 1306, loss = 0.31905412673950195
In grad_steps = 1307, loss = 0.1532694399356842
In grad_steps = 1308, loss = 0.35504084825515747
In grad_steps = 1309, loss = 0.31848427653312683
In grad_steps = 1310, loss = 0.3349779546260834
In grad_steps = 1311, loss = 0.043392203748226166
In grad_steps = 1312, loss = 0.5343260765075684
In grad_steps = 1313, loss = 0.02278578281402588
In grad_steps = 1314, loss = 0.333761990070343
In grad_steps = 1315, loss = 0.053476206958293915
In grad_steps = 1316, loss = 1.1598106622695923
In grad_steps = 1317, loss = 0.2113601267337799
In grad_steps = 1318, loss = 0.5683306455612183
In grad_steps = 1319, loss = 0.529823899269104
In grad_steps = 1320, loss = 0.48354122042655945
In grad_steps = 1321, loss = 0.18533068895339966
In grad_steps = 1322, loss = 0.6877788305282593
In grad_steps = 1323, loss = 0.046625226736068726
In grad_steps = 1324, loss = 0.8205450177192688
In grad_steps = 1325, loss = 0.3420470356941223
In grad_steps = 1326, loss = 0.5189963579177856
In grad_steps = 1327, loss = 0.3797435462474823
In grad_steps = 1328, loss = 0.9028691053390503
In grad_steps = 1329, loss = 0.14774659276008606
In grad_steps = 1330, loss = 0.3156828284263611
In grad_steps = 1331, loss = 0.15936115384101868
In grad_steps = 1332, loss = 0.5345308780670166
In grad_steps = 1333, loss = 0.09064535796642303
In grad_steps = 1334, loss = 0.17102883756160736
In grad_steps = 1335, loss = 0.2856726348400116
In grad_steps = 1336, loss = 0.133407324552536
In grad_steps = 1337, loss = 0.3194953501224518
In grad_steps = 1338, loss = 0.6419956088066101
In grad_steps = 1339, loss = 0.5540758967399597
In grad_steps = 1340, loss = 0.8106034398078918
In grad_steps = 1341, loss = 0.42148181796073914
In grad_steps = 1342, loss = 0.1082390546798706
In grad_steps = 1343, loss = 0.14825372397899628
In grad_steps = 1344, loss = 0.2652668356895447
In grad_steps = 1345, loss = 0.6637707352638245
In grad_steps = 1346, loss = 0.16824181377887726
In grad_steps = 1347, loss = 0.2510833442211151
In grad_steps = 1348, loss = 0.5552164316177368
In grad_steps = 1349, loss = 0.09988115727901459
In grad_steps = 1350, loss = 0.08599992096424103
In grad_steps = 1351, loss = 0.1313762366771698
In grad_steps = 1352, loss = 0.12659938633441925
In grad_steps = 1353, loss = 0.21447303891181946
In grad_steps = 1354, loss = 0.3805919587612152
In grad_steps = 1355, loss = 0.328846275806427
In grad_steps = 1356, loss = 0.04036823660135269
In grad_steps = 1357, loss = 0.2335978001356125
In grad_steps = 1358, loss = 0.8575273156166077
In grad_steps = 1359, loss = 0.059701789170503616
In grad_steps = 1360, loss = 0.34353965520858765
In grad_steps = 1361, loss = 0.041210826486349106
In grad_steps = 1362, loss = 0.025148052722215652
In grad_steps = 1363, loss = 0.041095152497291565
In grad_steps = 1364, loss = 0.14555728435516357
In grad_steps = 1365, loss = 0.26501578092575073
In grad_steps = 1366, loss = 0.39766618609428406
In grad_steps = 1367, loss = 1.535507321357727
In grad_steps = 1368, loss = 0.565064013004303
In grad_steps = 1369, loss = 0.29924169182777405
In grad_steps = 1370, loss = 0.7400643229484558
In grad_steps = 1371, loss = 0.095308318734169
In grad_steps = 1372, loss = 1.4223217964172363
In grad_steps = 1373, loss = 0.3598911166191101
In grad_steps = 1374, loss = 0.05753403156995773
In grad_steps = 1375, loss = 0.348021537065506
In grad_steps = 1376, loss = 0.13534507155418396
In grad_steps = 1377, loss = 0.6612282395362854
In grad_steps = 1378, loss = 0.12041816860437393
In grad_steps = 1379, loss = 0.5226069092750549
In grad_steps = 1380, loss = 0.28608864545822144
In grad_steps = 1381, loss = 0.14849720895290375
In grad_steps = 1382, loss = 0.8937864899635315
In grad_steps = 1383, loss = 0.26071757078170776
In grad_steps = 1384, loss = 0.04862702637910843
In grad_steps = 1385, loss = 0.2962941825389862
In grad_steps = 1386, loss = 0.15569360554218292
In grad_steps = 1387, loss = 0.36438286304473877
In grad_steps = 1388, loss = 0.4822286367416382
In grad_steps = 1389, loss = 0.06758725643157959
In grad_steps = 1390, loss = 0.11535198986530304
In grad_steps = 1391, loss = 0.09453124552965164
In grad_steps = 1392, loss = 0.056906264275312424
In grad_steps = 1393, loss = 0.8570194840431213
In grad_steps = 1394, loss = 0.16186507046222687
In grad_steps = 1395, loss = 0.39432209730148315
In grad_steps = 1396, loss = 0.15779507160186768
In grad_steps = 1397, loss = 1.027449131011963
In grad_steps = 1398, loss = 0.16559092700481415
In grad_steps = 1399, loss = 0.7742933630943298
In grad_steps = 1400, loss = 0.4045586585998535
In grad_steps = 1401, loss = 0.614832878112793
In grad_steps = 1402, loss = 0.06959857791662216
In grad_steps = 1403, loss = 0.23254841566085815
In grad_steps = 1404, loss = 0.18103954195976257
In grad_steps = 1405, loss = 0.05942940711975098
In grad_steps = 1406, loss = 0.33128735423088074
In grad_steps = 1407, loss = 0.33181294798851013
In grad_steps = 1408, loss = 0.4431021213531494
In grad_steps = 1409, loss = 0.34607967734336853
In grad_steps = 1410, loss = 0.6183252334594727
In grad_steps = 1411, loss = 0.10911257565021515
In grad_steps = 1412, loss = 0.17565275728702545
In grad_steps = 1413, loss = 0.12352077662944794
In grad_steps = 1414, loss = 0.08025963604450226
In grad_steps = 1415, loss = 0.06421635299921036
In grad_steps = 1416, loss = 0.21477682888507843
In grad_steps = 1417, loss = 0.5029088258743286
In grad_steps = 1418, loss = 0.03765972703695297
In grad_steps = 1419, loss = 0.17797647416591644
In grad_steps = 1420, loss = 0.1085706576704979
In grad_steps = 1421, loss = 0.8206322193145752
In grad_steps = 1422, loss = 0.12228888273239136
In grad_steps = 1423, loss = 0.025773005560040474
In grad_steps = 1424, loss = 0.5461548566818237
In grad_steps = 1425, loss = 0.1330634355545044
In grad_steps = 1426, loss = 0.13363638520240784
In grad_steps = 1427, loss = 0.15163441002368927
In grad_steps = 1428, loss = 0.006007014308124781
In grad_steps = 1429, loss = 1.3200801610946655
In grad_steps = 1430, loss = 1.4331049919128418
In grad_steps = 1431, loss = 0.07041462510824203
In grad_steps = 1432, loss = 0.2709773778915405
In grad_steps = 1433, loss = 0.5725920796394348
In grad_steps = 1434, loss = 0.7178844213485718
In grad_steps = 1435, loss = 0.17825162410736084
In grad_steps = 1436, loss = 2.205461025238037
In grad_steps = 1437, loss = 0.8700190782546997
In grad_steps = 1438, loss = 0.07324802875518799
In grad_steps = 1439, loss = 0.15059079229831696
In grad_steps = 1440, loss = 0.6623557806015015
In grad_steps = 1441, loss = 0.2225264012813568
In grad_steps = 1442, loss = 0.1030215322971344
In grad_steps = 1443, loss = 0.33633214235305786
In grad_steps = 1444, loss = 0.36922985315322876
In grad_steps = 1445, loss = 0.40606123208999634
In grad_steps = 1446, loss = 0.14904814958572388
In grad_steps = 1447, loss = 0.1522676795721054
In grad_steps = 1448, loss = 0.33735543489456177
In grad_steps = 1449, loss = 0.2466403692960739
In grad_steps = 1450, loss = 0.1382596343755722
In grad_steps = 1451, loss = 0.28692829608917236
In grad_steps = 1452, loss = 0.6747493147850037
In grad_steps = 1453, loss = 0.07649596035480499
In grad_steps = 1454, loss = 0.3704445958137512
In grad_steps = 1455, loss = 0.7024950981140137
In grad_steps = 1456, loss = 0.5196885466575623
In grad_steps = 1457, loss = 0.5120960474014282
In grad_steps = 1458, loss = 0.26725661754608154
In grad_steps = 1459, loss = 0.4147055447101593
In grad_steps = 1460, loss = 0.06298328936100006
In grad_steps = 1461, loss = 0.437605082988739
In grad_steps = 1462, loss = 0.28067123889923096
In grad_steps = 1463, loss = 0.6150238513946533
In grad_steps = 1464, loss = 0.16234935820102692
In grad_steps = 1465, loss = 0.8760194778442383
In grad_steps = 1466, loss = 0.1736183762550354
In grad_steps = 1467, loss = 0.1092972382903099
In grad_steps = 1468, loss = 0.1359943449497223
In grad_steps = 1469, loss = 0.5716917514801025
In grad_steps = 1470, loss = 0.059009797871112823
In grad_steps = 1471, loss = 0.32245469093322754
In grad_steps = 1472, loss = 0.9795742034912109
In grad_steps = 1473, loss = 0.11178342998027802
In grad_steps = 1474, loss = 0.33220210671424866
In grad_steps = 1475, loss = 1.1231040954589844
In grad_steps = 1476, loss = 0.23834708333015442
In grad_steps = 1477, loss = 0.13802137970924377
In grad_steps = 1478, loss = 0.7554816603660583
In grad_steps = 1479, loss = 0.34730756282806396
In grad_steps = 1480, loss = 0.2227141410112381
In grad_steps = 1481, loss = 0.24449172616004944
In grad_steps = 1482, loss = 0.0780225396156311
In grad_steps = 1483, loss = 0.4592278003692627
In grad_steps = 1484, loss = 0.10407664626836777
In grad_steps = 1485, loss = 0.10954416543245316
In grad_steps = 1486, loss = 0.4743897020816803
In grad_steps = 1487, loss = 0.18195539712905884
In grad_steps = 1488, loss = 0.06058792769908905
In grad_steps = 1489, loss = 0.47931286692619324
In grad_steps = 1490, loss = 0.09131358563899994
In grad_steps = 1491, loss = 0.18906128406524658
In grad_steps = 1492, loss = 0.18163864314556122
In grad_steps = 1493, loss = 0.3640683889389038
In grad_steps = 1494, loss = 0.6608453989028931
In grad_steps = 1495, loss = 0.13399076461791992
In grad_steps = 1496, loss = 0.3868766725063324
In grad_steps = 1497, loss = 0.30212318897247314
In grad_steps = 1498, loss = 0.38904881477355957
In grad_steps = 1499, loss = 0.2720036804676056
In grad_steps = 1500, loss = 0.253485769033432
In grad_steps = 1501, loss = 0.5643824338912964
In grad_steps = 1502, loss = 0.05797935277223587
In grad_steps = 1503, loss = 0.2585676312446594
In grad_steps = 1504, loss = 0.09265245497226715
In grad_steps = 1505, loss = 0.09588270634412766
In grad_steps = 1506, loss = 0.2457749843597412
In grad_steps = 1507, loss = 0.06119251996278763
In grad_steps = 1508, loss = 0.4250873923301697
In grad_steps = 1509, loss = 0.6848109364509583
In grad_steps = 1510, loss = 0.3551753759384155
In grad_steps = 1511, loss = 0.15507365763187408
In grad_steps = 1512, loss = 0.09872181713581085
In grad_steps = 1513, loss = 1.1278115510940552
In grad_steps = 1514, loss = 0.7976182103157043
In grad_steps = 1515, loss = 0.06509117037057877
In grad_steps = 1516, loss = 0.045833662152290344
In grad_steps = 1517, loss = 0.2337314933538437
In grad_steps = 1518, loss = 0.14923863112926483
In grad_steps = 1519, loss = 0.4895292818546295
In grad_steps = 1520, loss = 0.053697679191827774
In grad_steps = 1521, loss = 0.2783597409725189
In grad_steps = 1522, loss = 0.26300662755966187
In grad_steps = 1523, loss = 0.0846739113330841
In grad_steps = 1524, loss = 0.04488111287355423
In grad_steps = 1525, loss = 0.5486085414886475
In grad_steps = 1526, loss = 0.06683802604675293
In grad_steps = 1527, loss = 0.07556881010532379
In grad_steps = 1528, loss = 0.0769459679722786
In grad_steps = 1529, loss = 0.042495232075452805
In grad_steps = 1530, loss = 0.06276292353868484
In grad_steps = 1531, loss = 0.029165372252464294
In grad_steps = 1532, loss = 0.25868916511535645
In grad_steps = 1533, loss = 0.5587536692619324
In grad_steps = 1534, loss = 0.06826775521039963
In grad_steps = 1535, loss = 0.025022637099027634
In grad_steps = 1536, loss = 0.05577096343040466
In grad_steps = 1537, loss = 0.06535138934850693
In grad_steps = 1538, loss = 1.640409231185913
In grad_steps = 1539, loss = 0.015054409392178059
In grad_steps = 1540, loss = 0.02410983480513096
In grad_steps = 1541, loss = 1.2838023900985718
In grad_steps = 1542, loss = 0.7651577591896057
In grad_steps = 1543, loss = 0.4617350101470947
In grad_steps = 1544, loss = 0.24554622173309326
In grad_steps = 1545, loss = 0.05606503039598465
In grad_steps = 1546, loss = 0.8887174725532532
In grad_steps = 1547, loss = 0.0919966995716095
In grad_steps = 1548, loss = 0.2821890413761139
In grad_steps = 1549, loss = 0.2806380093097687
In grad_steps = 1550, loss = 0.18781328201293945
In grad_steps = 1551, loss = 0.2912772297859192
In grad_steps = 1552, loss = 0.6646659970283508
In grad_steps = 1553, loss = 0.1850426197052002
In grad_steps = 1554, loss = 0.7228349447250366
In grad_steps = 1555, loss = 0.040127258747816086
In grad_steps = 1556, loss = 0.06785547733306885
In grad_steps = 1557, loss = 0.5082859992980957
In grad_steps = 1558, loss = 0.6706888675689697
In grad_steps = 1559, loss = 0.30425065755844116
In grad_steps = 1560, loss = 0.3732231855392456
In grad_steps = 1561, loss = 0.1775144785642624
In grad_steps = 1562, loss = 0.20224876701831818
In grad_steps = 1563, loss = 0.616950273513794
In grad_steps = 1564, loss = 0.11731040477752686
In grad_steps = 1565, loss = 0.3347967863082886
In grad_steps = 1566, loss = 0.860945999622345
In grad_steps = 1567, loss = 0.2843753695487976
In grad_steps = 1568, loss = 0.739086389541626
In grad_steps = 1569, loss = 0.32420822978019714
In grad_steps = 1570, loss = 0.38968223333358765
In grad_steps = 1571, loss = 0.49014654755592346
In grad_steps = 1572, loss = 0.6636592149734497
In grad_steps = 1573, loss = 0.29365983605384827
In grad_steps = 1574, loss = 0.4812193512916565
In grad_steps = 1575, loss = 0.1039372980594635
In grad_steps = 1576, loss = 0.17901641130447388
In grad_steps = 1577, loss = 0.25922608375549316
In grad_steps = 1578, loss = 0.6212273240089417
In grad_steps = 1579, loss = 0.12949252128601074
In grad_steps = 1580, loss = 0.07409800589084625
In grad_steps = 1581, loss = 0.23344899713993073
In grad_steps = 1582, loss = 0.15618883073329926
In grad_steps = 1583, loss = 0.6996787786483765
In grad_steps = 1584, loss = 0.5779641270637512
In grad_steps = 1585, loss = 1.0967172384262085
In grad_steps = 1586, loss = 0.6642654538154602
In grad_steps = 1587, loss = 0.3334977626800537
In grad_steps = 1588, loss = 0.1639576405286789
In grad_steps = 1589, loss = 0.1084478497505188
In grad_steps = 1590, loss = 0.29475998878479004
In grad_steps = 1591, loss = 0.08377531170845032
In grad_steps = 1592, loss = 0.5847141742706299
In grad_steps = 1593, loss = 0.11241985112428665
In grad_steps = 1594, loss = 0.20854730904102325
In grad_steps = 1595, loss = 0.5553498268127441
In grad_steps = 1596, loss = 0.46084660291671753
In grad_steps = 1597, loss = 0.3511672616004944
In grad_steps = 1598, loss = 0.2578761577606201
In grad_steps = 1599, loss = 0.6790375709533691
In grad_steps = 1600, loss = 0.6172564029693604
In grad_steps = 1601, loss = 0.5479633808135986
In grad_steps = 1602, loss = 0.41084030270576477
In grad_steps = 1603, loss = 0.11518332362174988
In grad_steps = 1604, loss = 0.19940878450870514
In grad_steps = 1605, loss = 0.1331232190132141
In grad_steps = 1606, loss = 0.5227867960929871
In grad_steps = 1607, loss = 0.24239763617515564
In grad_steps = 1608, loss = 0.11556638777256012
In grad_steps = 1609, loss = 0.40766236186027527
In grad_steps = 1610, loss = 0.34734562039375305
In grad_steps = 1611, loss = 0.183180570602417
In grad_steps = 1612, loss = 0.817645788192749
In grad_steps = 1613, loss = 0.28888288140296936
In grad_steps = 1614, loss = 0.12854096293449402
In grad_steps = 1615, loss = 0.21914048492908478
In grad_steps = 1616, loss = 1.074057936668396
In grad_steps = 1617, loss = 0.016455303877592087
In grad_steps = 1618, loss = 0.38186535239219666
In grad_steps = 1619, loss = 0.4039034843444824
In grad_steps = 1620, loss = 0.02244454063475132
In grad_steps = 1621, loss = 0.1274881511926651
In grad_steps = 1622, loss = 0.7827774286270142
In grad_steps = 1623, loss = 0.32854312658309937
In grad_steps = 1624, loss = 0.45078253746032715
In grad_steps = 1625, loss = 0.2008177936077118
In grad_steps = 1626, loss = 0.030919533222913742
In grad_steps = 1627, loss = 0.10369136929512024
In grad_steps = 1628, loss = 0.24701645970344543
In grad_steps = 1629, loss = 0.13249094784259796
In grad_steps = 1630, loss = 0.30889230966567993
In grad_steps = 1631, loss = 0.23617297410964966
In grad_steps = 1632, loss = 0.6133404970169067
In grad_steps = 1633, loss = 0.03784897178411484
In grad_steps = 1634, loss = 0.08392014354467392
In grad_steps = 1635, loss = 0.2914312183856964
In grad_steps = 1636, loss = 0.8768347501754761
In grad_steps = 1637, loss = 0.02583102136850357
In grad_steps = 1638, loss = 0.06479617208242416
In grad_steps = 1639, loss = 0.8946220874786377
In grad_steps = 1640, loss = 0.32072168588638306
In grad_steps = 1641, loss = 0.10180074721574783
In grad_steps = 1642, loss = 0.8664883375167847
In grad_steps = 1643, loss = 0.0950489193201065
In grad_steps = 1644, loss = 0.9981210827827454
In grad_steps = 1645, loss = 0.28750160336494446
In grad_steps = 1646, loss = 0.685135006904602
In grad_steps = 1647, loss = 0.4503331184387207
In grad_steps = 1648, loss = 0.09975674748420715
In grad_steps = 1649, loss = 0.06015732139348984
In grad_steps = 1650, loss = 0.4780714809894562
In grad_steps = 1651, loss = 0.26504382491111755
In grad_steps = 1652, loss = 0.20329880714416504
In grad_steps = 1653, loss = 0.36114048957824707
In grad_steps = 1654, loss = 0.2900407016277313
In grad_steps = 1655, loss = 0.4243506193161011
In grad_steps = 1656, loss = 0.6749515533447266
In grad_steps = 1657, loss = 0.17235396802425385
In grad_steps = 1658, loss = 0.15935862064361572
In grad_steps = 1659, loss = 0.6788142919540405
In grad_steps = 1660, loss = 0.3927350342273712
In grad_steps = 1661, loss = 0.3577539324760437
In grad_steps = 1662, loss = 1.1401591300964355
In grad_steps = 1663, loss = 0.3998509347438812
In grad_steps = 1664, loss = 0.28765907883644104
In grad_steps = 1665, loss = 0.5782771706581116
In grad_steps = 1666, loss = 0.06238342076539993
In grad_steps = 1667, loss = 0.13976433873176575
In grad_steps = 1668, loss = 0.8015021681785583
In grad_steps = 1669, loss = 0.3311002850532532
In grad_steps = 1670, loss = 0.10980676859617233
In grad_steps = 1671, loss = 0.21538642048835754
In grad_steps = 1672, loss = 0.6850265860557556
In grad_steps = 1673, loss = 0.2642374336719513
In grad_steps = 1674, loss = 0.13949573040008545
In grad_steps = 1675, loss = 0.757247269153595
In grad_steps = 1676, loss = 0.4858326017856598
In grad_steps = 1677, loss = 0.2798486351966858
In grad_steps = 1678, loss = 0.22884391248226166
In grad_steps = 1679, loss = 0.6202307939529419
In grad_steps = 1680, loss = 0.14374014735221863
In grad_steps = 1681, loss = 0.5127754211425781
In grad_steps = 1682, loss = 0.21506348252296448
In grad_steps = 1683, loss = 0.18016836047172546
In grad_steps = 1684, loss = 0.4324425458908081
In grad_steps = 1685, loss = 0.1471918225288391
In grad_steps = 1686, loss = 0.4532540738582611
In grad_steps = 1687, loss = 0.4466298520565033
In grad_steps = 1688, loss = 0.39085572957992554
In grad_steps = 1689, loss = 0.2873469889163971
In grad_steps = 1690, loss = 0.08167614787817001
In grad_steps = 1691, loss = 0.362379789352417
In grad_steps = 1692, loss = 0.2101917415857315
In grad_steps = 1693, loss = 0.25554123520851135
In grad_steps = 1694, loss = 0.11386539041996002
In grad_steps = 1695, loss = 0.06682950258255005
In grad_steps = 1696, loss = 0.434550404548645
In grad_steps = 1697, loss = 0.188466876745224
In grad_steps = 1698, loss = 0.02718781679868698
In grad_steps = 1699, loss = 0.0947527065873146
In grad_steps = 1700, loss = 0.07781969755887985
In grad_steps = 1701, loss = 0.8725281953811646
In grad_steps = 1702, loss = 0.009394876658916473
In grad_steps = 1703, loss = 0.11302752792835236
In grad_steps = 1704, loss = 0.054442182183265686
In grad_steps = 1705, loss = 0.41831615567207336
In grad_steps = 1706, loss = 0.2850266695022583
In grad_steps = 1707, loss = 1.0079139471054077
In grad_steps = 1708, loss = 1.0677440166473389
In grad_steps = 1709, loss = 0.11296311765909195
In grad_steps = 1710, loss = 0.09832763671875
In grad_steps = 1711, loss = 0.0321643091738224
In grad_steps = 1712, loss = 0.5045618414878845
In grad_steps = 1713, loss = 0.8768467307090759
In grad_steps = 1714, loss = 0.08540880680084229
In grad_steps = 1715, loss = 0.6065106987953186
In grad_steps = 1716, loss = 0.6577224135398865
In grad_steps = 1717, loss = 0.8345665335655212
In grad_steps = 1718, loss = 0.0506715327501297
In grad_steps = 1719, loss = 0.39452600479125977
In grad_steps = 1720, loss = 0.4106137454509735
In grad_steps = 1721, loss = 0.06666970998048782
In grad_steps = 1722, loss = 0.5011584162712097
In grad_steps = 1723, loss = 0.14266541600227356
In grad_steps = 1724, loss = 0.7327350378036499
In grad_steps = 1725, loss = 0.19689294695854187
In grad_steps = 1726, loss = 0.3207383155822754
In grad_steps = 1727, loss = 0.43284326791763306
In grad_steps = 1728, loss = 0.24876481294631958
In grad_steps = 1729, loss = 0.3410351276397705
In grad_steps = 1730, loss = 0.15420593321323395
In grad_steps = 1731, loss = 0.32403290271759033
In grad_steps = 1732, loss = 0.08846777677536011
In grad_steps = 1733, loss = 0.059075333178043365
In grad_steps = 1734, loss = 0.1436590999364853
In grad_steps = 1735, loss = 0.2734149396419525
In grad_steps = 1736, loss = 0.28949955105781555
In grad_steps = 1737, loss = 0.5753108263015747
In grad_steps = 1738, loss = 0.12319336831569672
In grad_steps = 1739, loss = 0.03532521799206734
In grad_steps = 1740, loss = 0.038042373955249786
In grad_steps = 1741, loss = 0.17251889407634735
In grad_steps = 1742, loss = 0.4624508023262024
In grad_steps = 1743, loss = 0.022822493687272072
In grad_steps = 1744, loss = 0.630533754825592
In grad_steps = 1745, loss = 0.11090350151062012
In grad_steps = 1746, loss = 0.7010738849639893
In grad_steps = 1747, loss = 0.03680061921477318
In grad_steps = 1748, loss = 0.775772750377655
In grad_steps = 1749, loss = 0.022165950387716293
In grad_steps = 1750, loss = 0.7920222282409668
In grad_steps = 1751, loss = 0.15163572132587433
In grad_steps = 1752, loss = 0.1236317902803421
In grad_steps = 1753, loss = 0.16430167853832245
In grad_steps = 1754, loss = 0.08993068337440491
In grad_steps = 1755, loss = 0.05851764976978302
In grad_steps = 1756, loss = 0.15346494317054749
In grad_steps = 1757, loss = 1.112341284751892
In grad_steps = 1758, loss = 0.11479737609624863
In grad_steps = 1759, loss = 0.10178817808628082
In grad_steps = 1760, loss = 0.39454784989356995
In grad_steps = 1761, loss = 0.07697859406471252
In grad_steps = 1762, loss = 0.6981076002120972
In grad_steps = 1763, loss = 0.1842327117919922
In grad_steps = 1764, loss = 0.21148236095905304
In grad_steps = 1765, loss = 0.31609439849853516
In grad_steps = 1766, loss = 0.04770511016249657
In grad_steps = 1767, loss = 0.09650174528360367
In grad_steps = 1768, loss = 0.1927650421857834
In grad_steps = 1769, loss = 0.31649255752563477
In grad_steps = 1770, loss = 1.1402491331100464
In grad_steps = 1771, loss = 0.6779751181602478
In grad_steps = 1772, loss = 0.05620472878217697
In grad_steps = 1773, loss = 0.04792150855064392
In grad_steps = 1774, loss = 0.04347442835569382
In grad_steps = 1775, loss = 0.8303331732749939
In grad_steps = 1776, loss = 0.5320263504981995
In grad_steps = 1777, loss = 0.16812270879745483
In grad_steps = 1778, loss = 0.4511789083480835
In grad_steps = 1779, loss = 0.3453933000564575
In grad_steps = 1780, loss = 0.03688747435808182
In grad_steps = 1781, loss = 0.08663409948348999
In grad_steps = 1782, loss = 0.1982509046792984
In grad_steps = 1783, loss = 0.49507981538772583
In grad_steps = 1784, loss = 1.2254818677902222
In grad_steps = 1785, loss = 0.08770821243524551
In grad_steps = 1786, loss = 0.10185106098651886
In grad_steps = 1787, loss = 0.1651109755039215
In grad_steps = 1788, loss = 0.21194498240947723
In grad_steps = 1789, loss = 0.394620418548584
In grad_steps = 1790, loss = 0.4708576798439026
In grad_steps = 1791, loss = 0.5240287184715271
In grad_steps = 1792, loss = 0.0683288648724556
In grad_steps = 1793, loss = 0.12001952528953552
In grad_steps = 1794, loss = 0.11770707368850708
In grad_steps = 1795, loss = 0.17295700311660767
In grad_steps = 1796, loss = 0.07390179485082626
In grad_steps = 1797, loss = 0.2557450532913208
In grad_steps = 1798, loss = 0.07122897356748581
In grad_steps = 1799, loss = 0.9001283049583435
In grad_steps = 1800, loss = 0.6219184398651123
In grad_steps = 1801, loss = 0.4783710837364197
In grad_steps = 1802, loss = 1.1673073768615723
In grad_steps = 1803, loss = 0.08944527804851532
In grad_steps = 1804, loss = 0.7131393551826477
In grad_steps = 1805, loss = 0.21816280484199524
In grad_steps = 1806, loss = 0.10682099312543869
In grad_steps = 1807, loss = 0.20554743707180023
In grad_steps = 1808, loss = 0.6277914643287659
In grad_steps = 1809, loss = 0.2819964587688446
In grad_steps = 1810, loss = 0.04042733460664749
In grad_steps = 1811, loss = 0.07637423276901245
In grad_steps = 1812, loss = 0.30576837062835693
In grad_steps = 1813, loss = 0.3735968768596649
In grad_steps = 1814, loss = 0.09285932779312134
In grad_steps = 1815, loss = 0.27553635835647583
In grad_steps = 1816, loss = 0.25823915004730225
In grad_steps = 1817, loss = 0.42547309398651123
In grad_steps = 1818, loss = 0.18235383927822113
In grad_steps = 1819, loss = 0.38923701643943787
In grad_steps = 1820, loss = 1.184561848640442
In grad_steps = 1821, loss = 0.39218205213546753
In grad_steps = 1822, loss = 0.044942308217287064
In grad_steps = 1823, loss = 1.243733286857605
In grad_steps = 1824, loss = 0.07424617558717728
In grad_steps = 1825, loss = 0.09670807421207428
In grad_steps = 1826, loss = 0.8206520080566406
In grad_steps = 1827, loss = 1.0653892755508423
In grad_steps = 1828, loss = 0.8844287395477295
In grad_steps = 1829, loss = 0.7116310000419617
In grad_steps = 1830, loss = 0.23237432539463043
In grad_steps = 1831, loss = 0.1754007637500763
In grad_steps = 1832, loss = 0.6177919507026672
In grad_steps = 1833, loss = 1.4484210014343262
In grad_steps = 1834, loss = 0.11240968108177185
In grad_steps = 1835, loss = 0.35887783765792847
In grad_steps = 1836, loss = 0.5668148398399353
In grad_steps = 1837, loss = 0.6220354437828064
In grad_steps = 1838, loss = 0.5784226655960083
In grad_steps = 1839, loss = 0.304531455039978
In grad_steps = 1840, loss = 0.10760015994310379
In grad_steps = 1841, loss = 0.3322790265083313
In grad_steps = 1842, loss = 0.2715952694416046
In grad_steps = 1843, loss = 0.2543836534023285
In grad_steps = 1844, loss = 0.8051134347915649
In grad_steps = 1845, loss = 0.2770824134349823
In grad_steps = 1846, loss = 0.37763476371765137
In grad_steps = 1847, loss = 0.37013643980026245
In grad_steps = 1848, loss = 0.16397295892238617
In grad_steps = 1849, loss = 0.20043659210205078
In grad_steps = 1850, loss = 0.1736791431903839
In grad_steps = 1851, loss = 0.3215366005897522
In grad_steps = 1852, loss = 0.1379595547914505
In grad_steps = 1853, loss = 0.3393545150756836
In grad_steps = 1854, loss = 0.29122626781463623
In grad_steps = 1855, loss = 0.05933944135904312
In grad_steps = 1856, loss = 0.084525465965271
In grad_steps = 1857, loss = 0.08658211678266525
In grad_steps = 1858, loss = 0.1303655058145523
In grad_steps = 1859, loss = 0.5219025611877441
In grad_steps = 1860, loss = 1.1141318082809448
In grad_steps = 1861, loss = 0.051062509417533875
In grad_steps = 1862, loss = 0.3942968249320984
In grad_steps = 1863, loss = 0.041911158710718155
In grad_steps = 1864, loss = 0.12491057813167572
In grad_steps = 1865, loss = 0.544247567653656
In grad_steps = 1866, loss = 0.2879597544670105
In grad_steps = 1867, loss = 1.217287302017212
In grad_steps = 1868, loss = 0.25349095463752747
In grad_steps = 1869, loss = 0.07120031863451004
In grad_steps = 1870, loss = 0.08236275613307953
In grad_steps = 1871, loss = 0.0579763725399971
In grad_steps = 1872, loss = 0.13233308494091034
In grad_steps = 1873, loss = 0.023955095559358597
In grad_steps = 1874, loss = 0.0453878752887249
In grad_steps = 1875, loss = 0.08928772807121277
In grad_steps = 1876, loss = 0.06706614047288895
In grad_steps = 1877, loss = 0.638614296913147
In grad_steps = 1878, loss = 0.11785656213760376
In grad_steps = 1879, loss = 0.6229007244110107
In grad_steps = 1880, loss = 0.06397638469934464
In grad_steps = 1881, loss = 0.2820531129837036
In grad_steps = 1882, loss = 0.07147066295146942
In grad_steps = 1883, loss = 0.5338564515113831
In grad_steps = 1884, loss = 0.050898902118206024
In grad_steps = 1885, loss = 0.19591742753982544
In grad_steps = 1886, loss = 0.5420163869857788
In grad_steps = 1887, loss = 0.04925525188446045
In grad_steps = 1888, loss = 0.15660065412521362
In grad_steps = 1889, loss = 0.06607755273580551
In grad_steps = 1890, loss = 0.06554772704839706
In grad_steps = 1891, loss = 0.27672451734542847
In grad_steps = 1892, loss = 0.36862218379974365
In grad_steps = 1893, loss = 0.031648751348257065
In grad_steps = 1894, loss = 0.028961418196558952
In grad_steps = 1895, loss = 0.33531174063682556
In grad_steps = 1896, loss = 0.6580600142478943
In grad_steps = 1897, loss = 0.04714309051632881
In grad_steps = 1898, loss = 0.02056110091507435
In grad_steps = 1899, loss = 0.06014373153448105
In grad_steps = 1900, loss = 0.10302577912807465
In grad_steps = 1901, loss = 0.33959922194480896
In grad_steps = 1902, loss = 0.1622561365365982
In grad_steps = 1903, loss = 0.13787946105003357
In grad_steps = 1904, loss = 0.5333041548728943
In grad_steps = 1905, loss = 0.0416988767683506
In grad_steps = 1906, loss = 0.06622855365276337
In grad_steps = 1907, loss = 1.1977484226226807
In grad_steps = 1908, loss = 0.5446422696113586
In grad_steps = 1909, loss = 0.07707668840885162
In grad_steps = 1910, loss = 0.5740524530410767
In grad_steps = 1911, loss = 0.25245699286460876
In grad_steps = 1912, loss = 0.21715542674064636
In grad_steps = 1913, loss = 0.06096617504954338
In grad_steps = 1914, loss = 0.032077956944704056
In grad_steps = 1915, loss = 0.39761120080947876
In grad_steps = 1916, loss = 0.42506837844848633
In grad_steps = 1917, loss = 0.07960760593414307
In grad_steps = 1918, loss = 0.3032052516937256
In grad_steps = 1919, loss = 0.37235143780708313
In grad_steps = 1920, loss = 0.7909368276596069
In grad_steps = 1921, loss = 0.4883994460105896
In grad_steps = 1922, loss = 0.24608883261680603
In grad_steps = 1923, loss = 0.008223829790949821
In grad_steps = 1924, loss = 0.41076919436454773
In grad_steps = 1925, loss = 0.7260981202125549
In grad_steps = 1926, loss = 0.3526979982852936
In grad_steps = 1927, loss = 0.2836759686470032
In grad_steps = 1928, loss = 1.0523667335510254
In grad_steps = 1929, loss = 0.4678381085395813
In grad_steps = 1930, loss = 0.26642337441444397
In grad_steps = 1931, loss = 0.7572283744812012
In grad_steps = 1932, loss = 0.2337053269147873
In grad_steps = 1933, loss = 0.7951444983482361
In grad_steps = 1934, loss = 0.24158866703510284
In grad_steps = 1935, loss = 0.1472575068473816
In grad_steps = 1936, loss = 0.0907381922006607
In grad_steps = 1937, loss = 0.05802687257528305
In grad_steps = 1938, loss = 0.31573477387428284
In grad_steps = 1939, loss = 0.34836894273757935
In grad_steps = 1940, loss = 0.26056718826293945
In grad_steps = 1941, loss = 0.3531005084514618
In grad_steps = 1942, loss = 0.19313427805900574
In grad_steps = 1943, loss = 0.5560658574104309
In grad_steps = 1944, loss = 0.08317576348781586
In grad_steps = 1945, loss = 0.32593849301338196
In grad_steps = 1946, loss = 0.10168524086475372
In grad_steps = 1947, loss = 0.2508130967617035
In grad_steps = 1948, loss = 0.10857531428337097
In grad_steps = 1949, loss = 0.10469698160886765
In grad_steps = 1950, loss = 0.2764607071876526
In grad_steps = 1951, loss = 1.1897737979888916
In grad_steps = 1952, loss = 0.10010149329900742
In grad_steps = 1953, loss = 0.032496899366378784
In grad_steps = 1954, loss = 0.5042863488197327
In grad_steps = 1955, loss = 0.004754993133246899
In grad_steps = 1956, loss = 0.12944798171520233
In grad_steps = 1957, loss = 0.03752733767032623
In grad_steps = 1958, loss = 0.06668993830680847
In grad_steps = 1959, loss = 0.03917930647730827
In grad_steps = 1960, loss = 0.3035706579685211
In grad_steps = 1961, loss = 0.5499262809753418
In grad_steps = 1962, loss = 0.031208282336592674
In grad_steps = 1963, loss = 0.5779063105583191
In grad_steps = 1964, loss = 0.8772578835487366
In grad_steps = 1965, loss = 0.14155414700508118
In grad_steps = 1966, loss = 1.6842159032821655
In grad_steps = 1967, loss = 0.24542437493801117
In grad_steps = 1968, loss = 0.27484676241874695
In grad_steps = 1969, loss = 0.06690351665019989
In grad_steps = 1970, loss = 0.2289290577173233
In grad_steps = 1971, loss = 0.36468708515167236
In grad_steps = 1972, loss = 0.5380648970603943
In grad_steps = 1973, loss = 0.6882041692733765
In grad_steps = 1974, loss = 0.3447732925415039
In grad_steps = 1975, loss = 0.21750296652317047
In grad_steps = 1976, loss = 0.17161214351654053
In grad_steps = 1977, loss = 0.7931250333786011
In grad_steps = 1978, loss = 0.17966917157173157
In grad_steps = 1979, loss = 0.10072897374629974
In grad_steps = 1980, loss = 0.32387569546699524
In grad_steps = 1981, loss = 0.5939405560493469
In grad_steps = 1982, loss = 0.05185160040855408
In grad_steps = 1983, loss = 0.1914174109697342
In grad_steps = 1984, loss = 0.14150182902812958
In grad_steps = 1985, loss = 0.22418779134750366
In grad_steps = 1986, loss = 0.38785600662231445
In grad_steps = 1987, loss = 0.29484742879867554
In grad_steps = 1988, loss = 0.6140710115432739
In grad_steps = 1989, loss = 0.35793736577033997
In grad_steps = 1990, loss = 0.3266388773918152
In grad_steps = 1991, loss = 0.10604868084192276
In grad_steps = 1992, loss = 0.22640261054039001
In grad_steps = 1993, loss = 0.11236952990293503
In grad_steps = 1994, loss = 0.5527631044387817
In grad_steps = 1995, loss = 0.5741666555404663
In grad_steps = 1996, loss = 0.07524461299180984
In grad_steps = 1997, loss = 0.20691654086112976
In grad_steps = 1998, loss = 0.3332670331001282
In grad_steps = 1999, loss = 0.08900367468595505
In grad_steps = 2000, loss = 0.16008849442005157
In grad_steps = 2001, loss = 0.2340521365404129
In grad_steps = 2002, loss = 0.39481595158576965
In grad_steps = 2003, loss = 0.07354448735713959
In grad_steps = 2004, loss = 0.08081784844398499
In grad_steps = 2005, loss = 0.03510412946343422
In grad_steps = 2006, loss = 0.3005041778087616
In grad_steps = 2007, loss = 0.055051762610673904
In grad_steps = 2008, loss = 0.07977128028869629
In grad_steps = 2009, loss = 0.6439829468727112
In grad_steps = 2010, loss = 0.24603168666362762
In grad_steps = 2011, loss = 0.23622386157512665
In grad_steps = 2012, loss = 0.2158498466014862
In grad_steps = 2013, loss = 0.03801073133945465
In grad_steps = 2014, loss = 0.13440173864364624
In grad_steps = 2015, loss = 0.006600633729249239
In grad_steps = 2016, loss = 0.012633852660655975
In grad_steps = 2017, loss = 0.07764140516519547
In grad_steps = 2018, loss = 0.03623170778155327
In grad_steps = 2019, loss = 0.3563113212585449
In grad_steps = 2020, loss = 0.1383616030216217
In grad_steps = 2021, loss = 0.11657880246639252
In grad_steps = 2022, loss = 1.1739320755004883
In grad_steps = 2023, loss = 0.44399207830429077
In grad_steps = 2024, loss = 0.05138446018099785
In grad_steps = 2025, loss = 0.08014250546693802
In grad_steps = 2026, loss = 0.011941765435039997
In grad_steps = 2027, loss = 0.5651845932006836
In grad_steps = 2028, loss = 0.10064578801393509
In grad_steps = 2029, loss = 0.3383183479309082
In grad_steps = 2030, loss = 0.18649114668369293
In grad_steps = 2031, loss = 0.019070863723754883
In grad_steps = 2032, loss = 0.013217495754361153
In grad_steps = 2033, loss = 0.021739941090345383
In grad_steps = 2034, loss = 0.09065335988998413
In grad_steps = 2035, loss = 0.2605893909931183
In grad_steps = 2036, loss = 0.025141114369034767
In grad_steps = 2037, loss = 0.2160312682390213
In grad_steps = 2038, loss = 0.5998232960700989
In grad_steps = 2039, loss = 0.6675324440002441
In grad_steps = 2040, loss = 0.3227546215057373
In grad_steps = 2041, loss = 0.22270745038986206
In grad_steps = 2042, loss = 0.4382173418998718
In grad_steps = 2043, loss = 0.08691368997097015
In grad_steps = 2044, loss = 0.09143222868442535
In grad_steps = 2045, loss = 1.084444522857666
In grad_steps = 2046, loss = 0.47050875425338745
In grad_steps = 2047, loss = 0.024996723979711533
In grad_steps = 2048, loss = 0.20331253111362457
In grad_steps = 2049, loss = 1.6479086875915527
In grad_steps = 2050, loss = 0.7282229065895081
In grad_steps = 2051, loss = 0.08504390716552734
In grad_steps = 2052, loss = 0.10318687558174133
In grad_steps = 2053, loss = 0.9328354597091675
In grad_steps = 2054, loss = 0.29132023453712463
In grad_steps = 2055, loss = 0.4621928334236145
In grad_steps = 2056, loss = 0.5229812264442444
In grad_steps = 2057, loss = 0.17030790448188782
In grad_steps = 2058, loss = 0.33466753363609314
In grad_steps = 2059, loss = 0.21433913707733154
In grad_steps = 2060, loss = 0.4627729058265686
In grad_steps = 2061, loss = 0.39767909049987793
In grad_steps = 2062, loss = 0.20346973836421967
In grad_steps = 2063, loss = 0.21553769707679749
In grad_steps = 2064, loss = 0.4564799666404724
In grad_steps = 2065, loss = 0.16391809284687042
In grad_steps = 2066, loss = 0.13253214955329895
In grad_steps = 2067, loss = 0.14317819476127625
In grad_steps = 2068, loss = 0.18007034063339233
In grad_steps = 2069, loss = 0.6042178273200989
In grad_steps = 2070, loss = 0.5180955529212952
In grad_steps = 2071, loss = 0.5715667009353638
In grad_steps = 2072, loss = 0.029464462772011757
In grad_steps = 2073, loss = 0.6756622791290283
In grad_steps = 2074, loss = 0.14198344945907593
In grad_steps = 2075, loss = 0.045986905694007874
In grad_steps = 2076, loss = 0.033958543092012405
In grad_steps = 2077, loss = 0.08589335530996323
In grad_steps = 2078, loss = 1.0285028219223022
In grad_steps = 2079, loss = 0.43800097703933716
In grad_steps = 2080, loss = 0.22280225157737732
In grad_steps = 2081, loss = 0.014774538576602936
In grad_steps = 2082, loss = 0.09670906513929367
In grad_steps = 2083, loss = 0.11481836438179016
In grad_steps = 2084, loss = 0.079464852809906
In grad_steps = 2085, loss = 0.10612935572862625
In grad_steps = 2086, loss = 0.3080507814884186
In grad_steps = 2087, loss = 0.18361815810203552
In grad_steps = 2088, loss = 0.014173631556332111
In grad_steps = 2089, loss = 0.0201665461063385
In grad_steps = 2090, loss = 0.047195203602313995
In grad_steps = 2091, loss = 0.01628166437149048
In grad_steps = 2092, loss = 0.28774815797805786
In grad_steps = 2093, loss = 0.010313514620065689
In grad_steps = 2094, loss = 0.12655659019947052
In grad_steps = 2095, loss = 0.05321739241480827
In grad_steps = 2096, loss = 0.005823896266520023
In grad_steps = 2097, loss = 1.8144333362579346
In grad_steps = 2098, loss = 0.007594618014991283
In grad_steps = 2099, loss = 0.00634306576102972
In grad_steps = 2100, loss = 0.6103169918060303
In grad_steps = 2101, loss = 0.12448006123304367
In grad_steps = 2102, loss = 0.12973298132419586
In grad_steps = 2103, loss = 0.3226870894432068
In grad_steps = 2104, loss = 0.8526623249053955
In grad_steps = 2105, loss = 1.3605117797851562
In grad_steps = 2106, loss = 0.1118616983294487
In grad_steps = 2107, loss = 1.2559173107147217
In grad_steps = 2108, loss = 0.11039236187934875
In grad_steps = 2109, loss = 0.1081351637840271
In grad_steps = 2110, loss = 0.5355859398841858
In grad_steps = 2111, loss = 0.15218140184879303
In grad_steps = 2112, loss = 0.45952722430229187
In grad_steps = 2113, loss = 0.4125387370586395
In grad_steps = 2114, loss = 0.22719085216522217
In grad_steps = 2115, loss = 0.0968276634812355
In grad_steps = 2116, loss = 0.23080460727214813
In grad_steps = 2117, loss = 0.5046953558921814
In grad_steps = 2118, loss = 0.2662906348705292
In grad_steps = 2119, loss = 0.08170013129711151
In grad_steps = 2120, loss = 0.01546705886721611
In grad_steps = 2121, loss = 0.13419264554977417
In grad_steps = 2122, loss = 0.16076651215553284
In grad_steps = 2123, loss = 0.555879533290863
In grad_steps = 2124, loss = 0.1557157039642334
In grad_steps = 2125, loss = 0.1011824831366539
In grad_steps = 2126, loss = 1.0500786304473877
In grad_steps = 2127, loss = 0.27760446071624756
In grad_steps = 2128, loss = 0.19061945378780365
In grad_steps = 2129, loss = 0.28271105885505676
In grad_steps = 2130, loss = 0.4848865568637848
In grad_steps = 2131, loss = 0.6614217758178711
In grad_steps = 2132, loss = 0.5474862456321716
In grad_steps = 2133, loss = 0.040707387030124664
In grad_steps = 2134, loss = 0.09086775779724121
In grad_steps = 2135, loss = 0.5255028009414673
In grad_steps = 2136, loss = 0.73479163646698
In grad_steps = 2137, loss = 0.45708557963371277
In grad_steps = 2138, loss = 0.17659278213977814
In grad_steps = 2139, loss = 0.2518007159233093
In grad_steps = 2140, loss = 0.08846014738082886
In grad_steps = 2141, loss = 0.13092738389968872
In grad_steps = 2142, loss = 0.037512026727199554
In grad_steps = 2143, loss = 0.14974074065685272
In grad_steps = 2144, loss = 0.9164761900901794
In grad_steps = 2145, loss = 0.22289222478866577
In grad_steps = 2146, loss = 0.27854201197624207
In grad_steps = 2147, loss = 0.800143301486969
In grad_steps = 2148, loss = 0.715840220451355
In grad_steps = 2149, loss = 0.07553496211767197
In grad_steps = 2150, loss = 0.2945650517940521
In grad_steps = 2151, loss = 0.8066662549972534
In grad_steps = 2152, loss = 0.2433314174413681
In grad_steps = 2153, loss = 0.34393367171287537
In grad_steps = 2154, loss = 0.08431068062782288
In grad_steps = 2155, loss = 0.35604041814804077
In grad_steps = 2156, loss = 0.15370097756385803
In grad_steps = 2157, loss = 0.07772862911224365
In grad_steps = 2158, loss = 0.3777133524417877
In grad_steps = 2159, loss = 0.6404051184654236
In grad_steps = 2160, loss = 0.1203565001487732
In grad_steps = 2161, loss = 0.05700494349002838
In grad_steps = 2162, loss = 0.7665902972221375
In grad_steps = 2163, loss = 0.09639499336481094
In grad_steps = 2164, loss = 0.09318873286247253
In grad_steps = 2165, loss = 0.35414552688598633
In grad_steps = 2166, loss = 0.7775647640228271
In grad_steps = 2167, loss = 0.08458644151687622
In grad_steps = 2168, loss = 0.12045285105705261
In grad_steps = 2169, loss = 0.344233900308609
In grad_steps = 2170, loss = 0.023632822558283806
In grad_steps = 2171, loss = 0.7086039781570435
In grad_steps = 2172, loss = 0.7405388951301575
In grad_steps = 2173, loss = 0.034453827887773514
In grad_steps = 2174, loss = 0.04733816534280777
In grad_steps = 2175, loss = 0.2512492537498474
In grad_steps = 2176, loss = 0.517289936542511
In grad_steps = 2177, loss = 0.09487492591142654
In grad_steps = 2178, loss = 0.06291453540325165
In grad_steps = 2179, loss = 0.43511390686035156
In grad_steps = 2180, loss = 0.06297941505908966
In grad_steps = 2181, loss = 0.08817455172538757
In grad_steps = 2182, loss = 0.40654510259628296
In grad_steps = 2183, loss = 0.2848953902721405
In grad_steps = 2184, loss = 0.13405905663967133
In grad_steps = 2185, loss = 0.11908651888370514
In grad_steps = 2186, loss = 0.14794424176216125
In grad_steps = 2187, loss = 0.11459064483642578
In grad_steps = 2188, loss = 0.04473242908716202
In grad_steps = 2189, loss = 0.20090705156326294
In grad_steps = 2190, loss = 0.056184396147727966
In grad_steps = 2191, loss = 0.21659044921398163
In grad_steps = 2192, loss = 0.5538231730461121
In grad_steps = 2193, loss = 0.06441056728363037
In grad_steps = 2194, loss = 0.0482446514070034
In grad_steps = 2195, loss = 0.02129915915429592
In grad_steps = 2196, loss = 0.06753841042518616
In grad_steps = 2197, loss = 0.021012859418988228
In grad_steps = 2198, loss = 1.4372284412384033
In grad_steps = 2199, loss = 0.8036253452301025
In grad_steps = 2200, loss = 0.1588226556777954
In grad_steps = 2201, loss = 0.5471101403236389
In grad_steps = 2202, loss = 0.4028453230857849
In grad_steps = 2203, loss = 0.11221572756767273
In grad_steps = 2204, loss = 0.15527261793613434
In grad_steps = 2205, loss = 0.02080066129565239
In grad_steps = 2206, loss = 0.3545103073120117
In grad_steps = 2207, loss = 0.05289642512798309
In grad_steps = 2208, loss = 0.5610469579696655
In grad_steps = 2209, loss = 0.08211103081703186
In grad_steps = 2210, loss = 0.14650066196918488
In grad_steps = 2211, loss = 0.1253873109817505
In grad_steps = 2212, loss = 1.1654633283615112
In grad_steps = 2213, loss = 1.04401695728302
In grad_steps = 2214, loss = 0.2735525369644165
In grad_steps = 2215, loss = 0.3375513553619385
In grad_steps = 2216, loss = 0.3259601593017578
In grad_steps = 2217, loss = 1.4149175882339478
In grad_steps = 2218, loss = 0.6360641717910767
In grad_steps = 2219, loss = 0.4047876000404358
In grad_steps = 2220, loss = 0.12080360949039459
In grad_steps = 2221, loss = 0.19931261241436005
In grad_steps = 2222, loss = 0.30950993299484253
In grad_steps = 2223, loss = 0.3232191801071167
In grad_steps = 2224, loss = 0.11977514624595642
In grad_steps = 2225, loss = 0.4535622298717499
In grad_steps = 2226, loss = 0.37146496772766113
In grad_steps = 2227, loss = 0.7135285139083862
In grad_steps = 2228, loss = 0.3198402523994446
In grad_steps = 2229, loss = 0.26852935552597046
In grad_steps = 2230, loss = 0.21819639205932617
In grad_steps = 2231, loss = 0.07274841517210007
In grad_steps = 2232, loss = 0.0991525650024414
In grad_steps = 2233, loss = 0.13273672759532928
In grad_steps = 2234, loss = 0.4855857193470001
In grad_steps = 2235, loss = 0.07813684642314911
In grad_steps = 2236, loss = 0.583517849445343
In grad_steps = 2237, loss = 0.6951072812080383
In grad_steps = 2238, loss = 0.3764236569404602
In grad_steps = 2239, loss = 0.2972258925437927
In grad_steps = 2240, loss = 0.2131059169769287
In grad_steps = 2241, loss = 0.12440197169780731
In grad_steps = 2242, loss = 0.40268731117248535
In grad_steps = 2243, loss = 0.1425241082906723
In grad_steps = 2244, loss = 0.27508044242858887
In grad_steps = 2245, loss = 1.0525951385498047
In grad_steps = 2246, loss = 0.5418432354927063
In grad_steps = 2247, loss = 0.20059522986412048
In grad_steps = 2248, loss = 0.6874191164970398
In grad_steps = 2249, loss = 0.3224065601825714
In grad_steps = 2250, loss = 0.20221054553985596
In grad_steps = 2251, loss = 0.1341373771429062
In grad_steps = 2252, loss = 0.6886793971061707
In grad_steps = 2253, loss = 0.6347446441650391
In grad_steps = 2254, loss = 0.2199849635362625
In grad_steps = 2255, loss = 0.3884052634239197
In grad_steps = 2256, loss = 0.5286433100700378
In grad_steps = 2257, loss = 0.30544230341911316
In grad_steps = 2258, loss = 0.16823691129684448
In grad_steps = 2259, loss = 0.2998389005661011
In grad_steps = 2260, loss = 0.16621047258377075
In grad_steps = 2261, loss = 0.1752062886953354
In grad_steps = 2262, loss = 0.1988513171672821
In grad_steps = 2263, loss = 0.28055527806282043
In grad_steps = 2264, loss = 0.3994215726852417
In grad_steps = 2265, loss = 0.05568535998463631
In grad_steps = 2266, loss = 0.18021854758262634
In grad_steps = 2267, loss = 0.202554851770401
In grad_steps = 2268, loss = 0.040747590363025665
In grad_steps = 2269, loss = 0.6929311156272888
In grad_steps = 2270, loss = 0.09147903323173523
In grad_steps = 2271, loss = 0.14211313426494598
In grad_steps = 2272, loss = 0.042094871401786804
In grad_steps = 2273, loss = 0.2484353482723236
In grad_steps = 2274, loss = 0.031117793172597885
In grad_steps = 2275, loss = 0.019748061895370483
In grad_steps = 2276, loss = 0.040919531136751175
In grad_steps = 2277, loss = 0.22760304808616638
In grad_steps = 2278, loss = 0.058786727488040924
In grad_steps = 2279, loss = 0.012279068119823933
In grad_steps = 2280, loss = 0.012297865934669971
In grad_steps = 2281, loss = 0.5400087237358093
In grad_steps = 2282, loss = 0.03628065437078476
In grad_steps = 2283, loss = 0.9490284323692322
In grad_steps = 2284, loss = 0.0474882498383522
In grad_steps = 2285, loss = 0.038085922598838806
In grad_steps = 2286, loss = 0.09212812781333923
In grad_steps = 2287, loss = 0.7445498108863831
In grad_steps = 2288, loss = 0.23156943917274475
In grad_steps = 2289, loss = 0.02391461655497551
In grad_steps = 2290, loss = 0.12510165572166443
In grad_steps = 2291, loss = 0.04195239394903183
In grad_steps = 2292, loss = 0.20844514667987823
In grad_steps = 2293, loss = 0.6936134696006775
In grad_steps = 2294, loss = 0.03062576614320278
In grad_steps = 2295, loss = 0.7652415633201599
In grad_steps = 2296, loss = 0.09427277743816376
In grad_steps = 2297, loss = 0.5055278539657593
In grad_steps = 2298, loss = 0.1842900812625885
In grad_steps = 2299, loss = 0.18542632460594177
In grad_steps = 2300, loss = 0.07376616448163986
In grad_steps = 2301, loss = 0.19052164256572723
In grad_steps = 2302, loss = 0.06232251599431038
In grad_steps = 2303, loss = 0.015414068475365639
In grad_steps = 2304, loss = 0.03637193515896797
In grad_steps = 2305, loss = 0.349956214427948
In grad_steps = 2306, loss = 0.12203967571258545
In grad_steps = 2307, loss = 0.4567183554172516
In grad_steps = 2308, loss = 0.2874504625797272
In grad_steps = 2309, loss = 0.01496266108006239
In grad_steps = 2310, loss = 0.8922796845436096
In grad_steps = 2311, loss = 0.10025434195995331
In grad_steps = 2312, loss = 1.0496176481246948
In grad_steps = 2313, loss = 0.7037672400474548
In grad_steps = 2314, loss = 0.06995508074760437
In grad_steps = 2315, loss = 0.18006998300552368
In grad_steps = 2316, loss = 0.22597885131835938
In grad_steps = 2317, loss = 0.08818526566028595
In grad_steps = 2318, loss = 0.3815127909183502
In grad_steps = 2319, loss = 1.3969917297363281
In grad_steps = 2320, loss = 0.16350314021110535
In grad_steps = 2321, loss = 0.07038833945989609
In grad_steps = 2322, loss = 0.48469048738479614
In grad_steps = 2323, loss = 0.07781199365854263
In grad_steps = 2324, loss = 0.22978176176548004
In grad_steps = 2325, loss = 0.14079251885414124
In grad_steps = 2326, loss = 0.04976213350892067
In grad_steps = 2327, loss = 0.6707046031951904
In grad_steps = 2328, loss = 0.4638065695762634
In grad_steps = 2329, loss = 0.20721550285816193
In grad_steps = 2330, loss = 0.11077476292848587
In grad_steps = 2331, loss = 0.5625733733177185
In grad_steps = 2332, loss = 0.11814998090267181
In grad_steps = 2333, loss = 0.6033144593238831
In grad_steps = 2334, loss = 0.13214585185050964
In grad_steps = 2335, loss = 0.442395955324173
In grad_steps = 2336, loss = 0.28796055912971497
In grad_steps = 2337, loss = 0.09453901648521423
In grad_steps = 2338, loss = 0.05373426154255867
In grad_steps = 2339, loss = 0.09248367697000504
In grad_steps = 2340, loss = 0.20133128762245178
In grad_steps = 2341, loss = 0.15858377516269684
In grad_steps = 2342, loss = 1.0889934301376343
In grad_steps = 2343, loss = 0.038246676325798035
In grad_steps = 2344, loss = 0.9579398036003113
In grad_steps = 2345, loss = 0.08958251774311066
In grad_steps = 2346, loss = 0.22945180535316467
In grad_steps = 2347, loss = 0.11400876939296722
In grad_steps = 2348, loss = 0.06530323624610901
In grad_steps = 2349, loss = 0.15791350603103638
In grad_steps = 2350, loss = 0.1682591736316681
In grad_steps = 2351, loss = 0.46450063586235046
In grad_steps = 2352, loss = 0.006575258448719978
Beginning epoch 2
In grad_steps = 2353, loss = 0.06152058765292168
In grad_steps = 2354, loss = 1.323829174041748
In grad_steps = 2355, loss = 0.2828940451145172
In grad_steps = 2356, loss = 0.3976166546344757
In grad_steps = 2357, loss = 0.07775136828422546
In grad_steps = 2358, loss = 0.486611545085907
In grad_steps = 2359, loss = 0.1565595269203186
In grad_steps = 2360, loss = 0.10439684242010117
In grad_steps = 2361, loss = 0.06385039538145065
In grad_steps = 2362, loss = 0.8751797676086426
In grad_steps = 2363, loss = 0.17438092827796936
In grad_steps = 2364, loss = 0.26750850677490234
In grad_steps = 2365, loss = 0.1678653210401535
In grad_steps = 2366, loss = 0.07512722909450531
In grad_steps = 2367, loss = 0.5117059350013733
In grad_steps = 2368, loss = 0.060426242649555206
In grad_steps = 2369, loss = 0.12766484916210175
In grad_steps = 2370, loss = 0.5817035436630249
In grad_steps = 2371, loss = 0.534328281879425
In grad_steps = 2372, loss = 0.2795186936855316
In grad_steps = 2373, loss = 0.0453665554523468
In grad_steps = 2374, loss = 0.07755661010742188
In grad_steps = 2375, loss = 0.3311042785644531
In grad_steps = 2376, loss = 0.8201219439506531
In grad_steps = 2377, loss = 0.6628707051277161
In grad_steps = 2378, loss = 0.32136762142181396
In grad_steps = 2379, loss = 0.04456678032875061
In grad_steps = 2380, loss = 0.06528524309396744
In grad_steps = 2381, loss = 0.06912773102521896
In grad_steps = 2382, loss = 0.038284607231616974
In grad_steps = 2383, loss = 0.06798771768808365
In grad_steps = 2384, loss = 0.3825516104698181
In grad_steps = 2385, loss = 0.08591306954622269
In grad_steps = 2386, loss = 0.1959836781024933
In grad_steps = 2387, loss = 0.36456453800201416
In grad_steps = 2388, loss = 0.37103453278541565
In grad_steps = 2389, loss = 0.024741988629102707
In grad_steps = 2390, loss = 0.029212363064289093
In grad_steps = 2391, loss = 0.3774769902229309
In grad_steps = 2392, loss = 0.5010383725166321
In grad_steps = 2393, loss = 0.029197096824645996
In grad_steps = 2394, loss = 0.1672830879688263
In grad_steps = 2395, loss = 0.19596552848815918
In grad_steps = 2396, loss = 0.04861273989081383
In grad_steps = 2397, loss = 0.3109065294265747
In grad_steps = 2398, loss = 0.26426029205322266
In grad_steps = 2399, loss = 0.14400167763233185
In grad_steps = 2400, loss = 0.7487058639526367
In grad_steps = 2401, loss = 0.09039010107517242
In grad_steps = 2402, loss = 0.4673931300640106
In grad_steps = 2403, loss = 0.6390307545661926
In grad_steps = 2404, loss = 1.4848484992980957
In grad_steps = 2405, loss = 0.15570193529129028
In grad_steps = 2406, loss = 0.8650147318840027
In grad_steps = 2407, loss = 0.05814642831683159
In grad_steps = 2408, loss = 0.09139153361320496
In grad_steps = 2409, loss = 0.3513117730617523
In grad_steps = 2410, loss = 0.11089038848876953
In grad_steps = 2411, loss = 0.23319576680660248
In grad_steps = 2412, loss = 0.08701448142528534
In grad_steps = 2413, loss = 0.3606301546096802
In grad_steps = 2414, loss = 0.17882011830806732
In grad_steps = 2415, loss = 0.3732914924621582
In grad_steps = 2416, loss = 0.25477051734924316
In grad_steps = 2417, loss = 0.2903364598751068
In grad_steps = 2418, loss = 0.0663025751709938
In grad_steps = 2419, loss = 0.07167551666498184
In grad_steps = 2420, loss = 0.06930020451545715
In grad_steps = 2421, loss = 0.20259806513786316
In grad_steps = 2422, loss = 1.2679342031478882
In grad_steps = 2423, loss = 0.6163395643234253
In grad_steps = 2424, loss = 0.034453656524419785
In grad_steps = 2425, loss = 1.2023767232894897
In grad_steps = 2426, loss = 0.04388601332902908
In grad_steps = 2427, loss = 0.12617459893226624
In grad_steps = 2428, loss = 0.11307119578123093
In grad_steps = 2429, loss = 0.18888963758945465
In grad_steps = 2430, loss = 0.5016159415245056
In grad_steps = 2431, loss = 0.15683411061763763
In grad_steps = 2432, loss = 0.18536031246185303
In grad_steps = 2433, loss = 0.25024059414863586
In grad_steps = 2434, loss = 0.11356981843709946
In grad_steps = 2435, loss = 0.09450338780879974
In grad_steps = 2436, loss = 0.36794009804725647
In grad_steps = 2437, loss = 0.09161020815372467
In grad_steps = 2438, loss = 0.336010217666626
In grad_steps = 2439, loss = 0.997096836566925
In grad_steps = 2440, loss = 0.12306658923625946
In grad_steps = 2441, loss = 0.3733333647251129
In grad_steps = 2442, loss = 0.14465084671974182
In grad_steps = 2443, loss = 1.2612872123718262
In grad_steps = 2444, loss = 0.3638540208339691
In grad_steps = 2445, loss = 0.3099587559700012
In grad_steps = 2446, loss = 0.15288092195987701
In grad_steps = 2447, loss = 0.044678859412670135
In grad_steps = 2448, loss = 0.46560606360435486
In grad_steps = 2449, loss = 0.13473090529441833
In grad_steps = 2450, loss = 0.03952796757221222
In grad_steps = 2451, loss = 0.13075155019760132
In grad_steps = 2452, loss = 0.411531001329422
In grad_steps = 2453, loss = 0.6839162111282349
In grad_steps = 2454, loss = 0.05668073147535324
In grad_steps = 2455, loss = 0.5612163543701172
In grad_steps = 2456, loss = 0.6667553186416626
In grad_steps = 2457, loss = 0.3038564920425415
In grad_steps = 2458, loss = 0.4253738224506378
In grad_steps = 2459, loss = 0.0287819541990757
In grad_steps = 2460, loss = 0.05204290524125099
In grad_steps = 2461, loss = 0.11708232760429382
In grad_steps = 2462, loss = 0.18389448523521423
In grad_steps = 2463, loss = 0.24720758199691772
In grad_steps = 2464, loss = 0.09640916436910629
In grad_steps = 2465, loss = 0.1898089349269867
In grad_steps = 2466, loss = 0.30274805426597595
In grad_steps = 2467, loss = 0.31985652446746826
In grad_steps = 2468, loss = 0.21660444140434265
In grad_steps = 2469, loss = 0.879773736000061
In grad_steps = 2470, loss = 0.15886743366718292
In grad_steps = 2471, loss = 0.18421024084091187
In grad_steps = 2472, loss = 0.6775265336036682
In grad_steps = 2473, loss = 0.9533044695854187
In grad_steps = 2474, loss = 0.08120346814393997
In grad_steps = 2475, loss = 0.3469093143939972
In grad_steps = 2476, loss = 0.07817395031452179
In grad_steps = 2477, loss = 0.24844799935817719
In grad_steps = 2478, loss = 0.9479949474334717
In grad_steps = 2479, loss = 0.46015870571136475
In grad_steps = 2480, loss = 0.08980908244848251
In grad_steps = 2481, loss = 0.5817677974700928
In grad_steps = 2482, loss = 0.13759122788906097
In grad_steps = 2483, loss = 0.3175806403160095
In grad_steps = 2484, loss = 0.210773304104805
In grad_steps = 2485, loss = 0.2256428599357605
In grad_steps = 2486, loss = 0.09039497375488281
In grad_steps = 2487, loss = 0.5807810425758362
In grad_steps = 2488, loss = 0.06995714455842972
In grad_steps = 2489, loss = 0.15233367681503296
In grad_steps = 2490, loss = 0.149150088429451
In grad_steps = 2491, loss = 0.1967172771692276
In grad_steps = 2492, loss = 0.10264278948307037
In grad_steps = 2493, loss = 0.020808719098567963
In grad_steps = 2494, loss = 0.315968781709671
In grad_steps = 2495, loss = 0.6968343257904053
In grad_steps = 2496, loss = 0.2734692394733429
In grad_steps = 2497, loss = 0.16625192761421204
In grad_steps = 2498, loss = 0.2802741527557373
In grad_steps = 2499, loss = 0.09423032402992249
In grad_steps = 2500, loss = 0.6905901432037354
In grad_steps = 2501, loss = 0.9970623254776001
In grad_steps = 2502, loss = 0.9361369609832764
In grad_steps = 2503, loss = 0.07058596611022949
In grad_steps = 2504, loss = 0.20395833253860474
In grad_steps = 2505, loss = 0.05554153770208359
In grad_steps = 2506, loss = 0.5457649230957031
In grad_steps = 2507, loss = 0.6033414602279663
In grad_steps = 2508, loss = 0.19522596895694733
In grad_steps = 2509, loss = 0.21681112051010132
In grad_steps = 2510, loss = 0.5780943632125854
In grad_steps = 2511, loss = 0.1066480427980423
In grad_steps = 2512, loss = 0.3951284885406494
In grad_steps = 2513, loss = 0.6731653809547424
In grad_steps = 2514, loss = 0.05617544800043106
In grad_steps = 2515, loss = 0.4234195053577423
In grad_steps = 2516, loss = 0.13479489088058472
In grad_steps = 2517, loss = 0.10218241065740585
In grad_steps = 2518, loss = 0.17735019326210022
In grad_steps = 2519, loss = 0.24371132254600525
In grad_steps = 2520, loss = 0.09113089740276337
In grad_steps = 2521, loss = 0.0994310975074768
In grad_steps = 2522, loss = 0.5538679361343384
In grad_steps = 2523, loss = 0.05774451047182083
In grad_steps = 2524, loss = 0.29215508699417114
In grad_steps = 2525, loss = 0.09437139332294464
In grad_steps = 2526, loss = 0.06549936532974243
In grad_steps = 2527, loss = 0.6582783460617065
In grad_steps = 2528, loss = 0.129692941904068
In grad_steps = 2529, loss = 0.0753440260887146
In grad_steps = 2530, loss = 0.05661921203136444
In grad_steps = 2531, loss = 0.6302194595336914
In grad_steps = 2532, loss = 0.1449742466211319
In grad_steps = 2533, loss = 0.04470287263393402
In grad_steps = 2534, loss = 0.040006231516599655
In grad_steps = 2535, loss = 0.0576152428984642
In grad_steps = 2536, loss = 0.05263761430978775
In grad_steps = 2537, loss = 0.06245073303580284
In grad_steps = 2538, loss = 0.208974689245224
In grad_steps = 2539, loss = 0.0428101122379303
In grad_steps = 2540, loss = 0.011186111718416214
In grad_steps = 2541, loss = 1.1125521659851074
In grad_steps = 2542, loss = 0.006889906711876392
In grad_steps = 2543, loss = 0.04384142532944679
In grad_steps = 2544, loss = 0.44180235266685486
In grad_steps = 2545, loss = 0.1613587886095047
In grad_steps = 2546, loss = 0.02451634779572487
In grad_steps = 2547, loss = 0.3782729208469391
In grad_steps = 2548, loss = 0.020657481625676155
In grad_steps = 2549, loss = 0.8877562880516052
In grad_steps = 2550, loss = 0.006378006190061569
In grad_steps = 2551, loss = 0.037579916417598724
In grad_steps = 2552, loss = 0.028238575905561447
In grad_steps = 2553, loss = 0.03560853749513626
In grad_steps = 2554, loss = 1.4262694120407104
In grad_steps = 2555, loss = 0.2988404929637909
In grad_steps = 2556, loss = 0.01591671258211136
In grad_steps = 2557, loss = 0.06327321380376816
In grad_steps = 2558, loss = 0.37885427474975586
In grad_steps = 2559, loss = 0.8255404233932495
In grad_steps = 2560, loss = 0.3800422251224518
In grad_steps = 2561, loss = 0.06506884843111038
In grad_steps = 2562, loss = 0.6737678647041321
In grad_steps = 2563, loss = 0.15727823972702026
In grad_steps = 2564, loss = 0.07316135615110397
In grad_steps = 2565, loss = 0.1606021523475647
In grad_steps = 2566, loss = 0.08513185381889343
In grad_steps = 2567, loss = 0.3748648464679718
In grad_steps = 2568, loss = 0.22930172085762024
In grad_steps = 2569, loss = 0.11704600602388382
In grad_steps = 2570, loss = 0.4419589340686798
In grad_steps = 2571, loss = 0.3497266173362732
In grad_steps = 2572, loss = 0.06623075902462006
In grad_steps = 2573, loss = 0.04920779541134834
In grad_steps = 2574, loss = 0.4669094383716583
In grad_steps = 2575, loss = 0.02884460799396038
In grad_steps = 2576, loss = 0.09369754791259766
In grad_steps = 2577, loss = 0.37284499406814575
In grad_steps = 2578, loss = 0.7790138125419617
In grad_steps = 2579, loss = 0.1561831831932068
In grad_steps = 2580, loss = 0.669529914855957
In grad_steps = 2581, loss = 0.37643909454345703
In grad_steps = 2582, loss = 0.5635112524032593
In grad_steps = 2583, loss = 0.2839866280555725
In grad_steps = 2584, loss = 0.12357998639345169
In grad_steps = 2585, loss = 0.09798639267683029
In grad_steps = 2586, loss = 0.8448565602302551
In grad_steps = 2587, loss = 0.25569412112236023
In grad_steps = 2588, loss = 0.1741461455821991
In grad_steps = 2589, loss = 0.2882434129714966
In grad_steps = 2590, loss = 0.36739060282707214
In grad_steps = 2591, loss = 0.1748242974281311
In grad_steps = 2592, loss = 0.4709681272506714
In grad_steps = 2593, loss = 0.1408536732196808
In grad_steps = 2594, loss = 0.16500113904476166
In grad_steps = 2595, loss = 0.02734968066215515
In grad_steps = 2596, loss = 0.09038706123828888
In grad_steps = 2597, loss = 0.08433786034584045
In grad_steps = 2598, loss = 0.09597934037446976
In grad_steps = 2599, loss = 0.034481193870306015
In grad_steps = 2600, loss = 0.3236392140388489
In grad_steps = 2601, loss = 0.015827879309654236
In grad_steps = 2602, loss = 0.03701804578304291
In grad_steps = 2603, loss = 0.1864752471446991
In grad_steps = 2604, loss = 0.3144143521785736
In grad_steps = 2605, loss = 0.9999480247497559
In grad_steps = 2606, loss = 0.048337168991565704
In grad_steps = 2607, loss = 0.15442903339862823
In grad_steps = 2608, loss = 0.9659601449966431
In grad_steps = 2609, loss = 0.20154468715190887
In grad_steps = 2610, loss = 0.6017512083053589
In grad_steps = 2611, loss = 0.06491556763648987
In grad_steps = 2612, loss = 0.1404038369655609
In grad_steps = 2613, loss = 0.6537197828292847
In grad_steps = 2614, loss = 0.5004862546920776
In grad_steps = 2615, loss = 0.07355449348688126
In grad_steps = 2616, loss = 0.5785044431686401
In grad_steps = 2617, loss = 0.09710047394037247
In grad_steps = 2618, loss = 0.14629137516021729
In grad_steps = 2619, loss = 0.2432890385389328
In grad_steps = 2620, loss = 0.1266179084777832
In grad_steps = 2621, loss = 0.3375388979911804
In grad_steps = 2622, loss = 0.14828920364379883
In grad_steps = 2623, loss = 0.12924067676067352
In grad_steps = 2624, loss = 0.14904294908046722
In grad_steps = 2625, loss = 0.08065683394670486
In grad_steps = 2626, loss = 0.399551123380661
In grad_steps = 2627, loss = 0.05440006032586098
In grad_steps = 2628, loss = 0.06472823023796082
In grad_steps = 2629, loss = 0.18928350508213043
In grad_steps = 2630, loss = 0.5830806493759155
In grad_steps = 2631, loss = 0.019774816930294037
In grad_steps = 2632, loss = 1.1252940893173218
In grad_steps = 2633, loss = 0.6652706861495972
In grad_steps = 2634, loss = 0.8249784708023071
In grad_steps = 2635, loss = 0.14806818962097168
In grad_steps = 2636, loss = 0.5336459279060364
In grad_steps = 2637, loss = 0.10933835059404373
In grad_steps = 2638, loss = 0.5986868739128113
In grad_steps = 2639, loss = 0.5332538485527039
In grad_steps = 2640, loss = 0.02346249297261238
In grad_steps = 2641, loss = 0.02914559096097946
In grad_steps = 2642, loss = 0.4072924852371216
In grad_steps = 2643, loss = 0.08211514353752136
In grad_steps = 2644, loss = 0.25292083621025085
In grad_steps = 2645, loss = 0.25266915559768677
In grad_steps = 2646, loss = 0.2422800064086914
In grad_steps = 2647, loss = 0.16322138905525208
In grad_steps = 2648, loss = 0.1742396056652069
In grad_steps = 2649, loss = 0.06004004925489426
In grad_steps = 2650, loss = 0.18705543875694275
In grad_steps = 2651, loss = 0.3143687844276428
In grad_steps = 2652, loss = 0.08364695310592651
In grad_steps = 2653, loss = 0.15907005965709686
In grad_steps = 2654, loss = 0.13688945770263672
In grad_steps = 2655, loss = 0.014368304051458836
In grad_steps = 2656, loss = 0.2843535542488098
In grad_steps = 2657, loss = 0.3396293520927429
In grad_steps = 2658, loss = 1.0180633068084717
In grad_steps = 2659, loss = 0.03808650001883507
In grad_steps = 2660, loss = 0.256719708442688
In grad_steps = 2661, loss = 0.13917586207389832
In grad_steps = 2662, loss = 0.5803803205490112
In grad_steps = 2663, loss = 0.1746925711631775
In grad_steps = 2664, loss = 0.2951437532901764
In grad_steps = 2665, loss = 0.1733100265264511
In grad_steps = 2666, loss = 0.238308846950531
In grad_steps = 2667, loss = 0.5328494310379028
In grad_steps = 2668, loss = 0.45643845200538635
In grad_steps = 2669, loss = 0.0701935738325119
In grad_steps = 2670, loss = 0.1617870181798935
In grad_steps = 2671, loss = 0.3798239529132843
In grad_steps = 2672, loss = 0.06998876482248306
In grad_steps = 2673, loss = 0.13797903060913086
In grad_steps = 2674, loss = 0.16519881784915924
In grad_steps = 2675, loss = 0.24577055871486664
In grad_steps = 2676, loss = 0.5582717061042786
In grad_steps = 2677, loss = 0.07953781634569168
In grad_steps = 2678, loss = 0.09444441646337509
In grad_steps = 2679, loss = 0.032066091895103455
In grad_steps = 2680, loss = 0.06827227771282196
In grad_steps = 2681, loss = 0.03338965028524399
In grad_steps = 2682, loss = 0.0035689191427081823
In grad_steps = 2683, loss = 0.0950140580534935
In grad_steps = 2684, loss = 0.2514539659023285
In grad_steps = 2685, loss = 0.13964888453483582
In grad_steps = 2686, loss = 1.2893705368041992
In grad_steps = 2687, loss = 0.007795133627951145
In grad_steps = 2688, loss = 0.053822409361600876
In grad_steps = 2689, loss = 0.07559462636709213
In grad_steps = 2690, loss = 0.027687616646289825
In grad_steps = 2691, loss = 0.048780862241983414
In grad_steps = 2692, loss = 0.10452352464199066
In grad_steps = 2693, loss = 0.6062987446784973
In grad_steps = 2694, loss = 0.04866928234696388
In grad_steps = 2695, loss = 0.7214187979698181
In grad_steps = 2696, loss = 0.17441117763519287
In grad_steps = 2697, loss = 0.23596161603927612
In grad_steps = 2698, loss = 0.13550731539726257
In grad_steps = 2699, loss = 0.10205688327550888
In grad_steps = 2700, loss = 0.07902522385120392
In grad_steps = 2701, loss = 0.9093768000602722
In grad_steps = 2702, loss = 1.0338106155395508
In grad_steps = 2703, loss = 0.9224197268486023
In grad_steps = 2704, loss = 0.14547227323055267
In grad_steps = 2705, loss = 0.16430902481079102
In grad_steps = 2706, loss = 0.0776425376534462
In grad_steps = 2707, loss = 0.16490812599658966
In grad_steps = 2708, loss = 0.6512157917022705
In grad_steps = 2709, loss = 0.36164212226867676
In grad_steps = 2710, loss = 0.34526294469833374
In grad_steps = 2711, loss = 0.14047153294086456
In grad_steps = 2712, loss = 0.2859673500061035
In grad_steps = 2713, loss = 0.322197288274765
In grad_steps = 2714, loss = 0.12912708520889282
In grad_steps = 2715, loss = 0.14141365885734558
In grad_steps = 2716, loss = 0.03549938648939133
In grad_steps = 2717, loss = 0.06531128287315369
In grad_steps = 2718, loss = 0.09910991787910461
In grad_steps = 2719, loss = 0.027428174391388893
In grad_steps = 2720, loss = 0.1723175048828125
In grad_steps = 2721, loss = 0.05660240352153778
In grad_steps = 2722, loss = 0.04674728587269783
In grad_steps = 2723, loss = 1.1141173839569092
In grad_steps = 2724, loss = 0.03496816009283066
In grad_steps = 2725, loss = 0.06500086188316345
In grad_steps = 2726, loss = 0.18467360734939575
In grad_steps = 2727, loss = 0.037130337208509445
In grad_steps = 2728, loss = 0.039402782917022705
In grad_steps = 2729, loss = 0.03982485085725784
In grad_steps = 2730, loss = 0.12275786697864532
In grad_steps = 2731, loss = 0.34123891592025757
In grad_steps = 2732, loss = 0.08575358986854553
In grad_steps = 2733, loss = 0.08673322200775146
In grad_steps = 2734, loss = 0.5007820129394531
In grad_steps = 2735, loss = 0.21619407832622528
In grad_steps = 2736, loss = 0.018498949706554413
In grad_steps = 2737, loss = 0.175606831908226
In grad_steps = 2738, loss = 0.11711963266134262
In grad_steps = 2739, loss = 1.057051658630371
In grad_steps = 2740, loss = 0.040549106895923615
In grad_steps = 2741, loss = 0.28997987508773804
In grad_steps = 2742, loss = 1.3767462968826294
In grad_steps = 2743, loss = 0.05853009596467018
In grad_steps = 2744, loss = 0.04724006727337837
In grad_steps = 2745, loss = 0.04890447109937668
In grad_steps = 2746, loss = 0.6124365925788879
In grad_steps = 2747, loss = 0.02669229730963707
In grad_steps = 2748, loss = 0.20048512518405914
In grad_steps = 2749, loss = 0.0317889042198658
In grad_steps = 2750, loss = 0.14066201448440552
In grad_steps = 2751, loss = 0.10429633408784866
In grad_steps = 2752, loss = 0.2980857491493225
In grad_steps = 2753, loss = 0.257826566696167
In grad_steps = 2754, loss = 0.10158547759056091
In grad_steps = 2755, loss = 0.04069516435265541
In grad_steps = 2756, loss = 0.6914840936660767
In grad_steps = 2757, loss = 0.11354932188987732
In grad_steps = 2758, loss = 0.04068208113312721
In grad_steps = 2759, loss = 0.7210532426834106
In grad_steps = 2760, loss = 0.17063789069652557
In grad_steps = 2761, loss = 0.04613813757896423
In grad_steps = 2762, loss = 0.33087393641471863
In grad_steps = 2763, loss = 0.041005689650774
In grad_steps = 2764, loss = 0.5128219723701477
In grad_steps = 2765, loss = 0.17910712957382202
In grad_steps = 2766, loss = 0.08164290338754654
In grad_steps = 2767, loss = 0.3066176176071167
In grad_steps = 2768, loss = 0.09043557941913605
In grad_steps = 2769, loss = 0.05123743787407875
In grad_steps = 2770, loss = 0.0810750424861908
In grad_steps = 2771, loss = 0.28851863741874695
In grad_steps = 2772, loss = 0.0516989603638649
In grad_steps = 2773, loss = 0.008583205752074718
In grad_steps = 2774, loss = 0.24762699007987976
In grad_steps = 2775, loss = 0.22227880358695984
In grad_steps = 2776, loss = 0.0369277186691761
In grad_steps = 2777, loss = 0.314405232667923
In grad_steps = 2778, loss = 0.011973176151514053
In grad_steps = 2779, loss = 0.058664776384830475
In grad_steps = 2780, loss = 0.04113152623176575
In grad_steps = 2781, loss = 0.017413059249520302
In grad_steps = 2782, loss = 0.757041871547699
In grad_steps = 2783, loss = 0.4640398323535919
In grad_steps = 2784, loss = 0.12022126466035843
In grad_steps = 2785, loss = 0.02548093907535076
In grad_steps = 2786, loss = 0.3882986307144165
In grad_steps = 2787, loss = 0.7711617946624756
In grad_steps = 2788, loss = 0.02268601953983307
In grad_steps = 2789, loss = 0.1701665222644806
In grad_steps = 2790, loss = 0.4659394323825836
In grad_steps = 2791, loss = 0.8659849762916565
In grad_steps = 2792, loss = 0.05216062813997269
In grad_steps = 2793, loss = 0.016994403675198555
In grad_steps = 2794, loss = 0.14464262127876282
In grad_steps = 2795, loss = 0.5156943798065186
In grad_steps = 2796, loss = 0.022266589105129242
In grad_steps = 2797, loss = 0.06120520085096359
In grad_steps = 2798, loss = 0.022016121074557304
In grad_steps = 2799, loss = 0.4775688052177429
In grad_steps = 2800, loss = 0.045972470194101334
In grad_steps = 2801, loss = 0.5032002925872803
In grad_steps = 2802, loss = 0.06735339760780334
In grad_steps = 2803, loss = 0.04638112708926201
In grad_steps = 2804, loss = 0.08982358872890472
In grad_steps = 2805, loss = 0.11937227100133896
In grad_steps = 2806, loss = 0.2141924500465393
In grad_steps = 2807, loss = 0.5017452836036682
In grad_steps = 2808, loss = 0.021898891776800156
In grad_steps = 2809, loss = 0.6382187604904175
In grad_steps = 2810, loss = 0.10055764019489288
In grad_steps = 2811, loss = 0.47022125124931335
In grad_steps = 2812, loss = 0.2160801887512207
In grad_steps = 2813, loss = 0.04731692746281624
In grad_steps = 2814, loss = 0.006560183130204678
In grad_steps = 2815, loss = 1.0141525268554688
In grad_steps = 2816, loss = 0.049572333693504333
In grad_steps = 2817, loss = 0.11855348944664001
In grad_steps = 2818, loss = 0.7060757279396057
In grad_steps = 2819, loss = 0.11925698071718216
In grad_steps = 2820, loss = 0.3024061322212219
In grad_steps = 2821, loss = 0.17548063397407532
In grad_steps = 2822, loss = 0.22432221472263336
In grad_steps = 2823, loss = 0.21307435631752014
In grad_steps = 2824, loss = 0.03973469138145447
In grad_steps = 2825, loss = 0.06377745419740677
In grad_steps = 2826, loss = 0.04604272171854973
In grad_steps = 2827, loss = 0.5669719576835632
In grad_steps = 2828, loss = 0.29219263792037964
In grad_steps = 2829, loss = 0.013333719223737717
In grad_steps = 2830, loss = 0.07340824604034424
In grad_steps = 2831, loss = 0.05236741155385971
In grad_steps = 2832, loss = 0.6405807733535767
In grad_steps = 2833, loss = 0.03988161310553551
In grad_steps = 2834, loss = 0.19846737384796143
In grad_steps = 2835, loss = 0.0631255954504013
In grad_steps = 2836, loss = 0.08208763599395752
In grad_steps = 2837, loss = 0.4546061158180237
In grad_steps = 2838, loss = 0.7919111847877502
In grad_steps = 2839, loss = 0.02446025237441063
In grad_steps = 2840, loss = 0.5751276612281799
In grad_steps = 2841, loss = 1.0406646728515625
In grad_steps = 2842, loss = 0.4597857892513275
In grad_steps = 2843, loss = 0.19678127765655518
In grad_steps = 2844, loss = 0.2117784023284912
In grad_steps = 2845, loss = 0.1572713702917099
In grad_steps = 2846, loss = 0.040994126349687576
In grad_steps = 2847, loss = 0.6557747721672058
In grad_steps = 2848, loss = 0.14960432052612305
In grad_steps = 2849, loss = 0.2697066068649292
In grad_steps = 2850, loss = 0.05006703361868858
In grad_steps = 2851, loss = 0.4236114025115967
In grad_steps = 2852, loss = 0.19666337966918945
In grad_steps = 2853, loss = 0.07920335978269577
In grad_steps = 2854, loss = 0.33326399326324463
In grad_steps = 2855, loss = 0.5556350350379944
In grad_steps = 2856, loss = 0.5044446587562561
In grad_steps = 2857, loss = 0.23729495704174042
In grad_steps = 2858, loss = 0.7035131454467773
In grad_steps = 2859, loss = 0.19034725427627563
In grad_steps = 2860, loss = 0.3936172127723694
In grad_steps = 2861, loss = 0.799927294254303
In grad_steps = 2862, loss = 0.2258983999490738
In grad_steps = 2863, loss = 0.028308099135756493
In grad_steps = 2864, loss = 0.29657837748527527
In grad_steps = 2865, loss = 0.10436289012432098
In grad_steps = 2866, loss = 0.1632840782403946
In grad_steps = 2867, loss = 0.20316678285598755
In grad_steps = 2868, loss = 0.2011069655418396
In grad_steps = 2869, loss = 0.35824301838874817
In grad_steps = 2870, loss = 0.2617623805999756
In grad_steps = 2871, loss = 0.06856030970811844
In grad_steps = 2872, loss = 0.26994937658309937
In grad_steps = 2873, loss = 0.08780771493911743
In grad_steps = 2874, loss = 0.3837956488132477
In grad_steps = 2875, loss = 0.09947177767753601
In grad_steps = 2876, loss = 0.03216589242219925
In grad_steps = 2877, loss = 0.016550209373235703
In grad_steps = 2878, loss = 0.1748286634683609
In grad_steps = 2879, loss = 0.06622055172920227
In grad_steps = 2880, loss = 0.26831120252609253
In grad_steps = 2881, loss = 0.08308158069849014
In grad_steps = 2882, loss = 0.02441360428929329
In grad_steps = 2883, loss = 0.01732250489294529
In grad_steps = 2884, loss = 0.36531054973602295
In grad_steps = 2885, loss = 0.0860414206981659
In grad_steps = 2886, loss = 0.009850781410932541
In grad_steps = 2887, loss = 1.5670450925827026
In grad_steps = 2888, loss = 0.7841750383377075
In grad_steps = 2889, loss = 0.39774632453918457
In grad_steps = 2890, loss = 0.0796390175819397
In grad_steps = 2891, loss = 0.14322306215763092
In grad_steps = 2892, loss = 0.06719405204057693
In grad_steps = 2893, loss = 0.0713491439819336
In grad_steps = 2894, loss = 0.03106902539730072
In grad_steps = 2895, loss = 0.09030237048864365
In grad_steps = 2896, loss = 0.00960073247551918
In grad_steps = 2897, loss = 1.624873399734497
In grad_steps = 2898, loss = 0.9362148642539978
In grad_steps = 2899, loss = 0.11577527225017548
In grad_steps = 2900, loss = 1.2544493675231934
In grad_steps = 2901, loss = 0.08236263692378998
In grad_steps = 2902, loss = 0.3550208806991577
In grad_steps = 2903, loss = 0.626234233379364
In grad_steps = 2904, loss = 0.25696033239364624
In grad_steps = 2905, loss = 0.29593220353126526
In grad_steps = 2906, loss = 0.057261548936367035
In grad_steps = 2907, loss = 0.23907142877578735
In grad_steps = 2908, loss = 0.22782763838768005
In grad_steps = 2909, loss = 0.6651285886764526
In grad_steps = 2910, loss = 0.25545331835746765
In grad_steps = 2911, loss = 0.6417044997215271
In grad_steps = 2912, loss = 0.31611350178718567
In grad_steps = 2913, loss = 0.7010205984115601
In grad_steps = 2914, loss = 0.10065552592277527
In grad_steps = 2915, loss = 0.14628572762012482
In grad_steps = 2916, loss = 0.07667418569326401
In grad_steps = 2917, loss = 0.8944967985153198
In grad_steps = 2918, loss = 0.5028380751609802
In grad_steps = 2919, loss = 0.2378949224948883
In grad_steps = 2920, loss = 0.8578550815582275
In grad_steps = 2921, loss = 0.3525702953338623
In grad_steps = 2922, loss = 0.06248735263943672
In grad_steps = 2923, loss = 0.1512807309627533
In grad_steps = 2924, loss = 0.1798989474773407
In grad_steps = 2925, loss = 0.19896453619003296
In grad_steps = 2926, loss = 0.07293955236673355
In grad_steps = 2927, loss = 0.14530512690544128
In grad_steps = 2928, loss = 0.1021185964345932
In grad_steps = 2929, loss = 0.2665192782878876
In grad_steps = 2930, loss = 0.014816228300333023
In grad_steps = 2931, loss = 0.19292402267456055
In grad_steps = 2932, loss = 0.44231116771698
In grad_steps = 2933, loss = 0.776654064655304
In grad_steps = 2934, loss = 0.21135295927524567
In grad_steps = 2935, loss = 0.13763512670993805
In grad_steps = 2936, loss = 0.11999781429767609
In grad_steps = 2937, loss = 0.034760504961013794
In grad_steps = 2938, loss = 0.04778296500444412
In grad_steps = 2939, loss = 0.06482046097517014
In grad_steps = 2940, loss = 0.2556903064250946
In grad_steps = 2941, loss = 0.04931318759918213
In grad_steps = 2942, loss = 1.006697654724121
In grad_steps = 2943, loss = 0.3253606855869293
In grad_steps = 2944, loss = 0.06622445583343506
In grad_steps = 2945, loss = 1.0563905239105225
In grad_steps = 2946, loss = 0.07151387631893158
In grad_steps = 2947, loss = 0.06806733459234238
In grad_steps = 2948, loss = 0.37114793062210083
In grad_steps = 2949, loss = 0.2834394872188568
In grad_steps = 2950, loss = 0.26803624629974365
In grad_steps = 2951, loss = 0.15529945492744446
In grad_steps = 2952, loss = 0.7117528319358826
In grad_steps = 2953, loss = 0.5221017599105835
In grad_steps = 2954, loss = 0.022475263103842735
In grad_steps = 2955, loss = 0.026070918887853622
In grad_steps = 2956, loss = 0.08763813227415085
In grad_steps = 2957, loss = 0.07452476024627686
In grad_steps = 2958, loss = 0.15428875386714935
In grad_steps = 2959, loss = 0.038040220737457275
In grad_steps = 2960, loss = 0.04072156175971031
In grad_steps = 2961, loss = 0.2506229281425476
In grad_steps = 2962, loss = 0.3073318600654602
In grad_steps = 2963, loss = 0.42846864461898804
In grad_steps = 2964, loss = 0.5633890628814697
In grad_steps = 2965, loss = 0.1253136396408081
In grad_steps = 2966, loss = 0.76408851146698
In grad_steps = 2967, loss = 0.1805567741394043
In grad_steps = 2968, loss = 0.19620367884635925
In grad_steps = 2969, loss = 0.1853082776069641
In grad_steps = 2970, loss = 0.019933907315135002
In grad_steps = 2971, loss = 0.23232212662696838
In grad_steps = 2972, loss = 0.02587372064590454
In grad_steps = 2973, loss = 0.10357508063316345
In grad_steps = 2974, loss = 1.1494672298431396
In grad_steps = 2975, loss = 0.2526599168777466
In grad_steps = 2976, loss = 0.050151526927948
In grad_steps = 2977, loss = 0.12111210823059082
In grad_steps = 2978, loss = 0.28006359934806824
In grad_steps = 2979, loss = 0.5225370526313782
In grad_steps = 2980, loss = 0.0242350734770298
In grad_steps = 2981, loss = 0.15917623043060303
In grad_steps = 2982, loss = 0.08628758788108826
In grad_steps = 2983, loss = 0.10553291440010071
In grad_steps = 2984, loss = 0.07033517956733704
In grad_steps = 2985, loss = 0.03200062736868858
In grad_steps = 2986, loss = 0.42605507373809814
In grad_steps = 2987, loss = 0.022030234336853027
In grad_steps = 2988, loss = 0.34854334592819214
In grad_steps = 2989, loss = 0.1185809001326561
In grad_steps = 2990, loss = 0.30575937032699585
In grad_steps = 2991, loss = 0.04427987337112427
In grad_steps = 2992, loss = 1.0241378545761108
In grad_steps = 2993, loss = 0.4449059069156647
In grad_steps = 2994, loss = 0.0464119091629982
In grad_steps = 2995, loss = 0.04205368459224701
In grad_steps = 2996, loss = 0.13153502345085144
In grad_steps = 2997, loss = 0.2557087242603302
In grad_steps = 2998, loss = 1.224083423614502
In grad_steps = 2999, loss = 0.1703713983297348
In grad_steps = 3000, loss = 0.1055598109960556
In grad_steps = 3001, loss = 0.36248311400413513
In grad_steps = 3002, loss = 0.06944642961025238
In grad_steps = 3003, loss = 0.5902879238128662
In grad_steps = 3004, loss = 0.213436096906662
In grad_steps = 3005, loss = 1.3326637744903564
In grad_steps = 3006, loss = 0.45810315012931824
In grad_steps = 3007, loss = 0.4094313085079193
In grad_steps = 3008, loss = 0.38329413533210754
In grad_steps = 3009, loss = 0.06204892694950104
In grad_steps = 3010, loss = 0.048033103346824646
In grad_steps = 3011, loss = 0.1904579997062683
In grad_steps = 3012, loss = 0.029767826199531555
In grad_steps = 3013, loss = 0.0772876888513565
In grad_steps = 3014, loss = 0.1029641255736351
In grad_steps = 3015, loss = 0.38825464248657227
In grad_steps = 3016, loss = 0.034125667065382004
In grad_steps = 3017, loss = 0.043823447078466415
In grad_steps = 3018, loss = 0.12938283383846283
In grad_steps = 3019, loss = 0.031768813729286194
In grad_steps = 3020, loss = 0.17029841244220734
In grad_steps = 3021, loss = 0.07048983126878738
In grad_steps = 3022, loss = 0.3145376443862915
In grad_steps = 3023, loss = 0.4427410364151001
In grad_steps = 3024, loss = 0.7320561408996582
In grad_steps = 3025, loss = 0.04531379044055939
In grad_steps = 3026, loss = 0.6845528483390808
In grad_steps = 3027, loss = 0.0327291302382946
In grad_steps = 3028, loss = 0.13536852598190308
In grad_steps = 3029, loss = 0.040566638112068176
In grad_steps = 3030, loss = 0.11877769976854324
In grad_steps = 3031, loss = 0.06924986839294434
In grad_steps = 3032, loss = 0.14037980139255524
In grad_steps = 3033, loss = 0.011683739721775055
In grad_steps = 3034, loss = 0.8478647470474243
In grad_steps = 3035, loss = 0.19785118103027344
In grad_steps = 3036, loss = 0.15268288552761078
In grad_steps = 3037, loss = 0.059698138386011124
In grad_steps = 3038, loss = 0.1568617820739746
In grad_steps = 3039, loss = 0.07766152918338776
In grad_steps = 3040, loss = 0.07742462307214737
In grad_steps = 3041, loss = 0.5725396275520325
In grad_steps = 3042, loss = 0.01744602620601654
In grad_steps = 3043, loss = 0.23627795279026031
In grad_steps = 3044, loss = 0.6757022738456726
In grad_steps = 3045, loss = 0.06180200353264809
In grad_steps = 3046, loss = 0.05346217751502991
In grad_steps = 3047, loss = 0.03180793672800064
In grad_steps = 3048, loss = 0.1917140930891037
In grad_steps = 3049, loss = 0.11734379827976227
In grad_steps = 3050, loss = 0.1806979775428772
In grad_steps = 3051, loss = 0.045141756534576416
In grad_steps = 3052, loss = 0.21454095840454102
In grad_steps = 3053, loss = 0.43851616978645325
In grad_steps = 3054, loss = 0.032965973019599915
In grad_steps = 3055, loss = 0.08343186974525452
In grad_steps = 3056, loss = 0.11025988310575485
In grad_steps = 3057, loss = 0.07067852467298508
In grad_steps = 3058, loss = 0.04845406115055084
In grad_steps = 3059, loss = 0.28172600269317627
In grad_steps = 3060, loss = 0.027167214080691338
In grad_steps = 3061, loss = 0.013346667401492596
In grad_steps = 3062, loss = 0.042382799088954926
In grad_steps = 3063, loss = 0.04442581534385681
In grad_steps = 3064, loss = 0.06550102680921555
In grad_steps = 3065, loss = 0.535502552986145
In grad_steps = 3066, loss = 0.42793911695480347
In grad_steps = 3067, loss = 1.466063141822815
In grad_steps = 3068, loss = 1.7596526145935059
In grad_steps = 3069, loss = 0.15485817193984985
In grad_steps = 3070, loss = 0.03131440281867981
In grad_steps = 3071, loss = 0.028746385127305984
In grad_steps = 3072, loss = 0.021621864289045334
In grad_steps = 3073, loss = 0.008546818979084492
In grad_steps = 3074, loss = 1.134698748588562
In grad_steps = 3075, loss = 0.19549715518951416
In grad_steps = 3076, loss = 0.2792288362979889
In grad_steps = 3077, loss = 0.04645593836903572
In grad_steps = 3078, loss = 0.07260127365589142
In grad_steps = 3079, loss = 0.03531057387590408
In grad_steps = 3080, loss = 0.17429475486278534
In grad_steps = 3081, loss = 0.03260910138487816
In grad_steps = 3082, loss = 0.01535705290734768
In grad_steps = 3083, loss = 0.15767526626586914
In grad_steps = 3084, loss = 0.13130152225494385
In grad_steps = 3085, loss = 0.34880223870277405
In grad_steps = 3086, loss = 0.7844941020011902
In grad_steps = 3087, loss = 0.06403344869613647
In grad_steps = 3088, loss = 0.3668636381626129
In grad_steps = 3089, loss = 0.06887820363044739
In grad_steps = 3090, loss = 0.0282988753169775
In grad_steps = 3091, loss = 0.6385055184364319
In grad_steps = 3092, loss = 0.09877806156873703
In grad_steps = 3093, loss = 0.14649638533592224
In grad_steps = 3094, loss = 0.3163920044898987
In grad_steps = 3095, loss = 0.053301915526390076
In grad_steps = 3096, loss = 0.045573338866233826
In grad_steps = 3097, loss = 0.3419554829597473
In grad_steps = 3098, loss = 0.6116744875907898
In grad_steps = 3099, loss = 0.09216048568487167
In grad_steps = 3100, loss = 0.38016051054000854
In grad_steps = 3101, loss = 0.08606347441673279
In grad_steps = 3102, loss = 0.7626947164535522
In grad_steps = 3103, loss = 0.2155328094959259
In grad_steps = 3104, loss = 0.08532778918743134
In grad_steps = 3105, loss = 0.07812019437551498
In grad_steps = 3106, loss = 0.04416186735033989
In grad_steps = 3107, loss = 0.257927805185318
In grad_steps = 3108, loss = 0.39584845304489136
In grad_steps = 3109, loss = 0.3232242166996002
In grad_steps = 3110, loss = 0.11770415306091309
In grad_steps = 3111, loss = 0.7169815301895142
In grad_steps = 3112, loss = 0.39863836765289307
In grad_steps = 3113, loss = 0.19971182942390442
In grad_steps = 3114, loss = 0.4769492447376251
In grad_steps = 3115, loss = 0.4742330312728882
In grad_steps = 3116, loss = 0.17644119262695312
In grad_steps = 3117, loss = 0.4234498143196106
In grad_steps = 3118, loss = 0.7049778699874878
In grad_steps = 3119, loss = 0.019227243959903717
In grad_steps = 3120, loss = 0.12878522276878357
In grad_steps = 3121, loss = 0.24258723855018616
In grad_steps = 3122, loss = 0.51643967628479
In grad_steps = 3123, loss = 0.10161589086055756
In grad_steps = 3124, loss = 0.08431686460971832
In grad_steps = 3125, loss = 0.17995163798332214
In grad_steps = 3126, loss = 0.6918200254440308
In grad_steps = 3127, loss = 0.024536263197660446
In grad_steps = 3128, loss = 0.08636587113142014
In grad_steps = 3129, loss = 0.13195189833641052
In grad_steps = 3130, loss = 0.03879493847489357
In grad_steps = 3131, loss = 0.24182765185832977
In grad_steps = 3132, loss = 0.22460076212882996
In grad_steps = 3133, loss = 0.08046165108680725
In grad_steps = 3134, loss = 0.13894158601760864
In grad_steps = 3135, loss = 0.03137075901031494
In grad_steps = 3136, loss = 0.6688675880432129
In grad_steps = 3137, loss = 0.7210230827331543
In grad_steps = 3138, loss = 0.009767760522663593
In grad_steps = 3139, loss = 0.019167182967066765
In grad_steps = 3140, loss = 0.18249116837978363
In grad_steps = 3141, loss = 0.36112451553344727
In grad_steps = 3142, loss = 0.06031937152147293
In grad_steps = 3143, loss = 0.06912793219089508
In grad_steps = 3144, loss = 0.02920518070459366
In grad_steps = 3145, loss = 0.0643073245882988
In grad_steps = 3146, loss = 0.073309026658535
In grad_steps = 3147, loss = 1.3388984203338623
In grad_steps = 3148, loss = 0.7514474391937256
In grad_steps = 3149, loss = 0.04619649425148964
In grad_steps = 3150, loss = 0.7664241790771484
In grad_steps = 3151, loss = 0.06331735104322433
In grad_steps = 3152, loss = 0.2011512964963913
In grad_steps = 3153, loss = 0.3095301687717438
In grad_steps = 3154, loss = 0.08338592946529388
In grad_steps = 3155, loss = 0.18975487351417542
In grad_steps = 3156, loss = 0.04185514897108078
In grad_steps = 3157, loss = 0.016746191307902336
In grad_steps = 3158, loss = 0.37854915857315063
In grad_steps = 3159, loss = 0.03444332629442215
In grad_steps = 3160, loss = 0.07961490750312805
In grad_steps = 3161, loss = 0.1927480250597
In grad_steps = 3162, loss = 0.02058817818760872
In grad_steps = 3163, loss = 0.9095523357391357
In grad_steps = 3164, loss = 0.03741319850087166
In grad_steps = 3165, loss = 0.1141040176153183
In grad_steps = 3166, loss = 0.3563404083251953
In grad_steps = 3167, loss = 0.07803980261087418
In grad_steps = 3168, loss = 0.06474882364273071
In grad_steps = 3169, loss = 0.7100183367729187
In grad_steps = 3170, loss = 0.03777838498353958
In grad_steps = 3171, loss = 0.4473898708820343
In grad_steps = 3172, loss = 0.07876884192228317
In grad_steps = 3173, loss = 0.01630726270377636
In grad_steps = 3174, loss = 0.33594533801078796
In grad_steps = 3175, loss = 0.12878111004829407
In grad_steps = 3176, loss = 0.25039416551589966
In grad_steps = 3177, loss = 0.027150608599185944
In grad_steps = 3178, loss = 0.02544126659631729
In grad_steps = 3179, loss = 0.38379061222076416
In grad_steps = 3180, loss = 0.22245429456233978
In grad_steps = 3181, loss = 0.5314431190490723
In grad_steps = 3182, loss = 0.5763766765594482
In grad_steps = 3183, loss = 0.4798465967178345
In grad_steps = 3184, loss = 0.047777462750673294
In grad_steps = 3185, loss = 1.0011589527130127
In grad_steps = 3186, loss = 0.6556885838508606
In grad_steps = 3187, loss = 0.0684472844004631
In grad_steps = 3188, loss = 1.2827271223068237
In grad_steps = 3189, loss = 0.2341831624507904
In grad_steps = 3190, loss = 0.07012966275215149
In grad_steps = 3191, loss = 0.3102066218852997
In grad_steps = 3192, loss = 0.1606927365064621
In grad_steps = 3193, loss = 0.11211265623569489
In grad_steps = 3194, loss = 0.4947851598262787
In grad_steps = 3195, loss = 0.06779032945632935
In grad_steps = 3196, loss = 0.8581676483154297
In grad_steps = 3197, loss = 0.5861451625823975
In grad_steps = 3198, loss = 0.06073128804564476
In grad_steps = 3199, loss = 0.5447068810462952
In grad_steps = 3200, loss = 0.09964859485626221
In grad_steps = 3201, loss = 1.5190908908843994
In grad_steps = 3202, loss = 0.4875395894050598
In grad_steps = 3203, loss = 0.5273244380950928
In grad_steps = 3204, loss = 0.6066830158233643
In grad_steps = 3205, loss = 0.5843449234962463
In grad_steps = 3206, loss = 0.1506337970495224
In grad_steps = 3207, loss = 0.13260602951049805
In grad_steps = 3208, loss = 0.6043804883956909
In grad_steps = 3209, loss = 0.23417171835899353
In grad_steps = 3210, loss = 0.3040313124656677
In grad_steps = 3211, loss = 0.5272378921508789
In grad_steps = 3212, loss = 0.21042495965957642
In grad_steps = 3213, loss = 0.22388111054897308
In grad_steps = 3214, loss = 0.3351151943206787
In grad_steps = 3215, loss = 0.08191900700330734
In grad_steps = 3216, loss = 0.16014845669269562
In grad_steps = 3217, loss = 0.2395397424697876
In grad_steps = 3218, loss = 0.21646596491336823
In grad_steps = 3219, loss = 0.12070143967866898
In grad_steps = 3220, loss = 0.15703251957893372
In grad_steps = 3221, loss = 0.4383499026298523
In grad_steps = 3222, loss = 0.21404874324798584
In grad_steps = 3223, loss = 0.38008564710617065
In grad_steps = 3224, loss = 0.10178501158952713
In grad_steps = 3225, loss = 0.2679199278354645
In grad_steps = 3226, loss = 0.16222229599952698
In grad_steps = 3227, loss = 0.1278221309185028
In grad_steps = 3228, loss = 0.10112787038087845
In grad_steps = 3229, loss = 0.11898553371429443
In grad_steps = 3230, loss = 0.022504258900880814
In grad_steps = 3231, loss = 0.042713992297649384
In grad_steps = 3232, loss = 0.3287658393383026
In grad_steps = 3233, loss = 0.07681499421596527
In grad_steps = 3234, loss = 0.6607612371444702
In grad_steps = 3235, loss = 0.14757108688354492
In grad_steps = 3236, loss = 0.021720457822084427
In grad_steps = 3237, loss = 0.008710918016731739
In grad_steps = 3238, loss = 0.03255697712302208
In grad_steps = 3239, loss = 0.20334847271442413
In grad_steps = 3240, loss = 0.013082105666399002
In grad_steps = 3241, loss = 0.020288214087486267
In grad_steps = 3242, loss = 0.92401123046875
In grad_steps = 3243, loss = 0.3493202328681946
In grad_steps = 3244, loss = 0.007519283797591925
In grad_steps = 3245, loss = 1.0143182277679443
In grad_steps = 3246, loss = 0.05478803813457489
In grad_steps = 3247, loss = 0.06047629192471504
In grad_steps = 3248, loss = 0.19706003367900848
In grad_steps = 3249, loss = 0.12627536058425903
In grad_steps = 3250, loss = 0.15781550109386444
In grad_steps = 3251, loss = 0.9927976131439209
In grad_steps = 3252, loss = 0.07680252939462662
In grad_steps = 3253, loss = 0.11888332664966583
In grad_steps = 3254, loss = 0.16247189044952393
In grad_steps = 3255, loss = 0.05979105830192566
In grad_steps = 3256, loss = 1.2788238525390625
In grad_steps = 3257, loss = 0.16227996349334717
In grad_steps = 3258, loss = 0.2399139404296875
In grad_steps = 3259, loss = 0.07292758673429489
In grad_steps = 3260, loss = 0.03494710847735405
In grad_steps = 3261, loss = 0.09711794555187225
In grad_steps = 3262, loss = 0.27200600504875183
In grad_steps = 3263, loss = 0.04423737898468971
In grad_steps = 3264, loss = 0.04115693271160126
In grad_steps = 3265, loss = 0.08584682643413544
In grad_steps = 3266, loss = 0.5911865830421448
In grad_steps = 3267, loss = 0.6484233736991882
In grad_steps = 3268, loss = 0.12959326803684235
In grad_steps = 3269, loss = 0.2124684751033783
In grad_steps = 3270, loss = 0.04314526170492172
In grad_steps = 3271, loss = 0.7638848423957825
In grad_steps = 3272, loss = 1.0190508365631104
In grad_steps = 3273, loss = 0.9375121593475342
In grad_steps = 3274, loss = 0.4053784906864166
In grad_steps = 3275, loss = 0.040377113968133926
In grad_steps = 3276, loss = 1.4184645414352417
In grad_steps = 3277, loss = 0.5459244251251221
In grad_steps = 3278, loss = 0.05724204704165459
In grad_steps = 3279, loss = 0.10916723310947418
In grad_steps = 3280, loss = 0.3219006657600403
In grad_steps = 3281, loss = 0.9509157538414001
In grad_steps = 3282, loss = 0.11444388329982758
In grad_steps = 3283, loss = 0.3800535202026367
In grad_steps = 3284, loss = 0.2056359350681305
In grad_steps = 3285, loss = 0.570724606513977
In grad_steps = 3286, loss = 0.48914504051208496
In grad_steps = 3287, loss = 0.1107977107167244
In grad_steps = 3288, loss = 0.13702887296676636
In grad_steps = 3289, loss = 0.21849879622459412
In grad_steps = 3290, loss = 0.5842654705047607
In grad_steps = 3291, loss = 0.20466400682926178
In grad_steps = 3292, loss = 0.31724652647972107
In grad_steps = 3293, loss = 0.24394947290420532
In grad_steps = 3294, loss = 0.122164785861969
In grad_steps = 3295, loss = 0.16908541321754456
In grad_steps = 3296, loss = 0.46615201234817505
In grad_steps = 3297, loss = 0.1062258780002594
In grad_steps = 3298, loss = 0.09274349361658096
In grad_steps = 3299, loss = 0.2147168517112732
In grad_steps = 3300, loss = 0.46977078914642334
In grad_steps = 3301, loss = 0.13184833526611328
In grad_steps = 3302, loss = 0.5730444192886353
In grad_steps = 3303, loss = 0.052720922976732254
In grad_steps = 3304, loss = 0.6685049533843994
In grad_steps = 3305, loss = 0.1874183863401413
In grad_steps = 3306, loss = 0.36190012097358704
In grad_steps = 3307, loss = 0.3530292809009552
In grad_steps = 3308, loss = 0.3548237085342407
In grad_steps = 3309, loss = 0.6127311587333679
In grad_steps = 3310, loss = 0.4284110963344574
In grad_steps = 3311, loss = 0.21854785084724426
In grad_steps = 3312, loss = 0.07521726191043854
In grad_steps = 3313, loss = 0.0711522102355957
In grad_steps = 3314, loss = 0.10475387424230576
In grad_steps = 3315, loss = 0.22143793106079102
In grad_steps = 3316, loss = 0.6823613047599792
In grad_steps = 3317, loss = 0.05123642832040787
In grad_steps = 3318, loss = 0.6479425430297852
In grad_steps = 3319, loss = 0.038818955421447754
In grad_steps = 3320, loss = 0.15016871690750122
In grad_steps = 3321, loss = 0.05494770035147667
In grad_steps = 3322, loss = 0.1180935651063919
In grad_steps = 3323, loss = 0.20188385248184204
In grad_steps = 3324, loss = 0.026626788079738617
In grad_steps = 3325, loss = 0.043031878769397736
In grad_steps = 3326, loss = 0.3116488456726074
In grad_steps = 3327, loss = 0.025746867060661316
In grad_steps = 3328, loss = 0.0829751044511795
In grad_steps = 3329, loss = 0.09080541878938675
In grad_steps = 3330, loss = 0.08097004890441895
In grad_steps = 3331, loss = 0.060200028121471405
In grad_steps = 3332, loss = 0.03562876582145691
In grad_steps = 3333, loss = 0.04781627655029297
In grad_steps = 3334, loss = 0.07022906094789505
In grad_steps = 3335, loss = 0.2381017953157425
In grad_steps = 3336, loss = 0.006275172811001539
In grad_steps = 3337, loss = 0.01208907924592495
In grad_steps = 3338, loss = 0.06382778286933899
In grad_steps = 3339, loss = 0.08575411885976791
In grad_steps = 3340, loss = 0.09347188472747803
In grad_steps = 3341, loss = 0.14216364920139313
In grad_steps = 3342, loss = 0.013611659407615662
In grad_steps = 3343, loss = 0.009259520098567009
In grad_steps = 3344, loss = 0.5379767417907715
In grad_steps = 3345, loss = 0.10659395903348923
In grad_steps = 3346, loss = 0.11486431211233139
In grad_steps = 3347, loss = 1.1857447624206543
In grad_steps = 3348, loss = 0.002959100529551506
In grad_steps = 3349, loss = 0.05044720694422722
In grad_steps = 3350, loss = 0.06415059417486191
In grad_steps = 3351, loss = 0.017032930627465248
In grad_steps = 3352, loss = 0.04496905952692032
In grad_steps = 3353, loss = 0.07251691073179245
In grad_steps = 3354, loss = 0.1154416874051094
In grad_steps = 3355, loss = 0.04320794716477394
In grad_steps = 3356, loss = 0.0352158322930336
In grad_steps = 3357, loss = 0.018558979034423828
In grad_steps = 3358, loss = 0.05232744291424751
In grad_steps = 3359, loss = 0.9664673805236816
In grad_steps = 3360, loss = 0.0275504719465971
In grad_steps = 3361, loss = 0.0089100431650877
In grad_steps = 3362, loss = 0.017980337142944336
In grad_steps = 3363, loss = 0.0033805868588387966
In grad_steps = 3364, loss = 0.09126536548137665
In grad_steps = 3365, loss = 0.5082097053527832
In grad_steps = 3366, loss = 0.06873185932636261
In grad_steps = 3367, loss = 0.08502814918756485
In grad_steps = 3368, loss = 0.04969394579529762
In grad_steps = 3369, loss = 0.005479347892105579
In grad_steps = 3370, loss = 0.3298671841621399
In grad_steps = 3371, loss = 0.5540712475776672
In grad_steps = 3372, loss = 0.6813478469848633
In grad_steps = 3373, loss = 0.713062047958374
In grad_steps = 3374, loss = 0.07732881605625153
In grad_steps = 3375, loss = 0.6335641145706177
In grad_steps = 3376, loss = 0.0553484745323658
In grad_steps = 3377, loss = 0.11680129915475845
In grad_steps = 3378, loss = 0.319242924451828
In grad_steps = 3379, loss = 0.2867015600204468
In grad_steps = 3380, loss = 0.22065721452236176
In grad_steps = 3381, loss = 0.02298077382147312
In grad_steps = 3382, loss = 0.29890668392181396
In grad_steps = 3383, loss = 0.6144850254058838
In grad_steps = 3384, loss = 0.4738136827945709
In grad_steps = 3385, loss = 0.011693557724356651
In grad_steps = 3386, loss = 0.10883074253797531
In grad_steps = 3387, loss = 0.034549131989479065
In grad_steps = 3388, loss = 0.11858903616666794
In grad_steps = 3389, loss = 0.017765721306204796
In grad_steps = 3390, loss = 0.0572049617767334
In grad_steps = 3391, loss = 0.07698220759630203
In grad_steps = 3392, loss = 0.12613320350646973
In grad_steps = 3393, loss = 0.5687925815582275
In grad_steps = 3394, loss = 0.004487246740609407
In grad_steps = 3395, loss = 0.06380221247673035
In grad_steps = 3396, loss = 0.07797184586524963
In grad_steps = 3397, loss = 0.9131158590316772
In grad_steps = 3398, loss = 0.17117229104042053
In grad_steps = 3399, loss = 0.10624754428863525
In grad_steps = 3400, loss = 0.011909699998795986
In grad_steps = 3401, loss = 0.05695626884698868
In grad_steps = 3402, loss = 0.13787499070167542
In grad_steps = 3403, loss = 1.5153744220733643
In grad_steps = 3404, loss = 0.03195830434560776
In grad_steps = 3405, loss = 0.9860671758651733
In grad_steps = 3406, loss = 0.016361061483621597
In grad_steps = 3407, loss = 0.19833099842071533
In grad_steps = 3408, loss = 0.7418164014816284
In grad_steps = 3409, loss = 0.12817682325839996
In grad_steps = 3410, loss = 0.03168431669473648
In grad_steps = 3411, loss = 0.3632636070251465
In grad_steps = 3412, loss = 0.11641395837068558
In grad_steps = 3413, loss = 0.12851214408874512
In grad_steps = 3414, loss = 0.13087832927703857
In grad_steps = 3415, loss = 0.027340279892086983
In grad_steps = 3416, loss = 0.8366671800613403
In grad_steps = 3417, loss = 0.0380430594086647
In grad_steps = 3418, loss = 0.15657922625541687
In grad_steps = 3419, loss = 0.12192879617214203
In grad_steps = 3420, loss = 0.10836222767829895
In grad_steps = 3421, loss = 0.04325045645236969
In grad_steps = 3422, loss = 0.2748602330684662
In grad_steps = 3423, loss = 0.5687087178230286
In grad_steps = 3424, loss = 0.16631916165351868
In grad_steps = 3425, loss = 0.09474064409732819
In grad_steps = 3426, loss = 0.718604326248169
In grad_steps = 3427, loss = 0.033594436943531036
In grad_steps = 3428, loss = 0.03067777119576931
In grad_steps = 3429, loss = 0.8022197484970093
In grad_steps = 3430, loss = 0.9007989764213562
In grad_steps = 3431, loss = 0.23004382848739624
In grad_steps = 3432, loss = 0.48501378297805786
In grad_steps = 3433, loss = 0.07791125029325485
In grad_steps = 3434, loss = 0.09456976503133774
In grad_steps = 3435, loss = 0.34046483039855957
In grad_steps = 3436, loss = 0.07679896801710129
In grad_steps = 3437, loss = 0.07333426177501678
In grad_steps = 3438, loss = 0.09267304092645645
In grad_steps = 3439, loss = 0.06621430814266205
In grad_steps = 3440, loss = 0.15222348272800446
In grad_steps = 3441, loss = 1.0907855033874512
In grad_steps = 3442, loss = 0.27531111240386963
In grad_steps = 3443, loss = 0.1153983473777771
In grad_steps = 3444, loss = 0.33539146184921265
In grad_steps = 3445, loss = 0.22282524406909943
In grad_steps = 3446, loss = 0.26056402921676636
In grad_steps = 3447, loss = 0.12008637934923172
In grad_steps = 3448, loss = 0.10081962496042252
In grad_steps = 3449, loss = 0.03956860303878784
In grad_steps = 3450, loss = 0.06638883799314499
In grad_steps = 3451, loss = 0.16150745749473572
In grad_steps = 3452, loss = 0.05418720096349716
In grad_steps = 3453, loss = 0.0763595849275589
In grad_steps = 3454, loss = 0.031108418479561806
In grad_steps = 3455, loss = 0.09813584387302399
In grad_steps = 3456, loss = 0.4690403342247009
In grad_steps = 3457, loss = 0.021643269807100296
In grad_steps = 3458, loss = 0.015301823616027832
In grad_steps = 3459, loss = 0.552614152431488
In grad_steps = 3460, loss = 0.15607783198356628
In grad_steps = 3461, loss = 0.09116418659687042
In grad_steps = 3462, loss = 0.23719142377376556
In grad_steps = 3463, loss = 0.028334077447652817
In grad_steps = 3464, loss = 0.09228023141622543
In grad_steps = 3465, loss = 0.46066373586654663
In grad_steps = 3466, loss = 0.14804869890213013
In grad_steps = 3467, loss = 0.5849777460098267
In grad_steps = 3468, loss = 0.016851050779223442
In grad_steps = 3469, loss = 0.027784937992691994
In grad_steps = 3470, loss = 0.30406829714775085
In grad_steps = 3471, loss = 0.061665128916502
In grad_steps = 3472, loss = 0.02161911316215992
In grad_steps = 3473, loss = 0.015802808105945587
In grad_steps = 3474, loss = 0.043904200196266174
In grad_steps = 3475, loss = 0.22513605654239655
In grad_steps = 3476, loss = 0.04530845582485199
In grad_steps = 3477, loss = 0.5013977289199829
In grad_steps = 3478, loss = 0.006797223351895809
In grad_steps = 3479, loss = 0.01244201697409153
In grad_steps = 3480, loss = 0.03360636904835701
In grad_steps = 3481, loss = 0.011647269129753113
In grad_steps = 3482, loss = 0.031656138598918915
In grad_steps = 3483, loss = 1.1416223049163818
In grad_steps = 3484, loss = 0.020507637411355972
In grad_steps = 3485, loss = 0.04888301342725754
In grad_steps = 3486, loss = 0.013122485019266605
In grad_steps = 3487, loss = 1.0277762413024902
In grad_steps = 3488, loss = 0.028009960427880287
In grad_steps = 3489, loss = 0.021662816405296326
In grad_steps = 3490, loss = 0.8908575177192688
In grad_steps = 3491, loss = 0.6927832961082458
In grad_steps = 3492, loss = 0.0167287178337574
In grad_steps = 3493, loss = 0.3766494393348694
In grad_steps = 3494, loss = 0.05330545827746391
In grad_steps = 3495, loss = 0.9296733140945435
In grad_steps = 3496, loss = 0.16654522716999054
In grad_steps = 3497, loss = 0.02367313951253891
In grad_steps = 3498, loss = 0.1297951340675354
In grad_steps = 3499, loss = 0.11767219007015228
In grad_steps = 3500, loss = 0.1245565265417099
In grad_steps = 3501, loss = 0.5093408823013306
In grad_steps = 3502, loss = 0.21016113460063934
In grad_steps = 3503, loss = 0.08651167154312134
In grad_steps = 3504, loss = 0.3548990786075592
In grad_steps = 3505, loss = 0.8195549845695496
In grad_steps = 3506, loss = 0.39622801542282104
In grad_steps = 3507, loss = 0.09821006655693054
In grad_steps = 3508, loss = 0.1662452667951584
In grad_steps = 3509, loss = 0.1300523728132248
In grad_steps = 3510, loss = 0.02937883697450161
In grad_steps = 3511, loss = 0.2509443163871765
In grad_steps = 3512, loss = 0.02739959955215454
In grad_steps = 3513, loss = 0.34540659189224243
In grad_steps = 3514, loss = 0.2566525340080261
In grad_steps = 3515, loss = 0.08257351815700531
In grad_steps = 3516, loss = 0.03517176955938339
In grad_steps = 3517, loss = 0.015371458604931831
In grad_steps = 3518, loss = 0.22507111728191376
In grad_steps = 3519, loss = 0.15898095071315765
In grad_steps = 3520, loss = 0.028025975450873375
In grad_steps = 3521, loss = 0.4679471254348755
In grad_steps = 3522, loss = 1.311420202255249
In grad_steps = 3523, loss = 0.6554291248321533
In grad_steps = 3524, loss = 0.06687168031930923
In grad_steps = 3525, loss = 0.01581566594541073
In grad_steps = 3526, loss = 0.07634637504816055
In grad_steps = 3527, loss = 0.3904520571231842
In grad_steps = 3528, loss = 0.2305532842874527
In grad_steps = 3529, loss = 0.6820505261421204
In grad_steps = 3530, loss = 0.13496772944927216
In grad_steps = 3531, loss = 0.042653799057006836
In grad_steps = 3532, loss = 0.0589292049407959
In grad_steps = 3533, loss = 0.06466826796531677
In grad_steps = 3534, loss = 0.7016880512237549
In grad_steps = 3535, loss = 0.39821481704711914
In grad_steps = 3536, loss = 0.026041995733976364
In grad_steps = 3537, loss = 0.09914453327655792
In grad_steps = 3538, loss = 0.5822492837905884
In grad_steps = 3539, loss = 0.12858420610427856
In grad_steps = 3540, loss = 0.3453880250453949
In grad_steps = 3541, loss = 0.042432501912117004
In grad_steps = 3542, loss = 0.17073024809360504
In grad_steps = 3543, loss = 0.381538063287735
In grad_steps = 3544, loss = 0.08580779284238815
In grad_steps = 3545, loss = 0.07557721436023712
In grad_steps = 3546, loss = 0.19041894376277924
In grad_steps = 3547, loss = 0.07293975353240967
In grad_steps = 3548, loss = 0.76200270652771
In grad_steps = 3549, loss = 0.22374625504016876
In grad_steps = 3550, loss = 0.047437720000743866
In grad_steps = 3551, loss = 0.016478782519698143
In grad_steps = 3552, loss = 0.2832711338996887
In grad_steps = 3553, loss = 0.30500251054763794
In grad_steps = 3554, loss = 0.3048499822616577
In grad_steps = 3555, loss = 0.6167913675308228
In grad_steps = 3556, loss = 0.39187857508659363
In grad_steps = 3557, loss = 0.052972786128520966
In grad_steps = 3558, loss = 0.01325485110282898
In grad_steps = 3559, loss = 0.7798665165901184
In grad_steps = 3560, loss = 0.14459370076656342
In grad_steps = 3561, loss = 0.09037293493747711
In grad_steps = 3562, loss = 1.011109709739685
In grad_steps = 3563, loss = 0.0358789786696434
In grad_steps = 3564, loss = 0.010547063313424587
In grad_steps = 3565, loss = 0.014877134934067726
In grad_steps = 3566, loss = 0.5539456605911255
In grad_steps = 3567, loss = 0.09507326036691666
In grad_steps = 3568, loss = 0.12811538577079773
In grad_steps = 3569, loss = 0.0888037160038948
In grad_steps = 3570, loss = 0.08341991156339645
In grad_steps = 3571, loss = 0.02649276703596115
In grad_steps = 3572, loss = 0.145923912525177
In grad_steps = 3573, loss = 0.24787050485610962
In grad_steps = 3574, loss = 0.3308931589126587
In grad_steps = 3575, loss = 0.03883757442235947
In grad_steps = 3576, loss = 0.5282004475593567
In grad_steps = 3577, loss = 0.011047056876122952
In grad_steps = 3578, loss = 0.10485734790563583
In grad_steps = 3579, loss = 0.22951814532279968
In grad_steps = 3580, loss = 0.14133912324905396
In grad_steps = 3581, loss = 0.05623990297317505
In grad_steps = 3582, loss = 0.0672011598944664
In grad_steps = 3583, loss = 0.18878990411758423
In grad_steps = 3584, loss = 0.47338220477104187
In grad_steps = 3585, loss = 0.41709256172180176
In grad_steps = 3586, loss = 0.025169139727950096
In grad_steps = 3587, loss = 0.27725908160209656
In grad_steps = 3588, loss = 0.6140875816345215
In grad_steps = 3589, loss = 0.03286189213395119
In grad_steps = 3590, loss = 0.1889987289905548
In grad_steps = 3591, loss = 0.3353879153728485
In grad_steps = 3592, loss = 0.6950064897537231
In grad_steps = 3593, loss = 1.3125262260437012
In grad_steps = 3594, loss = 0.6672951579093933
In grad_steps = 3595, loss = 0.01808331161737442
In grad_steps = 3596, loss = 1.9058811664581299
In grad_steps = 3597, loss = 0.12075608223676682
In grad_steps = 3598, loss = 0.048918791115283966
In grad_steps = 3599, loss = 0.7096772193908691
In grad_steps = 3600, loss = 0.16394434869289398
In grad_steps = 3601, loss = 0.5092870593070984
In grad_steps = 3602, loss = 0.1596001535654068
In grad_steps = 3603, loss = 0.24295195937156677
In grad_steps = 3604, loss = 0.13844409584999084
In grad_steps = 3605, loss = 0.1475779414176941
In grad_steps = 3606, loss = 0.23302139341831207
In grad_steps = 3607, loss = 0.13003769516944885
In grad_steps = 3608, loss = 0.08001735061407089
In grad_steps = 3609, loss = 0.3173244297504425
In grad_steps = 3610, loss = 0.03447715565562248
In grad_steps = 3611, loss = 0.41048070788383484
In grad_steps = 3612, loss = 0.12283942103385925
In grad_steps = 3613, loss = 0.1564648598432541
In grad_steps = 3614, loss = 0.13490034639835358
In grad_steps = 3615, loss = 0.06129743903875351
In grad_steps = 3616, loss = 0.05414601042866707
In grad_steps = 3617, loss = 0.42901504039764404
In grad_steps = 3618, loss = 0.08283328264951706
In grad_steps = 3619, loss = 0.523896336555481
In grad_steps = 3620, loss = 0.08809265494346619
In grad_steps = 3621, loss = 0.19329982995986938
In grad_steps = 3622, loss = 0.10676060616970062
In grad_steps = 3623, loss = 0.12184341251850128
In grad_steps = 3624, loss = 0.28869736194610596
In grad_steps = 3625, loss = 0.01244858093559742
In grad_steps = 3626, loss = 0.01891261525452137
In grad_steps = 3627, loss = 0.03177483007311821
In grad_steps = 3628, loss = 0.008819708600640297
In grad_steps = 3629, loss = 0.4110344350337982
In grad_steps = 3630, loss = 0.49297550320625305
In grad_steps = 3631, loss = 0.014378846623003483
In grad_steps = 3632, loss = 0.007187697570770979
In grad_steps = 3633, loss = 0.012098549865186214
In grad_steps = 3634, loss = 0.011754676699638367
In grad_steps = 3635, loss = 0.0542491190135479
In grad_steps = 3636, loss = 0.018625425174832344
In grad_steps = 3637, loss = 0.013397857546806335
In grad_steps = 3638, loss = 0.7112807035446167
In grad_steps = 3639, loss = 0.15810859203338623
In grad_steps = 3640, loss = 0.7180014252662659
In grad_steps = 3641, loss = 0.12381723523139954
In grad_steps = 3642, loss = 0.01769087463617325
In grad_steps = 3643, loss = 0.10058046877384186
In grad_steps = 3644, loss = 0.034084856510162354
In grad_steps = 3645, loss = 0.08343908935785294
In grad_steps = 3646, loss = 0.024391232058405876
In grad_steps = 3647, loss = 0.6234496235847473
In grad_steps = 3648, loss = 0.01453065499663353
In grad_steps = 3649, loss = 0.5545710921287537
In grad_steps = 3650, loss = 0.06976348906755447
In grad_steps = 3651, loss = 0.0484449677169323
In grad_steps = 3652, loss = 0.047953397035598755
In grad_steps = 3653, loss = 0.11446411162614822
In grad_steps = 3654, loss = 0.01377157587558031
In grad_steps = 3655, loss = 0.27366864681243896
In grad_steps = 3656, loss = 0.06066936254501343
In grad_steps = 3657, loss = 0.12346113473176956
In grad_steps = 3658, loss = 0.059569619596004486
In grad_steps = 3659, loss = 0.23378029465675354
In grad_steps = 3660, loss = 0.06272834539413452
In grad_steps = 3661, loss = 0.055431585758924484
In grad_steps = 3662, loss = 1.3245035409927368
In grad_steps = 3663, loss = 0.20513632893562317
In grad_steps = 3664, loss = 0.01458750944584608
In grad_steps = 3665, loss = 0.06697259843349457
In grad_steps = 3666, loss = 0.3494095206260681
In grad_steps = 3667, loss = 0.09141888469457626
In grad_steps = 3668, loss = 0.1003090962767601
In grad_steps = 3669, loss = 0.7272279262542725
In grad_steps = 3670, loss = 0.5812490582466125
In grad_steps = 3671, loss = 0.29160118103027344
In grad_steps = 3672, loss = 0.051338210701942444
In grad_steps = 3673, loss = 0.21811017394065857
In grad_steps = 3674, loss = 0.11566312611103058
In grad_steps = 3675, loss = 0.5280207395553589
In grad_steps = 3676, loss = 0.04348760098218918
In grad_steps = 3677, loss = 0.3845539391040802
In grad_steps = 3678, loss = 0.15687346458435059
In grad_steps = 3679, loss = 0.0583735853433609
In grad_steps = 3680, loss = 0.01896592229604721
In grad_steps = 3681, loss = 0.9334856271743774
In grad_steps = 3682, loss = 0.029217258095741272
In grad_steps = 3683, loss = 0.25335726141929626
In grad_steps = 3684, loss = 0.04221932217478752
In grad_steps = 3685, loss = 0.226412832736969
In grad_steps = 3686, loss = 0.050080668181180954
In grad_steps = 3687, loss = 0.059541210532188416
In grad_steps = 3688, loss = 0.05097450688481331
In grad_steps = 3689, loss = 0.021541843190789223
In grad_steps = 3690, loss = 0.456415593624115
In grad_steps = 3691, loss = 0.9713341593742371
In grad_steps = 3692, loss = 0.8468363881111145
In grad_steps = 3693, loss = 0.14585845172405243
In grad_steps = 3694, loss = 0.08705545961856842
In grad_steps = 3695, loss = 0.0868193656206131
In grad_steps = 3696, loss = 0.057372819632291794
In grad_steps = 3697, loss = 0.3606952130794525
In grad_steps = 3698, loss = 0.0915878638625145
In grad_steps = 3699, loss = 0.06011618673801422
In grad_steps = 3700, loss = 0.18197007477283478
In grad_steps = 3701, loss = 0.14882467687129974
In grad_steps = 3702, loss = 0.22893333435058594
In grad_steps = 3703, loss = 0.028593111783266068
In grad_steps = 3704, loss = 0.060432352125644684
In grad_steps = 3705, loss = 0.05774862319231033
In grad_steps = 3706, loss = 0.07794003188610077
In grad_steps = 3707, loss = 0.09939918667078018
In grad_steps = 3708, loss = 0.10108251124620438
In grad_steps = 3709, loss = 0.02599993534386158
In grad_steps = 3710, loss = 0.07637771964073181
In grad_steps = 3711, loss = 0.1220209077000618
In grad_steps = 3712, loss = 0.2224552035331726
In grad_steps = 3713, loss = 0.42512625455856323
In grad_steps = 3714, loss = 0.023578433319926262
In grad_steps = 3715, loss = 0.024831879884004593
In grad_steps = 3716, loss = 0.016574135050177574
In grad_steps = 3717, loss = 0.07703208178281784
In grad_steps = 3718, loss = 0.024469619616866112
In grad_steps = 3719, loss = 0.26294201612472534
In grad_steps = 3720, loss = 0.46479156613349915
In grad_steps = 3721, loss = 0.009551078081130981
In grad_steps = 3722, loss = 0.08466396480798721
In grad_steps = 3723, loss = 0.04443379491567612
In grad_steps = 3724, loss = 0.5343891978263855
In grad_steps = 3725, loss = 1.273368239402771
In grad_steps = 3726, loss = 0.0553688183426857
In grad_steps = 3727, loss = 0.00905236229300499
In grad_steps = 3728, loss = 0.09206189960241318
In grad_steps = 3729, loss = 0.011473363265395164
In grad_steps = 3730, loss = 0.9850325584411621
In grad_steps = 3731, loss = 0.09604540467262268
In grad_steps = 3732, loss = 0.8174068331718445
In grad_steps = 3733, loss = 0.06040705740451813
In grad_steps = 3734, loss = 0.018034107983112335
In grad_steps = 3735, loss = 0.8365298509597778
In grad_steps = 3736, loss = 0.053008005023002625
In grad_steps = 3737, loss = 0.011279504746198654
In grad_steps = 3738, loss = 0.12849928438663483
In grad_steps = 3739, loss = 0.0711810365319252
In grad_steps = 3740, loss = 0.2134723961353302
In grad_steps = 3741, loss = 0.2037719488143921
In grad_steps = 3742, loss = 0.016944922506809235
In grad_steps = 3743, loss = 0.02391764335334301
In grad_steps = 3744, loss = 0.05119744688272476
In grad_steps = 3745, loss = 0.024699542671442032
In grad_steps = 3746, loss = 0.1645093411207199
In grad_steps = 3747, loss = 0.03650156408548355
In grad_steps = 3748, loss = 0.13229641318321228
In grad_steps = 3749, loss = 0.18153923749923706
In grad_steps = 3750, loss = 1.1134889125823975
In grad_steps = 3751, loss = 0.13706685602664948
In grad_steps = 3752, loss = 0.6082648038864136
In grad_steps = 3753, loss = 0.044835127890110016
In grad_steps = 3754, loss = 0.44896066188812256
In grad_steps = 3755, loss = 0.036656711250543594
In grad_steps = 3756, loss = 0.11269387602806091
In grad_steps = 3757, loss = 0.10130549222230911
In grad_steps = 3758, loss = 0.028886493295431137
In grad_steps = 3759, loss = 0.20026056468486786
In grad_steps = 3760, loss = 0.07161471247673035
In grad_steps = 3761, loss = 1.0403499603271484
In grad_steps = 3762, loss = 0.141647607088089
In grad_steps = 3763, loss = 0.09466244280338287
In grad_steps = 3764, loss = 0.034059811383485794
In grad_steps = 3765, loss = 0.08838094025850296
In grad_steps = 3766, loss = 0.170476496219635
In grad_steps = 3767, loss = 0.018238622695207596
In grad_steps = 3768, loss = 0.031192226335406303
In grad_steps = 3769, loss = 0.08082224428653717
In grad_steps = 3770, loss = 0.21586641669273376
In grad_steps = 3771, loss = 0.026478702202439308
In grad_steps = 3772, loss = 0.04681961238384247
In grad_steps = 3773, loss = 0.11115652322769165
In grad_steps = 3774, loss = 0.4391382336616516
In grad_steps = 3775, loss = 0.5202747583389282
In grad_steps = 3776, loss = 0.012419648468494415
In grad_steps = 3777, loss = 0.16803687810897827
In grad_steps = 3778, loss = 0.1114824041724205
In grad_steps = 3779, loss = 0.025645814836025238
In grad_steps = 3780, loss = 0.03036656603217125
In grad_steps = 3781, loss = 0.007711072452366352
In grad_steps = 3782, loss = 0.8949583172798157
In grad_steps = 3783, loss = 1.1413863897323608
In grad_steps = 3784, loss = 0.10109832137823105
In grad_steps = 3785, loss = 0.0750368982553482
In grad_steps = 3786, loss = 0.1294591724872589
In grad_steps = 3787, loss = 0.1307765692472458
In grad_steps = 3788, loss = 0.025109468027949333
In grad_steps = 3789, loss = 1.156738042831421
In grad_steps = 3790, loss = 0.42114391922950745
In grad_steps = 3791, loss = 0.024717269465327263
In grad_steps = 3792, loss = 0.03676357492804527
In grad_steps = 3793, loss = 0.19497694075107574
In grad_steps = 3794, loss = 0.06201385706663132
In grad_steps = 3795, loss = 0.010416870936751366
In grad_steps = 3796, loss = 0.3835064768791199
In grad_steps = 3797, loss = 0.1612018495798111
In grad_steps = 3798, loss = 0.5951598882675171
In grad_steps = 3799, loss = 0.013877897523343563
In grad_steps = 3800, loss = 0.5110611915588379
In grad_steps = 3801, loss = 0.1721114069223404
In grad_steps = 3802, loss = 0.09299354255199432
In grad_steps = 3803, loss = 0.03677225112915039
In grad_steps = 3804, loss = 0.20836175978183746
In grad_steps = 3805, loss = 0.4175454378128052
In grad_steps = 3806, loss = 0.013970844447612762
In grad_steps = 3807, loss = 0.20162363350391388
In grad_steps = 3808, loss = 0.22623758018016815
In grad_steps = 3809, loss = 0.23409266769886017
In grad_steps = 3810, loss = 0.927280068397522
In grad_steps = 3811, loss = 0.14028412103652954
In grad_steps = 3812, loss = 0.05744919925928116
In grad_steps = 3813, loss = 0.012611163780093193
In grad_steps = 3814, loss = 0.26631805300712585
In grad_steps = 3815, loss = 0.17561858892440796
In grad_steps = 3816, loss = 0.5189172029495239
In grad_steps = 3817, loss = 0.056390032172203064
In grad_steps = 3818, loss = 1.176320195198059
In grad_steps = 3819, loss = 0.2951699495315552
In grad_steps = 3820, loss = 0.06586669385433197
In grad_steps = 3821, loss = 0.01635291799902916
In grad_steps = 3822, loss = 0.80064857006073
In grad_steps = 3823, loss = 0.019887492060661316
In grad_steps = 3824, loss = 0.061750661581754684
In grad_steps = 3825, loss = 0.40228724479675293
In grad_steps = 3826, loss = 0.03116271086037159
In grad_steps = 3827, loss = 0.5546591877937317
In grad_steps = 3828, loss = 0.4174390435218811
In grad_steps = 3829, loss = 0.46098124980926514
In grad_steps = 3830, loss = 0.048290833830833435
In grad_steps = 3831, loss = 0.7884566783905029
In grad_steps = 3832, loss = 0.15948763489723206
In grad_steps = 3833, loss = 0.11695651710033417
In grad_steps = 3834, loss = 0.1815703809261322
In grad_steps = 3835, loss = 0.04668712988495827
In grad_steps = 3836, loss = 0.10591011494398117
In grad_steps = 3837, loss = 0.06716731190681458
In grad_steps = 3838, loss = 0.02205406129360199
In grad_steps = 3839, loss = 0.23766537010669708
In grad_steps = 3840, loss = 0.27597659826278687
In grad_steps = 3841, loss = 0.09142113476991653
In grad_steps = 3842, loss = 0.15260767936706543
In grad_steps = 3843, loss = 0.5697427988052368
In grad_steps = 3844, loss = 0.02974526584148407
In grad_steps = 3845, loss = 0.14905759692192078
In grad_steps = 3846, loss = 0.11709083616733551
In grad_steps = 3847, loss = 0.2547568678855896
In grad_steps = 3848, loss = 0.04493027925491333
In grad_steps = 3849, loss = 0.3869582712650299
In grad_steps = 3850, loss = 0.29682546854019165
In grad_steps = 3851, loss = 0.06914839148521423
In grad_steps = 3852, loss = 0.01853880286216736
In grad_steps = 3853, loss = 0.01843159645795822
In grad_steps = 3854, loss = 0.3731653094291687
In grad_steps = 3855, loss = 0.015332835726439953
In grad_steps = 3856, loss = 0.527630627155304
In grad_steps = 3857, loss = 0.1391068696975708
In grad_steps = 3858, loss = 0.013406931422650814
In grad_steps = 3859, loss = 0.005525863263756037
In grad_steps = 3860, loss = 0.4474634528160095
In grad_steps = 3861, loss = 0.5975640416145325
In grad_steps = 3862, loss = 0.09354846924543381
In grad_steps = 3863, loss = 0.47678834199905396
In grad_steps = 3864, loss = 0.13742457330226898
In grad_steps = 3865, loss = 0.02160029113292694
In grad_steps = 3866, loss = 1.5954145193099976
In grad_steps = 3867, loss = 0.36900508403778076
In grad_steps = 3868, loss = 0.07997483760118484
In grad_steps = 3869, loss = 0.08619300276041031
In grad_steps = 3870, loss = 0.1086394190788269
In grad_steps = 3871, loss = 0.04160960018634796
In grad_steps = 3872, loss = 0.5225890278816223
In grad_steps = 3873, loss = 0.02413841336965561
In grad_steps = 3874, loss = 0.6501055955886841
In grad_steps = 3875, loss = 0.1866329312324524
In grad_steps = 3876, loss = 0.11547213047742844
In grad_steps = 3877, loss = 0.14438818395137787
In grad_steps = 3878, loss = 0.4011227786540985
In grad_steps = 3879, loss = 0.05175764486193657
In grad_steps = 3880, loss = 0.04384862631559372
In grad_steps = 3881, loss = 0.09637269377708435
In grad_steps = 3882, loss = 0.09822885692119598
In grad_steps = 3883, loss = 0.07272174954414368
In grad_steps = 3884, loss = 0.03155927732586861
In grad_steps = 3885, loss = 0.16911433637142181
In grad_steps = 3886, loss = 0.1361662745475769
In grad_steps = 3887, loss = 0.09114255756139755
In grad_steps = 3888, loss = 0.014660331420600414
In grad_steps = 3889, loss = 0.20034262537956238
In grad_steps = 3890, loss = 0.5164540410041809
In grad_steps = 3891, loss = 0.44827842712402344
In grad_steps = 3892, loss = 0.0158340185880661
In grad_steps = 3893, loss = 0.00598070677369833
In grad_steps = 3894, loss = 0.039810262620449066
In grad_steps = 3895, loss = 0.011943342164158821
In grad_steps = 3896, loss = 0.2837449908256531
In grad_steps = 3897, loss = 0.02270444482564926
In grad_steps = 3898, loss = 0.006990918889641762
In grad_steps = 3899, loss = 0.8392025232315063
In grad_steps = 3900, loss = 0.011148292571306229
In grad_steps = 3901, loss = 0.12470781058073044
In grad_steps = 3902, loss = 0.042322199791669846
In grad_steps = 3903, loss = 0.11489136517047882
In grad_steps = 3904, loss = 0.020905692130327225
In grad_steps = 3905, loss = 0.14101043343544006
In grad_steps = 3906, loss = 0.21541395783424377
In grad_steps = 3907, loss = 0.22229492664337158
In grad_steps = 3908, loss = 0.005663582123816013
In grad_steps = 3909, loss = 0.0034249266609549522
In grad_steps = 3910, loss = 1.1066582202911377
In grad_steps = 3911, loss = 0.38016432523727417
In grad_steps = 3912, loss = 1.2207821607589722
In grad_steps = 3913, loss = 0.590086042881012
In grad_steps = 3914, loss = 0.05958827584981918
In grad_steps = 3915, loss = 0.02280394732952118
In grad_steps = 3916, loss = 0.026003215461969376
In grad_steps = 3917, loss = 0.03238290175795555
In grad_steps = 3918, loss = 0.15377095341682434
In grad_steps = 3919, loss = 1.0739333629608154
In grad_steps = 3920, loss = 0.045962028205394745
In grad_steps = 3921, loss = 0.14898920059204102
In grad_steps = 3922, loss = 0.14395606517791748
In grad_steps = 3923, loss = 0.44182369112968445
In grad_steps = 3924, loss = 0.1677674502134323
In grad_steps = 3925, loss = 0.6458200812339783
In grad_steps = 3926, loss = 0.16820812225341797
In grad_steps = 3927, loss = 0.6019430756568909
In grad_steps = 3928, loss = 0.0800839364528656
In grad_steps = 3929, loss = 0.13231241703033447
In grad_steps = 3930, loss = 0.33558645844459534
In grad_steps = 3931, loss = 0.5156810879707336
In grad_steps = 3932, loss = 0.11309487372636795
In grad_steps = 3933, loss = 0.07107962667942047
In grad_steps = 3934, loss = 0.17776048183441162
In grad_steps = 3935, loss = 0.046862803399562836
In grad_steps = 3936, loss = 0.1664494127035141
In grad_steps = 3937, loss = 0.5019458532333374
In grad_steps = 3938, loss = 0.7829872369766235
In grad_steps = 3939, loss = 0.0702006071805954
In grad_steps = 3940, loss = 0.181893989443779
In grad_steps = 3941, loss = 0.0954757034778595
In grad_steps = 3942, loss = 0.03297654539346695
In grad_steps = 3943, loss = 0.17972952127456665
In grad_steps = 3944, loss = 0.03738211840391159
In grad_steps = 3945, loss = 0.4928496778011322
In grad_steps = 3946, loss = 0.059968575835227966
In grad_steps = 3947, loss = 0.05810881406068802
In grad_steps = 3948, loss = 0.15607582032680511
In grad_steps = 3949, loss = 0.24016886949539185
In grad_steps = 3950, loss = 0.12129775434732437
In grad_steps = 3951, loss = 0.0988675057888031
In grad_steps = 3952, loss = 0.25983959436416626
In grad_steps = 3953, loss = 0.36142757534980774
In grad_steps = 3954, loss = 0.14265076816082
In grad_steps = 3955, loss = 0.6896930932998657
In grad_steps = 3956, loss = 0.034620583057403564
In grad_steps = 3957, loss = 0.06778430938720703
In grad_steps = 3958, loss = 0.03242375701665878
In grad_steps = 3959, loss = 0.697679877281189
In grad_steps = 3960, loss = 0.0746854618191719
In grad_steps = 3961, loss = 0.03861820325255394
In grad_steps = 3962, loss = 0.09451478719711304
In grad_steps = 3963, loss = 0.03196500614285469
In grad_steps = 3964, loss = 0.05556833744049072
In grad_steps = 3965, loss = 0.6261522769927979
In grad_steps = 3966, loss = 0.10006588697433472
In grad_steps = 3967, loss = 0.08775856345891953
In grad_steps = 3968, loss = 0.006407212931662798
In grad_steps = 3969, loss = 0.3176502585411072
In grad_steps = 3970, loss = 0.00944726262241602
In grad_steps = 3971, loss = 0.37833333015441895
In grad_steps = 3972, loss = 0.26421207189559937
In grad_steps = 3973, loss = 0.007901892997324467
In grad_steps = 3974, loss = 0.4600336253643036
In grad_steps = 3975, loss = 0.15783363580703735
In grad_steps = 3976, loss = 0.3773927688598633
In grad_steps = 3977, loss = 0.13684700429439545
In grad_steps = 3978, loss = 0.14100824296474457
In grad_steps = 3979, loss = 0.009815487079322338
In grad_steps = 3980, loss = 0.02335153892636299
In grad_steps = 3981, loss = 0.01511321123689413
In grad_steps = 3982, loss = 0.0825568214058876
In grad_steps = 3983, loss = 1.3998730182647705
In grad_steps = 3984, loss = 0.060748305171728134
In grad_steps = 3985, loss = 0.09098773449659348
In grad_steps = 3986, loss = 0.006626678630709648
In grad_steps = 3987, loss = 0.02371523529291153
In grad_steps = 3988, loss = 0.027764903381466866
In grad_steps = 3989, loss = 0.6498168706893921
In grad_steps = 3990, loss = 0.004879975691437721
In grad_steps = 3991, loss = 0.05628299340605736
In grad_steps = 3992, loss = 1.3168281316757202
In grad_steps = 3993, loss = 0.04065123200416565
In grad_steps = 3994, loss = 0.01726873591542244
In grad_steps = 3995, loss = 0.8454347252845764
In grad_steps = 3996, loss = 0.009725519455969334
In grad_steps = 3997, loss = 0.7548025846481323
In grad_steps = 3998, loss = 0.03920533135533333
In grad_steps = 3999, loss = 0.11958417296409607
In grad_steps = 4000, loss = 0.18656226992607117
In grad_steps = 4001, loss = 0.04992549121379852
In grad_steps = 4002, loss = 0.02694111503660679
In grad_steps = 4003, loss = 0.456553190946579
In grad_steps = 4004, loss = 0.1817743182182312
In grad_steps = 4005, loss = 0.08652880787849426
In grad_steps = 4006, loss = 0.2455570548772812
In grad_steps = 4007, loss = 0.5227079391479492
In grad_steps = 4008, loss = 0.1923741102218628
In grad_steps = 4009, loss = 0.7455750703811646
In grad_steps = 4010, loss = 0.10382544994354248
In grad_steps = 4011, loss = 0.17605167627334595
In grad_steps = 4012, loss = 0.4131426513195038
In grad_steps = 4013, loss = 0.6002365946769714
In grad_steps = 4014, loss = 0.2737463712692261
In grad_steps = 4015, loss = 0.22589434683322906
In grad_steps = 4016, loss = 0.194374218583107
In grad_steps = 4017, loss = 0.062212005257606506
In grad_steps = 4018, loss = 0.26580333709716797
In grad_steps = 4019, loss = 0.037958044558763504
In grad_steps = 4020, loss = 0.154768168926239
In grad_steps = 4021, loss = 0.720951497554779
In grad_steps = 4022, loss = 0.21574628353118896
In grad_steps = 4023, loss = 0.042872823774814606
In grad_steps = 4024, loss = 0.15793123841285706
In grad_steps = 4025, loss = 0.48906823992729187
In grad_steps = 4026, loss = 0.056519072502851486
In grad_steps = 4027, loss = 0.02892596647143364
In grad_steps = 4028, loss = 0.607117772102356
In grad_steps = 4029, loss = 0.2461344599723816
In grad_steps = 4030, loss = 0.546538770198822
In grad_steps = 4031, loss = 0.06517055630683899
In grad_steps = 4032, loss = 0.9288254380226135
In grad_steps = 4033, loss = 0.17291289567947388
In grad_steps = 4034, loss = 0.42145973443984985
In grad_steps = 4035, loss = 0.1483290195465088
In grad_steps = 4036, loss = 0.12745758891105652
In grad_steps = 4037, loss = 0.2947930693626404
In grad_steps = 4038, loss = 0.06594154983758926
In grad_steps = 4039, loss = 0.5138478875160217
In grad_steps = 4040, loss = 0.2822785973548889
In grad_steps = 4041, loss = 0.0778062641620636
In grad_steps = 4042, loss = 0.11865845322608948
In grad_steps = 4043, loss = 0.06629235297441483
In grad_steps = 4044, loss = 0.08723084628582001
In grad_steps = 4045, loss = 0.1864631623029709
In grad_steps = 4046, loss = 0.24792340397834778
In grad_steps = 4047, loss = 0.22052040696144104
In grad_steps = 4048, loss = 0.11919679492712021
In grad_steps = 4049, loss = 0.27932316064834595
In grad_steps = 4050, loss = 0.06753705441951752
In grad_steps = 4051, loss = 0.027406102046370506
In grad_steps = 4052, loss = 0.18395927548408508
In grad_steps = 4053, loss = 0.047246988862752914
In grad_steps = 4054, loss = 0.613046407699585
In grad_steps = 4055, loss = 0.0064824349246919155
In grad_steps = 4056, loss = 0.24059812724590302
In grad_steps = 4057, loss = 0.2066081166267395
In grad_steps = 4058, loss = 0.08893293887376785
In grad_steps = 4059, loss = 0.1534147709608078
In grad_steps = 4060, loss = 0.06612338125705719
In grad_steps = 4061, loss = 0.1060028001666069
In grad_steps = 4062, loss = 0.027884909883141518
In grad_steps = 4063, loss = 0.04801267385482788
In grad_steps = 4064, loss = 0.010538875125348568
In grad_steps = 4065, loss = 0.00800370704382658
In grad_steps = 4066, loss = 2.0321204662323
In grad_steps = 4067, loss = 0.009896278381347656
In grad_steps = 4068, loss = 0.3180934190750122
In grad_steps = 4069, loss = 0.6849269270896912
In grad_steps = 4070, loss = 0.9215839505195618
In grad_steps = 4071, loss = 0.007535040378570557
In grad_steps = 4072, loss = 0.03296917676925659
In grad_steps = 4073, loss = 0.12376869469881058
In grad_steps = 4074, loss = 0.036903299391269684
In grad_steps = 4075, loss = 0.6944857239723206
In grad_steps = 4076, loss = 0.03125122934579849
In grad_steps = 4077, loss = 0.4704587161540985
In grad_steps = 4078, loss = 0.18151554465293884
In grad_steps = 4079, loss = 0.3664873242378235
In grad_steps = 4080, loss = 0.11190105974674225
In grad_steps = 4081, loss = 0.10102035850286484
In grad_steps = 4082, loss = 0.6750543117523193
In grad_steps = 4083, loss = 0.08038763701915741
In grad_steps = 4084, loss = 0.09345117956399918
In grad_steps = 4085, loss = 0.058797843754291534
In grad_steps = 4086, loss = 0.037630822509527206
In grad_steps = 4087, loss = 0.19825124740600586
In grad_steps = 4088, loss = 0.11778367310762405
In grad_steps = 4089, loss = 0.16887372732162476
In grad_steps = 4090, loss = 0.6794664859771729
In grad_steps = 4091, loss = 0.08404630422592163
In grad_steps = 4092, loss = 0.06485773622989655
In grad_steps = 4093, loss = 0.029824836179614067
In grad_steps = 4094, loss = 0.21384839713573456
In grad_steps = 4095, loss = 0.18685482442378998
In grad_steps = 4096, loss = 0.04881061986088753
In grad_steps = 4097, loss = 0.3754968047142029
In grad_steps = 4098, loss = 0.2016686201095581
In grad_steps = 4099, loss = 0.14864227175712585
In grad_steps = 4100, loss = 0.03448665514588356
In grad_steps = 4101, loss = 0.29582029581069946
In grad_steps = 4102, loss = 0.009180388413369656
In grad_steps = 4103, loss = 0.45906150341033936
In grad_steps = 4104, loss = 0.05217200145125389
In grad_steps = 4105, loss = 0.02641826495528221
In grad_steps = 4106, loss = 0.1610979586839676
In grad_steps = 4107, loss = 0.03631491959095001
In grad_steps = 4108, loss = 0.005658761598169804
In grad_steps = 4109, loss = 0.023746201768517494
In grad_steps = 4110, loss = 0.8297804594039917
In grad_steps = 4111, loss = 0.024073734879493713
In grad_steps = 4112, loss = 0.030984535813331604
In grad_steps = 4113, loss = 0.0893094539642334
In grad_steps = 4114, loss = 0.037185654044151306
In grad_steps = 4115, loss = 0.29319626092910767
In grad_steps = 4116, loss = 0.025255803018808365
In grad_steps = 4117, loss = 0.1311304271221161
In grad_steps = 4118, loss = 0.100000761449337
In grad_steps = 4119, loss = 0.0008524560835212469
In grad_steps = 4120, loss = 0.015532293356955051
In grad_steps = 4121, loss = 0.15863332152366638
In grad_steps = 4122, loss = 0.11749157309532166
In grad_steps = 4123, loss = 0.5476927161216736
In grad_steps = 4124, loss = 0.5619741082191467
In grad_steps = 4125, loss = 0.048083797097206116
In grad_steps = 4126, loss = 0.01807500049471855
In grad_steps = 4127, loss = 0.05075674504041672
In grad_steps = 4128, loss = 1.0388414859771729
In grad_steps = 4129, loss = 0.15419581532478333
In grad_steps = 4130, loss = 0.019754748791456223
In grad_steps = 4131, loss = 0.13376982510089874
In grad_steps = 4132, loss = 0.3710975646972656
In grad_steps = 4133, loss = 0.007999598048627377
In grad_steps = 4134, loss = 0.025293603539466858
In grad_steps = 4135, loss = 0.10418693721294403
In grad_steps = 4136, loss = 0.9174323081970215
In grad_steps = 4137, loss = 0.6463444828987122
In grad_steps = 4138, loss = 0.014295903965830803
In grad_steps = 4139, loss = 0.012282952666282654
In grad_steps = 4140, loss = 0.20402151346206665
In grad_steps = 4141, loss = 0.05978263169527054
In grad_steps = 4142, loss = 0.049293890595436096
In grad_steps = 4143, loss = 0.2159465104341507
In grad_steps = 4144, loss = 0.14272837340831757
In grad_steps = 4145, loss = 0.013013124465942383
In grad_steps = 4146, loss = 0.05834885314106941
In grad_steps = 4147, loss = 0.09630940109491348
In grad_steps = 4148, loss = 0.03219101205468178
In grad_steps = 4149, loss = 0.03506229445338249
In grad_steps = 4150, loss = 0.09661979973316193
In grad_steps = 4151, loss = 0.012440658174455166
In grad_steps = 4152, loss = 1.0884909629821777
In grad_steps = 4153, loss = 0.6999028325080872
In grad_steps = 4154, loss = 0.4760631322860718
In grad_steps = 4155, loss = 0.9394880533218384
In grad_steps = 4156, loss = 0.015302025713026524
In grad_steps = 4157, loss = 0.34007972478866577
In grad_steps = 4158, loss = 0.13139310479164124
In grad_steps = 4159, loss = 0.24268393218517303
In grad_steps = 4160, loss = 0.15595844388008118
In grad_steps = 4161, loss = 0.7854098081588745
In grad_steps = 4162, loss = 0.24651674926280975
In grad_steps = 4163, loss = 0.02018265798687935
In grad_steps = 4164, loss = 0.042462896555662155
In grad_steps = 4165, loss = 0.2239203006029129
In grad_steps = 4166, loss = 0.1399417221546173
In grad_steps = 4167, loss = 0.04697110876441002
In grad_steps = 4168, loss = 0.32919594645500183
In grad_steps = 4169, loss = 0.07019102573394775
In grad_steps = 4170, loss = 0.2025887370109558
In grad_steps = 4171, loss = 0.09710132330656052
In grad_steps = 4172, loss = 0.24169692397117615
In grad_steps = 4173, loss = 0.7521253228187561
In grad_steps = 4174, loss = 0.14797642827033997
In grad_steps = 4175, loss = 0.03592865169048309
In grad_steps = 4176, loss = 1.4672549962997437
In grad_steps = 4177, loss = 0.06965046375989914
In grad_steps = 4178, loss = 0.02276652120053768
In grad_steps = 4179, loss = 0.9039039015769958
In grad_steps = 4180, loss = 0.8013582825660706
In grad_steps = 4181, loss = 0.8165716528892517
In grad_steps = 4182, loss = 0.23863299190998077
In grad_steps = 4183, loss = 0.09085989743471146
In grad_steps = 4184, loss = 0.07497265189886093
In grad_steps = 4185, loss = 0.8655294179916382
In grad_steps = 4186, loss = 0.9597734212875366
In grad_steps = 4187, loss = 0.07538165152072906
In grad_steps = 4188, loss = 0.08004368096590042
In grad_steps = 4189, loss = 0.3935298025608063
In grad_steps = 4190, loss = 0.6239380836486816
In grad_steps = 4191, loss = 0.28555190563201904
In grad_steps = 4192, loss = 0.26141026616096497
In grad_steps = 4193, loss = 0.021169422194361687
In grad_steps = 4194, loss = 0.11032937467098236
In grad_steps = 4195, loss = 0.09971929341554642
In grad_steps = 4196, loss = 0.16015712916851044
In grad_steps = 4197, loss = 0.6940474510192871
In grad_steps = 4198, loss = 0.14834482967853546
In grad_steps = 4199, loss = 0.4067758321762085
In grad_steps = 4200, loss = 0.17562639713287354
In grad_steps = 4201, loss = 0.07002328336238861
In grad_steps = 4202, loss = 0.06905324012041092
In grad_steps = 4203, loss = 0.15519845485687256
In grad_steps = 4204, loss = 0.08237504959106445
In grad_steps = 4205, loss = 0.3136657476425171
In grad_steps = 4206, loss = 0.10603757947683334
In grad_steps = 4207, loss = 0.06654661893844604
In grad_steps = 4208, loss = 0.008349083364009857
In grad_steps = 4209, loss = 0.04766835272312164
In grad_steps = 4210, loss = 0.26960647106170654
In grad_steps = 4211, loss = 0.04715840891003609
In grad_steps = 4212, loss = 0.06940662860870361
In grad_steps = 4213, loss = 0.26974329352378845
In grad_steps = 4214, loss = 0.02200360968708992
In grad_steps = 4215, loss = 0.05785209313035011
In grad_steps = 4216, loss = 0.019112497568130493
In grad_steps = 4217, loss = 0.05121100693941116
In grad_steps = 4218, loss = 0.5103382468223572
In grad_steps = 4219, loss = 0.008669351227581501
In grad_steps = 4220, loss = 0.06047629192471504
In grad_steps = 4221, loss = 0.006458763964474201
In grad_steps = 4222, loss = 0.002906442154198885
In grad_steps = 4223, loss = 0.010886665433645248
In grad_steps = 4224, loss = 0.023899545893073082
In grad_steps = 4225, loss = 0.009693814441561699
In grad_steps = 4226, loss = 0.007574370130896568
In grad_steps = 4227, loss = 0.004557470791041851
In grad_steps = 4228, loss = 0.0109320729970932
In grad_steps = 4229, loss = 0.0021137979347258806
In grad_steps = 4230, loss = 1.2622891664505005
In grad_steps = 4231, loss = 0.05530117079615593
In grad_steps = 4232, loss = 0.307228684425354
In grad_steps = 4233, loss = 0.0038112234324216843
In grad_steps = 4234, loss = 0.03486574813723564
In grad_steps = 4235, loss = 0.17102712392807007
In grad_steps = 4236, loss = 0.22813352942466736
In grad_steps = 4237, loss = 0.008662888780236244
In grad_steps = 4238, loss = 0.0811447873711586
In grad_steps = 4239, loss = 0.11258651316165924
In grad_steps = 4240, loss = 0.004394246265292168
In grad_steps = 4241, loss = 0.0457601323723793
In grad_steps = 4242, loss = 0.008333444595336914
In grad_steps = 4243, loss = 0.10285860300064087
In grad_steps = 4244, loss = 0.0211882796138525
In grad_steps = 4245, loss = 0.2988044321537018
In grad_steps = 4246, loss = 0.004424131941050291
In grad_steps = 4247, loss = 0.004316907841712236
In grad_steps = 4248, loss = 0.017222648486495018
In grad_steps = 4249, loss = 0.027640895918011665
In grad_steps = 4250, loss = 0.00392572209239006
In grad_steps = 4251, loss = 0.0030237173195928335
In grad_steps = 4252, loss = 0.2031404823064804
In grad_steps = 4253, loss = 0.010081260465085506
In grad_steps = 4254, loss = 0.04983298107981682
In grad_steps = 4255, loss = 0.04304327070713043
In grad_steps = 4256, loss = 0.4143739640712738
In grad_steps = 4257, loss = 0.04842834919691086
In grad_steps = 4258, loss = 0.009803890250623226
In grad_steps = 4259, loss = 0.47238612174987793
In grad_steps = 4260, loss = 1.3449060916900635
In grad_steps = 4261, loss = 0.5697832107543945
In grad_steps = 4262, loss = 0.0783497542142868
In grad_steps = 4263, loss = 0.7659717798233032
In grad_steps = 4264, loss = 0.04338683560490608
In grad_steps = 4265, loss = 0.05642887204885483
In grad_steps = 4266, loss = 0.05720290169119835
In grad_steps = 4267, loss = 0.013333585113286972
In grad_steps = 4268, loss = 0.6350047588348389
In grad_steps = 4269, loss = 0.33611011505126953
In grad_steps = 4270, loss = 0.06325197219848633
In grad_steps = 4271, loss = 0.20372486114501953
In grad_steps = 4272, loss = 0.09186258167028427
In grad_steps = 4273, loss = 0.086710125207901
In grad_steps = 4274, loss = 0.1423576921224594
In grad_steps = 4275, loss = 0.04427496716380119
In grad_steps = 4276, loss = 0.049205973744392395
In grad_steps = 4277, loss = 0.35720476508140564
In grad_steps = 4278, loss = 0.2581918239593506
In grad_steps = 4279, loss = 0.19679801166057587
In grad_steps = 4280, loss = 0.08817192912101746
In grad_steps = 4281, loss = 0.9533668756484985
In grad_steps = 4282, loss = 0.25382310152053833
In grad_steps = 4283, loss = 0.24601107835769653
In grad_steps = 4284, loss = 1.1784502267837524
In grad_steps = 4285, loss = 0.2553270757198334
In grad_steps = 4286, loss = 0.3116537630558014
In grad_steps = 4287, loss = 0.10377676784992218
In grad_steps = 4288, loss = 0.04478849470615387
In grad_steps = 4289, loss = 0.036064375191926956
In grad_steps = 4290, loss = 0.043961070477962494
In grad_steps = 4291, loss = 0.14509862661361694
In grad_steps = 4292, loss = 0.5642547011375427
In grad_steps = 4293, loss = 0.05177794024348259
In grad_steps = 4294, loss = 0.050786785781383514
In grad_steps = 4295, loss = 0.09245482087135315
In grad_steps = 4296, loss = 1.5962097644805908
In grad_steps = 4297, loss = 0.0971735417842865
In grad_steps = 4298, loss = 0.15102355182170868
In grad_steps = 4299, loss = 0.31307223439216614
In grad_steps = 4300, loss = 0.1964236944913864
In grad_steps = 4301, loss = 0.1807793378829956
In grad_steps = 4302, loss = 0.16116029024124146
In grad_steps = 4303, loss = 0.12282291799783707
In grad_steps = 4304, loss = 0.2846740782260895
In grad_steps = 4305, loss = 0.13220065832138062
In grad_steps = 4306, loss = 0.18268045783042908
In grad_steps = 4307, loss = 0.3485323488712311
In grad_steps = 4308, loss = 0.015219652093946934
In grad_steps = 4309, loss = 0.034733034670352936
In grad_steps = 4310, loss = 0.23444832861423492
In grad_steps = 4311, loss = 0.11406463384628296
In grad_steps = 4312, loss = 0.029344266280531883
In grad_steps = 4313, loss = 0.05134915933012962
In grad_steps = 4314, loss = 0.10463801771402359
In grad_steps = 4315, loss = 0.08567370474338531
In grad_steps = 4316, loss = 0.6200743317604065
In grad_steps = 4317, loss = 0.3394593596458435
In grad_steps = 4318, loss = 0.15919218957424164
In grad_steps = 4319, loss = 1.332104206085205
In grad_steps = 4320, loss = 0.26440805196762085
In grad_steps = 4321, loss = 0.04301697760820389
In grad_steps = 4322, loss = 0.04140137508511543
In grad_steps = 4323, loss = 0.16360190510749817
In grad_steps = 4324, loss = 0.6550043225288391
In grad_steps = 4325, loss = 0.3357982635498047
In grad_steps = 4326, loss = 0.021701980382204056
In grad_steps = 4327, loss = 0.17080608010292053
In grad_steps = 4328, loss = 0.07253194600343704
In grad_steps = 4329, loss = 0.05155065655708313
In grad_steps = 4330, loss = 0.9283598065376282
In grad_steps = 4331, loss = 0.05267786234617233
In grad_steps = 4332, loss = 0.030921345576643944
In grad_steps = 4333, loss = 0.12280315905809402
In grad_steps = 4334, loss = 1.119583010673523
In grad_steps = 4335, loss = 0.02725757658481598
In grad_steps = 4336, loss = 0.020297346636652946
In grad_steps = 4337, loss = 0.05058598518371582
In grad_steps = 4338, loss = 0.09467458724975586
In grad_steps = 4339, loss = 0.357846736907959
In grad_steps = 4340, loss = 0.05077822506427765
In grad_steps = 4341, loss = 0.07961609959602356
In grad_steps = 4342, loss = 0.44359299540519714
In grad_steps = 4343, loss = 0.17749403417110443
In grad_steps = 4344, loss = 0.22342437505722046
In grad_steps = 4345, loss = 0.08007235825061798
In grad_steps = 4346, loss = 0.10532811284065247
In grad_steps = 4347, loss = 0.39603686332702637
In grad_steps = 4348, loss = 0.1479109823703766
In grad_steps = 4349, loss = 0.03288659080862999
In grad_steps = 4350, loss = 0.11198368668556213
In grad_steps = 4351, loss = 0.14134950935840607
In grad_steps = 4352, loss = 0.058478884398937225
In grad_steps = 4353, loss = 0.13460016250610352
In grad_steps = 4354, loss = 0.06067710369825363
In grad_steps = 4355, loss = 0.504690408706665
In grad_steps = 4356, loss = 0.06186327338218689
In grad_steps = 4357, loss = 0.014710895717144012
In grad_steps = 4358, loss = 0.008003725670278072
In grad_steps = 4359, loss = 0.020765412598848343
In grad_steps = 4360, loss = 0.5732423663139343
In grad_steps = 4361, loss = 0.015871120616793633
In grad_steps = 4362, loss = 0.5116749405860901
In grad_steps = 4363, loss = 0.11489303410053253
In grad_steps = 4364, loss = 0.06746677309274673
In grad_steps = 4365, loss = 0.12067808210849762
In grad_steps = 4366, loss = 0.017300346866250038
In grad_steps = 4367, loss = 0.061494503170251846
In grad_steps = 4368, loss = 0.003794223302975297
In grad_steps = 4369, loss = 0.008891697973012924
In grad_steps = 4370, loss = 0.19539107382297516
In grad_steps = 4371, loss = 0.025969551876187325
In grad_steps = 4372, loss = 0.21553157269954681
In grad_steps = 4373, loss = 0.14327608048915863
In grad_steps = 4374, loss = 0.02763211727142334
In grad_steps = 4375, loss = 0.8228555917739868
In grad_steps = 4376, loss = 0.09780337661504745
In grad_steps = 4377, loss = 0.039142731577157974
In grad_steps = 4378, loss = 0.02469385787844658
In grad_steps = 4379, loss = 0.003828075248748064
In grad_steps = 4380, loss = 0.04207536578178406
In grad_steps = 4381, loss = 0.021139463409781456
In grad_steps = 4382, loss = 0.035950712859630585
In grad_steps = 4383, loss = 0.02924996055662632
In grad_steps = 4384, loss = 0.012759098783135414
In grad_steps = 4385, loss = 0.013004995882511139
In grad_steps = 4386, loss = 0.008490857668220997
In grad_steps = 4387, loss = 0.009196678176522255
In grad_steps = 4388, loss = 0.06618105620145798
In grad_steps = 4389, loss = 0.013164416886866093
In grad_steps = 4390, loss = 0.06542646884918213
In grad_steps = 4391, loss = 0.3388848900794983
In grad_steps = 4392, loss = 0.21930789947509766
In grad_steps = 4393, loss = 0.5790742039680481
In grad_steps = 4394, loss = 0.019042208790779114
In grad_steps = 4395, loss = 0.04039822891354561
In grad_steps = 4396, loss = 0.018001055344939232
In grad_steps = 4397, loss = 0.029121555387973785
In grad_steps = 4398, loss = 0.11056774109601974
In grad_steps = 4399, loss = 0.27887430787086487
In grad_steps = 4400, loss = 0.003640983486548066
In grad_steps = 4401, loss = 0.017822686582803726
In grad_steps = 4402, loss = 1.1447856426239014
In grad_steps = 4403, loss = 0.05198509618639946
In grad_steps = 4404, loss = 0.0979439839720726
In grad_steps = 4405, loss = 0.007807855028659105
In grad_steps = 4406, loss = 1.3501914739608765
In grad_steps = 4407, loss = 0.12092316895723343
In grad_steps = 4408, loss = 0.24750055372714996
In grad_steps = 4409, loss = 0.010218785144388676
In grad_steps = 4410, loss = 0.016798149794340134
In grad_steps = 4411, loss = 0.42026159167289734
In grad_steps = 4412, loss = 0.04175958037376404
In grad_steps = 4413, loss = 0.574279248714447
In grad_steps = 4414, loss = 0.06429347395896912
In grad_steps = 4415, loss = 0.0187106691300869
In grad_steps = 4416, loss = 0.011433832347393036
In grad_steps = 4417, loss = 0.41797375679016113
In grad_steps = 4418, loss = 0.06086358428001404
In grad_steps = 4419, loss = 0.013785196468234062
In grad_steps = 4420, loss = 0.011721392162144184
In grad_steps = 4421, loss = 0.01995854452252388
In grad_steps = 4422, loss = 0.08607374131679535
In grad_steps = 4423, loss = 0.3905520737171173
In grad_steps = 4424, loss = 0.20388349890708923
In grad_steps = 4425, loss = 0.023230811581015587
In grad_steps = 4426, loss = 0.2533985674381256
In grad_steps = 4427, loss = 0.06552431732416153
In grad_steps = 4428, loss = 0.039776723831892014
In grad_steps = 4429, loss = 0.020169945433735847
In grad_steps = 4430, loss = 0.2929691672325134
In grad_steps = 4431, loss = 0.5808377861976624
In grad_steps = 4432, loss = 0.44610217213630676
In grad_steps = 4433, loss = 0.14547404646873474
In grad_steps = 4434, loss = 0.001891616964712739
In grad_steps = 4435, loss = 0.016231803223490715
In grad_steps = 4436, loss = 0.011823095381259918
In grad_steps = 4437, loss = 0.019617483019828796
In grad_steps = 4438, loss = 0.24307984113693237
In grad_steps = 4439, loss = 0.058274541050195694
In grad_steps = 4440, loss = 0.17390435934066772
In grad_steps = 4441, loss = 0.012092076241970062
In grad_steps = 4442, loss = 0.017504991963505745
In grad_steps = 4443, loss = 0.015047007240355015
In grad_steps = 4444, loss = 0.011283334344625473
In grad_steps = 4445, loss = 0.4556708037853241
In grad_steps = 4446, loss = 0.008168268948793411
In grad_steps = 4447, loss = 0.39533472061157227
In grad_steps = 4448, loss = 0.02132260799407959
In grad_steps = 4449, loss = 0.0017970073968172073
In grad_steps = 4450, loss = 0.3211168646812439
In grad_steps = 4451, loss = 0.02308361977338791
In grad_steps = 4452, loss = 0.012651382014155388
In grad_steps = 4453, loss = 0.07829272001981735
In grad_steps = 4454, loss = 1.274936556816101
In grad_steps = 4455, loss = 0.0035940834786742926
In grad_steps = 4456, loss = 0.003047473728656769
In grad_steps = 4457, loss = 0.7438768148422241
In grad_steps = 4458, loss = 0.6860198974609375
In grad_steps = 4459, loss = 0.10671621561050415
In grad_steps = 4460, loss = 0.40669819712638855
In grad_steps = 4461, loss = 0.2924654483795166
In grad_steps = 4462, loss = 0.06311323493719101
In grad_steps = 4463, loss = 0.17588037252426147
In grad_steps = 4464, loss = 0.01629137247800827
In grad_steps = 4465, loss = 0.5490466952323914
In grad_steps = 4466, loss = 0.30583664774894714
In grad_steps = 4467, loss = 0.33690422773361206
In grad_steps = 4468, loss = 0.048350486904382706
In grad_steps = 4469, loss = 0.13127394020557404
In grad_steps = 4470, loss = 0.20580893754959106
In grad_steps = 4471, loss = 0.16137385368347168
In grad_steps = 4472, loss = 0.06535764783620834
In grad_steps = 4473, loss = 0.016067247837781906
In grad_steps = 4474, loss = 0.11169802397489548
In grad_steps = 4475, loss = 0.0720687210559845
In grad_steps = 4476, loss = 0.258040189743042
In grad_steps = 4477, loss = 0.04993496835231781
In grad_steps = 4478, loss = 0.037040989845991135
In grad_steps = 4479, loss = 1.287255883216858
In grad_steps = 4480, loss = 0.04667817801237106
In grad_steps = 4481, loss = 0.014162600971758366
In grad_steps = 4482, loss = 0.22110974788665771
In grad_steps = 4483, loss = 0.3438592553138733
In grad_steps = 4484, loss = 0.10965797305107117
In grad_steps = 4485, loss = 0.05571502447128296
In grad_steps = 4486, loss = 0.006193217821419239
In grad_steps = 4487, loss = 0.027876731008291245
In grad_steps = 4488, loss = 0.5456350445747375
In grad_steps = 4489, loss = 0.5404943823814392
In grad_steps = 4490, loss = 0.23741157352924347
In grad_steps = 4491, loss = 0.4561234414577484
In grad_steps = 4492, loss = 0.02591509185731411
In grad_steps = 4493, loss = 0.02395690605044365
In grad_steps = 4494, loss = 0.06912239640951157
In grad_steps = 4495, loss = 0.02474241331219673
In grad_steps = 4496, loss = 0.08475732058286667
In grad_steps = 4497, loss = 0.8189208507537842
In grad_steps = 4498, loss = 0.11238911002874374
In grad_steps = 4499, loss = 0.05168130248785019
In grad_steps = 4500, loss = 0.14980554580688477
In grad_steps = 4501, loss = 0.2283337414264679
In grad_steps = 4502, loss = 0.017823824658989906
In grad_steps = 4503, loss = 0.12742862105369568
In grad_steps = 4504, loss = 0.0685458704829216
In grad_steps = 4505, loss = 0.17284302413463593
In grad_steps = 4506, loss = 0.08939708024263382
In grad_steps = 4507, loss = 0.3060208261013031
In grad_steps = 4508, loss = 0.274326890707016
In grad_steps = 4509, loss = 0.06928310543298721
In grad_steps = 4510, loss = 0.015772119164466858
In grad_steps = 4511, loss = 0.33672240376472473
In grad_steps = 4512, loss = 1.0033742189407349
In grad_steps = 4513, loss = 0.05065613240003586
In grad_steps = 4514, loss = 0.012489914894104004
In grad_steps = 4515, loss = 1.0706665515899658
In grad_steps = 4516, loss = 0.021669892594218254
In grad_steps = 4517, loss = 0.01848607137799263
In grad_steps = 4518, loss = 0.16654393076896667
In grad_steps = 4519, loss = 1.401207447052002
In grad_steps = 4520, loss = 0.02009458839893341
In grad_steps = 4521, loss = 0.060854051262140274
In grad_steps = 4522, loss = 0.35815632343292236
In grad_steps = 4523, loss = 0.00648174062371254
In grad_steps = 4524, loss = 0.7456128001213074
In grad_steps = 4525, loss = 0.8268994688987732
In grad_steps = 4526, loss = 0.01094502117484808
In grad_steps = 4527, loss = 0.01684577576816082
In grad_steps = 4528, loss = 0.14031124114990234
In grad_steps = 4529, loss = 0.25240302085876465
In grad_steps = 4530, loss = 0.3180444538593292
In grad_steps = 4531, loss = 0.07605651021003723
In grad_steps = 4532, loss = 0.5023778080940247
In grad_steps = 4533, loss = 0.06812672317028046
In grad_steps = 4534, loss = 0.04864374175667763
In grad_steps = 4535, loss = 0.4195576310157776
In grad_steps = 4536, loss = 0.297609806060791
In grad_steps = 4537, loss = 0.12223262339830399
In grad_steps = 4538, loss = 0.13731123507022858
In grad_steps = 4539, loss = 0.0853334292769432
In grad_steps = 4540, loss = 0.18846474587917328
In grad_steps = 4541, loss = 0.029880840331315994
In grad_steps = 4542, loss = 0.14967572689056396
In grad_steps = 4543, loss = 0.08339245617389679
In grad_steps = 4544, loss = 0.04668266326189041
In grad_steps = 4545, loss = 0.37399405241012573
In grad_steps = 4546, loss = 0.18405617773532867
In grad_steps = 4547, loss = 0.03894924744963646
In grad_steps = 4548, loss = 0.013438543304800987
In grad_steps = 4549, loss = 0.0699877217411995
In grad_steps = 4550, loss = 0.03393125906586647
In grad_steps = 4551, loss = 0.9241686463356018
In grad_steps = 4552, loss = 0.04035466909408569
In grad_steps = 4553, loss = 0.18312278389930725
In grad_steps = 4554, loss = 0.13904792070388794
In grad_steps = 4555, loss = 0.10354539752006531
In grad_steps = 4556, loss = 0.22259488701820374
In grad_steps = 4557, loss = 0.07174913585186005
In grad_steps = 4558, loss = 0.010881469585001469
In grad_steps = 4559, loss = 0.12199757993221283
In grad_steps = 4560, loss = 0.020507358014583588
In grad_steps = 4561, loss = 0.13266222178936005
In grad_steps = 4562, loss = 0.015199722722172737
In grad_steps = 4563, loss = 1.031432867050171
In grad_steps = 4564, loss = 0.059896353632211685
In grad_steps = 4565, loss = 0.6349656581878662
In grad_steps = 4566, loss = 1.0850441455841064
In grad_steps = 4567, loss = 0.029546942561864853
In grad_steps = 4568, loss = 0.20589926838874817
In grad_steps = 4569, loss = 0.08659740537405014
In grad_steps = 4570, loss = 1.0149027109146118
In grad_steps = 4571, loss = 0.13470014929771423
In grad_steps = 4572, loss = 0.32874366641044617
In grad_steps = 4573, loss = 0.2447790801525116
In grad_steps = 4574, loss = 0.06017524003982544
In grad_steps = 4575, loss = 0.34944573044776917
In grad_steps = 4576, loss = 0.3393426835536957
In grad_steps = 4577, loss = 0.020671963691711426
In grad_steps = 4578, loss = 0.059763845056295395
In grad_steps = 4579, loss = 0.1131758764386177
In grad_steps = 4580, loss = 0.6792546510696411
In grad_steps = 4581, loss = 0.0705442726612091
In grad_steps = 4582, loss = 0.11265829205513
In grad_steps = 4583, loss = 0.09374694526195526
In grad_steps = 4584, loss = 0.033551059663295746
In grad_steps = 4585, loss = 0.05451813340187073
In grad_steps = 4586, loss = 0.10578569769859314
In grad_steps = 4587, loss = 0.15539973974227905
In grad_steps = 4588, loss = 0.014571542851626873
In grad_steps = 4589, loss = 0.14598718285560608
In grad_steps = 4590, loss = 0.5830690264701843
In grad_steps = 4591, loss = 0.203765869140625
In grad_steps = 4592, loss = 0.026946058496832848
In grad_steps = 4593, loss = 0.1691591888666153
In grad_steps = 4594, loss = 0.04563964903354645
In grad_steps = 4595, loss = 0.6329483389854431
In grad_steps = 4596, loss = 0.25056859850883484
In grad_steps = 4597, loss = 0.040411826223134995
In grad_steps = 4598, loss = 1.067259669303894
In grad_steps = 4599, loss = 0.1310654878616333
In grad_steps = 4600, loss = 0.022183246910572052
In grad_steps = 4601, loss = 0.20676855742931366
In grad_steps = 4602, loss = 0.47816210985183716
In grad_steps = 4603, loss = 0.05201248824596405
In grad_steps = 4604, loss = 0.032702382653951645
In grad_steps = 4605, loss = 0.27511656284332275
In grad_steps = 4606, loss = 0.11571184545755386
In grad_steps = 4607, loss = 0.05416317656636238
In grad_steps = 4608, loss = 0.08362535387277603
In grad_steps = 4609, loss = 0.22998876869678497
In grad_steps = 4610, loss = 0.02251560240983963
In grad_steps = 4611, loss = 0.046680547297000885
In grad_steps = 4612, loss = 0.31805020570755005
In grad_steps = 4613, loss = 0.6929559707641602
In grad_steps = 4614, loss = 0.1208038404583931
In grad_steps = 4615, loss = 0.021626221016049385
In grad_steps = 4616, loss = 0.09392128884792328
In grad_steps = 4617, loss = 0.1351543664932251
In grad_steps = 4618, loss = 0.06402885168790817
In grad_steps = 4619, loss = 0.28691044449806213
In grad_steps = 4620, loss = 0.038156356662511826
In grad_steps = 4621, loss = 0.007550305221229792
In grad_steps = 4622, loss = 0.9576910138130188
In grad_steps = 4623, loss = 0.03511587530374527
In grad_steps = 4624, loss = 0.03092571720480919
In grad_steps = 4625, loss = 0.0299392007291317
In grad_steps = 4626, loss = 0.04143757000565529
In grad_steps = 4627, loss = 0.09976053982973099
In grad_steps = 4628, loss = 0.008645880967378616
In grad_steps = 4629, loss = 0.051075998693704605
In grad_steps = 4630, loss = 0.020375609397888184
In grad_steps = 4631, loss = 0.023704659193754196
In grad_steps = 4632, loss = 0.003989072982221842
In grad_steps = 4633, loss = 0.01781536638736725
In grad_steps = 4634, loss = 0.10096628963947296
In grad_steps = 4635, loss = 0.03257414326071739
In grad_steps = 4636, loss = 1.0353647470474243
In grad_steps = 4637, loss = 0.018796710297465324
In grad_steps = 4638, loss = 0.012640309520065784
In grad_steps = 4639, loss = 0.04086945205926895
In grad_steps = 4640, loss = 0.7689374685287476
In grad_steps = 4641, loss = 0.0784168392419815
In grad_steps = 4642, loss = 0.009197497740387917
In grad_steps = 4643, loss = 0.06038069352507591
In grad_steps = 4644, loss = 0.03618951141834259
In grad_steps = 4645, loss = 0.11193979531526566
In grad_steps = 4646, loss = 0.02377881109714508
In grad_steps = 4647, loss = 0.06322187185287476
In grad_steps = 4648, loss = 0.2471320927143097
In grad_steps = 4649, loss = 0.02157193049788475
In grad_steps = 4650, loss = 0.08949026465415955
In grad_steps = 4651, loss = 0.14192204177379608
In grad_steps = 4652, loss = 0.033826883882284164
In grad_steps = 4653, loss = 0.04560341686010361
In grad_steps = 4654, loss = 0.023201191797852516
In grad_steps = 4655, loss = 0.39138826727867126
In grad_steps = 4656, loss = 0.004707688465714455
In grad_steps = 4657, loss = 0.027133483439683914
In grad_steps = 4658, loss = 0.020304124802350998
In grad_steps = 4659, loss = 0.026290172711014748
In grad_steps = 4660, loss = 0.03514692187309265
In grad_steps = 4661, loss = 0.04419851303100586
In grad_steps = 4662, loss = 0.0011378176277503371
In grad_steps = 4663, loss = 1.2414878606796265
In grad_steps = 4664, loss = 0.02683544158935547
In grad_steps = 4665, loss = 0.5416615009307861
In grad_steps = 4666, loss = 0.9157420992851257
In grad_steps = 4667, loss = 0.11735327541828156
In grad_steps = 4668, loss = 0.04843227565288544
In grad_steps = 4669, loss = 0.05621197074651718
In grad_steps = 4670, loss = 0.028748637065291405
In grad_steps = 4671, loss = 0.6153603792190552
In grad_steps = 4672, loss = 1.5708295106887817
In grad_steps = 4673, loss = 0.104306161403656
In grad_steps = 4674, loss = 0.036373432725667953
In grad_steps = 4675, loss = 0.14181363582611084
In grad_steps = 4676, loss = 0.03565319627523422
In grad_steps = 4677, loss = 0.2820093333721161
In grad_steps = 4678, loss = 0.08766758441925049
In grad_steps = 4679, loss = 0.011789416894316673
In grad_steps = 4680, loss = 0.39376503229141235
In grad_steps = 4681, loss = 0.11172541975975037
In grad_steps = 4682, loss = 0.18660196661949158
In grad_steps = 4683, loss = 0.031920310109853745
In grad_steps = 4684, loss = 0.3711259663105011
In grad_steps = 4685, loss = 0.027406904846429825
In grad_steps = 4686, loss = 0.4180305302143097
In grad_steps = 4687, loss = 0.146543949842453
In grad_steps = 4688, loss = 0.21615815162658691
In grad_steps = 4689, loss = 0.22377043962478638
In grad_steps = 4690, loss = 0.032539863139390945
In grad_steps = 4691, loss = 0.02137298323214054
In grad_steps = 4692, loss = 0.1446305513381958
In grad_steps = 4693, loss = 0.11650919914245605
In grad_steps = 4694, loss = 0.01850251667201519
In grad_steps = 4695, loss = 0.7660198211669922
In grad_steps = 4696, loss = 0.009547533467411995
In grad_steps = 4697, loss = 1.1256186962127686
In grad_steps = 4698, loss = 0.08685039728879929
In grad_steps = 4699, loss = 0.04871118813753128
In grad_steps = 4700, loss = 0.026725761592388153
In grad_steps = 4701, loss = 0.11210669577121735
In grad_steps = 4702, loss = 0.06253179162740707
In grad_steps = 4703, loss = 0.16491404175758362
In grad_steps = 4704, loss = 0.5951492190361023
In grad_steps = 4705, loss = 0.001793087343685329
Elapsed time: 2675.671719789505 seconds for ensemble 4 with 2 epochs
LoRA instance 4 evaluation complete. Data saved to /pscratch/sd/t/tianle/lucid/other_source/SURP_2024/results/tmp_lora_ensemble/Llama3-8B-set-4/test_data_instance_4_seed_40373.npz.
lora instance i = 4 Successfully finished.
Final, Test average ensemble probabilities = 
[[0.30044788 0.69955206]
 [0.07409541 0.9259046 ]
 [0.6580914  0.34190854]
 ...
 [0.06306472 0.93693525]
 [0.92917    0.07082997]
 [0.972933   0.02706701]]
Accuracy: 0.8984
MCC: 0.8023
AUC: 0.9632
Confusion Matrix:
tensor([[1145,   49],
        [ 190,  969]])
Specificity: 0.9590
Precision (Macro): 0.9048
F1 Score (Macro): 0.8979
Expected Calibration Error (ECE): 0.0453
NLL loss: 0.2577
Ensemble evaluation complete.
