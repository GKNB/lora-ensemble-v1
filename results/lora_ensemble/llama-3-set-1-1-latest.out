Job is running on partition: gpu_ss11
Job is running under account: m2616_g
/pscratch/sd/t/tianle/conda/envs/lucid_surp_2024/lib/python3.11/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:10<00:32, 10.70s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:21<00:21, 10.69s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:31<00:10, 10.52s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:34<00:00,  7.28s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:34<00:00,  8.51s/it]
Llama3 has been loaded successfully.
Namespace(model_name='Llama3', config='/pscratch/sd/t/tianle/lucid/other_source/SURP_2024/src/config.json', repo_dir='/pscratch/sd/t/tianle/lucid/other_source/SURP_2024/', dataset='1.1', n_ensemble=5, seed=1, use_model_snapshot=False)
self.num_epochs = 12, self.batch_size = 8, self.max_length = 120
Model:  LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(128256, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)
          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)
          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)
          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)
)
Train dataset looks like:  Dataset({
    features: ['question', 'answer'],
    num_rows: 802
})
Test dataset looks like:  Dataset({
    features: ['question', 'answer'],
    num_rows: 90
})
Training lora instance 0
DEBUG: Total parameters: 8037076992
DEBUG: Trainable parameters: 6815744
Beginning epoch 1
In grad_steps = 0, loss = 1.6091442108154297
In grad_steps = 1, loss = 0.9505108594894409
In grad_steps = 2, loss = 0.7143058776855469
In grad_steps = 3, loss = 0.7483433485031128
In grad_steps = 4, loss = 1.1947731971740723
In grad_steps = 5, loss = 0.4234481155872345
In grad_steps = 6, loss = 0.8606953620910645
In grad_steps = 7, loss = 0.6864454746246338
In grad_steps = 8, loss = 0.8028537631034851
In grad_steps = 9, loss = 0.6530000567436218
In grad_steps = 10, loss = 0.7088554501533508
In grad_steps = 11, loss = 0.7100578546524048
In grad_steps = 12, loss = 0.6998376846313477
In grad_steps = 13, loss = 0.6383041143417358
In grad_steps = 14, loss = 0.7461022138595581
In grad_steps = 15, loss = 0.8343608379364014
In grad_steps = 16, loss = 0.699072003364563
In grad_steps = 17, loss = 0.6770431995391846
In grad_steps = 18, loss = 0.702297031879425
In grad_steps = 19, loss = 0.7180556654930115
In grad_steps = 20, loss = 0.6781305074691772
In grad_steps = 21, loss = 0.7097270488739014
In grad_steps = 22, loss = 0.6825861930847168
In grad_steps = 23, loss = 0.7079824209213257
In grad_steps = 24, loss = 0.7101917266845703
In grad_steps = 25, loss = 0.7477929592132568
In grad_steps = 26, loss = 0.6506673097610474
In grad_steps = 27, loss = 0.7311241626739502
In grad_steps = 28, loss = 0.6617582440376282
In grad_steps = 29, loss = 0.7004374265670776
In grad_steps = 30, loss = 0.6840956211090088
In grad_steps = 31, loss = 0.6968879699707031
In grad_steps = 32, loss = 0.7476065158843994
In grad_steps = 33, loss = 0.6851715445518494
In grad_steps = 34, loss = 0.7952466011047363
In grad_steps = 35, loss = 0.6903398633003235
In grad_steps = 36, loss = 0.7266228795051575
In grad_steps = 37, loss = 0.6733088493347168
In grad_steps = 38, loss = 0.7363433241844177
In grad_steps = 39, loss = 0.7010541558265686
In grad_steps = 40, loss = 0.6905612945556641
In grad_steps = 41, loss = 0.7046732306480408
In grad_steps = 42, loss = 0.6974695324897766
In grad_steps = 43, loss = 0.6846420764923096
In grad_steps = 44, loss = 0.6927847862243652
In grad_steps = 45, loss = 0.6840347647666931
In grad_steps = 46, loss = 0.7029131054878235
In grad_steps = 47, loss = 0.7274677753448486
In grad_steps = 48, loss = 0.6871544122695923
In grad_steps = 49, loss = 0.6750724911689758
In grad_steps = 50, loss = 0.6647840142250061
In grad_steps = 51, loss = 0.7211178541183472
In grad_steps = 52, loss = 0.7248793840408325
In grad_steps = 53, loss = 0.6598076820373535
In grad_steps = 54, loss = 0.5669521689414978
In grad_steps = 55, loss = 0.604131281375885
In grad_steps = 56, loss = 0.5690982341766357
In grad_steps = 57, loss = 0.5563927888870239
In grad_steps = 58, loss = 0.770983099937439
In grad_steps = 59, loss = 0.6940435767173767
In grad_steps = 60, loss = 0.8211270570755005
In grad_steps = 61, loss = 0.6720349192619324
In grad_steps = 62, loss = 0.9732082486152649
In grad_steps = 63, loss = 0.7584820985794067
In grad_steps = 64, loss = 0.5951685309410095
In grad_steps = 65, loss = 0.589717447757721
In grad_steps = 66, loss = 0.7926972508430481
In grad_steps = 67, loss = 0.66386878490448
In grad_steps = 68, loss = 0.73508220911026
In grad_steps = 69, loss = 0.6907678842544556
In grad_steps = 70, loss = 0.7362115383148193
In grad_steps = 71, loss = 0.7031644582748413
In grad_steps = 72, loss = 0.6930907368659973
In grad_steps = 73, loss = 0.7081165313720703
In grad_steps = 74, loss = 0.6694512963294983
In grad_steps = 75, loss = 0.6740550994873047
In grad_steps = 76, loss = 0.6576133966445923
In grad_steps = 77, loss = 1.0407910346984863
In grad_steps = 78, loss = 0.8201911449432373
In grad_steps = 79, loss = 0.6362139582633972
In grad_steps = 80, loss = 0.6901234984397888
In grad_steps = 81, loss = 0.6798475980758667
In grad_steps = 82, loss = 0.7262805104255676
In grad_steps = 83, loss = 0.699083149433136
In grad_steps = 84, loss = 0.7011756896972656
In grad_steps = 85, loss = 0.7053475975990295
In grad_steps = 86, loss = 0.6559908986091614
In grad_steps = 87, loss = 0.6920536756515503
In grad_steps = 88, loss = 0.7100545167922974
In grad_steps = 89, loss = 0.6792477369308472
In grad_steps = 90, loss = 0.7099568247795105
In grad_steps = 91, loss = 0.6602062582969666
In grad_steps = 92, loss = 0.679533064365387
In grad_steps = 93, loss = 0.7181829810142517
In grad_steps = 94, loss = 0.7336204051971436
In grad_steps = 95, loss = 0.6506196856498718
In grad_steps = 96, loss = 0.8512075543403625
In grad_steps = 97, loss = 0.7612028121948242
In grad_steps = 98, loss = 0.6473482847213745
In grad_steps = 99, loss = 0.6254456043243408
In grad_steps = 100, loss = 0.6875823736190796
Beginning epoch 2
In grad_steps = 101, loss = 0.6411621570587158
In grad_steps = 102, loss = 0.6461812853813171
In grad_steps = 103, loss = 0.6562339067459106
In grad_steps = 104, loss = 0.679675281047821
In grad_steps = 105, loss = 0.718104898929596
In grad_steps = 106, loss = 0.6152739524841309
In grad_steps = 107, loss = 0.655044436454773
In grad_steps = 108, loss = 0.639664351940155
In grad_steps = 109, loss = 0.7081755995750427
In grad_steps = 110, loss = 0.6532208919525146
In grad_steps = 111, loss = 0.7295923829078674
In grad_steps = 112, loss = 0.6585775017738342
In grad_steps = 113, loss = 0.7095780968666077
In grad_steps = 114, loss = 0.74348384141922
In grad_steps = 115, loss = 0.6991632580757141
In grad_steps = 116, loss = 0.6479486227035522
In grad_steps = 117, loss = 0.6500980854034424
In grad_steps = 118, loss = 0.6676801443099976
In grad_steps = 119, loss = 0.7130993008613586
In grad_steps = 120, loss = 0.6722519397735596
In grad_steps = 121, loss = 0.6072055101394653
In grad_steps = 122, loss = 0.697759747505188
In grad_steps = 123, loss = 0.7204517126083374
In grad_steps = 124, loss = 0.6991836428642273
In grad_steps = 125, loss = 0.6321555376052856
In grad_steps = 126, loss = 0.5693247318267822
In grad_steps = 127, loss = 0.7244880795478821
In grad_steps = 128, loss = 0.641942024230957
In grad_steps = 129, loss = 0.5869361758232117
In grad_steps = 130, loss = 0.6938563585281372
In grad_steps = 131, loss = 0.5652648210525513
In grad_steps = 132, loss = 0.6908692717552185
In grad_steps = 133, loss = 0.8600693345069885
In grad_steps = 134, loss = 0.6820403933525085
In grad_steps = 135, loss = 0.8070986866950989
In grad_steps = 136, loss = 0.6720165014266968
In grad_steps = 137, loss = 0.6593134999275208
In grad_steps = 138, loss = 0.6152135729789734
In grad_steps = 139, loss = 0.75929194688797
In grad_steps = 140, loss = 0.8540003895759583
In grad_steps = 141, loss = 0.6027851700782776
In grad_steps = 142, loss = 0.6834384202957153
In grad_steps = 143, loss = 0.6774505972862244
In grad_steps = 144, loss = 0.7036772966384888
In grad_steps = 145, loss = 0.6676000952720642
In grad_steps = 146, loss = 0.6353321075439453
In grad_steps = 147, loss = 0.6436871290206909
In grad_steps = 148, loss = 0.6304382681846619
In grad_steps = 149, loss = 0.6111096739768982
In grad_steps = 150, loss = 0.6561084389686584
In grad_steps = 151, loss = 0.5726156234741211
In grad_steps = 152, loss = 0.7512399554252625
In grad_steps = 153, loss = 0.7605668902397156
In grad_steps = 154, loss = 0.6155743598937988
In grad_steps = 155, loss = 0.5361821055412292
In grad_steps = 156, loss = 0.5848813056945801
In grad_steps = 157, loss = 0.49214908480644226
In grad_steps = 158, loss = 0.47166404128074646
In grad_steps = 159, loss = 0.7794681787490845
In grad_steps = 160, loss = 0.7242820262908936
In grad_steps = 161, loss = 0.7862049341201782
In grad_steps = 162, loss = 0.6272705793380737
In grad_steps = 163, loss = 0.8528799414634705
In grad_steps = 164, loss = 0.7061722278594971
In grad_steps = 165, loss = 0.5878546237945557
In grad_steps = 166, loss = 0.5765801072120667
In grad_steps = 167, loss = 0.6631693840026855
In grad_steps = 168, loss = 0.6305044293403625
In grad_steps = 169, loss = 0.6726233959197998
In grad_steps = 170, loss = 0.7098971605300903
In grad_steps = 171, loss = 0.6262943744659424
In grad_steps = 172, loss = 0.68117755651474
In grad_steps = 173, loss = 0.6650983691215515
In grad_steps = 174, loss = 0.6892560720443726
In grad_steps = 175, loss = 0.6622036695480347
In grad_steps = 176, loss = 0.659816563129425
In grad_steps = 177, loss = 0.6263547539710999
In grad_steps = 178, loss = 0.8948400616645813
In grad_steps = 179, loss = 0.7388152480125427
In grad_steps = 180, loss = 0.5509575605392456
In grad_steps = 181, loss = 0.5933451056480408
In grad_steps = 182, loss = 0.6399981379508972
In grad_steps = 183, loss = 0.7323704361915588
In grad_steps = 184, loss = 0.6918935775756836
In grad_steps = 185, loss = 0.7013700008392334
In grad_steps = 186, loss = 0.7149562835693359
In grad_steps = 187, loss = 0.5315899848937988
In grad_steps = 188, loss = 0.6551281809806824
In grad_steps = 189, loss = 0.6933301687240601
In grad_steps = 190, loss = 0.6943666338920593
In grad_steps = 191, loss = 0.6485501527786255
In grad_steps = 192, loss = 0.5853731632232666
In grad_steps = 193, loss = 0.6207273006439209
In grad_steps = 194, loss = 0.7430382370948792
In grad_steps = 195, loss = 0.8364299535751343
In grad_steps = 196, loss = 0.5785084366798401
In grad_steps = 197, loss = 0.6092699766159058
In grad_steps = 198, loss = 0.6906828880310059
In grad_steps = 199, loss = 0.49913889169692993
In grad_steps = 200, loss = 0.6104333996772766
In grad_steps = 201, loss = 0.6906646490097046
Beginning epoch 3
In grad_steps = 202, loss = 0.5967090129852295
In grad_steps = 203, loss = 0.5482427477836609
In grad_steps = 204, loss = 0.6677320003509521
In grad_steps = 205, loss = 0.7560489177703857
In grad_steps = 206, loss = 0.6462004780769348
In grad_steps = 207, loss = 0.5826096534729004
In grad_steps = 208, loss = 0.5821859240531921
In grad_steps = 209, loss = 0.5181341767311096
In grad_steps = 210, loss = 0.7489241361618042
In grad_steps = 211, loss = 0.5388416051864624
In grad_steps = 212, loss = 0.769295871257782
In grad_steps = 213, loss = 0.6105724573135376
In grad_steps = 214, loss = 0.6603177785873413
In grad_steps = 215, loss = 0.7706615924835205
In grad_steps = 216, loss = 0.6815543174743652
In grad_steps = 217, loss = 0.6264565587043762
In grad_steps = 218, loss = 0.6632847189903259
In grad_steps = 219, loss = 0.6234752535820007
In grad_steps = 220, loss = 0.6083494424819946
In grad_steps = 221, loss = 0.5900754928588867
In grad_steps = 222, loss = 0.5142672061920166
In grad_steps = 223, loss = 0.6754897832870483
In grad_steps = 224, loss = 0.7279640436172485
In grad_steps = 225, loss = 0.6941938400268555
In grad_steps = 226, loss = 0.5438595414161682
In grad_steps = 227, loss = 0.4154295027256012
In grad_steps = 228, loss = 0.6602211594581604
In grad_steps = 229, loss = 0.671280026435852
In grad_steps = 230, loss = 0.3978172540664673
In grad_steps = 231, loss = 0.5839424133300781
In grad_steps = 232, loss = 0.48173925280570984
In grad_steps = 233, loss = 0.67391037940979
In grad_steps = 234, loss = 0.640854001045227
In grad_steps = 235, loss = 0.5906086564064026
In grad_steps = 236, loss = 0.7102369666099548
In grad_steps = 237, loss = 0.41759318113327026
In grad_steps = 238, loss = 0.6230229139328003
In grad_steps = 239, loss = 0.5862318873405457
In grad_steps = 240, loss = 0.9469183683395386
In grad_steps = 241, loss = 0.90090012550354
In grad_steps = 242, loss = 0.49002426862716675
In grad_steps = 243, loss = 0.6425691843032837
In grad_steps = 244, loss = 0.6600775718688965
In grad_steps = 245, loss = 0.6980607509613037
In grad_steps = 246, loss = 0.6545246243476868
In grad_steps = 247, loss = 0.590389609336853
In grad_steps = 248, loss = 0.6437652111053467
In grad_steps = 249, loss = 0.5890110731124878
In grad_steps = 250, loss = 0.5804979801177979
In grad_steps = 251, loss = 0.6511361002922058
In grad_steps = 252, loss = 0.5491979718208313
In grad_steps = 253, loss = 0.749280571937561
In grad_steps = 254, loss = 0.7259622812271118
In grad_steps = 255, loss = 0.6061965227127075
In grad_steps = 256, loss = 0.5644514560699463
In grad_steps = 257, loss = 0.5871601104736328
In grad_steps = 258, loss = 0.4700137972831726
In grad_steps = 259, loss = 0.44964906573295593
In grad_steps = 260, loss = 0.7736834287643433
In grad_steps = 261, loss = 0.6801756620407104
In grad_steps = 262, loss = 0.7966527938842773
In grad_steps = 263, loss = 0.5827136039733887
In grad_steps = 264, loss = 0.7560611963272095
In grad_steps = 265, loss = 0.667523205280304
In grad_steps = 266, loss = 0.6139188408851624
In grad_steps = 267, loss = 0.6374415159225464
In grad_steps = 268, loss = 0.5638633966445923
In grad_steps = 269, loss = 0.638329267501831
In grad_steps = 270, loss = 0.6706084609031677
In grad_steps = 271, loss = 0.7458994388580322
In grad_steps = 272, loss = 0.6134620904922485
In grad_steps = 273, loss = 0.6504095792770386
In grad_steps = 274, loss = 0.6521894931793213
In grad_steps = 275, loss = 0.643036961555481
In grad_steps = 276, loss = 0.6838155388832092
In grad_steps = 277, loss = 0.6065015196800232
In grad_steps = 278, loss = 0.6138312816619873
In grad_steps = 279, loss = 0.7465663552284241
In grad_steps = 280, loss = 0.6718645691871643
In grad_steps = 281, loss = 0.5147014856338501
In grad_steps = 282, loss = 0.5026196837425232
In grad_steps = 283, loss = 0.6018785238265991
In grad_steps = 284, loss = 0.7249458432197571
In grad_steps = 285, loss = 0.6318656206130981
In grad_steps = 286, loss = 0.6318622827529907
In grad_steps = 287, loss = 0.5916672945022583
In grad_steps = 288, loss = 0.4142220616340637
In grad_steps = 289, loss = 0.5133799910545349
In grad_steps = 290, loss = 0.8290027976036072
In grad_steps = 291, loss = 0.6009023189544678
In grad_steps = 292, loss = 0.5930850505828857
In grad_steps = 293, loss = 0.4065900444984436
In grad_steps = 294, loss = 0.6155151128768921
In grad_steps = 295, loss = 1.001663088798523
In grad_steps = 296, loss = 0.8602535724639893
In grad_steps = 297, loss = 0.5113141536712646
In grad_steps = 298, loss = 0.4816953241825104
In grad_steps = 299, loss = 0.5687454342842102
In grad_steps = 300, loss = 0.5886367559432983
In grad_steps = 301, loss = 0.7169192433357239
In grad_steps = 302, loss = 0.6426588892936707
Beginning epoch 4
In grad_steps = 303, loss = 0.6544299125671387
In grad_steps = 304, loss = 0.5817348957061768
In grad_steps = 305, loss = 0.6454816460609436
In grad_steps = 306, loss = 0.6771175861358643
In grad_steps = 307, loss = 0.6400914192199707
In grad_steps = 308, loss = 0.5273426175117493
In grad_steps = 309, loss = 0.5834224820137024
In grad_steps = 310, loss = 0.49793779850006104
In grad_steps = 311, loss = 0.8473928570747375
In grad_steps = 312, loss = 0.4987221956253052
In grad_steps = 313, loss = 0.806056797504425
In grad_steps = 314, loss = 0.5665626525878906
In grad_steps = 315, loss = 0.6788786053657532
In grad_steps = 316, loss = 0.7236874103546143
In grad_steps = 317, loss = 0.6448875069618225
In grad_steps = 318, loss = 0.6619770526885986
In grad_steps = 319, loss = 0.6535486578941345
In grad_steps = 320, loss = 0.6037036180496216
In grad_steps = 321, loss = 0.4949280917644501
In grad_steps = 322, loss = 0.5329930782318115
In grad_steps = 323, loss = 0.4533071219921112
In grad_steps = 324, loss = 0.6035470962524414
In grad_steps = 325, loss = 0.7567334771156311
In grad_steps = 326, loss = 0.6584831476211548
In grad_steps = 327, loss = 0.4589765965938568
In grad_steps = 328, loss = 0.4076819121837616
In grad_steps = 329, loss = 0.4626765549182892
In grad_steps = 330, loss = 0.8845925331115723
In grad_steps = 331, loss = 0.24736136198043823
In grad_steps = 332, loss = 0.34168165922164917
In grad_steps = 333, loss = 0.32116955518722534
In grad_steps = 334, loss = 0.4739145040512085
In grad_steps = 335, loss = 0.20710328221321106
In grad_steps = 336, loss = 0.24377579987049103
In grad_steps = 337, loss = 0.4265581965446472
In grad_steps = 338, loss = 0.4874340891838074
In grad_steps = 339, loss = 0.18901373445987701
In grad_steps = 340, loss = 0.022184550762176514
In grad_steps = 341, loss = 0.4664852023124695
In grad_steps = 342, loss = 0.2821935713291168
In grad_steps = 343, loss = 0.8790872097015381
In grad_steps = 344, loss = 2.2790818214416504
In grad_steps = 345, loss = 0.8136106729507446
In grad_steps = 346, loss = 0.694900393486023
In grad_steps = 347, loss = 0.628311038017273
In grad_steps = 348, loss = 0.5273066163063049
In grad_steps = 349, loss = 0.6507568359375
In grad_steps = 350, loss = 0.6434837579727173
In grad_steps = 351, loss = 0.5993761420249939
In grad_steps = 352, loss = 0.602532148361206
In grad_steps = 353, loss = 0.5940869450569153
In grad_steps = 354, loss = 0.7636584043502808
In grad_steps = 355, loss = 0.7403159737586975
In grad_steps = 356, loss = 0.6183022856712341
In grad_steps = 357, loss = 0.5148280262947083
In grad_steps = 358, loss = 0.5281471014022827
In grad_steps = 359, loss = 0.47410333156585693
In grad_steps = 360, loss = 0.4917236566543579
In grad_steps = 361, loss = 0.907558262348175
In grad_steps = 362, loss = 0.7144570350646973
In grad_steps = 363, loss = 0.7551909685134888
In grad_steps = 364, loss = 0.5583416223526001
In grad_steps = 365, loss = 0.6579428911209106
In grad_steps = 366, loss = 0.6629674434661865
In grad_steps = 367, loss = 0.7554234862327576
In grad_steps = 368, loss = 0.7696526050567627
In grad_steps = 369, loss = 0.5760796666145325
In grad_steps = 370, loss = 0.6470292210578918
In grad_steps = 371, loss = 0.6388704776763916
In grad_steps = 372, loss = 0.6648333668708801
In grad_steps = 373, loss = 0.6117456555366516
In grad_steps = 374, loss = 0.6925203800201416
In grad_steps = 375, loss = 0.6625651717185974
In grad_steps = 376, loss = 0.6420958638191223
In grad_steps = 377, loss = 0.648734986782074
In grad_steps = 378, loss = 0.6165639758110046
In grad_steps = 379, loss = 0.5819966793060303
In grad_steps = 380, loss = 0.79533451795578
In grad_steps = 381, loss = 0.6800981760025024
In grad_steps = 382, loss = 0.4212973117828369
In grad_steps = 383, loss = 0.530722439289093
In grad_steps = 384, loss = 0.5822646617889404
In grad_steps = 385, loss = 0.670094907283783
In grad_steps = 386, loss = 0.6357556581497192
In grad_steps = 387, loss = 0.6609780788421631
In grad_steps = 388, loss = 0.53971928358078
In grad_steps = 389, loss = 0.4688144028186798
In grad_steps = 390, loss = 0.6439269781112671
In grad_steps = 391, loss = 0.5802987813949585
In grad_steps = 392, loss = 0.5576269030570984
In grad_steps = 393, loss = 0.38409361243247986
In grad_steps = 394, loss = 0.40840646624565125
In grad_steps = 395, loss = 0.40989258885383606
In grad_steps = 396, loss = 0.5497244596481323
In grad_steps = 397, loss = 0.4292832016944885
In grad_steps = 398, loss = 0.6415272355079651
In grad_steps = 399, loss = 0.6476773023605347
In grad_steps = 400, loss = 0.4196647107601166
In grad_steps = 401, loss = 0.9718891382217407
In grad_steps = 402, loss = 0.7138749957084656
In grad_steps = 403, loss = 0.3914221525192261
Beginning epoch 5
In grad_steps = 404, loss = 0.561045229434967
In grad_steps = 405, loss = 0.5589141845703125
In grad_steps = 406, loss = 0.657710611820221
In grad_steps = 407, loss = 0.5883791446685791
In grad_steps = 408, loss = 0.8274084329605103
In grad_steps = 409, loss = 0.5222694873809814
In grad_steps = 410, loss = 0.6051098704338074
In grad_steps = 411, loss = 0.48417389392852783
In grad_steps = 412, loss = 0.6849697232246399
In grad_steps = 413, loss = 0.5206221342086792
In grad_steps = 414, loss = 0.7071748971939087
In grad_steps = 415, loss = 0.5634090900421143
In grad_steps = 416, loss = 0.5937207341194153
In grad_steps = 417, loss = 0.7356137633323669
In grad_steps = 418, loss = 0.6546223759651184
In grad_steps = 419, loss = 0.714597225189209
In grad_steps = 420, loss = 0.6633905172348022
In grad_steps = 421, loss = 0.6158702373504639
In grad_steps = 422, loss = 0.5395340323448181
In grad_steps = 423, loss = 0.5673444271087646
In grad_steps = 424, loss = 0.4756603240966797
In grad_steps = 425, loss = 0.6255496144294739
In grad_steps = 426, loss = 0.6924483180046082
In grad_steps = 427, loss = 0.6185570955276489
In grad_steps = 428, loss = 0.4365829527378082
In grad_steps = 429, loss = 0.4820939600467682
In grad_steps = 430, loss = 0.5056637525558472
In grad_steps = 431, loss = 0.6472991704940796
In grad_steps = 432, loss = 0.3578781187534332
In grad_steps = 433, loss = 0.5726268291473389
In grad_steps = 434, loss = 0.3703494668006897
In grad_steps = 435, loss = 0.604820728302002
In grad_steps = 436, loss = 0.36268359422683716
In grad_steps = 437, loss = 0.31808745861053467
In grad_steps = 438, loss = 0.3059645891189575
In grad_steps = 439, loss = 0.30030766129493713
In grad_steps = 440, loss = 0.36964917182922363
In grad_steps = 441, loss = 0.2810787558555603
In grad_steps = 442, loss = 0.18640144169330597
In grad_steps = 443, loss = 0.6209355592727661
In grad_steps = 444, loss = 0.47502756118774414
In grad_steps = 445, loss = 0.3889387547969818
In grad_steps = 446, loss = 0.5103497505187988
In grad_steps = 447, loss = 1.018669843673706
In grad_steps = 448, loss = 0.49942731857299805
In grad_steps = 449, loss = 0.3255820572376251
In grad_steps = 450, loss = 0.4427390396595001
In grad_steps = 451, loss = 0.3010098934173584
In grad_steps = 452, loss = 0.36783191561698914
In grad_steps = 453, loss = 0.47466179728507996
In grad_steps = 454, loss = 0.539400041103363
In grad_steps = 455, loss = 0.573710024356842
In grad_steps = 456, loss = 0.5445559024810791
In grad_steps = 457, loss = 0.512945294380188
In grad_steps = 458, loss = 0.686911940574646
In grad_steps = 459, loss = 0.3530862629413605
In grad_steps = 460, loss = 0.4416440427303314
In grad_steps = 461, loss = 0.30933547019958496
In grad_steps = 462, loss = 1.050465703010559
In grad_steps = 463, loss = 0.7292321920394897
In grad_steps = 464, loss = 0.8593627214431763
In grad_steps = 465, loss = 0.4194584786891937
In grad_steps = 466, loss = 0.4763182997703552
In grad_steps = 467, loss = 0.5325456857681274
In grad_steps = 468, loss = 0.5466718077659607
In grad_steps = 469, loss = 0.5287715792655945
In grad_steps = 470, loss = 0.5113344788551331
In grad_steps = 471, loss = 0.7387825846672058
In grad_steps = 472, loss = 0.5771344900131226
In grad_steps = 473, loss = 0.7903837561607361
In grad_steps = 474, loss = 0.5353134274482727
In grad_steps = 475, loss = 0.556579053401947
In grad_steps = 476, loss = 0.5647627115249634
In grad_steps = 477, loss = 0.5122249126434326
In grad_steps = 478, loss = 0.6315030455589294
In grad_steps = 479, loss = 0.5317592620849609
In grad_steps = 480, loss = 0.5340117812156677
In grad_steps = 481, loss = 0.6816037893295288
In grad_steps = 482, loss = 0.5608629584312439
In grad_steps = 483, loss = 0.3574621379375458
In grad_steps = 484, loss = 0.44810062646865845
In grad_steps = 485, loss = 0.53553706407547
In grad_steps = 486, loss = 0.5218003392219543
In grad_steps = 487, loss = 0.5429614186286926
In grad_steps = 488, loss = 0.5629106760025024
In grad_steps = 489, loss = 0.3860861361026764
In grad_steps = 490, loss = 0.2804275155067444
In grad_steps = 491, loss = 0.4158719480037689
In grad_steps = 492, loss = 0.45334142446517944
In grad_steps = 493, loss = 0.2676372528076172
In grad_steps = 494, loss = 0.29356104135513306
In grad_steps = 495, loss = 0.19443464279174805
In grad_steps = 496, loss = 0.3665478527545929
In grad_steps = 497, loss = 0.5127063989639282
In grad_steps = 498, loss = 0.36498934030532837
In grad_steps = 499, loss = 0.38551685214042664
In grad_steps = 500, loss = 0.16757012903690338
In grad_steps = 501, loss = 0.1558394730091095
In grad_steps = 502, loss = 0.2308468073606491
In grad_steps = 503, loss = 0.5664029717445374
In grad_steps = 504, loss = 0.006275302730500698
Beginning epoch 6
In grad_steps = 505, loss = 1.129155158996582
In grad_steps = 506, loss = 0.2813391089439392
In grad_steps = 507, loss = 0.34412646293640137
In grad_steps = 508, loss = 0.560889482498169
In grad_steps = 509, loss = 0.9585596919059753
In grad_steps = 510, loss = 0.5343413949012756
In grad_steps = 511, loss = 0.7977023720741272
In grad_steps = 512, loss = 0.4856880009174347
In grad_steps = 513, loss = 0.6137599945068359
In grad_steps = 514, loss = 0.35012710094451904
In grad_steps = 515, loss = 0.6290147304534912
In grad_steps = 516, loss = 0.600775957107544
In grad_steps = 517, loss = 0.5678929090499878
In grad_steps = 518, loss = 0.6576439738273621
In grad_steps = 519, loss = 0.7069528102874756
In grad_steps = 520, loss = 0.7691499590873718
In grad_steps = 521, loss = 0.5777921676635742
In grad_steps = 522, loss = 0.5575662851333618
In grad_steps = 523, loss = 0.5656051635742188
In grad_steps = 524, loss = 0.5469614863395691
In grad_steps = 525, loss = 0.31026166677474976
In grad_steps = 526, loss = 0.5747120380401611
In grad_steps = 527, loss = 0.4590652585029602
In grad_steps = 528, loss = 0.516611635684967
In grad_steps = 529, loss = 0.2505452632904053
In grad_steps = 530, loss = 0.41419386863708496
In grad_steps = 531, loss = 0.32322755455970764
In grad_steps = 532, loss = 0.4426646828651428
In grad_steps = 533, loss = 0.3593159019947052
In grad_steps = 534, loss = 0.2674863338470459
In grad_steps = 535, loss = 0.10520851612091064
In grad_steps = 536, loss = 0.3093300461769104
In grad_steps = 537, loss = 0.607376217842102
In grad_steps = 538, loss = 0.12088322639465332
In grad_steps = 539, loss = 0.13374237716197968
In grad_steps = 540, loss = 0.3614851236343384
In grad_steps = 541, loss = 0.09744508564472198
In grad_steps = 542, loss = 0.1370273381471634
In grad_steps = 543, loss = 0.25817620754241943
In grad_steps = 544, loss = 0.040684401988983154
In grad_steps = 545, loss = 0.12676677107810974
In grad_steps = 546, loss = 0.05848423391580582
In grad_steps = 547, loss = 0.033591873943805695
In grad_steps = 548, loss = 0.6144768595695496
In grad_steps = 549, loss = 0.08961907029151917
In grad_steps = 550, loss = 0.030327139422297478
In grad_steps = 551, loss = 0.016776107251644135
In grad_steps = 552, loss = 0.531428873538971
In grad_steps = 553, loss = 0.009757479652762413
In grad_steps = 554, loss = 0.04215684160590172
In grad_steps = 555, loss = 0.39177656173706055
In grad_steps = 556, loss = 0.1832759827375412
In grad_steps = 557, loss = 0.7521822452545166
In grad_steps = 558, loss = 0.43592023849487305
In grad_steps = 559, loss = 0.4906725287437439
In grad_steps = 560, loss = 0.0672445297241211
In grad_steps = 561, loss = 0.3386128842830658
In grad_steps = 562, loss = 0.14585278928279877
In grad_steps = 563, loss = 0.684345006942749
In grad_steps = 564, loss = 0.4795979857444763
In grad_steps = 565, loss = 0.8010700345039368
In grad_steps = 566, loss = 0.295663058757782
In grad_steps = 567, loss = 0.1170792356133461
In grad_steps = 568, loss = 0.3029385507106781
In grad_steps = 569, loss = 0.596808910369873
In grad_steps = 570, loss = 0.38539770245552063
In grad_steps = 571, loss = 0.3276115357875824
In grad_steps = 572, loss = 0.6445485949516296
In grad_steps = 573, loss = 0.4644920825958252
In grad_steps = 574, loss = 0.6534377336502075
In grad_steps = 575, loss = 0.5258447527885437
In grad_steps = 576, loss = 0.6432709693908691
In grad_steps = 577, loss = 0.4585658013820648
In grad_steps = 578, loss = 0.38750022649765015
In grad_steps = 579, loss = 0.48608818650245667
In grad_steps = 580, loss = 0.34537070989608765
In grad_steps = 581, loss = 0.5747827291488647
In grad_steps = 582, loss = 0.4213997423648834
In grad_steps = 583, loss = 0.25053972005844116
In grad_steps = 584, loss = 0.25229179859161377
In grad_steps = 585, loss = 0.29838094115257263
In grad_steps = 586, loss = 0.4782429337501526
In grad_steps = 587, loss = 0.3951941132545471
In grad_steps = 588, loss = 0.501865804195404
In grad_steps = 589, loss = 0.4724823236465454
In grad_steps = 590, loss = 0.6561985611915588
In grad_steps = 591, loss = 0.08444851636886597
In grad_steps = 592, loss = 0.2752715051174164
In grad_steps = 593, loss = 0.41694962978363037
In grad_steps = 594, loss = 0.3192940354347229
In grad_steps = 595, loss = 0.42491012811660767
In grad_steps = 596, loss = 0.47514259815216064
In grad_steps = 597, loss = 0.12178432941436768
In grad_steps = 598, loss = 0.14929421246051788
In grad_steps = 599, loss = 0.06649568676948547
In grad_steps = 600, loss = 0.31487521529197693
In grad_steps = 601, loss = 0.411088764667511
In grad_steps = 602, loss = 0.9010116457939148
In grad_steps = 603, loss = 0.1828054040670395
In grad_steps = 604, loss = 0.5979564189910889
In grad_steps = 605, loss = 0.11204985529184341
Beginning epoch 7
In grad_steps = 606, loss = 0.3738999664783478
In grad_steps = 607, loss = 0.24935536086559296
In grad_steps = 608, loss = 0.45436233282089233
In grad_steps = 609, loss = 0.8718872666358948
In grad_steps = 610, loss = 0.26704147458076477
In grad_steps = 611, loss = 0.82558274269104
In grad_steps = 612, loss = 0.3968551754951477
In grad_steps = 613, loss = 0.47670233249664307
In grad_steps = 614, loss = 0.3057796359062195
In grad_steps = 615, loss = 0.5587281584739685
In grad_steps = 616, loss = 0.512411892414093
In grad_steps = 617, loss = 0.3352500796318054
In grad_steps = 618, loss = 0.5807581543922424
In grad_steps = 619, loss = 0.9331187009811401
In grad_steps = 620, loss = 0.435286283493042
In grad_steps = 621, loss = 0.6801705360412598
In grad_steps = 622, loss = 0.5929622650146484
In grad_steps = 623, loss = 0.4000313878059387
In grad_steps = 624, loss = 0.399291455745697
In grad_steps = 625, loss = 0.33247289061546326
In grad_steps = 626, loss = 0.39827701449394226
In grad_steps = 627, loss = 0.5000652074813843
In grad_steps = 628, loss = 0.5981413722038269
In grad_steps = 629, loss = 0.3664645552635193
In grad_steps = 630, loss = 0.16589607298374176
In grad_steps = 631, loss = 0.37032628059387207
In grad_steps = 632, loss = 0.2875370979309082
In grad_steps = 633, loss = 0.17337699234485626
In grad_steps = 634, loss = 0.18521332740783691
In grad_steps = 635, loss = 0.23539966344833374
In grad_steps = 636, loss = 0.1708083599805832
In grad_steps = 637, loss = 0.2750295400619507
In grad_steps = 638, loss = 0.10240984708070755
In grad_steps = 639, loss = 0.09591814875602722
In grad_steps = 640, loss = 0.043337248265743256
In grad_steps = 641, loss = 0.08894921094179153
In grad_steps = 642, loss = 0.12396173179149628
In grad_steps = 643, loss = 0.10165387392044067
In grad_steps = 644, loss = 0.08081703633069992
In grad_steps = 645, loss = 0.02157014235854149
In grad_steps = 646, loss = 0.08184397220611572
In grad_steps = 647, loss = 0.039286136627197266
In grad_steps = 648, loss = 0.03940823674201965
In grad_steps = 649, loss = 0.028620950877666473
In grad_steps = 650, loss = 0.0532180555164814
In grad_steps = 651, loss = 0.3331700265407562
In grad_steps = 652, loss = 0.0018706100527197123
In grad_steps = 653, loss = 0.002488589845597744
In grad_steps = 654, loss = 0.021291175857186317
In grad_steps = 655, loss = 0.2067752480506897
In grad_steps = 656, loss = 0.06036774441599846
In grad_steps = 657, loss = 0.03666296228766441
In grad_steps = 658, loss = 0.025478456169366837
In grad_steps = 659, loss = 0.07792965322732925
In grad_steps = 660, loss = 0.06332460045814514
In grad_steps = 661, loss = 0.4122218191623688
In grad_steps = 662, loss = 0.9610815048217773
In grad_steps = 663, loss = 0.0016361158341169357
In grad_steps = 664, loss = 0.12857331335544586
In grad_steps = 665, loss = 0.6833934783935547
In grad_steps = 666, loss = 1.0421520471572876
In grad_steps = 667, loss = 0.2658500373363495
In grad_steps = 668, loss = 0.11659249663352966
In grad_steps = 669, loss = 0.25172099471092224
In grad_steps = 670, loss = 0.3353845179080963
In grad_steps = 671, loss = 0.43098410964012146
In grad_steps = 672, loss = 0.20397105813026428
In grad_steps = 673, loss = 0.5462986826896667
In grad_steps = 674, loss = 0.48833486437797546
In grad_steps = 675, loss = 0.28748640418052673
In grad_steps = 676, loss = 0.5070410370826721
In grad_steps = 677, loss = 0.16693370044231415
In grad_steps = 678, loss = 0.2726185917854309
In grad_steps = 679, loss = 0.14793725311756134
In grad_steps = 680, loss = 0.4632761776447296
In grad_steps = 681, loss = 0.24583405256271362
In grad_steps = 682, loss = 0.36699986457824707
In grad_steps = 683, loss = 0.30241909623146057
In grad_steps = 684, loss = 0.23935043811798096
In grad_steps = 685, loss = 0.2334379404783249
In grad_steps = 686, loss = 0.2058906853199005
In grad_steps = 687, loss = 0.22680538892745972
In grad_steps = 688, loss = 0.24581940472126007
In grad_steps = 689, loss = 0.3885446786880493
In grad_steps = 690, loss = 0.13922756910324097
In grad_steps = 691, loss = 0.041450317949056625
In grad_steps = 692, loss = 0.03156504034996033
In grad_steps = 693, loss = 0.09605934470891953
In grad_steps = 694, loss = 0.09312311559915543
In grad_steps = 695, loss = 0.1189529076218605
In grad_steps = 696, loss = 0.30922797322273254
In grad_steps = 697, loss = 0.26726415753364563
In grad_steps = 698, loss = 0.040511876344680786
In grad_steps = 699, loss = 0.09319065511226654
In grad_steps = 700, loss = 0.041498951613903046
In grad_steps = 701, loss = 0.0057432944886386395
In grad_steps = 702, loss = 0.015189455822110176
In grad_steps = 703, loss = 0.01184211578220129
In grad_steps = 704, loss = 0.0008176409173756838
In grad_steps = 705, loss = 0.28095054626464844
In grad_steps = 706, loss = 0.00018362306582275778
Beginning epoch 8
In grad_steps = 707, loss = 0.08378858864307404
In grad_steps = 708, loss = 0.0007661340641789138
In grad_steps = 709, loss = 0.34683188796043396
In grad_steps = 710, loss = 0.649308443069458
In grad_steps = 711, loss = 0.02206854335963726
In grad_steps = 712, loss = 1.1573494672775269
In grad_steps = 713, loss = 0.3180074393749237
In grad_steps = 714, loss = 0.3642368018627167
In grad_steps = 715, loss = 0.1906755417585373
In grad_steps = 716, loss = 0.47963786125183105
In grad_steps = 717, loss = 0.45762646198272705
In grad_steps = 718, loss = 0.3151581287384033
In grad_steps = 719, loss = 0.5910571217536926
In grad_steps = 720, loss = 0.7526512145996094
In grad_steps = 721, loss = 0.3821393847465515
In grad_steps = 722, loss = 0.46138107776641846
In grad_steps = 723, loss = 0.49325501918792725
In grad_steps = 724, loss = 0.4277880787849426
In grad_steps = 725, loss = 0.30696719884872437
In grad_steps = 726, loss = 0.2732606828212738
In grad_steps = 727, loss = 0.3121713399887085
In grad_steps = 728, loss = 0.42540013790130615
In grad_steps = 729, loss = 0.3826427161693573
In grad_steps = 730, loss = 0.39041557908058167
In grad_steps = 731, loss = 0.11953409016132355
In grad_steps = 732, loss = 0.36640140414237976
In grad_steps = 733, loss = 0.10435754805803299
In grad_steps = 734, loss = 0.26795288920402527
In grad_steps = 735, loss = 0.12126889079809189
In grad_steps = 736, loss = 0.1426895260810852
In grad_steps = 737, loss = 0.396629273891449
In grad_steps = 738, loss = 0.8236125111579895
In grad_steps = 739, loss = 0.07875915616750717
In grad_steps = 740, loss = 0.05773106962442398
In grad_steps = 741, loss = 0.6814844012260437
In grad_steps = 742, loss = 0.1199672520160675
In grad_steps = 743, loss = 0.30332183837890625
In grad_steps = 744, loss = 0.13190652430057526
In grad_steps = 745, loss = 0.15546558797359467
In grad_steps = 746, loss = 0.07461141049861908
In grad_steps = 747, loss = 0.15605925023555756
In grad_steps = 748, loss = 0.2215726226568222
In grad_steps = 749, loss = 0.07100224494934082
In grad_steps = 750, loss = 0.09980309754610062
In grad_steps = 751, loss = 0.1329071968793869
In grad_steps = 752, loss = 0.24202269315719604
In grad_steps = 753, loss = 0.030599184334278107
In grad_steps = 754, loss = 0.33490535616874695
In grad_steps = 755, loss = 0.09034506976604462
In grad_steps = 756, loss = 0.135803684592247
In grad_steps = 757, loss = 0.013838838785886765
In grad_steps = 758, loss = 0.051860835403203964
In grad_steps = 759, loss = 0.4000142812728882
In grad_steps = 760, loss = 0.11117123812437057
In grad_steps = 761, loss = 0.15127240121364594
In grad_steps = 762, loss = 0.01129329577088356
In grad_steps = 763, loss = 0.2641265094280243
In grad_steps = 764, loss = 0.002998491981998086
In grad_steps = 765, loss = 0.30036845803260803
In grad_steps = 766, loss = 1.3384442329406738
In grad_steps = 767, loss = 0.4591083824634552
In grad_steps = 768, loss = 0.05108752101659775
In grad_steps = 769, loss = 0.02918194606900215
In grad_steps = 770, loss = 0.04071693494915962
In grad_steps = 771, loss = 0.5846173167228699
In grad_steps = 772, loss = 0.30236491560935974
In grad_steps = 773, loss = 0.4011003077030182
In grad_steps = 774, loss = 0.5865678191184998
In grad_steps = 775, loss = 0.25571295619010925
In grad_steps = 776, loss = 0.24356675148010254
In grad_steps = 777, loss = 0.9488265514373779
In grad_steps = 778, loss = 0.38335829973220825
In grad_steps = 779, loss = 0.24330629408359528
In grad_steps = 780, loss = 0.2188928872346878
In grad_steps = 781, loss = 0.7434834837913513
In grad_steps = 782, loss = 0.32116734981536865
In grad_steps = 783, loss = 0.3517771363258362
In grad_steps = 784, loss = 0.5761752128601074
In grad_steps = 785, loss = 0.4311286509037018
In grad_steps = 786, loss = 0.3543515205383301
In grad_steps = 787, loss = 0.2067083716392517
In grad_steps = 788, loss = 0.23425555229187012
In grad_steps = 789, loss = 0.3345421254634857
In grad_steps = 790, loss = 0.17028582096099854
In grad_steps = 791, loss = 0.35996049642562866
In grad_steps = 792, loss = 0.2147519290447235
In grad_steps = 793, loss = 0.37145179510116577
In grad_steps = 794, loss = 0.26035276055336
In grad_steps = 795, loss = 0.708777129650116
In grad_steps = 796, loss = 0.2819305658340454
In grad_steps = 797, loss = 0.7372744083404541
In grad_steps = 798, loss = 0.11659073084592819
In grad_steps = 799, loss = 0.04800622910261154
In grad_steps = 800, loss = 0.2680583894252777
In grad_steps = 801, loss = 0.1497965008020401
In grad_steps = 802, loss = 0.2138242870569229
In grad_steps = 803, loss = 0.1837756186723709
In grad_steps = 804, loss = 0.3297577202320099
In grad_steps = 805, loss = 0.013866693712770939
In grad_steps = 806, loss = 0.05422684922814369
In grad_steps = 807, loss = 0.0060628377832472324
Beginning epoch 9
In grad_steps = 808, loss = 0.36819013953208923
In grad_steps = 809, loss = 0.03743781894445419
In grad_steps = 810, loss = 0.11635853350162506
In grad_steps = 811, loss = 0.09459531307220459
In grad_steps = 812, loss = 0.34543338418006897
In grad_steps = 813, loss = 0.1573842167854309
In grad_steps = 814, loss = 0.18249712884426117
In grad_steps = 815, loss = 0.05086459219455719
In grad_steps = 816, loss = 0.020332109183073044
In grad_steps = 817, loss = 0.7293509244918823
In grad_steps = 818, loss = 0.23701900243759155
In grad_steps = 819, loss = 0.08928585052490234
In grad_steps = 820, loss = 0.18434341251850128
In grad_steps = 821, loss = 0.04233068600296974
In grad_steps = 822, loss = 0.08975999802350998
In grad_steps = 823, loss = 0.4972297251224518
In grad_steps = 824, loss = 0.2824743986129761
In grad_steps = 825, loss = 0.2883363366127014
In grad_steps = 826, loss = 0.27161210775375366
In grad_steps = 827, loss = 0.4099355936050415
In grad_steps = 828, loss = 0.08291511237621307
In grad_steps = 829, loss = 0.7130029201507568
In grad_steps = 830, loss = 0.11892016232013702
In grad_steps = 831, loss = 0.2500706613063812
In grad_steps = 832, loss = 0.2597343921661377
In grad_steps = 833, loss = 0.09516225755214691
In grad_steps = 834, loss = 0.20225954055786133
In grad_steps = 835, loss = 0.07948379218578339
In grad_steps = 836, loss = 0.22075550258159637
In grad_steps = 837, loss = 0.15754523873329163
In grad_steps = 838, loss = 0.17089402675628662
In grad_steps = 839, loss = 0.1296641081571579
In grad_steps = 840, loss = 0.08277146518230438
In grad_steps = 841, loss = 0.1494743674993515
In grad_steps = 842, loss = 0.08089713007211685
In grad_steps = 843, loss = 0.3162064552307129
In grad_steps = 844, loss = 0.08716334402561188
In grad_steps = 845, loss = 0.11501461267471313
In grad_steps = 846, loss = 0.6371521949768066
In grad_steps = 847, loss = 0.07933211326599121
In grad_steps = 848, loss = 0.053998708724975586
In grad_steps = 849, loss = 0.03276018798351288
In grad_steps = 850, loss = 0.33036163449287415
In grad_steps = 851, loss = 0.06255170702934265
In grad_steps = 852, loss = 0.02960977330803871
In grad_steps = 853, loss = 0.024573631584644318
In grad_steps = 854, loss = 0.04004349187016487
In grad_steps = 855, loss = 0.2687593102455139
In grad_steps = 856, loss = 0.05446094274520874
In grad_steps = 857, loss = 0.012466528452932835
In grad_steps = 858, loss = 0.10346844047307968
In grad_steps = 859, loss = 0.13482774794101715
In grad_steps = 860, loss = 0.10272152721881866
In grad_steps = 861, loss = 0.07654350996017456
In grad_steps = 862, loss = 0.1736840158700943
In grad_steps = 863, loss = 0.007297417614609003
In grad_steps = 864, loss = 0.024718156084418297
In grad_steps = 865, loss = 0.0032881279475986958
In grad_steps = 866, loss = 1.0344274044036865
In grad_steps = 867, loss = 0.046701524406671524
In grad_steps = 868, loss = 0.5746877789497375
In grad_steps = 869, loss = 0.05205964669585228
In grad_steps = 870, loss = 0.006988784298300743
In grad_steps = 871, loss = 0.09236722439527512
In grad_steps = 872, loss = 0.6117970943450928
In grad_steps = 873, loss = 0.05394497886300087
In grad_steps = 874, loss = 0.14734089374542236
In grad_steps = 875, loss = 0.694777250289917
In grad_steps = 876, loss = 0.05294262245297432
In grad_steps = 877, loss = 0.07034467905759811
In grad_steps = 878, loss = 0.8236764669418335
In grad_steps = 879, loss = 0.691062867641449
In grad_steps = 880, loss = 0.5733764171600342
In grad_steps = 881, loss = 0.16994264721870422
In grad_steps = 882, loss = 0.16566409170627594
In grad_steps = 883, loss = 0.14473900198936462
In grad_steps = 884, loss = 0.39280247688293457
In grad_steps = 885, loss = 0.5483464598655701
In grad_steps = 886, loss = 0.3787609338760376
In grad_steps = 887, loss = 0.1559651792049408
In grad_steps = 888, loss = 0.10275904089212418
In grad_steps = 889, loss = 0.2129857987165451
In grad_steps = 890, loss = 0.3315238654613495
In grad_steps = 891, loss = 0.15198571979999542
In grad_steps = 892, loss = 0.1208430752158165
In grad_steps = 893, loss = 0.05841308832168579
In grad_steps = 894, loss = 0.06068961322307587
In grad_steps = 895, loss = 0.07657302916049957
In grad_steps = 896, loss = 0.11001389473676682
In grad_steps = 897, loss = 0.19629733264446259
In grad_steps = 898, loss = 0.32763081789016724
In grad_steps = 899, loss = 0.12406527996063232
In grad_steps = 900, loss = 0.7565988898277283
In grad_steps = 901, loss = 0.2731485366821289
In grad_steps = 902, loss = 0.06010785326361656
In grad_steps = 903, loss = 0.011803989298641682
In grad_steps = 904, loss = 0.032892998307943344
In grad_steps = 905, loss = 0.18219462037086487
In grad_steps = 906, loss = 0.0226796455681324
In grad_steps = 907, loss = 0.15763932466506958
In grad_steps = 908, loss = 0.09729874134063721
Beginning epoch 10
In grad_steps = 909, loss = 0.022509509697556496
In grad_steps = 910, loss = 0.09844265878200531
In grad_steps = 911, loss = 0.1909998655319214
In grad_steps = 912, loss = 0.05081770196557045
In grad_steps = 913, loss = 0.6864747405052185
In grad_steps = 914, loss = 0.02455838955938816
In grad_steps = 915, loss = 0.08216924220323563
In grad_steps = 916, loss = 0.18522819876670837
In grad_steps = 917, loss = 0.022333839908242226
In grad_steps = 918, loss = 0.11339113861322403
In grad_steps = 919, loss = 0.08137726783752441
In grad_steps = 920, loss = 0.027986422181129456
In grad_steps = 921, loss = 0.02199561521410942
In grad_steps = 922, loss = 0.06183170527219772
In grad_steps = 923, loss = 0.028044160455465317
In grad_steps = 924, loss = 0.2873160243034363
In grad_steps = 925, loss = 0.08074969798326492
In grad_steps = 926, loss = 0.07799096405506134
In grad_steps = 927, loss = 0.03129645809531212
In grad_steps = 928, loss = 0.007319816388189793
In grad_steps = 929, loss = 0.08843592554330826
In grad_steps = 930, loss = 0.34589266777038574
In grad_steps = 931, loss = 0.03379201889038086
In grad_steps = 932, loss = 0.1850225031375885
In grad_steps = 933, loss = 0.006796448491513729
In grad_steps = 934, loss = 0.09568925201892853
In grad_steps = 935, loss = 0.0325339250266552
In grad_steps = 936, loss = 0.009873642586171627
In grad_steps = 937, loss = 0.05809418112039566
In grad_steps = 938, loss = 0.02942453883588314
In grad_steps = 939, loss = 0.0022528613917529583
In grad_steps = 940, loss = 0.05960499122738838
In grad_steps = 941, loss = 0.008952233009040356
In grad_steps = 942, loss = 0.04160615801811218
In grad_steps = 943, loss = 0.03901504725217819
In grad_steps = 944, loss = 0.029326243326067924
In grad_steps = 945, loss = 0.005577247589826584
In grad_steps = 946, loss = 0.030011188238859177
In grad_steps = 947, loss = 0.0070082941092550755
In grad_steps = 948, loss = 0.00899219885468483
In grad_steps = 949, loss = 0.04345064237713814
In grad_steps = 950, loss = 0.18399383127689362
In grad_steps = 951, loss = 0.004832631908357143
In grad_steps = 952, loss = 0.04183819890022278
In grad_steps = 953, loss = 0.006655935663729906
In grad_steps = 954, loss = 0.033590901643037796
In grad_steps = 955, loss = 0.4893963634967804
In grad_steps = 956, loss = 0.003593347268179059
In grad_steps = 957, loss = 0.0033518467098474503
In grad_steps = 958, loss = 0.014708013273775578
In grad_steps = 959, loss = 0.0032704405020922422
In grad_steps = 960, loss = 0.009695527143776417
In grad_steps = 961, loss = 0.07938826829195023
In grad_steps = 962, loss = 0.0012801154516637325
In grad_steps = 963, loss = 0.9367654323577881
In grad_steps = 964, loss = 0.004767498001456261
In grad_steps = 965, loss = 0.0273028165102005
In grad_steps = 966, loss = 0.002178770024329424
In grad_steps = 967, loss = 0.13010472059249878
In grad_steps = 968, loss = 0.0882246196269989
In grad_steps = 969, loss = 0.6270347833633423
In grad_steps = 970, loss = 0.38472628593444824
In grad_steps = 971, loss = 0.021070584654808044
In grad_steps = 972, loss = 0.041754648089408875
In grad_steps = 973, loss = 0.38810595870018005
In grad_steps = 974, loss = 0.024738261476159096
In grad_steps = 975, loss = 0.08927562832832336
In grad_steps = 976, loss = 0.30628108978271484
In grad_steps = 977, loss = 0.04796190932393074
In grad_steps = 978, loss = 0.19590258598327637
In grad_steps = 979, loss = 0.09386717528104782
In grad_steps = 980, loss = 0.24970701336860657
In grad_steps = 981, loss = 0.08430176973342896
In grad_steps = 982, loss = 0.10016681253910065
In grad_steps = 983, loss = 0.16794462502002716
In grad_steps = 984, loss = 0.06173915043473244
In grad_steps = 985, loss = 0.1528806984424591
In grad_steps = 986, loss = 0.06533920019865036
In grad_steps = 987, loss = 0.03707500174641609
In grad_steps = 988, loss = 0.01959593966603279
In grad_steps = 989, loss = 0.04383863881230354
In grad_steps = 990, loss = 0.013988600112497807
In grad_steps = 991, loss = 0.030172882601618767
In grad_steps = 992, loss = 0.055577002465724945
In grad_steps = 993, loss = 0.02799602411687374
In grad_steps = 994, loss = 0.03134923800826073
In grad_steps = 995, loss = 0.019112570211291313
In grad_steps = 996, loss = 0.009065539576113224
In grad_steps = 997, loss = 0.023699648678302765
In grad_steps = 998, loss = 0.0987393856048584
In grad_steps = 999, loss = 0.5608536005020142
In grad_steps = 1000, loss = 0.0012817506212741137
In grad_steps = 1001, loss = 0.0032426081597805023
In grad_steps = 1002, loss = 0.009891699068248272
In grad_steps = 1003, loss = 0.0029944581910967827
In grad_steps = 1004, loss = 0.0021563698537647724
In grad_steps = 1005, loss = 0.35628655552864075
In grad_steps = 1006, loss = 0.008127239532768726
In grad_steps = 1007, loss = 0.004373991396278143
In grad_steps = 1008, loss = 0.0025811810046434402
In grad_steps = 1009, loss = 0.00026454951148480177
Beginning epoch 11
In grad_steps = 1010, loss = 0.0029069981537759304
In grad_steps = 1011, loss = 0.06507797539234161
In grad_steps = 1012, loss = 0.017038317397236824
In grad_steps = 1013, loss = 0.16969995200634003
In grad_steps = 1014, loss = 0.24763338267803192
In grad_steps = 1015, loss = 0.0007831960683688521
In grad_steps = 1016, loss = 0.08837412297725677
In grad_steps = 1017, loss = 0.01589144766330719
In grad_steps = 1018, loss = 0.1341593712568283
In grad_steps = 1019, loss = 0.022015675902366638
In grad_steps = 1020, loss = 0.8898003697395325
In grad_steps = 1021, loss = 0.04105781018733978
In grad_steps = 1022, loss = 0.3845134973526001
In grad_steps = 1023, loss = 0.012691644951701164
In grad_steps = 1024, loss = 0.06259917467832565
In grad_steps = 1025, loss = 0.031114425510168076
In grad_steps = 1026, loss = 0.0416669063270092
In grad_steps = 1027, loss = 0.015150823630392551
In grad_steps = 1028, loss = 0.07100404053926468
In grad_steps = 1029, loss = 0.41390693187713623
In grad_steps = 1030, loss = 0.09068530797958374
In grad_steps = 1031, loss = 0.4850214421749115
In grad_steps = 1032, loss = 0.21459749341011047
In grad_steps = 1033, loss = 0.019331153482198715
In grad_steps = 1034, loss = 0.007507541682571173
In grad_steps = 1035, loss = 0.5479840636253357
In grad_steps = 1036, loss = 0.062122855335474014
In grad_steps = 1037, loss = 0.012926856987178326
In grad_steps = 1038, loss = 0.130174919962883
In grad_steps = 1039, loss = 0.3817368149757385
In grad_steps = 1040, loss = 0.007888510823249817
In grad_steps = 1041, loss = 0.03557950630784035
In grad_steps = 1042, loss = 0.024384571239352226
In grad_steps = 1043, loss = 0.057950541377067566
In grad_steps = 1044, loss = 0.018116258084774017
In grad_steps = 1045, loss = 0.033968038856983185
In grad_steps = 1046, loss = 0.052383601665496826
In grad_steps = 1047, loss = 0.03973091393709183
In grad_steps = 1048, loss = 0.012013093568384647
In grad_steps = 1049, loss = 0.030802898108959198
In grad_steps = 1050, loss = 0.19605977833271027
In grad_steps = 1051, loss = 0.04232349991798401
In grad_steps = 1052, loss = 0.02748260460793972
In grad_steps = 1053, loss = 0.06298910081386566
In grad_steps = 1054, loss = 0.0905962735414505
In grad_steps = 1055, loss = 0.018353231251239777
In grad_steps = 1056, loss = 0.012422535568475723
In grad_steps = 1057, loss = 0.008246893994510174
In grad_steps = 1058, loss = 0.005710509605705738
In grad_steps = 1059, loss = 0.024354830384254456
In grad_steps = 1060, loss = 0.1332530528306961
In grad_steps = 1061, loss = 0.41760146617889404
In grad_steps = 1062, loss = 0.25007957220077515
In grad_steps = 1063, loss = 0.09244555979967117
In grad_steps = 1064, loss = 0.2725828289985657
In grad_steps = 1065, loss = 0.027766600251197815
In grad_steps = 1066, loss = 0.019426394253969193
In grad_steps = 1067, loss = 0.0025968430563807487
In grad_steps = 1068, loss = 0.029370944947004318
In grad_steps = 1069, loss = 0.14365334808826447
In grad_steps = 1070, loss = 1.1783323287963867
In grad_steps = 1071, loss = 0.10224414616823196
In grad_steps = 1072, loss = 0.021029261872172356
In grad_steps = 1073, loss = 0.30346304178237915
In grad_steps = 1074, loss = 0.5145636796951294
In grad_steps = 1075, loss = 0.09122466295957565
In grad_steps = 1076, loss = 0.09425225108861923
In grad_steps = 1077, loss = 0.6990286707878113
In grad_steps = 1078, loss = 0.09586697816848755
In grad_steps = 1079, loss = 0.05354169011116028
In grad_steps = 1080, loss = 0.5899467468261719
In grad_steps = 1081, loss = 0.03615035116672516
In grad_steps = 1082, loss = 0.03020365536212921
In grad_steps = 1083, loss = 0.007060103118419647
In grad_steps = 1084, loss = 0.6297854781150818
In grad_steps = 1085, loss = 0.02543661743402481
In grad_steps = 1086, loss = 0.08310650289058685
In grad_steps = 1087, loss = 0.24834059178829193
In grad_steps = 1088, loss = 0.10129953175783157
In grad_steps = 1089, loss = 0.04009818285703659
In grad_steps = 1090, loss = 0.061621930450201035
In grad_steps = 1091, loss = 0.07694448530673981
In grad_steps = 1092, loss = 0.11636020988225937
In grad_steps = 1093, loss = 0.04541207104921341
In grad_steps = 1094, loss = 0.010917294770479202
In grad_steps = 1095, loss = 0.005621394608169794
In grad_steps = 1096, loss = 0.003340336959809065
In grad_steps = 1097, loss = 0.02697911113500595
In grad_steps = 1098, loss = 0.01428280770778656
In grad_steps = 1099, loss = 0.016377974301576614
In grad_steps = 1100, loss = 0.038836777210235596
In grad_steps = 1101, loss = 0.09585864841938019
In grad_steps = 1102, loss = 0.10352081805467606
In grad_steps = 1103, loss = 0.014286164194345474
In grad_steps = 1104, loss = 0.01905844919383526
In grad_steps = 1105, loss = 0.10100843757390976
In grad_steps = 1106, loss = 0.08328302204608917
In grad_steps = 1107, loss = 0.031529996544122696
In grad_steps = 1108, loss = 0.0015812801430001855
In grad_steps = 1109, loss = 0.00425644451752305
In grad_steps = 1110, loss = 0.00039408772136084735
Beginning epoch 12
In grad_steps = 1111, loss = 0.0022337143309414387
In grad_steps = 1112, loss = 0.0009504082263447344
In grad_steps = 1113, loss = 0.0021466142497956753
In grad_steps = 1114, loss = 0.0011846168199554086
In grad_steps = 1115, loss = 0.012780803255736828
In grad_steps = 1116, loss = 0.00037091487320140004
In grad_steps = 1117, loss = 0.20802919566631317
In grad_steps = 1118, loss = 0.0030691493302583694
In grad_steps = 1119, loss = 0.8566941618919373
In grad_steps = 1120, loss = 0.01416695211082697
In grad_steps = 1121, loss = 0.1394222378730774
In grad_steps = 1122, loss = 0.0035256275441497564
In grad_steps = 1123, loss = 0.00338412425480783
In grad_steps = 1124, loss = 0.15867769718170166
In grad_steps = 1125, loss = 0.4994592070579529
In grad_steps = 1126, loss = 0.6675861477851868
In grad_steps = 1127, loss = 0.07468104362487793
In grad_steps = 1128, loss = 0.010808810591697693
In grad_steps = 1129, loss = 0.027982046827673912
In grad_steps = 1130, loss = 0.0026803440414369106
In grad_steps = 1131, loss = 0.1497652530670166
In grad_steps = 1132, loss = 0.37706172466278076
In grad_steps = 1133, loss = 0.028528667986392975
In grad_steps = 1134, loss = 0.03673117235302925
In grad_steps = 1135, loss = 0.018246663734316826
In grad_steps = 1136, loss = 0.02124464511871338
In grad_steps = 1137, loss = 0.021768998354673386
In grad_steps = 1138, loss = 0.009219731204211712
In grad_steps = 1139, loss = 0.03198917582631111
In grad_steps = 1140, loss = 0.028021231293678284
In grad_steps = 1141, loss = 0.0057177115231752396
In grad_steps = 1142, loss = 0.020667020231485367
In grad_steps = 1143, loss = 0.04204947128891945
In grad_steps = 1144, loss = 0.010514303110539913
In grad_steps = 1145, loss = 0.009654483757913113
In grad_steps = 1146, loss = 0.026368891820311546
In grad_steps = 1147, loss = 0.039880916476249695
In grad_steps = 1148, loss = 0.12485112994909286
In grad_steps = 1149, loss = 0.12756088376045227
In grad_steps = 1150, loss = 0.036917705088853836
In grad_steps = 1151, loss = 0.3016340434551239
In grad_steps = 1152, loss = 0.23700150847434998
In grad_steps = 1153, loss = 0.02857123874127865
In grad_steps = 1154, loss = 0.03120737336575985
In grad_steps = 1155, loss = 0.05900968611240387
In grad_steps = 1156, loss = 0.021433794870972633
In grad_steps = 1157, loss = 0.013807279989123344
In grad_steps = 1158, loss = 0.0027797825168818235
In grad_steps = 1159, loss = 0.05313786491751671
In grad_steps = 1160, loss = 0.005263166967779398
In grad_steps = 1161, loss = 0.007512283977121115
In grad_steps = 1162, loss = 0.0033373455516994
In grad_steps = 1163, loss = 0.16087841987609863
In grad_steps = 1164, loss = 0.0016104443930089474
In grad_steps = 1165, loss = 0.235921710729599
In grad_steps = 1166, loss = 0.028513099998235703
In grad_steps = 1167, loss = 0.022543370723724365
In grad_steps = 1168, loss = 0.0026128957979381084
In grad_steps = 1169, loss = 0.005821287631988525
In grad_steps = 1170, loss = 0.008091608993709087
In grad_steps = 1171, loss = 0.4521256685256958
In grad_steps = 1172, loss = 0.0008057538070715964
In grad_steps = 1173, loss = 0.0162325669080019
In grad_steps = 1174, loss = 0.010150082409381866
In grad_steps = 1175, loss = 1.1056797504425049
In grad_steps = 1176, loss = 0.0015139860333874822
In grad_steps = 1177, loss = 0.6237216591835022
In grad_steps = 1178, loss = 0.5610847473144531
In grad_steps = 1179, loss = 0.004442129284143448
In grad_steps = 1180, loss = 0.1463731825351715
In grad_steps = 1181, loss = 0.4338915944099426
In grad_steps = 1182, loss = 0.25650882720947266
In grad_steps = 1183, loss = 0.44288399815559387
In grad_steps = 1184, loss = 0.011537614278495312
In grad_steps = 1185, loss = 0.10014738142490387
In grad_steps = 1186, loss = 0.15174467861652374
In grad_steps = 1187, loss = 0.08520825207233429
In grad_steps = 1188, loss = 0.2596527636051178
In grad_steps = 1189, loss = 0.2409028857946396
In grad_steps = 1190, loss = 0.02868053689599037
In grad_steps = 1191, loss = 0.08435927331447601
In grad_steps = 1192, loss = 0.2457214593887329
In grad_steps = 1193, loss = 0.30990105867385864
In grad_steps = 1194, loss = 0.20405793190002441
In grad_steps = 1195, loss = 0.026166845113039017
In grad_steps = 1196, loss = 0.028974929824471474
In grad_steps = 1197, loss = 0.015367328189313412
In grad_steps = 1198, loss = 0.10449905693531036
In grad_steps = 1199, loss = 0.05229323357343674
In grad_steps = 1200, loss = 0.051585596054792404
In grad_steps = 1201, loss = 0.11924365162849426
In grad_steps = 1202, loss = 0.029786214232444763
In grad_steps = 1203, loss = 0.02635110542178154
In grad_steps = 1204, loss = 0.054515257477760315
In grad_steps = 1205, loss = 0.017152346670627594
In grad_steps = 1206, loss = 0.10178826004266739
In grad_steps = 1207, loss = 0.004839187953621149
In grad_steps = 1208, loss = 0.023454366251826286
In grad_steps = 1209, loss = 0.9105499386787415
In grad_steps = 1210, loss = 0.01300769206136465
In grad_steps = 1211, loss = 0.00036135929985903203
Elapsed time: 2139.7764732837677 seconds for ensemble 0 with 12 epochs
LoRA instance 0 evaluation complete. Data saved to /pscratch/sd/t/tianle/lucid/other_source/SURP_2024/results/tmp_lora_ensemble/Llama3-8B-set-1.1/test_data_instance_0_seed_1.npz.
lora instance i = 0 Successfully finished.
Training lora instance 1
DEBUG: Total parameters: 8037076992
DEBUG: Trainable parameters: 6815744
Beginning epoch 1
In grad_steps = 0, loss = 1.6091442108154297
In grad_steps = 1, loss = 0.9671872854232788
In grad_steps = 2, loss = 0.7096237540245056
In grad_steps = 3, loss = 0.7473104596138
In grad_steps = 4, loss = 1.1837139129638672
In grad_steps = 5, loss = 0.42487314343452454
In grad_steps = 6, loss = 0.8639761805534363
In grad_steps = 7, loss = 0.6911374926567078
In grad_steps = 8, loss = 0.808884859085083
In grad_steps = 9, loss = 0.6516649723052979
In grad_steps = 10, loss = 0.7070003151893616
In grad_steps = 11, loss = 0.7026938199996948
In grad_steps = 12, loss = 0.7015763521194458
In grad_steps = 13, loss = 0.6372498869895935
In grad_steps = 14, loss = 0.7465065717697144
In grad_steps = 15, loss = 0.8308446407318115
In grad_steps = 16, loss = 0.6986275911331177
In grad_steps = 17, loss = 0.6825603246688843
In grad_steps = 18, loss = 0.699958324432373
In grad_steps = 19, loss = 0.7200936079025269
In grad_steps = 20, loss = 0.6750268340110779
In grad_steps = 21, loss = 0.7082390189170837
In grad_steps = 22, loss = 0.6762504577636719
In grad_steps = 23, loss = 0.7090904712677002
In grad_steps = 24, loss = 0.7091121077537537
In grad_steps = 25, loss = 0.7515947818756104
In grad_steps = 26, loss = 0.6420644521713257
In grad_steps = 27, loss = 0.734997570514679
In grad_steps = 28, loss = 0.6602951288223267
In grad_steps = 29, loss = 0.7038984894752502
In grad_steps = 30, loss = 0.6816426515579224
In grad_steps = 31, loss = 0.6929677724838257
In grad_steps = 32, loss = 0.7449572682380676
In grad_steps = 33, loss = 0.6842738389968872
In grad_steps = 34, loss = 0.8014577031135559
In grad_steps = 35, loss = 0.6880938410758972
In grad_steps = 36, loss = 0.7250312566757202
In grad_steps = 37, loss = 0.6770766377449036
In grad_steps = 38, loss = 0.7357543706893921
In grad_steps = 39, loss = 0.7041268348693848
In grad_steps = 40, loss = 0.6932456493377686
In grad_steps = 41, loss = 0.7069001197814941
In grad_steps = 42, loss = 0.6994292736053467
In grad_steps = 43, loss = 0.6853537559509277
In grad_steps = 44, loss = 0.6918513178825378
In grad_steps = 45, loss = 0.6853442788124084
In grad_steps = 46, loss = 0.7035767436027527
In grad_steps = 47, loss = 0.7304545640945435
In grad_steps = 48, loss = 0.6879445910453796
In grad_steps = 49, loss = 0.6766179800033569
In grad_steps = 50, loss = 0.6638002395629883
In grad_steps = 51, loss = 0.7207350134849548
In grad_steps = 52, loss = 0.7225139737129211
In grad_steps = 53, loss = 0.6584790945053101
In grad_steps = 54, loss = 0.5726759433746338
In grad_steps = 55, loss = 0.6047252416610718
In grad_steps = 56, loss = 0.5718538165092468
In grad_steps = 57, loss = 0.5584547519683838
In grad_steps = 58, loss = 0.7552897930145264
In grad_steps = 59, loss = 0.6901732087135315
In grad_steps = 60, loss = 0.8200153708457947
In grad_steps = 61, loss = 0.6720268726348877
In grad_steps = 62, loss = 0.9854229092597961
In grad_steps = 63, loss = 0.7552780508995056
In grad_steps = 64, loss = 0.5910843014717102
In grad_steps = 65, loss = 0.582229495048523
In grad_steps = 66, loss = 0.8068389296531677
In grad_steps = 67, loss = 0.6582298874855042
In grad_steps = 68, loss = 0.7431520223617554
In grad_steps = 69, loss = 0.6853153705596924
In grad_steps = 70, loss = 0.7385445833206177
In grad_steps = 71, loss = 0.7135712504386902
In grad_steps = 72, loss = 0.6928299069404602
In grad_steps = 73, loss = 0.7084203362464905
In grad_steps = 74, loss = 0.6691095232963562
In grad_steps = 75, loss = 0.6738441586494446
In grad_steps = 76, loss = 0.6643518209457397
In grad_steps = 77, loss = 1.042543649673462
In grad_steps = 78, loss = 0.821428656578064
In grad_steps = 79, loss = 0.6385974884033203
In grad_steps = 80, loss = 0.6902250051498413
In grad_steps = 81, loss = 0.6805023550987244
In grad_steps = 82, loss = 0.7255125045776367
In grad_steps = 83, loss = 0.6940531730651855
In grad_steps = 84, loss = 0.6999953389167786
In grad_steps = 85, loss = 0.7043585777282715
In grad_steps = 86, loss = 0.6599666476249695
In grad_steps = 87, loss = 0.6911211609840393
In grad_steps = 88, loss = 0.7114904522895813
In grad_steps = 89, loss = 0.6801306009292603
In grad_steps = 90, loss = 0.7140690684318542
In grad_steps = 91, loss = 0.6626009941101074
In grad_steps = 92, loss = 0.6813088655471802
In grad_steps = 93, loss = 0.7153539061546326
In grad_steps = 94, loss = 0.7324845194816589
In grad_steps = 95, loss = 0.6529341340065002
In grad_steps = 96, loss = 0.8471752405166626
In grad_steps = 97, loss = 0.7590429782867432
In grad_steps = 98, loss = 0.6491482257843018
In grad_steps = 99, loss = 0.624117374420166
In grad_steps = 100, loss = 0.6882167458534241
Beginning epoch 2
In grad_steps = 101, loss = 0.6438758969306946
In grad_steps = 102, loss = 0.6455202698707581
In grad_steps = 103, loss = 0.6571553349494934
In grad_steps = 104, loss = 0.6787621974945068
In grad_steps = 105, loss = 0.7185532450675964
In grad_steps = 106, loss = 0.614859402179718
In grad_steps = 107, loss = 0.650597333908081
In grad_steps = 108, loss = 0.6409196853637695
In grad_steps = 109, loss = 0.7115007042884827
In grad_steps = 110, loss = 0.6471596956253052
In grad_steps = 111, loss = 0.7275640964508057
In grad_steps = 112, loss = 0.6564674377441406
In grad_steps = 113, loss = 0.7139762043952942
In grad_steps = 114, loss = 0.7387295365333557
In grad_steps = 115, loss = 0.6977400779724121
In grad_steps = 116, loss = 0.65715092420578
In grad_steps = 117, loss = 0.6546909809112549
In grad_steps = 118, loss = 0.6674147844314575
In grad_steps = 119, loss = 0.7108929753303528
In grad_steps = 120, loss = 0.6727626323699951
In grad_steps = 121, loss = 0.6045761704444885
In grad_steps = 122, loss = 0.6930527091026306
In grad_steps = 123, loss = 0.7155117392539978
In grad_steps = 124, loss = 0.6972310543060303
In grad_steps = 125, loss = 0.6327119469642639
In grad_steps = 126, loss = 0.5697643160820007
In grad_steps = 127, loss = 0.71392822265625
In grad_steps = 128, loss = 0.6429864168167114
In grad_steps = 129, loss = 0.5730229616165161
In grad_steps = 130, loss = 0.6932466626167297
In grad_steps = 131, loss = 0.5693005323410034
In grad_steps = 132, loss = 0.6798391342163086
In grad_steps = 133, loss = 0.8656671047210693
In grad_steps = 134, loss = 0.6887481212615967
In grad_steps = 135, loss = 0.8194249272346497
In grad_steps = 136, loss = 0.7086563110351562
In grad_steps = 137, loss = 0.6553277969360352
In grad_steps = 138, loss = 0.6389023065567017
In grad_steps = 139, loss = 0.7596006989479065
In grad_steps = 140, loss = 0.8386710286140442
In grad_steps = 141, loss = 0.606397271156311
In grad_steps = 142, loss = 0.6754712462425232
In grad_steps = 143, loss = 0.6757857799530029
In grad_steps = 144, loss = 0.7019233703613281
In grad_steps = 145, loss = 0.6549303531646729
In grad_steps = 146, loss = 0.6376861333847046
In grad_steps = 147, loss = 0.6454219818115234
In grad_steps = 148, loss = 0.6294063925743103
In grad_steps = 149, loss = 0.6197852492332458
In grad_steps = 150, loss = 0.6582249402999878
In grad_steps = 151, loss = 0.584152102470398
In grad_steps = 152, loss = 0.7491126656532288
In grad_steps = 153, loss = 0.7429835796356201
In grad_steps = 154, loss = 0.6214742064476013
In grad_steps = 155, loss = 0.5467766523361206
In grad_steps = 156, loss = 0.5969665050506592
In grad_steps = 157, loss = 0.506169319152832
In grad_steps = 158, loss = 0.4926678538322449
In grad_steps = 159, loss = 0.758533239364624
In grad_steps = 160, loss = 0.7250491976737976
In grad_steps = 161, loss = 0.7891873121261597
In grad_steps = 162, loss = 0.6302412748336792
In grad_steps = 163, loss = 0.889376699924469
In grad_steps = 164, loss = 0.7209891080856323
In grad_steps = 165, loss = 0.5774933695793152
In grad_steps = 166, loss = 0.5680121779441833
In grad_steps = 167, loss = 0.6808183193206787
In grad_steps = 168, loss = 0.6372352838516235
In grad_steps = 169, loss = 0.6820633411407471
In grad_steps = 170, loss = 0.7092089056968689
In grad_steps = 171, loss = 0.6305751800537109
In grad_steps = 172, loss = 0.6870694160461426
In grad_steps = 173, loss = 0.6698894500732422
In grad_steps = 174, loss = 0.6973611116409302
In grad_steps = 175, loss = 0.6552290320396423
In grad_steps = 176, loss = 0.6630459427833557
In grad_steps = 177, loss = 0.6266940236091614
In grad_steps = 178, loss = 0.9079301357269287
In grad_steps = 179, loss = 0.7525800466537476
In grad_steps = 180, loss = 0.5585172772407532
In grad_steps = 181, loss = 0.5983449816703796
In grad_steps = 182, loss = 0.6681015491485596
In grad_steps = 183, loss = 0.7290486097335815
In grad_steps = 184, loss = 0.7026638984680176
In grad_steps = 185, loss = 0.7157702445983887
In grad_steps = 186, loss = 0.7097881436347961
In grad_steps = 187, loss = 0.5252034664154053
In grad_steps = 188, loss = 0.6495871543884277
In grad_steps = 189, loss = 0.7023123502731323
In grad_steps = 190, loss = 0.6971062421798706
In grad_steps = 191, loss = 0.6749237775802612
In grad_steps = 192, loss = 0.580517590045929
In grad_steps = 193, loss = 0.6242669820785522
In grad_steps = 194, loss = 0.7158323526382446
In grad_steps = 195, loss = 0.783129096031189
In grad_steps = 196, loss = 0.5776536464691162
In grad_steps = 197, loss = 0.6087495684623718
In grad_steps = 198, loss = 0.7010412216186523
In grad_steps = 199, loss = 0.48223263025283813
In grad_steps = 200, loss = 0.592624306678772
In grad_steps = 201, loss = 0.6854074001312256
Beginning epoch 3
In grad_steps = 202, loss = 0.5994347929954529
In grad_steps = 203, loss = 0.5319011211395264
In grad_steps = 204, loss = 0.6620309352874756
In grad_steps = 205, loss = 0.7577038407325745
In grad_steps = 206, loss = 0.6593369245529175
In grad_steps = 207, loss = 0.5533344149589539
In grad_steps = 208, loss = 0.5946299433708191
In grad_steps = 209, loss = 0.5109227299690247
In grad_steps = 210, loss = 0.7687764763832092
In grad_steps = 211, loss = 0.5396521687507629
In grad_steps = 212, loss = 0.7622013092041016
In grad_steps = 213, loss = 0.6017696857452393
In grad_steps = 214, loss = 0.6586370468139648
In grad_steps = 215, loss = 0.7332270741462708
In grad_steps = 216, loss = 0.679015040397644
In grad_steps = 217, loss = 0.6532615423202515
In grad_steps = 218, loss = 0.6503575444221497
In grad_steps = 219, loss = 0.6209433078765869
In grad_steps = 220, loss = 0.5820768475532532
In grad_steps = 221, loss = 0.5656112432479858
In grad_steps = 222, loss = 0.5188663601875305
In grad_steps = 223, loss = 0.68254154920578
In grad_steps = 224, loss = 0.7466807961463928
In grad_steps = 225, loss = 0.7035146951675415
In grad_steps = 226, loss = 0.5447710156440735
In grad_steps = 227, loss = 0.4218267500400543
In grad_steps = 228, loss = 0.5947141647338867
In grad_steps = 229, loss = 0.7554432153701782
In grad_steps = 230, loss = 0.34845641255378723
In grad_steps = 231, loss = 0.628758430480957
In grad_steps = 232, loss = 0.4665173292160034
In grad_steps = 233, loss = 0.6643540859222412
In grad_steps = 234, loss = 0.5944006443023682
In grad_steps = 235, loss = 0.580910861492157
In grad_steps = 236, loss = 0.869040310382843
In grad_steps = 237, loss = 0.38161182403564453
In grad_steps = 238, loss = 0.5720187425613403
In grad_steps = 239, loss = 0.6076749563217163
In grad_steps = 240, loss = 0.9716376066207886
In grad_steps = 241, loss = 0.9163222312927246
In grad_steps = 242, loss = 0.5173496603965759
In grad_steps = 243, loss = 0.6731176972389221
In grad_steps = 244, loss = 0.6532281041145325
In grad_steps = 245, loss = 0.7055383920669556
In grad_steps = 246, loss = 0.6391749978065491
In grad_steps = 247, loss = 0.5895792841911316
In grad_steps = 248, loss = 0.6389793753623962
In grad_steps = 249, loss = 0.5821788907051086
In grad_steps = 250, loss = 0.5735774636268616
In grad_steps = 251, loss = 0.6553225517272949
In grad_steps = 252, loss = 0.547347366809845
In grad_steps = 253, loss = 0.7465028166770935
In grad_steps = 254, loss = 0.7245291471481323
In grad_steps = 255, loss = 0.5909129977226257
In grad_steps = 256, loss = 0.5847854018211365
In grad_steps = 257, loss = 0.5796506404876709
In grad_steps = 258, loss = 0.46298661828041077
In grad_steps = 259, loss = 0.41413724422454834
In grad_steps = 260, loss = 0.8493437170982361
In grad_steps = 261, loss = 0.7283653616905212
In grad_steps = 262, loss = 0.7477782368659973
In grad_steps = 263, loss = 0.5349003672599792
In grad_steps = 264, loss = 0.6915188431739807
In grad_steps = 265, loss = 0.6836438179016113
In grad_steps = 266, loss = 0.6280320882797241
In grad_steps = 267, loss = 0.6471853852272034
In grad_steps = 268, loss = 0.5736029148101807
In grad_steps = 269, loss = 0.6031724810600281
In grad_steps = 270, loss = 0.6534185409545898
In grad_steps = 271, loss = 0.7226545214653015
In grad_steps = 272, loss = 0.6216510534286499
In grad_steps = 273, loss = 0.6627497673034668
In grad_steps = 274, loss = 0.6460371017456055
In grad_steps = 275, loss = 0.6684250831604004
In grad_steps = 276, loss = 0.6596509218215942
In grad_steps = 277, loss = 0.6287346482276917
In grad_steps = 278, loss = 0.6048470139503479
In grad_steps = 279, loss = 0.7857121229171753
In grad_steps = 280, loss = 0.6851467490196228
In grad_steps = 281, loss = 0.5137925148010254
In grad_steps = 282, loss = 0.5209917426109314
In grad_steps = 283, loss = 0.64490807056427
In grad_steps = 284, loss = 0.7367525100708008
In grad_steps = 285, loss = 0.663469672203064
In grad_steps = 286, loss = 0.6499777436256409
In grad_steps = 287, loss = 0.655555248260498
In grad_steps = 288, loss = 0.41887539625167847
In grad_steps = 289, loss = 0.5234678983688354
In grad_steps = 290, loss = 0.8046131134033203
In grad_steps = 291, loss = 0.7221236228942871
In grad_steps = 292, loss = 0.5271408557891846
In grad_steps = 293, loss = 0.41050949692726135
In grad_steps = 294, loss = 0.5500156283378601
In grad_steps = 295, loss = 0.8920393586158752
In grad_steps = 296, loss = 0.8763440847396851
In grad_steps = 297, loss = 0.5202580690383911
In grad_steps = 298, loss = 0.5306245684623718
In grad_steps = 299, loss = 0.585023820400238
In grad_steps = 300, loss = 0.519003689289093
In grad_steps = 301, loss = 0.6657758951187134
In grad_steps = 302, loss = 0.5914306640625
Beginning epoch 4
In grad_steps = 303, loss = 0.6568479537963867
In grad_steps = 304, loss = 0.5751432776451111
In grad_steps = 305, loss = 0.6776596903800964
In grad_steps = 306, loss = 0.677383303642273
In grad_steps = 307, loss = 0.5958794355392456
In grad_steps = 308, loss = 0.6132141351699829
In grad_steps = 309, loss = 0.5541264414787292
In grad_steps = 310, loss = 0.5141873359680176
In grad_steps = 311, loss = 0.7099044322967529
In grad_steps = 312, loss = 0.5305109024047852
In grad_steps = 313, loss = 0.7628485560417175
In grad_steps = 314, loss = 0.559626042842865
In grad_steps = 315, loss = 0.6933618187904358
In grad_steps = 316, loss = 0.7669581174850464
In grad_steps = 317, loss = 0.6270215511322021
In grad_steps = 318, loss = 0.6128104329109192
In grad_steps = 319, loss = 0.6845123767852783
In grad_steps = 320, loss = 0.5381922721862793
In grad_steps = 321, loss = 0.5256816148757935
In grad_steps = 322, loss = 0.5191423296928406
In grad_steps = 323, loss = 0.39572852849960327
In grad_steps = 324, loss = 0.6269529461860657
In grad_steps = 325, loss = 0.7477742433547974
In grad_steps = 326, loss = 0.635226845741272
In grad_steps = 327, loss = 0.465704083442688
In grad_steps = 328, loss = 0.40969714522361755
In grad_steps = 329, loss = 0.5384529232978821
In grad_steps = 330, loss = 0.6112872958183289
In grad_steps = 331, loss = 0.2909441888332367
In grad_steps = 332, loss = 0.32710033655166626
In grad_steps = 333, loss = 0.31927186250686646
In grad_steps = 334, loss = 0.5578153729438782
In grad_steps = 335, loss = 0.2647148072719574
In grad_steps = 336, loss = 0.4939461350440979
In grad_steps = 337, loss = 0.3335977792739868
In grad_steps = 338, loss = 0.22613930702209473
In grad_steps = 339, loss = 0.20621345937252045
In grad_steps = 340, loss = 0.08109253644943237
In grad_steps = 341, loss = 0.30391478538513184
In grad_steps = 342, loss = 0.7697399854660034
In grad_steps = 343, loss = 0.7007771134376526
In grad_steps = 344, loss = 1.65841543674469
In grad_steps = 345, loss = 0.8283347487449646
In grad_steps = 346, loss = 0.7267913222312927
In grad_steps = 347, loss = 0.5103757381439209
In grad_steps = 348, loss = 0.4434971213340759
In grad_steps = 349, loss = 0.655232846736908
In grad_steps = 350, loss = 0.7367623448371887
In grad_steps = 351, loss = 0.5319664478302002
In grad_steps = 352, loss = 0.604743480682373
In grad_steps = 353, loss = 0.639464795589447
In grad_steps = 354, loss = 0.7138077616691589
In grad_steps = 355, loss = 0.7036339640617371
In grad_steps = 356, loss = 0.6045290231704712
In grad_steps = 357, loss = 0.4916784465312958
In grad_steps = 358, loss = 0.4900307059288025
In grad_steps = 359, loss = 0.4722539484500885
In grad_steps = 360, loss = 0.4820099472999573
In grad_steps = 361, loss = 0.8921534419059753
In grad_steps = 362, loss = 0.7157636880874634
In grad_steps = 363, loss = 0.7820709943771362
In grad_steps = 364, loss = 0.5575350522994995
In grad_steps = 365, loss = 0.6367800235748291
In grad_steps = 366, loss = 0.6715423464775085
In grad_steps = 367, loss = 0.7296112179756165
In grad_steps = 368, loss = 0.7017973065376282
In grad_steps = 369, loss = 0.5886951684951782
In grad_steps = 370, loss = 0.6302071809768677
In grad_steps = 371, loss = 0.6258862018585205
In grad_steps = 372, loss = 0.636258065700531
In grad_steps = 373, loss = 0.6221181750297546
In grad_steps = 374, loss = 0.6439179182052612
In grad_steps = 375, loss = 0.6597744226455688
In grad_steps = 376, loss = 0.6281787753105164
In grad_steps = 377, loss = 0.6297813057899475
In grad_steps = 378, loss = 0.5594704151153564
In grad_steps = 379, loss = 0.5324276685714722
In grad_steps = 380, loss = 0.8011619448661804
In grad_steps = 381, loss = 0.6221804022789001
In grad_steps = 382, loss = 0.4571506381034851
In grad_steps = 383, loss = 0.5668072700500488
In grad_steps = 384, loss = 0.5801548957824707
In grad_steps = 385, loss = 0.6881147623062134
In grad_steps = 386, loss = 0.6263778209686279
In grad_steps = 387, loss = 0.5483580827713013
In grad_steps = 388, loss = 0.6891409754753113
In grad_steps = 389, loss = 0.4127175211906433
In grad_steps = 390, loss = 0.5284738540649414
In grad_steps = 391, loss = 0.5755613446235657
In grad_steps = 392, loss = 0.4899890422821045
In grad_steps = 393, loss = 0.4873974323272705
In grad_steps = 394, loss = 0.4704405665397644
In grad_steps = 395, loss = 0.5059784054756165
In grad_steps = 396, loss = 0.39749202132225037
In grad_steps = 397, loss = 0.3713611662387848
In grad_steps = 398, loss = 0.4173234701156616
In grad_steps = 399, loss = 0.5298845171928406
In grad_steps = 400, loss = 0.3940606713294983
In grad_steps = 401, loss = 0.8771969079971313
In grad_steps = 402, loss = 0.46209895610809326
In grad_steps = 403, loss = 0.33054065704345703
Beginning epoch 5
In grad_steps = 404, loss = 0.690228283405304
In grad_steps = 405, loss = 0.4246269166469574
In grad_steps = 406, loss = 0.70502769947052
In grad_steps = 407, loss = 0.5649068355560303
In grad_steps = 408, loss = 0.7677354216575623
In grad_steps = 409, loss = 0.5404251217842102
In grad_steps = 410, loss = 0.48995232582092285
In grad_steps = 411, loss = 0.5051529407501221
In grad_steps = 412, loss = 0.5248112082481384
In grad_steps = 413, loss = 0.5698546767234802
In grad_steps = 414, loss = 0.6811314821243286
In grad_steps = 415, loss = 0.6384072303771973
In grad_steps = 416, loss = 0.540179967880249
In grad_steps = 417, loss = 0.6419316530227661
In grad_steps = 418, loss = 0.6193534135818481
In grad_steps = 419, loss = 0.6732527613639832
In grad_steps = 420, loss = 0.634723961353302
In grad_steps = 421, loss = 0.5266106724739075
In grad_steps = 422, loss = 0.5312535762786865
In grad_steps = 423, loss = 0.5124750733375549
In grad_steps = 424, loss = 0.3362307548522949
In grad_steps = 425, loss = 0.6543158292770386
In grad_steps = 426, loss = 0.5459320545196533
In grad_steps = 427, loss = 0.5593174695968628
In grad_steps = 428, loss = 0.4134642779827118
In grad_steps = 429, loss = 0.4004107117652893
In grad_steps = 430, loss = 0.3526775538921356
In grad_steps = 431, loss = 0.5458733439445496
In grad_steps = 432, loss = 0.3077957332134247
In grad_steps = 433, loss = 0.3526645302772522
In grad_steps = 434, loss = 0.3454306423664093
In grad_steps = 435, loss = 0.5322585701942444
In grad_steps = 436, loss = 0.2355823516845703
In grad_steps = 437, loss = 0.17924164235591888
In grad_steps = 438, loss = 0.2505917549133301
In grad_steps = 439, loss = 0.1149192824959755
In grad_steps = 440, loss = 0.05136903002858162
In grad_steps = 441, loss = 0.10222341865301132
In grad_steps = 442, loss = 0.12691472470760345
In grad_steps = 443, loss = 1.56178879737854
In grad_steps = 444, loss = 0.08079515397548676
In grad_steps = 445, loss = 1.1468056440353394
In grad_steps = 446, loss = 1.0526180267333984
In grad_steps = 447, loss = 0.6419317126274109
In grad_steps = 448, loss = 0.6171280145645142
In grad_steps = 449, loss = 0.33681637048721313
In grad_steps = 450, loss = 0.2799263000488281
In grad_steps = 451, loss = 0.37991979718208313
In grad_steps = 452, loss = 0.3283580243587494
In grad_steps = 453, loss = 0.49531230330467224
In grad_steps = 454, loss = 0.8087735176086426
In grad_steps = 455, loss = 0.5713010430335999
In grad_steps = 456, loss = 0.6207796335220337
In grad_steps = 457, loss = 0.5214577913284302
In grad_steps = 458, loss = 0.5871596932411194
In grad_steps = 459, loss = 0.38659465312957764
In grad_steps = 460, loss = 0.461076021194458
In grad_steps = 461, loss = 0.38328051567077637
In grad_steps = 462, loss = 0.9784845113754272
In grad_steps = 463, loss = 0.8026684522628784
In grad_steps = 464, loss = 0.8655914664268494
In grad_steps = 465, loss = 0.4757311940193176
In grad_steps = 466, loss = 0.7210617661476135
In grad_steps = 467, loss = 0.4976775646209717
In grad_steps = 468, loss = 0.6636435389518738
In grad_steps = 469, loss = 0.5451716184616089
In grad_steps = 470, loss = 0.49774378538131714
In grad_steps = 471, loss = 0.6625399589538574
In grad_steps = 472, loss = 0.5475909113883972
In grad_steps = 473, loss = 0.7388469576835632
In grad_steps = 474, loss = 0.5093436241149902
In grad_steps = 475, loss = 0.5738280415534973
In grad_steps = 476, loss = 0.613055408000946
In grad_steps = 477, loss = 0.415241003036499
In grad_steps = 478, loss = 0.680565357208252
In grad_steps = 479, loss = 0.5470783114433289
In grad_steps = 480, loss = 0.524253785610199
In grad_steps = 481, loss = 0.5835357308387756
In grad_steps = 482, loss = 0.5622370839118958
In grad_steps = 483, loss = 0.25486767292022705
In grad_steps = 484, loss = 0.337656706571579
In grad_steps = 485, loss = 0.36656445264816284
In grad_steps = 486, loss = 0.5340631604194641
In grad_steps = 487, loss = 0.5795164704322815
In grad_steps = 488, loss = 0.7154127955436707
In grad_steps = 489, loss = 0.36991778016090393
In grad_steps = 490, loss = 0.09125120937824249
In grad_steps = 491, loss = 0.3717322051525116
In grad_steps = 492, loss = 0.5446388721466064
In grad_steps = 493, loss = 0.15240933001041412
In grad_steps = 494, loss = 0.32331010699272156
In grad_steps = 495, loss = 0.2644156813621521
In grad_steps = 496, loss = 0.3393518328666687
In grad_steps = 497, loss = 0.18933984637260437
In grad_steps = 498, loss = 0.12526674568653107
In grad_steps = 499, loss = 0.21485134959220886
In grad_steps = 500, loss = 0.1800018846988678
In grad_steps = 501, loss = 0.2528313994407654
In grad_steps = 502, loss = 0.20542576909065247
In grad_steps = 503, loss = 0.836301863193512
In grad_steps = 504, loss = 0.018182549625635147
Beginning epoch 6
In grad_steps = 505, loss = 0.4871944487094879
In grad_steps = 506, loss = 0.08127576857805252
In grad_steps = 507, loss = 0.5396731495857239
In grad_steps = 508, loss = 0.10429395735263824
In grad_steps = 509, loss = 1.1086679697036743
In grad_steps = 510, loss = 0.7705910205841064
In grad_steps = 511, loss = 0.3381049931049347
In grad_steps = 512, loss = 0.577250599861145
In grad_steps = 513, loss = 0.3240683674812317
In grad_steps = 514, loss = 0.6394250392913818
In grad_steps = 515, loss = 0.5436305999755859
In grad_steps = 516, loss = 0.5398313999176025
In grad_steps = 517, loss = 0.4541475772857666
In grad_steps = 518, loss = 0.7570705413818359
In grad_steps = 519, loss = 0.6346532106399536
In grad_steps = 520, loss = 0.6149488687515259
In grad_steps = 521, loss = 0.6107819080352783
In grad_steps = 522, loss = 0.5252589583396912
In grad_steps = 523, loss = 0.5543414354324341
In grad_steps = 524, loss = 0.46975430846214294
In grad_steps = 525, loss = 0.33667564392089844
In grad_steps = 526, loss = 0.6166063547134399
In grad_steps = 527, loss = 0.5686343312263489
In grad_steps = 528, loss = 0.5564098954200745
In grad_steps = 529, loss = 0.3463634252548218
In grad_steps = 530, loss = 0.3122682273387909
In grad_steps = 531, loss = 0.41038352251052856
In grad_steps = 532, loss = 0.4590161144733429
In grad_steps = 533, loss = 0.28317567706108093
In grad_steps = 534, loss = 0.3145639896392822
In grad_steps = 535, loss = 0.27600961923599243
In grad_steps = 536, loss = 0.40492331981658936
In grad_steps = 537, loss = 0.23722633719444275
In grad_steps = 538, loss = 0.2713285982608795
In grad_steps = 539, loss = 0.16653718054294586
In grad_steps = 540, loss = 0.19476886093616486
In grad_steps = 541, loss = 0.28921622037887573
In grad_steps = 542, loss = 0.0695289596915245
In grad_steps = 543, loss = 0.050047680735588074
In grad_steps = 544, loss = 0.14484262466430664
In grad_steps = 545, loss = 0.024395586922764778
In grad_steps = 546, loss = 0.27178627252578735
In grad_steps = 547, loss = 0.47858789563179016
In grad_steps = 548, loss = 0.9821781516075134
In grad_steps = 549, loss = 0.4945426285266876
In grad_steps = 550, loss = 0.02620864473283291
In grad_steps = 551, loss = 0.03993414714932442
In grad_steps = 552, loss = 0.15626879036426544
In grad_steps = 553, loss = 0.04478989169001579
In grad_steps = 554, loss = 0.5920583605766296
In grad_steps = 555, loss = 0.29358261823654175
In grad_steps = 556, loss = 0.3655722439289093
In grad_steps = 557, loss = 0.7837716341018677
In grad_steps = 558, loss = 0.5761080384254456
In grad_steps = 559, loss = 0.9850791692733765
In grad_steps = 560, loss = 0.7193411588668823
In grad_steps = 561, loss = 0.41884249448776245
In grad_steps = 562, loss = 0.3210453689098358
In grad_steps = 563, loss = 0.5731910467147827
In grad_steps = 564, loss = 0.954026997089386
In grad_steps = 565, loss = 0.8546883463859558
In grad_steps = 566, loss = 0.5224677324295044
In grad_steps = 567, loss = 0.6762201189994812
In grad_steps = 568, loss = 0.48931264877319336
In grad_steps = 569, loss = 0.6406726837158203
In grad_steps = 570, loss = 0.36145150661468506
In grad_steps = 571, loss = 0.5706361532211304
In grad_steps = 572, loss = 0.482374370098114
In grad_steps = 573, loss = 0.3600843846797943
In grad_steps = 574, loss = 0.6451495885848999
In grad_steps = 575, loss = 0.46292197704315186
In grad_steps = 576, loss = 0.5300836563110352
In grad_steps = 577, loss = 0.5211661458015442
In grad_steps = 578, loss = 0.42167139053344727
In grad_steps = 579, loss = 0.6532129049301147
In grad_steps = 580, loss = 0.285614550113678
In grad_steps = 581, loss = 0.4436893165111542
In grad_steps = 582, loss = 0.46250084042549133
In grad_steps = 583, loss = 0.306666761636734
In grad_steps = 584, loss = 0.3352293372154236
In grad_steps = 585, loss = 0.295783668756485
In grad_steps = 586, loss = 0.17695003747940063
In grad_steps = 587, loss = 0.4437943696975708
In grad_steps = 588, loss = 0.26684340834617615
In grad_steps = 589, loss = 0.30609190464019775
In grad_steps = 590, loss = 0.09114932268857956
In grad_steps = 591, loss = 0.0298935417085886
In grad_steps = 592, loss = 0.11826643347740173
In grad_steps = 593, loss = 0.18165217339992523
In grad_steps = 594, loss = 0.05178256332874298
In grad_steps = 595, loss = 0.9688661694526672
In grad_steps = 596, loss = 0.022752314805984497
In grad_steps = 597, loss = 0.25749471783638
In grad_steps = 598, loss = 0.8343899846076965
In grad_steps = 599, loss = 0.37207648158073425
In grad_steps = 600, loss = 0.08282031118869781
In grad_steps = 601, loss = 0.060083527117967606
In grad_steps = 602, loss = 0.08749277144670486
In grad_steps = 603, loss = 0.2977112829685211
In grad_steps = 604, loss = 0.2916756272315979
In grad_steps = 605, loss = 0.1494712382555008
Beginning epoch 7
In grad_steps = 606, loss = 0.62958163022995
In grad_steps = 607, loss = 0.18602930009365082
In grad_steps = 608, loss = 0.5331066846847534
In grad_steps = 609, loss = 0.13479886949062347
In grad_steps = 610, loss = 0.10857188701629639
In grad_steps = 611, loss = 0.2740578353404999
In grad_steps = 612, loss = 1.3006699085235596
In grad_steps = 613, loss = 0.6582639217376709
In grad_steps = 614, loss = 1.3199200630187988
In grad_steps = 615, loss = 0.3254449665546417
In grad_steps = 616, loss = 0.6869602203369141
In grad_steps = 617, loss = 0.3181934356689453
In grad_steps = 618, loss = 0.5099227428436279
In grad_steps = 619, loss = 0.47650259733200073
In grad_steps = 620, loss = 0.5253040194511414
In grad_steps = 621, loss = 0.6413713097572327
In grad_steps = 622, loss = 0.5828249454498291
In grad_steps = 623, loss = 0.46319326758384705
In grad_steps = 624, loss = 0.33493369817733765
In grad_steps = 625, loss = 0.4236115515232086
In grad_steps = 626, loss = 0.4669363498687744
In grad_steps = 627, loss = 0.6121506094932556
In grad_steps = 628, loss = 0.4648321866989136
In grad_steps = 629, loss = 0.4401019811630249
In grad_steps = 630, loss = 0.18906261026859283
In grad_steps = 631, loss = 0.37777623534202576
In grad_steps = 632, loss = 0.15214040875434875
In grad_steps = 633, loss = 0.5880857706069946
In grad_steps = 634, loss = 0.28745535016059875
In grad_steps = 635, loss = 0.23223930597305298
In grad_steps = 636, loss = 0.1406678706407547
In grad_steps = 637, loss = 0.5235938429832458
In grad_steps = 638, loss = 0.21566757559776306
In grad_steps = 639, loss = 0.20970706641674042
In grad_steps = 640, loss = 0.5198805332183838
In grad_steps = 641, loss = 0.1112714484333992
In grad_steps = 642, loss = 0.04744614660739899
In grad_steps = 643, loss = 0.09520617127418518
In grad_steps = 644, loss = 0.1793929636478424
In grad_steps = 645, loss = 0.4142146110534668
In grad_steps = 646, loss = 0.04730115830898285
In grad_steps = 647, loss = 0.01741723157465458
In grad_steps = 648, loss = 0.05571860074996948
In grad_steps = 649, loss = 0.06779219955205917
In grad_steps = 650, loss = 0.0797976553440094
In grad_steps = 651, loss = 0.10298959910869598
In grad_steps = 652, loss = 0.018397098407149315
In grad_steps = 653, loss = 0.0030006468296051025
In grad_steps = 654, loss = 0.012142805382609367
In grad_steps = 655, loss = 0.009708220139145851
In grad_steps = 656, loss = 0.033292170614004135
In grad_steps = 657, loss = 0.8317147493362427
In grad_steps = 658, loss = 1.0610907077789307
In grad_steps = 659, loss = 0.42125964164733887
In grad_steps = 660, loss = 1.0584008693695068
In grad_steps = 661, loss = 0.7483566999435425
In grad_steps = 662, loss = 1.0881626605987549
In grad_steps = 663, loss = 0.4597564935684204
In grad_steps = 664, loss = 0.44851139187812805
In grad_steps = 665, loss = 0.5938178300857544
In grad_steps = 666, loss = 0.6875555515289307
In grad_steps = 667, loss = 0.26139241456985474
In grad_steps = 668, loss = 0.43079519271850586
In grad_steps = 669, loss = 0.40765929222106934
In grad_steps = 670, loss = 0.6779260039329529
In grad_steps = 671, loss = 0.3331415355205536
In grad_steps = 672, loss = 0.3869149684906006
In grad_steps = 673, loss = 0.6640248894691467
In grad_steps = 674, loss = 0.40968918800354004
In grad_steps = 675, loss = 0.7502822875976562
In grad_steps = 676, loss = 0.3622177839279175
In grad_steps = 677, loss = 0.36822521686553955
In grad_steps = 678, loss = 0.51493239402771
In grad_steps = 679, loss = 0.2971581220626831
In grad_steps = 680, loss = 0.46769535541534424
In grad_steps = 681, loss = 0.1987699270248413
In grad_steps = 682, loss = 0.4543759226799011
In grad_steps = 683, loss = 0.4874369204044342
In grad_steps = 684, loss = 0.26065632700920105
In grad_steps = 685, loss = 0.2492513358592987
In grad_steps = 686, loss = 0.1735900342464447
In grad_steps = 687, loss = 0.20221713185310364
In grad_steps = 688, loss = 0.17128773033618927
In grad_steps = 689, loss = 0.2754577398300171
In grad_steps = 690, loss = 0.6855580806732178
In grad_steps = 691, loss = 0.6448196172714233
In grad_steps = 692, loss = 0.3494901955127716
In grad_steps = 693, loss = 0.17935046553611755
In grad_steps = 694, loss = 0.19403080642223358
In grad_steps = 695, loss = 0.11418835818767548
In grad_steps = 696, loss = 0.06364770233631134
In grad_steps = 697, loss = 0.21180076897144318
In grad_steps = 698, loss = 0.876215934753418
In grad_steps = 699, loss = 0.1819104105234146
In grad_steps = 700, loss = 0.18567733466625214
In grad_steps = 701, loss = 0.3088650405406952
In grad_steps = 702, loss = 0.583859384059906
In grad_steps = 703, loss = 0.22209681570529938
In grad_steps = 704, loss = 0.27326226234436035
In grad_steps = 705, loss = 0.09126879274845123
In grad_steps = 706, loss = 0.019385963678359985
Beginning epoch 8
In grad_steps = 707, loss = 0.608251690864563
In grad_steps = 708, loss = 0.4630838632583618
In grad_steps = 709, loss = 0.36054500937461853
In grad_steps = 710, loss = 0.9349715709686279
In grad_steps = 711, loss = 0.19315937161445618
In grad_steps = 712, loss = 0.14412224292755127
In grad_steps = 713, loss = 0.23110058903694153
In grad_steps = 714, loss = 0.1307433396577835
In grad_steps = 715, loss = 0.13892826437950134
In grad_steps = 716, loss = 0.1817011535167694
In grad_steps = 717, loss = 0.964738130569458
In grad_steps = 718, loss = 0.3345955014228821
In grad_steps = 719, loss = 0.9287793636322021
In grad_steps = 720, loss = 0.6204284429550171
In grad_steps = 721, loss = 0.4007633328437805
In grad_steps = 722, loss = 0.6132530570030212
In grad_steps = 723, loss = 0.46820011734962463
In grad_steps = 724, loss = 0.45990967750549316
In grad_steps = 725, loss = 0.16947069764137268
In grad_steps = 726, loss = 0.16679266095161438
In grad_steps = 727, loss = 0.5001420378684998
In grad_steps = 728, loss = 0.5814896821975708
In grad_steps = 729, loss = 0.6693076491355896
In grad_steps = 730, loss = 0.6222862005233765
In grad_steps = 731, loss = 0.20417794585227966
In grad_steps = 732, loss = 0.2169174700975418
In grad_steps = 733, loss = 0.40311217308044434
In grad_steps = 734, loss = 0.2478814721107483
In grad_steps = 735, loss = 0.35441508889198303
In grad_steps = 736, loss = 0.31984883546829224
In grad_steps = 737, loss = 0.1637546718120575
In grad_steps = 738, loss = 0.40609365701675415
In grad_steps = 739, loss = 0.22347530722618103
In grad_steps = 740, loss = 0.4055716395378113
In grad_steps = 741, loss = 0.15945769846439362
In grad_steps = 742, loss = 0.3126896619796753
In grad_steps = 743, loss = 0.21071219444274902
In grad_steps = 744, loss = 0.24075812101364136
In grad_steps = 745, loss = 0.32206618785858154
In grad_steps = 746, loss = 0.13237935304641724
In grad_steps = 747, loss = 0.10913323611021042
In grad_steps = 748, loss = 0.2940247654914856
In grad_steps = 749, loss = 0.2800341546535492
In grad_steps = 750, loss = 0.11115159839391708
In grad_steps = 751, loss = 0.21360467374324799
In grad_steps = 752, loss = 0.08015120774507523
In grad_steps = 753, loss = 0.20282875001430511
In grad_steps = 754, loss = 0.010147257708013058
In grad_steps = 755, loss = 0.11668525636196136
In grad_steps = 756, loss = 0.31517595052719116
In grad_steps = 757, loss = 0.09768936038017273
In grad_steps = 758, loss = 0.0993046760559082
In grad_steps = 759, loss = 0.21199092268943787
In grad_steps = 760, loss = 0.09545635432004929
In grad_steps = 761, loss = 0.41931426525115967
In grad_steps = 762, loss = 0.008840295486152172
In grad_steps = 763, loss = 0.25559374690055847
In grad_steps = 764, loss = 0.024105481803417206
In grad_steps = 765, loss = 0.07547848671674728
In grad_steps = 766, loss = 0.3515338897705078
In grad_steps = 767, loss = 0.2614816427230835
In grad_steps = 768, loss = 0.026618901640176773
In grad_steps = 769, loss = 0.023632701486349106
In grad_steps = 770, loss = 0.033504195511341095
In grad_steps = 771, loss = 1.121330976486206
In grad_steps = 772, loss = 0.15299777686595917
In grad_steps = 773, loss = 0.03401884064078331
In grad_steps = 774, loss = 0.7932060956954956
In grad_steps = 775, loss = 0.06634700298309326
In grad_steps = 776, loss = 0.167212575674057
In grad_steps = 777, loss = 0.491425096988678
In grad_steps = 778, loss = 0.020853711292147636
In grad_steps = 779, loss = 0.11566636711359024
In grad_steps = 780, loss = 0.5902526378631592
In grad_steps = 781, loss = 0.37669479846954346
In grad_steps = 782, loss = 0.1133381575345993
In grad_steps = 783, loss = 0.244527205824852
In grad_steps = 784, loss = 0.42787790298461914
In grad_steps = 785, loss = 0.19754065573215485
In grad_steps = 786, loss = 0.04206811636686325
In grad_steps = 787, loss = 0.06084252893924713
In grad_steps = 788, loss = 0.06290222704410553
In grad_steps = 789, loss = 0.06812480092048645
In grad_steps = 790, loss = 0.06751607358455658
In grad_steps = 791, loss = 0.052407048642635345
In grad_steps = 792, loss = 0.04384174570441246
In grad_steps = 793, loss = 0.11394873261451721
In grad_steps = 794, loss = 0.14294257760047913
In grad_steps = 795, loss = 0.18545597791671753
In grad_steps = 796, loss = 0.2224871665239334
In grad_steps = 797, loss = 0.3373362421989441
In grad_steps = 798, loss = 0.2663172483444214
In grad_steps = 799, loss = 0.5877139568328857
In grad_steps = 800, loss = 0.12329193204641342
In grad_steps = 801, loss = 0.030119000002741814
In grad_steps = 802, loss = 0.02400861121714115
In grad_steps = 803, loss = 0.1720184087753296
In grad_steps = 804, loss = 0.38545912504196167
In grad_steps = 805, loss = 0.046361614018678665
In grad_steps = 806, loss = 0.3826715350151062
In grad_steps = 807, loss = 0.22612349689006805
Beginning epoch 9
In grad_steps = 808, loss = 0.5533546209335327
In grad_steps = 809, loss = 0.018703140318393707
In grad_steps = 810, loss = 0.10561825335025787
In grad_steps = 811, loss = 0.04112894460558891
In grad_steps = 812, loss = 0.23556289076805115
In grad_steps = 813, loss = 0.08536434173583984
In grad_steps = 814, loss = 0.14871688187122345
In grad_steps = 815, loss = 0.16122755408287048
In grad_steps = 816, loss = 0.023603569716215134
In grad_steps = 817, loss = 1.242511510848999
In grad_steps = 818, loss = 0.21608465909957886
In grad_steps = 819, loss = 0.2912575602531433
In grad_steps = 820, loss = 0.29505449533462524
In grad_steps = 821, loss = 0.3415394723415375
In grad_steps = 822, loss = 0.16084876656532288
In grad_steps = 823, loss = 0.04243600741028786
In grad_steps = 824, loss = 0.13512973487377167
In grad_steps = 825, loss = 0.1950460821390152
In grad_steps = 826, loss = 0.3652954399585724
In grad_steps = 827, loss = 0.26510071754455566
In grad_steps = 828, loss = 0.034596994519233704
In grad_steps = 829, loss = 0.33968493342399597
In grad_steps = 830, loss = 0.231353297829628
In grad_steps = 831, loss = 0.2634968161582947
In grad_steps = 832, loss = 0.08176583796739578
In grad_steps = 833, loss = 0.4753050208091736
In grad_steps = 834, loss = 0.059500258415937424
In grad_steps = 835, loss = 0.07595930993556976
In grad_steps = 836, loss = 0.23810580372810364
In grad_steps = 837, loss = 0.17253342270851135
In grad_steps = 838, loss = 0.028021568432450294
In grad_steps = 839, loss = 0.1715053766965866
In grad_steps = 840, loss = 0.21470984816551208
In grad_steps = 841, loss = 0.1833116114139557
In grad_steps = 842, loss = 0.21708892285823822
In grad_steps = 843, loss = 0.26373156905174255
In grad_steps = 844, loss = 0.4674164652824402
In grad_steps = 845, loss = 0.1772386133670807
In grad_steps = 846, loss = 0.1289370208978653
In grad_steps = 847, loss = 0.04822103679180145
In grad_steps = 848, loss = 0.21494054794311523
In grad_steps = 849, loss = 0.033506590873003006
In grad_steps = 850, loss = 0.039159320294857025
In grad_steps = 851, loss = 0.060550667345523834
In grad_steps = 852, loss = 0.08051399886608124
In grad_steps = 853, loss = 0.011395992711186409
In grad_steps = 854, loss = 0.7376595139503479
In grad_steps = 855, loss = 0.45459553599357605
In grad_steps = 856, loss = 0.04795876517891884
In grad_steps = 857, loss = 0.023165877908468246
In grad_steps = 858, loss = 0.1398678421974182
In grad_steps = 859, loss = 0.13998550176620483
In grad_steps = 860, loss = 0.2368369847536087
In grad_steps = 861, loss = 0.30578428506851196
In grad_steps = 862, loss = 0.3789401650428772
In grad_steps = 863, loss = 0.038751546293497086
In grad_steps = 864, loss = 0.043863099068403244
In grad_steps = 865, loss = 0.010918104089796543
In grad_steps = 866, loss = 0.05202380567789078
In grad_steps = 867, loss = 0.04988326132297516
In grad_steps = 868, loss = 0.6834862232208252
In grad_steps = 869, loss = 0.03876572474837303
In grad_steps = 870, loss = 0.23947970569133759
In grad_steps = 871, loss = 0.7902354001998901
In grad_steps = 872, loss = 0.5852639675140381
In grad_steps = 873, loss = 0.01367136836051941
In grad_steps = 874, loss = 0.029005713760852814
In grad_steps = 875, loss = 0.30803853273391724
In grad_steps = 876, loss = 0.11755260080099106
In grad_steps = 877, loss = 0.2090444713830948
In grad_steps = 878, loss = 0.18384578824043274
In grad_steps = 879, loss = 0.07964089512825012
In grad_steps = 880, loss = 0.1556331217288971
In grad_steps = 881, loss = 0.147864431142807
In grad_steps = 882, loss = 0.12286139279603958
In grad_steps = 883, loss = 0.3332546055316925
In grad_steps = 884, loss = 0.2894824147224426
In grad_steps = 885, loss = 0.09980151802301407
In grad_steps = 886, loss = 0.18619702756404877
In grad_steps = 887, loss = 0.026806574314832687
In grad_steps = 888, loss = 0.030231254175305367
In grad_steps = 889, loss = 0.09980659931898117
In grad_steps = 890, loss = 0.7247889637947083
In grad_steps = 891, loss = 0.3755730390548706
In grad_steps = 892, loss = 0.01894727349281311
In grad_steps = 893, loss = 0.13329343497753143
In grad_steps = 894, loss = 0.007298843935132027
In grad_steps = 895, loss = 0.030712617561221123
In grad_steps = 896, loss = 0.023769108578562737
In grad_steps = 897, loss = 0.016375478357076645
In grad_steps = 898, loss = 0.5988510251045227
In grad_steps = 899, loss = 0.11428196728229523
In grad_steps = 900, loss = 0.12485167384147644
In grad_steps = 901, loss = 0.13396108150482178
In grad_steps = 902, loss = 0.07716076821088791
In grad_steps = 903, loss = 0.08164864778518677
In grad_steps = 904, loss = 0.04631594941020012
In grad_steps = 905, loss = 0.027638673782348633
In grad_steps = 906, loss = 0.015544292517006397
In grad_steps = 907, loss = 0.18418101966381073
In grad_steps = 908, loss = 0.003588913707062602
Beginning epoch 10
In grad_steps = 909, loss = 0.01101777609437704
In grad_steps = 910, loss = 0.006878336891531944
In grad_steps = 911, loss = 0.12288297712802887
In grad_steps = 912, loss = 0.11815091967582703
In grad_steps = 913, loss = 0.08891310542821884
In grad_steps = 914, loss = 0.020817486569285393
In grad_steps = 915, loss = 0.07877258956432343
In grad_steps = 916, loss = 0.006994094233959913
In grad_steps = 917, loss = 0.17117735743522644
In grad_steps = 918, loss = 0.49988484382629395
In grad_steps = 919, loss = 0.25643688440322876
In grad_steps = 920, loss = 0.04160649701952934
In grad_steps = 921, loss = 0.0203762948513031
In grad_steps = 922, loss = 0.5374302864074707
In grad_steps = 923, loss = 0.09853073209524155
In grad_steps = 924, loss = 0.015155596658587456
In grad_steps = 925, loss = 0.03994910418987274
In grad_steps = 926, loss = 0.016063610091805458
In grad_steps = 927, loss = 0.013644251972436905
In grad_steps = 928, loss = 0.010395625606179237
In grad_steps = 929, loss = 0.05786612257361412
In grad_steps = 930, loss = 0.030106641352176666
In grad_steps = 931, loss = 0.18733778595924377
In grad_steps = 932, loss = 0.16251784563064575
In grad_steps = 933, loss = 0.006356788799166679
In grad_steps = 934, loss = 0.015130146406590939
In grad_steps = 935, loss = 0.012730205431580544
In grad_steps = 936, loss = 0.018279636278748512
In grad_steps = 937, loss = 0.02963016927242279
In grad_steps = 938, loss = 0.09642761200666428
In grad_steps = 939, loss = 0.011770191602408886
In grad_steps = 940, loss = 0.013823851943016052
In grad_steps = 941, loss = 0.00746557954698801
In grad_steps = 942, loss = 0.06873738765716553
In grad_steps = 943, loss = 0.0064100101590156555
In grad_steps = 944, loss = 0.06754057109355927
In grad_steps = 945, loss = 0.013279342092573643
In grad_steps = 946, loss = 0.3417910039424896
In grad_steps = 947, loss = 0.0062203723937273026
In grad_steps = 948, loss = 0.009984247386455536
In grad_steps = 949, loss = 0.027143215760588646
In grad_steps = 950, loss = 0.010630562901496887
In grad_steps = 951, loss = 0.016526490449905396
In grad_steps = 952, loss = 0.03698299452662468
In grad_steps = 953, loss = 0.19014087319374084
In grad_steps = 954, loss = 0.012219859287142754
In grad_steps = 955, loss = 0.008154059760272503
In grad_steps = 956, loss = 0.0022298372350633144
In grad_steps = 957, loss = 0.004491619765758514
In grad_steps = 958, loss = 0.005058849696069956
In grad_steps = 959, loss = 0.0021525470074266195
In grad_steps = 960, loss = 0.004866736009716988
In grad_steps = 961, loss = 0.018590889871120453
In grad_steps = 962, loss = 0.02993353269994259
In grad_steps = 963, loss = 0.01916733756661415
In grad_steps = 964, loss = 0.0026652636006474495
In grad_steps = 965, loss = 0.04507428780198097
In grad_steps = 966, loss = 0.0031354764942079782
In grad_steps = 967, loss = 0.014369185082614422
In grad_steps = 968, loss = 0.5223892331123352
In grad_steps = 969, loss = 0.03246646746993065
In grad_steps = 970, loss = 0.00699744513258338
In grad_steps = 971, loss = 0.00469162268564105
In grad_steps = 972, loss = 0.00446223933249712
In grad_steps = 973, loss = 0.6563525199890137
In grad_steps = 974, loss = 0.47009870409965515
In grad_steps = 975, loss = 1.469547152519226
In grad_steps = 976, loss = 0.8773637413978577
In grad_steps = 977, loss = 0.5518286824226379
In grad_steps = 978, loss = 0.6473831534385681
In grad_steps = 979, loss = 0.5671392679214478
In grad_steps = 980, loss = 0.1061956137418747
In grad_steps = 981, loss = 0.11015646904706955
In grad_steps = 982, loss = 0.08336441218852997
In grad_steps = 983, loss = 0.45423898100852966
In grad_steps = 984, loss = 0.03844253718852997
In grad_steps = 985, loss = 0.19094893336296082
In grad_steps = 986, loss = 0.480760782957077
In grad_steps = 987, loss = 0.41494038701057434
In grad_steps = 988, loss = 0.1346025913953781
In grad_steps = 989, loss = 0.10045108199119568
In grad_steps = 990, loss = 0.1800779104232788
In grad_steps = 991, loss = 0.07814206928014755
In grad_steps = 992, loss = 0.0707947239279747
In grad_steps = 993, loss = 0.2274993509054184
In grad_steps = 994, loss = 0.04474334418773651
In grad_steps = 995, loss = 0.02438686229288578
In grad_steps = 996, loss = 0.08670926839113235
In grad_steps = 997, loss = 0.06585138291120529
In grad_steps = 998, loss = 0.3550552427768707
In grad_steps = 999, loss = 0.6513441205024719
In grad_steps = 1000, loss = 0.018597567453980446
In grad_steps = 1001, loss = 0.019049759954214096
In grad_steps = 1002, loss = 0.02790665626525879
In grad_steps = 1003, loss = 0.012728506699204445
In grad_steps = 1004, loss = 0.05508089438080788
In grad_steps = 1005, loss = 0.12332681566476822
In grad_steps = 1006, loss = 0.07715853303670883
In grad_steps = 1007, loss = 0.24394848942756653
In grad_steps = 1008, loss = 0.011563261970877647
In grad_steps = 1009, loss = 0.009140809997916222
Beginning epoch 11
In grad_steps = 1010, loss = 0.026179367676377296
In grad_steps = 1011, loss = 0.01394856907427311
In grad_steps = 1012, loss = 0.01307716779410839
In grad_steps = 1013, loss = 0.16231361031532288
In grad_steps = 1014, loss = 0.08686703443527222
In grad_steps = 1015, loss = 0.0032291554380208254
In grad_steps = 1016, loss = 0.15814326703548431
In grad_steps = 1017, loss = 0.04176848381757736
In grad_steps = 1018, loss = 0.007957630790770054
In grad_steps = 1019, loss = 0.05031964182853699
In grad_steps = 1020, loss = 0.04902079701423645
In grad_steps = 1021, loss = 0.008417332544922829
In grad_steps = 1022, loss = 0.010543746873736382
In grad_steps = 1023, loss = 0.04335392266511917
In grad_steps = 1024, loss = 0.010078882798552513
In grad_steps = 1025, loss = 0.015892356634140015
In grad_steps = 1026, loss = 0.2689516246318817
In grad_steps = 1027, loss = 0.005264639854431152
In grad_steps = 1028, loss = 0.010414938442409039
In grad_steps = 1029, loss = 0.002605087123811245
In grad_steps = 1030, loss = 0.08987215906381607
In grad_steps = 1031, loss = 0.34377703070640564
In grad_steps = 1032, loss = 0.1316080242395401
In grad_steps = 1033, loss = 0.6406856775283813
In grad_steps = 1034, loss = 0.00848831981420517
In grad_steps = 1035, loss = 0.15062202513217926
In grad_steps = 1036, loss = 0.0034274680074304342
In grad_steps = 1037, loss = 0.286539763212204
In grad_steps = 1038, loss = 0.004891430027782917
In grad_steps = 1039, loss = 0.21653325855731964
In grad_steps = 1040, loss = 0.05191466212272644
In grad_steps = 1041, loss = 0.395643949508667
In grad_steps = 1042, loss = 0.21674512326717377
In grad_steps = 1043, loss = 0.01996871829032898
In grad_steps = 1044, loss = 0.02237766981124878
In grad_steps = 1045, loss = 0.015144797042012215
In grad_steps = 1046, loss = 0.0324278362095356
In grad_steps = 1047, loss = 0.09598570317029953
In grad_steps = 1048, loss = 0.15638023614883423
In grad_steps = 1049, loss = 0.038652852177619934
In grad_steps = 1050, loss = 0.22786055505275726
In grad_steps = 1051, loss = 0.49799519777297974
In grad_steps = 1052, loss = 0.3555053174495697
In grad_steps = 1053, loss = 0.04465232416987419
In grad_steps = 1054, loss = 0.04235141724348068
In grad_steps = 1055, loss = 0.06131521612405777
In grad_steps = 1056, loss = 0.048751987516880035
In grad_steps = 1057, loss = 0.017769034951925278
In grad_steps = 1058, loss = 0.0822388082742691
In grad_steps = 1059, loss = 0.0342477522790432
In grad_steps = 1060, loss = 0.02766210027039051
In grad_steps = 1061, loss = 0.08924438804388046
In grad_steps = 1062, loss = 0.10670330375432968
In grad_steps = 1063, loss = 0.023041825741529465
In grad_steps = 1064, loss = 0.5290945172309875
In grad_steps = 1065, loss = 0.1832284778356552
In grad_steps = 1066, loss = 0.41350245475769043
In grad_steps = 1067, loss = 0.1371619552373886
In grad_steps = 1068, loss = 0.01855919510126114
In grad_steps = 1069, loss = 0.07713496685028076
In grad_steps = 1070, loss = 0.28292736411094666
In grad_steps = 1071, loss = 0.527633786201477
In grad_steps = 1072, loss = 0.4139195382595062
In grad_steps = 1073, loss = 0.0918014794588089
In grad_steps = 1074, loss = 0.8407929539680481
In grad_steps = 1075, loss = 0.008495127782225609
In grad_steps = 1076, loss = 0.045457251369953156
In grad_steps = 1077, loss = 0.2915308177471161
In grad_steps = 1078, loss = 0.03279392421245575
In grad_steps = 1079, loss = 0.1851484179496765
In grad_steps = 1080, loss = 0.2246454358100891
In grad_steps = 1081, loss = 0.24896200001239777
In grad_steps = 1082, loss = 0.36018040776252747
In grad_steps = 1083, loss = 0.1890372931957245
In grad_steps = 1084, loss = 0.3015490770339966
In grad_steps = 1085, loss = 0.1485128253698349
In grad_steps = 1086, loss = 0.17586199939250946
In grad_steps = 1087, loss = 0.11384298652410507
In grad_steps = 1088, loss = 0.17697927355766296
In grad_steps = 1089, loss = 0.02275492437183857
In grad_steps = 1090, loss = 0.08069517463445663
In grad_steps = 1091, loss = 0.27548450231552124
In grad_steps = 1092, loss = 0.5708743929862976
In grad_steps = 1093, loss = 0.09780503064393997
In grad_steps = 1094, loss = 0.027972225099802017
In grad_steps = 1095, loss = 0.12058453261852264
In grad_steps = 1096, loss = 0.11405237764120102
In grad_steps = 1097, loss = 0.16270457208156586
In grad_steps = 1098, loss = 0.13295283913612366
In grad_steps = 1099, loss = 0.07632623612880707
In grad_steps = 1100, loss = 0.012535798363387585
In grad_steps = 1101, loss = 0.017204826697707176
In grad_steps = 1102, loss = 0.033937837928533554
In grad_steps = 1103, loss = 0.04129938408732414
In grad_steps = 1104, loss = 0.11262091249227524
In grad_steps = 1105, loss = 0.016290316358208656
In grad_steps = 1106, loss = 0.0054902867414057255
In grad_steps = 1107, loss = 0.016943693161010742
In grad_steps = 1108, loss = 0.019377658143639565
In grad_steps = 1109, loss = 0.00745258666574955
In grad_steps = 1110, loss = 0.0014291085535660386
Beginning epoch 12
In grad_steps = 1111, loss = 0.07862375676631927
In grad_steps = 1112, loss = 0.007490264251828194
In grad_steps = 1113, loss = 0.0032487008720636368
In grad_steps = 1114, loss = 0.06328674405813217
In grad_steps = 1115, loss = 0.007726186886429787
In grad_steps = 1116, loss = 0.4823737144470215
In grad_steps = 1117, loss = 0.008530301973223686
In grad_steps = 1118, loss = 0.014775018207728863
In grad_steps = 1119, loss = 0.000973172893282026
In grad_steps = 1120, loss = 0.06914807111024857
In grad_steps = 1121, loss = 0.010514359921216965
In grad_steps = 1122, loss = 0.00609644316136837
In grad_steps = 1123, loss = 0.011520328931510448
In grad_steps = 1124, loss = 0.013262724503874779
In grad_steps = 1125, loss = 0.021999366581439972
In grad_steps = 1126, loss = 0.5687204003334045
In grad_steps = 1127, loss = 0.43703189492225647
In grad_steps = 1128, loss = 0.17658287286758423
In grad_steps = 1129, loss = 0.0052910614758729935
In grad_steps = 1130, loss = 0.010596789419651031
In grad_steps = 1131, loss = 0.010965203866362572
In grad_steps = 1132, loss = 0.011684531345963478
In grad_steps = 1133, loss = 0.029390506446361542
In grad_steps = 1134, loss = 0.01721631921827793
In grad_steps = 1135, loss = 0.0209505558013916
In grad_steps = 1136, loss = 0.01643681526184082
In grad_steps = 1137, loss = 0.11931193619966507
In grad_steps = 1138, loss = 0.1025707870721817
In grad_steps = 1139, loss = 0.35823380947113037
In grad_steps = 1140, loss = 0.01801091805100441
In grad_steps = 1141, loss = 0.0060690781101584435
In grad_steps = 1142, loss = 0.051773037761449814
In grad_steps = 1143, loss = 0.07033594697713852
In grad_steps = 1144, loss = 0.11217208951711655
In grad_steps = 1145, loss = 0.008850225247442722
In grad_steps = 1146, loss = 0.028895463794469833
In grad_steps = 1147, loss = 0.027998963370919228
In grad_steps = 1148, loss = 0.18416669964790344
In grad_steps = 1149, loss = 0.09367348998785019
In grad_steps = 1150, loss = 0.03605975955724716
In grad_steps = 1151, loss = 0.15973177552223206
In grad_steps = 1152, loss = 0.28175032138824463
In grad_steps = 1153, loss = 0.014494940638542175
In grad_steps = 1154, loss = 0.036310743540525436
In grad_steps = 1155, loss = 0.04661010950803757
In grad_steps = 1156, loss = 0.00882655568420887
In grad_steps = 1157, loss = 0.03981972113251686
In grad_steps = 1158, loss = 0.003830237779766321
In grad_steps = 1159, loss = 0.03245038166642189
In grad_steps = 1160, loss = 0.005242199636995792
In grad_steps = 1161, loss = 0.036432284861803055
In grad_steps = 1162, loss = 0.007043658755719662
In grad_steps = 1163, loss = 0.01310168206691742
In grad_steps = 1164, loss = 0.00817393884062767
In grad_steps = 1165, loss = 0.1349487155675888
In grad_steps = 1166, loss = 0.02736346237361431
In grad_steps = 1167, loss = 0.03007117658853531
In grad_steps = 1168, loss = 0.008641927503049374
In grad_steps = 1169, loss = 0.450489342212677
In grad_steps = 1170, loss = 0.026095189154148102
In grad_steps = 1171, loss = 0.03332192823290825
In grad_steps = 1172, loss = 0.03816969692707062
In grad_steps = 1173, loss = 0.0015183890936896205
In grad_steps = 1174, loss = 0.0505368672311306
In grad_steps = 1175, loss = 0.35282567143440247
In grad_steps = 1176, loss = 0.0019259426044300199
In grad_steps = 1177, loss = 0.006730744615197182
In grad_steps = 1178, loss = 0.1898275464773178
In grad_steps = 1179, loss = 0.006071910727769136
In grad_steps = 1180, loss = 0.006910894066095352
In grad_steps = 1181, loss = 0.32506781816482544
In grad_steps = 1182, loss = 0.04439312219619751
In grad_steps = 1183, loss = 0.04599776118993759
In grad_steps = 1184, loss = 0.011566121131181717
In grad_steps = 1185, loss = 0.013039413839578629
In grad_steps = 1186, loss = 0.06701233983039856
In grad_steps = 1187, loss = 0.18844392895698547
In grad_steps = 1188, loss = 0.0330069325864315
In grad_steps = 1189, loss = 0.006944235414266586
In grad_steps = 1190, loss = 0.0012057253625243902
In grad_steps = 1191, loss = 0.0014939290704205632
In grad_steps = 1192, loss = 0.0124387601390481
In grad_steps = 1193, loss = 0.014409422874450684
In grad_steps = 1194, loss = 0.03576083108782768
In grad_steps = 1195, loss = 0.0004130515444558114
In grad_steps = 1196, loss = 0.2981581389904022
In grad_steps = 1197, loss = 0.4480486810207367
In grad_steps = 1198, loss = 0.5974857807159424
In grad_steps = 1199, loss = 0.0024386211298406124
In grad_steps = 1200, loss = 0.044798970222473145
In grad_steps = 1201, loss = 0.05578695237636566
In grad_steps = 1202, loss = 0.6683907508850098
In grad_steps = 1203, loss = 0.5167962908744812
In grad_steps = 1204, loss = 0.011717269197106361
In grad_steps = 1205, loss = 0.02249707654118538
In grad_steps = 1206, loss = 0.026508329436182976
In grad_steps = 1207, loss = 0.017289424315094948
In grad_steps = 1208, loss = 0.013240622356534004
In grad_steps = 1209, loss = 0.018751436844468117
In grad_steps = 1210, loss = 0.02331017516553402
In grad_steps = 1211, loss = 0.008769090287387371
Elapsed time: 2139.020569562912 seconds for ensemble 1 with 12 epochs
LoRA instance 1 evaluation complete. Data saved to /pscratch/sd/t/tianle/lucid/other_source/SURP_2024/results/tmp_lora_ensemble/Llama3-8B-set-1.1/test_data_instance_1_seed_10094.npz.
lora instance i = 1 Successfully finished.
Training lora instance 2
DEBUG: Total parameters: 8037076992
DEBUG: Trainable parameters: 6815744
Beginning epoch 1
In grad_steps = 0, loss = 1.6091442108154297
In grad_steps = 1, loss = 0.9626079797744751
In grad_steps = 2, loss = 0.704777717590332
In grad_steps = 3, loss = 0.7464417219161987
In grad_steps = 4, loss = 1.195509910583496
In grad_steps = 5, loss = 0.4269177317619324
In grad_steps = 6, loss = 0.871624767780304
In grad_steps = 7, loss = 0.6933709979057312
In grad_steps = 8, loss = 0.8064722418785095
In grad_steps = 9, loss = 0.6538984775543213
In grad_steps = 10, loss = 0.7072902917861938
In grad_steps = 11, loss = 0.7027340531349182
In grad_steps = 12, loss = 0.7003489136695862
In grad_steps = 13, loss = 0.6378198266029358
In grad_steps = 14, loss = 0.7467021942138672
In grad_steps = 15, loss = 0.8341699838638306
In grad_steps = 16, loss = 0.7016894817352295
In grad_steps = 17, loss = 0.6830189824104309
In grad_steps = 18, loss = 0.695656418800354
In grad_steps = 19, loss = 0.7121304869651794
In grad_steps = 20, loss = 0.6762863993644714
In grad_steps = 21, loss = 0.7103642225265503
In grad_steps = 22, loss = 0.6787893772125244
In grad_steps = 23, loss = 0.706484317779541
In grad_steps = 24, loss = 0.7131320238113403
In grad_steps = 25, loss = 0.7524364590644836
In grad_steps = 26, loss = 0.6457366943359375
In grad_steps = 27, loss = 0.732415497303009
In grad_steps = 28, loss = 0.6564602851867676
In grad_steps = 29, loss = 0.7069055438041687
In grad_steps = 30, loss = 0.6825308799743652
In grad_steps = 31, loss = 0.6931843757629395
In grad_steps = 32, loss = 0.7427778840065002
In grad_steps = 33, loss = 0.6832053661346436
In grad_steps = 34, loss = 0.7925008535385132
In grad_steps = 35, loss = 0.6901129484176636
In grad_steps = 36, loss = 0.7281303405761719
In grad_steps = 37, loss = 0.6760427951812744
In grad_steps = 38, loss = 0.736045241355896
In grad_steps = 39, loss = 0.702405571937561
In grad_steps = 40, loss = 0.6920119524002075
In grad_steps = 41, loss = 0.7067201137542725
In grad_steps = 42, loss = 0.6964858770370483
In grad_steps = 43, loss = 0.6827147006988525
In grad_steps = 44, loss = 0.6903395056724548
In grad_steps = 45, loss = 0.6802799105644226
In grad_steps = 46, loss = 0.7045192718505859
In grad_steps = 47, loss = 0.7289692759513855
In grad_steps = 48, loss = 0.6849849820137024
In grad_steps = 49, loss = 0.6773174405097961
In grad_steps = 50, loss = 0.6663200259208679
In grad_steps = 51, loss = 0.7186412811279297
In grad_steps = 52, loss = 0.7262769341468811
In grad_steps = 53, loss = 0.6576552391052246
In grad_steps = 54, loss = 0.574283242225647
In grad_steps = 55, loss = 0.6092699766159058
In grad_steps = 56, loss = 0.569878876209259
In grad_steps = 57, loss = 0.5618203282356262
In grad_steps = 58, loss = 0.7481029629707336
In grad_steps = 59, loss = 0.6879041790962219
In grad_steps = 60, loss = 0.8009719848632812
In grad_steps = 61, loss = 0.6707591414451599
In grad_steps = 62, loss = 0.9897249341011047
In grad_steps = 63, loss = 0.7636917233467102
In grad_steps = 64, loss = 0.5870150923728943
In grad_steps = 65, loss = 0.5762726068496704
In grad_steps = 66, loss = 0.8154066801071167
In grad_steps = 67, loss = 0.66020667552948
In grad_steps = 68, loss = 0.7359225749969482
In grad_steps = 69, loss = 0.6826886534690857
In grad_steps = 70, loss = 0.7216418385505676
In grad_steps = 71, loss = 0.7007955312728882
In grad_steps = 72, loss = 0.6827012300491333
In grad_steps = 73, loss = 0.713283896446228
In grad_steps = 74, loss = 0.6687237620353699
In grad_steps = 75, loss = 0.6735690832138062
In grad_steps = 76, loss = 0.6566084623336792
In grad_steps = 77, loss = 1.0496838092803955
In grad_steps = 78, loss = 0.8154228329658508
In grad_steps = 79, loss = 0.6359598636627197
In grad_steps = 80, loss = 0.6799355745315552
In grad_steps = 81, loss = 0.6721610426902771
In grad_steps = 82, loss = 0.7146240472793579
In grad_steps = 83, loss = 0.7103579640388489
In grad_steps = 84, loss = 0.7213493585586548
In grad_steps = 85, loss = 0.7088746428489685
In grad_steps = 86, loss = 0.6445389986038208
In grad_steps = 87, loss = 0.6873583197593689
In grad_steps = 88, loss = 0.7115454077720642
In grad_steps = 89, loss = 0.67939692735672
In grad_steps = 90, loss = 0.7034456729888916
In grad_steps = 91, loss = 0.6564930081367493
In grad_steps = 92, loss = 0.6760820150375366
In grad_steps = 93, loss = 0.7247509360313416
In grad_steps = 94, loss = 0.7472859025001526
In grad_steps = 95, loss = 0.6528846025466919
In grad_steps = 96, loss = 0.8684064745903015
In grad_steps = 97, loss = 0.7666165828704834
In grad_steps = 98, loss = 0.6444017291069031
In grad_steps = 99, loss = 0.6251927614212036
In grad_steps = 100, loss = 0.6904903650283813
Beginning epoch 2
In grad_steps = 101, loss = 0.641327977180481
In grad_steps = 102, loss = 0.6460093855857849
In grad_steps = 103, loss = 0.662616491317749
In grad_steps = 104, loss = 0.6824257969856262
In grad_steps = 105, loss = 0.7100009918212891
In grad_steps = 106, loss = 0.6256827116012573
In grad_steps = 107, loss = 0.6502179503440857
In grad_steps = 108, loss = 0.6393888592720032
In grad_steps = 109, loss = 0.7041758894920349
In grad_steps = 110, loss = 0.6416774988174438
In grad_steps = 111, loss = 0.7286404967308044
In grad_steps = 112, loss = 0.6563345193862915
In grad_steps = 113, loss = 0.7052196860313416
In grad_steps = 114, loss = 0.7406546473503113
In grad_steps = 115, loss = 0.703366219997406
In grad_steps = 116, loss = 0.6550811529159546
In grad_steps = 117, loss = 0.6435568332672119
In grad_steps = 118, loss = 0.6628132462501526
In grad_steps = 119, loss = 0.7119646072387695
In grad_steps = 120, loss = 0.6659733057022095
In grad_steps = 121, loss = 0.5940977334976196
In grad_steps = 122, loss = 0.6959447860717773
In grad_steps = 123, loss = 0.7249764204025269
In grad_steps = 124, loss = 0.7036938667297363
In grad_steps = 125, loss = 0.6234477758407593
In grad_steps = 126, loss = 0.5542192459106445
In grad_steps = 127, loss = 0.7078903913497925
In grad_steps = 128, loss = 0.6570890545845032
In grad_steps = 129, loss = 0.5382009148597717
In grad_steps = 130, loss = 0.7153782248497009
In grad_steps = 131, loss = 0.5601925253868103
In grad_steps = 132, loss = 0.7067832946777344
In grad_steps = 133, loss = 0.871813952922821
In grad_steps = 134, loss = 0.6858243346214294
In grad_steps = 135, loss = 0.8155404925346375
In grad_steps = 136, loss = 0.6808047890663147
In grad_steps = 137, loss = 0.6623870730400085
In grad_steps = 138, loss = 0.612129807472229
In grad_steps = 139, loss = 0.7627750635147095
In grad_steps = 140, loss = 0.8296009302139282
In grad_steps = 141, loss = 0.603705644607544
In grad_steps = 142, loss = 0.6727787852287292
In grad_steps = 143, loss = 0.6761952042579651
In grad_steps = 144, loss = 0.6925795078277588
In grad_steps = 145, loss = 0.6519620418548584
In grad_steps = 146, loss = 0.6321658492088318
In grad_steps = 147, loss = 0.6493695974349976
In grad_steps = 148, loss = 0.6175755858421326
In grad_steps = 149, loss = 0.6101142764091492
In grad_steps = 150, loss = 0.653664231300354
In grad_steps = 151, loss = 0.585464596748352
In grad_steps = 152, loss = 0.7447664141654968
In grad_steps = 153, loss = 0.7445386052131653
In grad_steps = 154, loss = 0.6211686134338379
In grad_steps = 155, loss = 0.5416595935821533
In grad_steps = 156, loss = 0.581653892993927
In grad_steps = 157, loss = 0.5005164742469788
In grad_steps = 158, loss = 0.4950844943523407
In grad_steps = 159, loss = 0.7589815855026245
In grad_steps = 160, loss = 0.7274895310401917
In grad_steps = 161, loss = 0.7858651876449585
In grad_steps = 162, loss = 0.6137658953666687
In grad_steps = 163, loss = 0.8593254089355469
In grad_steps = 164, loss = 0.7007172107696533
In grad_steps = 165, loss = 0.5762763023376465
In grad_steps = 166, loss = 0.5627242922782898
In grad_steps = 167, loss = 0.662165641784668
In grad_steps = 168, loss = 0.6375792026519775
In grad_steps = 169, loss = 0.6888797283172607
In grad_steps = 170, loss = 0.7257300615310669
In grad_steps = 171, loss = 0.5924211144447327
In grad_steps = 172, loss = 0.6831236481666565
In grad_steps = 173, loss = 0.6731936931610107
In grad_steps = 174, loss = 0.6978744864463806
In grad_steps = 175, loss = 0.6707673072814941
In grad_steps = 176, loss = 0.6460015773773193
In grad_steps = 177, loss = 0.6207820177078247
In grad_steps = 178, loss = 0.864789605140686
In grad_steps = 179, loss = 0.7305420637130737
In grad_steps = 180, loss = 0.5465192794799805
In grad_steps = 181, loss = 0.5697188377380371
In grad_steps = 182, loss = 0.6494334936141968
In grad_steps = 183, loss = 0.7346827387809753
In grad_steps = 184, loss = 0.7116555571556091
In grad_steps = 185, loss = 0.7159080505371094
In grad_steps = 186, loss = 0.6872364282608032
In grad_steps = 187, loss = 0.5126817226409912
In grad_steps = 188, loss = 0.6534708738327026
In grad_steps = 189, loss = 0.6764683723449707
In grad_steps = 190, loss = 0.6930549740791321
In grad_steps = 191, loss = 0.6385458111763
In grad_steps = 192, loss = 0.570685625076294
In grad_steps = 193, loss = 0.5935720205307007
In grad_steps = 194, loss = 0.7097861170768738
In grad_steps = 195, loss = 0.8056204915046692
In grad_steps = 196, loss = 0.5843772292137146
In grad_steps = 197, loss = 0.5750016570091248
In grad_steps = 198, loss = 0.6715711951255798
In grad_steps = 199, loss = 0.479946494102478
In grad_steps = 200, loss = 0.6175251603126526
In grad_steps = 201, loss = 0.7038804888725281
Beginning epoch 3
In grad_steps = 202, loss = 0.6142069101333618
In grad_steps = 203, loss = 0.5381221771240234
In grad_steps = 204, loss = 0.667077362537384
In grad_steps = 205, loss = 0.7316458821296692
In grad_steps = 206, loss = 0.657801628112793
In grad_steps = 207, loss = 0.561805009841919
In grad_steps = 208, loss = 0.6069888472557068
In grad_steps = 209, loss = 0.5105074048042297
In grad_steps = 210, loss = 0.7741531729698181
In grad_steps = 211, loss = 0.541338324546814
In grad_steps = 212, loss = 0.760843813419342
In grad_steps = 213, loss = 0.6274706125259399
In grad_steps = 214, loss = 0.6826621294021606
In grad_steps = 215, loss = 0.7700504064559937
In grad_steps = 216, loss = 0.6834732294082642
In grad_steps = 217, loss = 0.6219354271888733
In grad_steps = 218, loss = 0.6741718053817749
In grad_steps = 219, loss = 0.6259652972221375
In grad_steps = 220, loss = 0.5964779853820801
In grad_steps = 221, loss = 0.5795657634735107
In grad_steps = 222, loss = 0.5401943325996399
In grad_steps = 223, loss = 0.6626046299934387
In grad_steps = 224, loss = 0.7564217448234558
In grad_steps = 225, loss = 0.6880812048912048
In grad_steps = 226, loss = 0.5463261008262634
In grad_steps = 227, loss = 0.4182226359844208
In grad_steps = 228, loss = 0.6527785658836365
In grad_steps = 229, loss = 0.6962392330169678
In grad_steps = 230, loss = 0.3949543833732605
In grad_steps = 231, loss = 0.6023133397102356
In grad_steps = 232, loss = 0.49289363622665405
In grad_steps = 233, loss = 0.6603851914405823
In grad_steps = 234, loss = 0.5901669859886169
In grad_steps = 235, loss = 0.5482145547866821
In grad_steps = 236, loss = 0.729729950428009
In grad_steps = 237, loss = 0.40644726157188416
In grad_steps = 238, loss = 0.5357397198677063
In grad_steps = 239, loss = 0.8009396195411682
In grad_steps = 240, loss = 1.056518793106079
In grad_steps = 241, loss = 1.0095832347869873
In grad_steps = 242, loss = 0.5298687219619751
In grad_steps = 243, loss = 0.8251304626464844
In grad_steps = 244, loss = 0.6327999830245972
In grad_steps = 245, loss = 0.6944190859794617
In grad_steps = 246, loss = 0.5938939452171326
In grad_steps = 247, loss = 0.5321231484413147
In grad_steps = 248, loss = 0.6222390532493591
In grad_steps = 249, loss = 0.5646783709526062
In grad_steps = 250, loss = 0.548510730266571
In grad_steps = 251, loss = 0.5793155431747437
In grad_steps = 252, loss = 0.587056040763855
In grad_steps = 253, loss = 0.6328569054603577
In grad_steps = 254, loss = 0.7220751643180847
In grad_steps = 255, loss = 0.586104154586792
In grad_steps = 256, loss = 0.5653538107872009
In grad_steps = 257, loss = 0.5029996037483215
In grad_steps = 258, loss = 0.433083176612854
In grad_steps = 259, loss = 0.4259517192840576
In grad_steps = 260, loss = 1.1330342292785645
In grad_steps = 261, loss = 0.8318770527839661
In grad_steps = 262, loss = 0.7494520545005798
In grad_steps = 263, loss = 0.5307186841964722
In grad_steps = 264, loss = 0.6425381898880005
In grad_steps = 265, loss = 0.6501553058624268
In grad_steps = 266, loss = 0.6584457159042358
In grad_steps = 267, loss = 0.707579493522644
In grad_steps = 268, loss = 0.589104950428009
In grad_steps = 269, loss = 0.6498319506645203
In grad_steps = 270, loss = 0.6268518567085266
In grad_steps = 271, loss = 0.6797062158584595
In grad_steps = 272, loss = 0.6167458295822144
In grad_steps = 273, loss = 0.665221095085144
In grad_steps = 274, loss = 0.6619322299957275
In grad_steps = 275, loss = 0.6427806615829468
In grad_steps = 276, loss = 0.6395566463470459
In grad_steps = 277, loss = 0.6155571341514587
In grad_steps = 278, loss = 0.5892043709754944
In grad_steps = 279, loss = 0.7986213564872742
In grad_steps = 280, loss = 0.7030052542686462
In grad_steps = 281, loss = 0.4806434214115143
In grad_steps = 282, loss = 0.5402839183807373
In grad_steps = 283, loss = 0.6377477645874023
In grad_steps = 284, loss = 0.7302308678627014
In grad_steps = 285, loss = 0.6694298386573792
In grad_steps = 286, loss = 0.638748288154602
In grad_steps = 287, loss = 0.6306548118591309
In grad_steps = 288, loss = 0.43935176730155945
In grad_steps = 289, loss = 0.6082394123077393
In grad_steps = 290, loss = 0.6224698424339294
In grad_steps = 291, loss = 0.6264665722846985
In grad_steps = 292, loss = 0.5464312434196472
In grad_steps = 293, loss = 0.44222620129585266
In grad_steps = 294, loss = 0.5569983720779419
In grad_steps = 295, loss = 0.7076900005340576
In grad_steps = 296, loss = 0.5440801382064819
In grad_steps = 297, loss = 0.5800856351852417
In grad_steps = 298, loss = 0.5050704479217529
In grad_steps = 299, loss = 0.5236791372299194
In grad_steps = 300, loss = 0.812919020652771
In grad_steps = 301, loss = 0.8031229376792908
In grad_steps = 302, loss = 0.6280214190483093
Beginning epoch 4
In grad_steps = 303, loss = 0.6106643676757812
In grad_steps = 304, loss = 0.5260254740715027
In grad_steps = 305, loss = 0.6310328841209412
In grad_steps = 306, loss = 0.6241250038146973
In grad_steps = 307, loss = 0.6748166084289551
In grad_steps = 308, loss = 0.5063141584396362
In grad_steps = 309, loss = 0.612883448600769
In grad_steps = 310, loss = 0.5234006643295288
In grad_steps = 311, loss = 0.7967272400856018
In grad_steps = 312, loss = 0.5339471101760864
In grad_steps = 313, loss = 0.756941556930542
In grad_steps = 314, loss = 0.5698346495628357
In grad_steps = 315, loss = 0.6387549042701721
In grad_steps = 316, loss = 0.7219718098640442
In grad_steps = 317, loss = 0.6892188787460327
In grad_steps = 318, loss = 0.678555965423584
In grad_steps = 319, loss = 0.6436152458190918
In grad_steps = 320, loss = 0.6120216250419617
In grad_steps = 321, loss = 0.5031067132949829
In grad_steps = 322, loss = 0.5180061459541321
In grad_steps = 323, loss = 0.4655894637107849
In grad_steps = 324, loss = 0.5918518900871277
In grad_steps = 325, loss = 0.6733543276786804
In grad_steps = 326, loss = 0.699817419052124
In grad_steps = 327, loss = 0.3915383219718933
In grad_steps = 328, loss = 0.4592948257923126
In grad_steps = 329, loss = 0.40769708156585693
In grad_steps = 330, loss = 0.8607311248779297
In grad_steps = 331, loss = 0.25461599230766296
In grad_steps = 332, loss = 0.43100714683532715
In grad_steps = 333, loss = 0.3210641145706177
In grad_steps = 334, loss = 0.5929381847381592
In grad_steps = 335, loss = 0.40460264682769775
In grad_steps = 336, loss = 0.3807413876056671
In grad_steps = 337, loss = 0.5225459337234497
In grad_steps = 338, loss = 0.2987639009952545
In grad_steps = 339, loss = 0.17340900003910065
In grad_steps = 340, loss = 0.10332036018371582
In grad_steps = 341, loss = 0.49124470353126526
In grad_steps = 342, loss = 0.4820772409439087
In grad_steps = 343, loss = 0.31108397245407104
In grad_steps = 344, loss = 0.7415921688079834
In grad_steps = 345, loss = 0.9034005403518677
In grad_steps = 346, loss = 0.8765044212341309
In grad_steps = 347, loss = 0.3044394552707672
In grad_steps = 348, loss = 0.4665042459964752
In grad_steps = 349, loss = 0.4401949346065521
In grad_steps = 350, loss = 0.4168148934841156
In grad_steps = 351, loss = 0.3963671624660492
In grad_steps = 352, loss = 0.46044087409973145
In grad_steps = 353, loss = 0.5502170324325562
In grad_steps = 354, loss = 0.6383649110794067
In grad_steps = 355, loss = 0.6447826623916626
In grad_steps = 356, loss = 0.5640919804573059
In grad_steps = 357, loss = 0.5643571019172668
In grad_steps = 358, loss = 0.4165785312652588
In grad_steps = 359, loss = 0.419731080532074
In grad_steps = 360, loss = 0.33722278475761414
In grad_steps = 361, loss = 0.8861622214317322
In grad_steps = 362, loss = 0.7288070321083069
In grad_steps = 363, loss = 0.8392398357391357
In grad_steps = 364, loss = 0.47410789132118225
In grad_steps = 365, loss = 0.6057092547416687
In grad_steps = 366, loss = 0.487430214881897
In grad_steps = 367, loss = 0.6458491086959839
In grad_steps = 368, loss = 0.6170106530189514
In grad_steps = 369, loss = 0.48336368799209595
In grad_steps = 370, loss = 0.7082085609436035
In grad_steps = 371, loss = 0.6156107783317566
In grad_steps = 372, loss = 0.7187787294387817
In grad_steps = 373, loss = 0.513855516910553
In grad_steps = 374, loss = 0.5654494166374207
In grad_steps = 375, loss = 0.665523886680603
In grad_steps = 376, loss = 0.5731242895126343
In grad_steps = 377, loss = 0.6444241404533386
In grad_steps = 378, loss = 0.5642856359481812
In grad_steps = 379, loss = 0.5545140504837036
In grad_steps = 380, loss = 0.6644386053085327
In grad_steps = 381, loss = 0.5858885645866394
In grad_steps = 382, loss = 0.4281921982765198
In grad_steps = 383, loss = 0.37368184328079224
In grad_steps = 384, loss = 0.5203850269317627
In grad_steps = 385, loss = 0.684009313583374
In grad_steps = 386, loss = 0.5494353771209717
In grad_steps = 387, loss = 0.4383167326450348
In grad_steps = 388, loss = 0.47659027576446533
In grad_steps = 389, loss = 0.2539987862110138
In grad_steps = 390, loss = 0.36615702509880066
In grad_steps = 391, loss = 0.5826982259750366
In grad_steps = 392, loss = 0.3353860378265381
In grad_steps = 393, loss = 0.3556399345397949
In grad_steps = 394, loss = 0.17565663158893585
In grad_steps = 395, loss = 0.3687817454338074
In grad_steps = 396, loss = 1.07453191280365
In grad_steps = 397, loss = 0.8913892507553101
In grad_steps = 398, loss = 0.4022337794303894
In grad_steps = 399, loss = 0.4293517470359802
In grad_steps = 400, loss = 0.32819700241088867
In grad_steps = 401, loss = 0.7732974886894226
In grad_steps = 402, loss = 0.8123147487640381
In grad_steps = 403, loss = 0.3153950870037079
Beginning epoch 5
In grad_steps = 404, loss = 0.5446298718452454
In grad_steps = 405, loss = 0.4155142903327942
In grad_steps = 406, loss = 0.6103963255882263
In grad_steps = 407, loss = 0.7063783407211304
In grad_steps = 408, loss = 0.9461312294006348
In grad_steps = 409, loss = 0.47510093450546265
In grad_steps = 410, loss = 0.5982415080070496
In grad_steps = 411, loss = 0.5048852562904358
In grad_steps = 412, loss = 0.7884309887886047
In grad_steps = 413, loss = 0.4202360510826111
In grad_steps = 414, loss = 0.6855413913726807
In grad_steps = 415, loss = 0.5153412222862244
In grad_steps = 416, loss = 0.5325168967247009
In grad_steps = 417, loss = 0.7013741135597229
In grad_steps = 418, loss = 0.7418500185012817
In grad_steps = 419, loss = 0.7674080729484558
In grad_steps = 420, loss = 0.6371654272079468
In grad_steps = 421, loss = 0.5785275101661682
In grad_steps = 422, loss = 0.5025382041931152
In grad_steps = 423, loss = 0.4949806034564972
In grad_steps = 424, loss = 0.27386918663978577
In grad_steps = 425, loss = 0.6640547513961792
In grad_steps = 426, loss = 0.48652219772338867
In grad_steps = 427, loss = 0.619886040687561
In grad_steps = 428, loss = 0.23049788177013397
In grad_steps = 429, loss = 0.4593025743961334
In grad_steps = 430, loss = 0.3592311441898346
In grad_steps = 431, loss = 0.6083320379257202
In grad_steps = 432, loss = 0.23451673984527588
In grad_steps = 433, loss = 0.3666001856327057
In grad_steps = 434, loss = 0.1541159301996231
In grad_steps = 435, loss = 0.6760277152061462
In grad_steps = 436, loss = 0.6914805173873901
In grad_steps = 437, loss = 0.21308544278144836
In grad_steps = 438, loss = 0.23216229677200317
In grad_steps = 439, loss = 0.28944236040115356
In grad_steps = 440, loss = 0.15157008171081543
In grad_steps = 441, loss = 0.12254409492015839
In grad_steps = 442, loss = 0.4511793255805969
In grad_steps = 443, loss = 0.4146377444267273
In grad_steps = 444, loss = 0.12448173016309738
In grad_steps = 445, loss = 0.12140612304210663
In grad_steps = 446, loss = 0.15310020744800568
In grad_steps = 447, loss = 0.25656503438949585
In grad_steps = 448, loss = 0.13926826417446136
In grad_steps = 449, loss = 0.08560985326766968
In grad_steps = 450, loss = 0.24022972583770752
In grad_steps = 451, loss = 0.09109824895858765
In grad_steps = 452, loss = 0.02737680822610855
In grad_steps = 453, loss = 0.9951767921447754
In grad_steps = 454, loss = 0.8320952653884888
In grad_steps = 455, loss = 0.756020188331604
In grad_steps = 456, loss = 0.8942989706993103
In grad_steps = 457, loss = 0.5451459884643555
In grad_steps = 458, loss = 0.3209856152534485
In grad_steps = 459, loss = 0.263056218624115
In grad_steps = 460, loss = 0.3920590877532959
In grad_steps = 461, loss = 0.2953777313232422
In grad_steps = 462, loss = 0.819373369216919
In grad_steps = 463, loss = 0.6068876385688782
In grad_steps = 464, loss = 0.8961105942726135
In grad_steps = 465, loss = 0.46325069665908813
In grad_steps = 466, loss = 0.5964449048042297
In grad_steps = 467, loss = 0.5231715440750122
In grad_steps = 468, loss = 0.6669241786003113
In grad_steps = 469, loss = 0.705690860748291
In grad_steps = 470, loss = 0.5120998024940491
In grad_steps = 471, loss = 0.7022486329078674
In grad_steps = 472, loss = 0.6384580135345459
In grad_steps = 473, loss = 0.697356641292572
In grad_steps = 474, loss = 0.5409531593322754
In grad_steps = 475, loss = 0.6051251292228699
In grad_steps = 476, loss = 0.6832879185676575
In grad_steps = 477, loss = 0.5563173890113831
In grad_steps = 478, loss = 0.672784686088562
In grad_steps = 479, loss = 0.4849485158920288
In grad_steps = 480, loss = 0.6561816930770874
In grad_steps = 481, loss = 0.6200696229934692
In grad_steps = 482, loss = 0.543095052242279
In grad_steps = 483, loss = 0.4121707081794739
In grad_steps = 484, loss = 0.37373849749565125
In grad_steps = 485, loss = 0.5392526388168335
In grad_steps = 486, loss = 0.6735360622406006
In grad_steps = 487, loss = 0.6346157789230347
In grad_steps = 488, loss = 0.47088178992271423
In grad_steps = 489, loss = 0.4710879921913147
In grad_steps = 490, loss = 0.23656989634037018
In grad_steps = 491, loss = 0.5821760296821594
In grad_steps = 492, loss = 0.4819168448448181
In grad_steps = 493, loss = 0.3362705111503601
In grad_steps = 494, loss = 0.35749268531799316
In grad_steps = 495, loss = 0.20515985786914825
In grad_steps = 496, loss = 0.3820086717605591
In grad_steps = 497, loss = 0.36833804845809937
In grad_steps = 498, loss = 0.177259162068367
In grad_steps = 499, loss = 0.33140885829925537
In grad_steps = 500, loss = 0.33083418011665344
In grad_steps = 501, loss = 0.2694161534309387
In grad_steps = 502, loss = 0.28504225611686707
In grad_steps = 503, loss = 0.7453715801239014
In grad_steps = 504, loss = 0.04806385561823845
Beginning epoch 6
In grad_steps = 505, loss = 0.8637316226959229
In grad_steps = 506, loss = 0.37634432315826416
In grad_steps = 507, loss = 0.27929332852363586
In grad_steps = 508, loss = 1.0241683721542358
In grad_steps = 509, loss = 0.7467159032821655
In grad_steps = 510, loss = 0.661433756351471
In grad_steps = 511, loss = 0.7633653879165649
In grad_steps = 512, loss = 0.4090665876865387
In grad_steps = 513, loss = 0.3701191842556
In grad_steps = 514, loss = 0.4053140878677368
In grad_steps = 515, loss = 0.563060462474823
In grad_steps = 516, loss = 0.44695597887039185
In grad_steps = 517, loss = 0.43016359210014343
In grad_steps = 518, loss = 0.5719924569129944
In grad_steps = 519, loss = 0.5954297184944153
In grad_steps = 520, loss = 0.5955552458763123
In grad_steps = 521, loss = 0.5134915709495544
In grad_steps = 522, loss = 0.5107795596122742
In grad_steps = 523, loss = 0.5119991302490234
In grad_steps = 524, loss = 0.3311035931110382
In grad_steps = 525, loss = 0.2235715538263321
In grad_steps = 526, loss = 0.4564912021160126
In grad_steps = 527, loss = 0.4508993923664093
In grad_steps = 528, loss = 0.3202035427093506
In grad_steps = 529, loss = 0.15140940248966217
In grad_steps = 530, loss = 0.26404938101768494
In grad_steps = 531, loss = 0.1762426346540451
In grad_steps = 532, loss = 0.4066840410232544
In grad_steps = 533, loss = 0.27146732807159424
In grad_steps = 534, loss = 0.10563632100820541
In grad_steps = 535, loss = 0.12506337463855743
In grad_steps = 536, loss = 0.16274723410606384
In grad_steps = 537, loss = 0.127707377076149
In grad_steps = 538, loss = 0.05572570860385895
In grad_steps = 539, loss = 0.6716203093528748
In grad_steps = 540, loss = 0.08715885132551193
In grad_steps = 541, loss = 0.011572840623557568
In grad_steps = 542, loss = 0.01979994960129261
In grad_steps = 543, loss = 0.08058762550354004
In grad_steps = 544, loss = 0.13500940799713135
In grad_steps = 545, loss = 0.11625541001558304
In grad_steps = 546, loss = 0.2639369070529938
In grad_steps = 547, loss = 0.03745950013399124
In grad_steps = 548, loss = 0.2338099628686905
In grad_steps = 549, loss = 0.15426796674728394
In grad_steps = 550, loss = 0.005845004227012396
In grad_steps = 551, loss = 0.04458951950073242
In grad_steps = 552, loss = 0.2136542797088623
In grad_steps = 553, loss = 0.13398556411266327
In grad_steps = 554, loss = 0.40048110485076904
In grad_steps = 555, loss = 0.0062691643834114075
In grad_steps = 556, loss = 0.4172016978263855
In grad_steps = 557, loss = 0.8645754456520081
In grad_steps = 558, loss = 0.16752353310585022
In grad_steps = 559, loss = 1.085588812828064
In grad_steps = 560, loss = 0.31554222106933594
In grad_steps = 561, loss = 0.5051690936088562
In grad_steps = 562, loss = 0.2871973216533661
In grad_steps = 563, loss = 0.41613292694091797
In grad_steps = 564, loss = 0.6049674153327942
In grad_steps = 565, loss = 0.7006138563156128
In grad_steps = 566, loss = 0.29504480957984924
In grad_steps = 567, loss = 0.6939651966094971
In grad_steps = 568, loss = 0.46738213300704956
In grad_steps = 569, loss = 0.6356536746025085
In grad_steps = 570, loss = 0.32503408193588257
In grad_steps = 571, loss = 0.540945291519165
In grad_steps = 572, loss = 0.7078208327293396
In grad_steps = 573, loss = 0.5389328598976135
In grad_steps = 574, loss = 0.5909503698348999
In grad_steps = 575, loss = 0.5027941465377808
In grad_steps = 576, loss = 0.4306476414203644
In grad_steps = 577, loss = 0.5344631671905518
In grad_steps = 578, loss = 0.47795525193214417
In grad_steps = 579, loss = 0.6572670340538025
In grad_steps = 580, loss = 0.3738722503185272
In grad_steps = 581, loss = 0.5169625282287598
In grad_steps = 582, loss = 0.6017768383026123
In grad_steps = 583, loss = 0.4717913269996643
In grad_steps = 584, loss = 0.275246798992157
In grad_steps = 585, loss = 0.271562397480011
In grad_steps = 586, loss = 0.42570802569389343
In grad_steps = 587, loss = 0.43176260590553284
In grad_steps = 588, loss = 0.7178030014038086
In grad_steps = 589, loss = 0.846265435218811
In grad_steps = 590, loss = 0.15993018448352814
In grad_steps = 591, loss = 0.0883614793419838
In grad_steps = 592, loss = 0.2768396735191345
In grad_steps = 593, loss = 0.5204593539237976
In grad_steps = 594, loss = 0.1619003415107727
In grad_steps = 595, loss = 0.08035213500261307
In grad_steps = 596, loss = 0.2508235275745392
In grad_steps = 597, loss = 0.1730911135673523
In grad_steps = 598, loss = 0.19002126157283783
In grad_steps = 599, loss = 0.1866033375263214
In grad_steps = 600, loss = 0.07922680675983429
In grad_steps = 601, loss = 0.5551214814186096
In grad_steps = 602, loss = 0.8575183153152466
In grad_steps = 603, loss = 0.23376761376857758
In grad_steps = 604, loss = 0.11139987409114838
In grad_steps = 605, loss = 0.018617141991853714
Beginning epoch 7
In grad_steps = 606, loss = 0.5916604995727539
In grad_steps = 607, loss = 0.20744259655475616
In grad_steps = 608, loss = 0.7962236404418945
In grad_steps = 609, loss = 0.8026086091995239
In grad_steps = 610, loss = 0.26995334029197693
In grad_steps = 611, loss = 0.639792799949646
In grad_steps = 612, loss = 0.1694900095462799
In grad_steps = 613, loss = 0.14899727702140808
In grad_steps = 614, loss = 0.24912258982658386
In grad_steps = 615, loss = 0.21462233364582062
In grad_steps = 616, loss = 0.7554903030395508
In grad_steps = 617, loss = 0.45954975485801697
In grad_steps = 618, loss = 0.4816029667854309
In grad_steps = 619, loss = 0.9876195192337036
In grad_steps = 620, loss = 0.4591441750526428
In grad_steps = 621, loss = 0.4979887306690216
In grad_steps = 622, loss = 0.5773310661315918
In grad_steps = 623, loss = 0.4138834774494171
In grad_steps = 624, loss = 0.25651854276657104
In grad_steps = 625, loss = 0.18607525527477264
In grad_steps = 626, loss = 0.4754510521888733
In grad_steps = 627, loss = 0.4325113594532013
In grad_steps = 628, loss = 0.7746121883392334
In grad_steps = 629, loss = 0.3538128137588501
In grad_steps = 630, loss = 0.16239944100379944
In grad_steps = 631, loss = 0.13057509064674377
In grad_steps = 632, loss = 0.18285447359085083
In grad_steps = 633, loss = 0.14728394150733948
In grad_steps = 634, loss = 0.14524275064468384
In grad_steps = 635, loss = 0.3488845229148865
In grad_steps = 636, loss = 0.17779411375522614
In grad_steps = 637, loss = 0.9322200417518616
In grad_steps = 638, loss = 0.19016411900520325
In grad_steps = 639, loss = 0.2434784471988678
In grad_steps = 640, loss = 0.09816011041402817
In grad_steps = 641, loss = 0.07419519871473312
In grad_steps = 642, loss = 0.7458942532539368
In grad_steps = 643, loss = 0.2727872431278229
In grad_steps = 644, loss = 0.7468373775482178
In grad_steps = 645, loss = 0.05855663865804672
In grad_steps = 646, loss = 0.6224700212478638
In grad_steps = 647, loss = 0.04286584630608559
In grad_steps = 648, loss = 0.06530120968818665
In grad_steps = 649, loss = 0.20589396357536316
In grad_steps = 650, loss = 0.4741945266723633
In grad_steps = 651, loss = 0.14200644195079803
In grad_steps = 652, loss = 0.5172439813613892
In grad_steps = 653, loss = 0.3335379660129547
In grad_steps = 654, loss = 0.13774049282073975
In grad_steps = 655, loss = 0.03757111728191376
In grad_steps = 656, loss = 0.11057889461517334
In grad_steps = 657, loss = 0.08380257338285446
In grad_steps = 658, loss = 0.39853549003601074
In grad_steps = 659, loss = 0.1297924816608429
In grad_steps = 660, loss = 0.35484257340431213
In grad_steps = 661, loss = 0.1523440033197403
In grad_steps = 662, loss = 0.24591782689094543
In grad_steps = 663, loss = 0.07077496498823166
In grad_steps = 664, loss = 0.4921901524066925
In grad_steps = 665, loss = 0.5996085405349731
In grad_steps = 666, loss = 0.6536287665367126
In grad_steps = 667, loss = 0.22892268002033234
In grad_steps = 668, loss = 0.11718063801527023
In grad_steps = 669, loss = 0.3563927412033081
In grad_steps = 670, loss = 0.592922031879425
In grad_steps = 671, loss = 0.14234820008277893
In grad_steps = 672, loss = 0.23210269212722778
In grad_steps = 673, loss = 0.7826859354972839
In grad_steps = 674, loss = 0.49832817912101746
In grad_steps = 675, loss = 0.31388428807258606
In grad_steps = 676, loss = 0.8758212924003601
In grad_steps = 677, loss = 0.39795026183128357
In grad_steps = 678, loss = 0.6997316479682922
In grad_steps = 679, loss = 0.22806833684444427
In grad_steps = 680, loss = 0.5555723309516907
In grad_steps = 681, loss = 0.12012701481580734
In grad_steps = 682, loss = 0.4385409951210022
In grad_steps = 683, loss = 0.4527129530906677
In grad_steps = 684, loss = 0.3691158592700958
In grad_steps = 685, loss = 0.09104000777006149
In grad_steps = 686, loss = 0.21428774297237396
In grad_steps = 687, loss = 0.16457714140415192
In grad_steps = 688, loss = 0.19407224655151367
In grad_steps = 689, loss = 0.2491205483675003
In grad_steps = 690, loss = 0.4102541208267212
In grad_steps = 691, loss = 0.16681258380413055
In grad_steps = 692, loss = 0.14158712327480316
In grad_steps = 693, loss = 0.14195212721824646
In grad_steps = 694, loss = 0.34268173575401306
In grad_steps = 695, loss = 0.19977881014347076
In grad_steps = 696, loss = 0.22927430272102356
In grad_steps = 697, loss = 0.0631176084280014
In grad_steps = 698, loss = 0.09859369695186615
In grad_steps = 699, loss = 0.04196683689951897
In grad_steps = 700, loss = 0.019581027328968048
In grad_steps = 701, loss = 0.08714292198419571
In grad_steps = 702, loss = 0.20355069637298584
In grad_steps = 703, loss = 0.056680064648389816
In grad_steps = 704, loss = 0.002373483730480075
In grad_steps = 705, loss = 0.31832650303840637
In grad_steps = 706, loss = 2.2825422286987305
Beginning epoch 8
In grad_steps = 707, loss = 0.2804321348667145
In grad_steps = 708, loss = 0.012167406268417835
In grad_steps = 709, loss = 0.16905996203422546
In grad_steps = 710, loss = 0.26698043942451477
In grad_steps = 711, loss = 0.31593644618988037
In grad_steps = 712, loss = 1.0630745887756348
In grad_steps = 713, loss = 0.5725487470626831
In grad_steps = 714, loss = 0.6173123121261597
In grad_steps = 715, loss = 0.2651229500770569
In grad_steps = 716, loss = 0.5098122954368591
In grad_steps = 717, loss = 0.4031248986721039
In grad_steps = 718, loss = 0.3516812324523926
In grad_steps = 719, loss = 0.6350007057189941
In grad_steps = 720, loss = 0.7743327617645264
In grad_steps = 721, loss = 0.5931724905967712
In grad_steps = 722, loss = 0.34930527210235596
In grad_steps = 723, loss = 0.6380960941314697
In grad_steps = 724, loss = 0.28341439366340637
In grad_steps = 725, loss = 0.24932464957237244
In grad_steps = 726, loss = 0.22831538319587708
In grad_steps = 727, loss = 0.21055196225643158
In grad_steps = 728, loss = 0.4552432894706726
In grad_steps = 729, loss = 0.86601722240448
In grad_steps = 730, loss = 0.32925257086753845
In grad_steps = 731, loss = 0.2151406854391098
In grad_steps = 732, loss = 0.44930243492126465
In grad_steps = 733, loss = 0.11441652476787567
In grad_steps = 734, loss = 0.22805717587471008
In grad_steps = 735, loss = 0.26183006167411804
In grad_steps = 736, loss = 0.28095728158950806
In grad_steps = 737, loss = 0.2967347502708435
In grad_steps = 738, loss = 0.34047451615333557
In grad_steps = 739, loss = 0.12240903824567795
In grad_steps = 740, loss = 0.3617110848426819
In grad_steps = 741, loss = 0.09248143434524536
In grad_steps = 742, loss = 0.12054324150085449
In grad_steps = 743, loss = 0.13071392476558685
In grad_steps = 744, loss = 0.10329128801822662
In grad_steps = 745, loss = 0.3503512144088745
In grad_steps = 746, loss = 0.3130018413066864
In grad_steps = 747, loss = 0.4748013913631439
In grad_steps = 748, loss = 0.023067841306328773
In grad_steps = 749, loss = 0.09247677773237228
In grad_steps = 750, loss = 0.10262477397918701
In grad_steps = 751, loss = 0.14086365699768066
In grad_steps = 752, loss = 0.09978234022855759
In grad_steps = 753, loss = 0.02023516595363617
In grad_steps = 754, loss = 0.00923982448875904
In grad_steps = 755, loss = 0.07233815640211105
In grad_steps = 756, loss = 0.045379623770713806
In grad_steps = 757, loss = 0.38843196630477905
In grad_steps = 758, loss = 0.03870353847742081
In grad_steps = 759, loss = 0.41699883341789246
In grad_steps = 760, loss = 0.18169935047626495
In grad_steps = 761, loss = 0.9187266230583191
In grad_steps = 762, loss = 0.05332194268703461
In grad_steps = 763, loss = 0.29935982823371887
In grad_steps = 764, loss = 0.09478504210710526
In grad_steps = 765, loss = 0.12042463570833206
In grad_steps = 766, loss = 0.2629244923591614
In grad_steps = 767, loss = 0.6275570392608643
In grad_steps = 768, loss = 0.24494647979736328
In grad_steps = 769, loss = 0.21280774474143982
In grad_steps = 770, loss = 0.28523823618888855
In grad_steps = 771, loss = 0.6451698541641235
In grad_steps = 772, loss = 0.08040840178728104
In grad_steps = 773, loss = 0.24457991123199463
In grad_steps = 774, loss = 0.7249647974967957
In grad_steps = 775, loss = 0.12535755336284637
In grad_steps = 776, loss = 0.1794729381799698
In grad_steps = 777, loss = 0.3730444610118866
In grad_steps = 778, loss = 0.18372009694576263
In grad_steps = 779, loss = 0.18995004892349243
In grad_steps = 780, loss = 0.14599083364009857
In grad_steps = 781, loss = 0.3546035885810852
In grad_steps = 782, loss = 0.26840493083000183
In grad_steps = 783, loss = 0.21621431410312653
In grad_steps = 784, loss = 1.0038714408874512
In grad_steps = 785, loss = 0.2599450647830963
In grad_steps = 786, loss = 0.13202565908432007
In grad_steps = 787, loss = 0.057832203805446625
In grad_steps = 788, loss = 0.24745671451091766
In grad_steps = 789, loss = 0.1142679750919342
In grad_steps = 790, loss = 0.1934507191181183
In grad_steps = 791, loss = 0.5413417220115662
In grad_steps = 792, loss = 0.0912613719701767
In grad_steps = 793, loss = 0.017929906025528908
In grad_steps = 794, loss = 0.060126375406980515
In grad_steps = 795, loss = 0.2046792358160019
In grad_steps = 796, loss = 0.33787891268730164
In grad_steps = 797, loss = 0.21467427909374237
In grad_steps = 798, loss = 0.07228947430849075
In grad_steps = 799, loss = 0.2856462895870209
In grad_steps = 800, loss = 0.10844025760889053
In grad_steps = 801, loss = 0.021092260256409645
In grad_steps = 802, loss = 0.013697999529540539
In grad_steps = 803, loss = 0.014709141105413437
In grad_steps = 804, loss = 0.02324320562183857
In grad_steps = 805, loss = 0.018678994849324226
In grad_steps = 806, loss = 0.03504090756177902
In grad_steps = 807, loss = 0.0015594654250890017
Beginning epoch 9
In grad_steps = 808, loss = 0.013154487125575542
In grad_steps = 809, loss = 0.06722281128168106
In grad_steps = 810, loss = 0.7593029737472534
In grad_steps = 811, loss = 0.03885190933942795
In grad_steps = 812, loss = 0.9337884187698364
In grad_steps = 813, loss = 0.06627129763364792
In grad_steps = 814, loss = 0.07667230069637299
In grad_steps = 815, loss = 0.03531506657600403
In grad_steps = 816, loss = 0.008567698299884796
In grad_steps = 817, loss = 0.29920655488967896
In grad_steps = 818, loss = 0.09582158178091049
In grad_steps = 819, loss = 0.234898641705513
In grad_steps = 820, loss = 0.2809407711029053
In grad_steps = 821, loss = 0.0863146036863327
In grad_steps = 822, loss = 0.07713528722524643
In grad_steps = 823, loss = 0.19770485162734985
In grad_steps = 824, loss = 0.37193551659584045
In grad_steps = 825, loss = 0.02312147058546543
In grad_steps = 826, loss = 0.3123776912689209
In grad_steps = 827, loss = 0.02142728678882122
In grad_steps = 828, loss = 0.06088747829198837
In grad_steps = 829, loss = 0.11103188991546631
In grad_steps = 830, loss = 0.2446385622024536
In grad_steps = 831, loss = 0.07278065383434296
In grad_steps = 832, loss = 0.024242829531431198
In grad_steps = 833, loss = 0.032451558858156204
In grad_steps = 834, loss = 0.1430041640996933
In grad_steps = 835, loss = 0.016699468716979027
In grad_steps = 836, loss = 0.24644117057323456
In grad_steps = 837, loss = 0.0210565272718668
In grad_steps = 838, loss = 0.023527832701802254
In grad_steps = 839, loss = 0.011619207449257374
In grad_steps = 840, loss = 0.04219057038426399
In grad_steps = 841, loss = 0.17308874428272247
In grad_steps = 842, loss = 0.15350961685180664
In grad_steps = 843, loss = 0.01808284968137741
In grad_steps = 844, loss = 0.012073810212314129
In grad_steps = 845, loss = 0.025701522827148438
In grad_steps = 846, loss = 0.05881677567958832
In grad_steps = 847, loss = 0.020881954580545425
In grad_steps = 848, loss = 0.03692757338285446
In grad_steps = 849, loss = 0.01970486529171467
In grad_steps = 850, loss = 0.013608907349407673
In grad_steps = 851, loss = 0.02151765301823616
In grad_steps = 852, loss = 0.0251280777156353
In grad_steps = 853, loss = 0.01686086319386959
In grad_steps = 854, loss = 0.012790683656930923
In grad_steps = 855, loss = 0.007825723849236965
In grad_steps = 856, loss = 0.0023820658680051565
In grad_steps = 857, loss = 0.0016436983132734895
In grad_steps = 858, loss = 0.002287786453962326
In grad_steps = 859, loss = 0.002733007073402405
In grad_steps = 860, loss = 0.0509893037378788
In grad_steps = 861, loss = 0.0619419626891613
In grad_steps = 862, loss = 0.6048267483711243
In grad_steps = 863, loss = 0.11088806390762329
In grad_steps = 864, loss = 0.4846756160259247
In grad_steps = 865, loss = 0.001777525874786079
In grad_steps = 866, loss = 0.022443169727921486
In grad_steps = 867, loss = 0.03549715504050255
In grad_steps = 868, loss = 0.5025441646575928
In grad_steps = 869, loss = 0.037863362580537796
In grad_steps = 870, loss = 0.0038478742353618145
In grad_steps = 871, loss = 0.14556358754634857
In grad_steps = 872, loss = 0.751689076423645
In grad_steps = 873, loss = 0.02969517931342125
In grad_steps = 874, loss = 0.38698631525039673
In grad_steps = 875, loss = 0.5508636236190796
In grad_steps = 876, loss = 0.03821959346532822
In grad_steps = 877, loss = 0.17456696927547455
In grad_steps = 878, loss = 0.48181119561195374
In grad_steps = 879, loss = 0.09227468073368073
In grad_steps = 880, loss = 0.08010637760162354
In grad_steps = 881, loss = 0.04824048653244972
In grad_steps = 882, loss = 0.3203156590461731
In grad_steps = 883, loss = 0.10598281770944595
In grad_steps = 884, loss = 0.10990510135889053
In grad_steps = 885, loss = 0.6662984490394592
In grad_steps = 886, loss = 0.2048017680644989
In grad_steps = 887, loss = 0.2439534068107605
In grad_steps = 888, loss = 0.15175792574882507
In grad_steps = 889, loss = 0.3531888425350189
In grad_steps = 890, loss = 0.6243963241577148
In grad_steps = 891, loss = 0.13780765235424042
In grad_steps = 892, loss = 0.07943940907716751
In grad_steps = 893, loss = 0.1578257977962494
In grad_steps = 894, loss = 0.023339422419667244
In grad_steps = 895, loss = 0.06114816293120384
In grad_steps = 896, loss = 0.15316186845302582
In grad_steps = 897, loss = 0.344828337430954
In grad_steps = 898, loss = 0.1613430380821228
In grad_steps = 899, loss = 0.19653049111366272
In grad_steps = 900, loss = 0.38243764638900757
In grad_steps = 901, loss = 0.03225606307387352
In grad_steps = 902, loss = 0.27578893303871155
In grad_steps = 903, loss = 0.050089478492736816
In grad_steps = 904, loss = 0.026701731607317924
In grad_steps = 905, loss = 0.024053867906332016
In grad_steps = 906, loss = 0.020523834973573685
In grad_steps = 907, loss = 0.07372791320085526
In grad_steps = 908, loss = 0.003174138255417347
Beginning epoch 10
In grad_steps = 909, loss = 0.10156991332769394
In grad_steps = 910, loss = 0.04706263169646263
In grad_steps = 911, loss = 0.012932787649333477
In grad_steps = 912, loss = 0.016666680574417114
In grad_steps = 913, loss = 0.1298127919435501
In grad_steps = 914, loss = 0.005210013594478369
In grad_steps = 915, loss = 0.18235209584236145
In grad_steps = 916, loss = 0.14527375996112823
In grad_steps = 917, loss = 0.019682835787534714
In grad_steps = 918, loss = 0.18087489902973175
In grad_steps = 919, loss = 0.3347783088684082
In grad_steps = 920, loss = 0.0083948178216815
In grad_steps = 921, loss = 0.012333719059824944
In grad_steps = 922, loss = 0.0865650624036789
In grad_steps = 923, loss = 0.08375535160303116
In grad_steps = 924, loss = 0.011939683929085732
In grad_steps = 925, loss = 0.02809765376150608
In grad_steps = 926, loss = 0.2660644054412842
In grad_steps = 927, loss = 0.005789770279079676
In grad_steps = 928, loss = 0.0016380376182496548
In grad_steps = 929, loss = 0.013131075538694859
In grad_steps = 930, loss = 0.01808318868279457
In grad_steps = 931, loss = 0.03219156339764595
In grad_steps = 932, loss = 0.009071671403944492
In grad_steps = 933, loss = 0.008322588168084621
In grad_steps = 934, loss = 0.003879652824252844
In grad_steps = 935, loss = 0.06283341348171234
In grad_steps = 936, loss = 0.01744460128247738
In grad_steps = 937, loss = 0.009263237938284874
In grad_steps = 938, loss = 0.006231274921447039
In grad_steps = 939, loss = 0.001841299468651414
In grad_steps = 940, loss = 0.003860128577798605
In grad_steps = 941, loss = 0.0008047224255278707
In grad_steps = 942, loss = 0.21141698956489563
In grad_steps = 943, loss = 0.004093078430742025
In grad_steps = 944, loss = 0.002450157655403018
In grad_steps = 945, loss = 0.005843430291861296
In grad_steps = 946, loss = 0.5276352763175964
In grad_steps = 947, loss = 0.20008522272109985
In grad_steps = 948, loss = 0.007138160988688469
In grad_steps = 949, loss = 0.04937112331390381
In grad_steps = 950, loss = 0.013620900921523571
In grad_steps = 951, loss = 0.01041069719940424
In grad_steps = 952, loss = 0.08079995214939117
In grad_steps = 953, loss = 0.15586918592453003
In grad_steps = 954, loss = 0.20835162699222565
In grad_steps = 955, loss = 0.014663441106677055
In grad_steps = 956, loss = 0.0011581254657357931
In grad_steps = 957, loss = 0.014169461093842983
In grad_steps = 958, loss = 0.0016192932380363345
In grad_steps = 959, loss = 0.02305416390299797
In grad_steps = 960, loss = 0.0038200239650905132
In grad_steps = 961, loss = 0.04185246303677559
In grad_steps = 962, loss = 0.04382040724158287
In grad_steps = 963, loss = 0.3154878318309784
In grad_steps = 964, loss = 0.6216078400611877
In grad_steps = 965, loss = 0.120637446641922
In grad_steps = 966, loss = 0.25927993655204773
In grad_steps = 967, loss = 0.12665484845638275
In grad_steps = 968, loss = 0.07290714234113693
In grad_steps = 969, loss = 0.027688797563314438
In grad_steps = 970, loss = 0.002953452756628394
In grad_steps = 971, loss = 0.0033758485224097967
In grad_steps = 972, loss = 0.00705390889197588
In grad_steps = 973, loss = 0.4999332129955292
In grad_steps = 974, loss = 0.0040800380520522594
In grad_steps = 975, loss = 0.06451088190078735
In grad_steps = 976, loss = 0.30310511589050293
In grad_steps = 977, loss = 0.49034661054611206
In grad_steps = 978, loss = 0.07951825857162476
In grad_steps = 979, loss = 0.421102374792099
In grad_steps = 980, loss = 0.49739938974380493
In grad_steps = 981, loss = 0.026625338941812515
In grad_steps = 982, loss = 0.03999389335513115
In grad_steps = 983, loss = 0.0861007571220398
In grad_steps = 984, loss = 0.2005644291639328
In grad_steps = 985, loss = 0.18393045663833618
In grad_steps = 986, loss = 0.0815126895904541
In grad_steps = 987, loss = 0.03237246349453926
In grad_steps = 988, loss = 0.1379847526550293
In grad_steps = 989, loss = 0.05177782475948334
In grad_steps = 990, loss = 0.07540678977966309
In grad_steps = 991, loss = 0.21908125281333923
In grad_steps = 992, loss = 0.13327421247959137
In grad_steps = 993, loss = 0.05272247642278671
In grad_steps = 994, loss = 0.5916252732276917
In grad_steps = 995, loss = 0.11968113481998444
In grad_steps = 996, loss = 0.047181859612464905
In grad_steps = 997, loss = 0.06331370025873184
In grad_steps = 998, loss = 0.038803573697805405
In grad_steps = 999, loss = 0.1019723042845726
In grad_steps = 1000, loss = 0.00817660428583622
In grad_steps = 1001, loss = 0.07111062854528427
In grad_steps = 1002, loss = 0.014479903504252434
In grad_steps = 1003, loss = 0.03989866003394127
In grad_steps = 1004, loss = 0.02294263429939747
In grad_steps = 1005, loss = 0.008728688582777977
In grad_steps = 1006, loss = 0.2517086863517761
In grad_steps = 1007, loss = 0.42754465341567993
In grad_steps = 1008, loss = 0.07408784329891205
In grad_steps = 1009, loss = 0.004780528135597706
Beginning epoch 11
In grad_steps = 1010, loss = 0.44645267724990845
In grad_steps = 1011, loss = 0.0059371027164161205
In grad_steps = 1012, loss = 0.009447237476706505
In grad_steps = 1013, loss = 0.4127904176712036
In grad_steps = 1014, loss = 0.09482449293136597
In grad_steps = 1015, loss = 0.13693943619728088
In grad_steps = 1016, loss = 0.17601096630096436
In grad_steps = 1017, loss = 0.014529946260154247
In grad_steps = 1018, loss = 0.3205186724662781
In grad_steps = 1019, loss = 0.18104690313339233
In grad_steps = 1020, loss = 0.18218672275543213
In grad_steps = 1021, loss = 0.020992280915379524
In grad_steps = 1022, loss = 0.03721661865711212
In grad_steps = 1023, loss = 0.1080443263053894
In grad_steps = 1024, loss = 0.1133272647857666
In grad_steps = 1025, loss = 0.022528547793626785
In grad_steps = 1026, loss = 0.07231951504945755
In grad_steps = 1027, loss = 0.604992151260376
In grad_steps = 1028, loss = 0.02463267371058464
In grad_steps = 1029, loss = 0.013915197923779488
In grad_steps = 1030, loss = 0.008972840383648872
In grad_steps = 1031, loss = 0.14006760716438293
In grad_steps = 1032, loss = 0.5377365350723267
In grad_steps = 1033, loss = 0.174883633852005
In grad_steps = 1034, loss = 0.009183350019156933
In grad_steps = 1035, loss = 0.014880049042403698
In grad_steps = 1036, loss = 0.025963684543967247
In grad_steps = 1037, loss = 0.012903686612844467
In grad_steps = 1038, loss = 0.04338180273771286
In grad_steps = 1039, loss = 0.13023130595684052
In grad_steps = 1040, loss = 0.00989025179296732
In grad_steps = 1041, loss = 0.06814512610435486
In grad_steps = 1042, loss = 0.01028929091989994
In grad_steps = 1043, loss = 0.07047265022993088
In grad_steps = 1044, loss = 0.028103696182370186
In grad_steps = 1045, loss = 0.007295873016119003
In grad_steps = 1046, loss = 0.025059528648853302
In grad_steps = 1047, loss = 0.026071136817336082
In grad_steps = 1048, loss = 0.020721886307001114
In grad_steps = 1049, loss = 0.039757661521434784
In grad_steps = 1050, loss = 0.021268615499138832
In grad_steps = 1051, loss = 0.6841701865196228
In grad_steps = 1052, loss = 0.09248322993516922
In grad_steps = 1053, loss = 0.145301952958107
In grad_steps = 1054, loss = 0.020339617505669594
In grad_steps = 1055, loss = 0.007969807833433151
In grad_steps = 1056, loss = 0.5544450283050537
In grad_steps = 1057, loss = 0.0055299187079072
In grad_steps = 1058, loss = 0.02476152591407299
In grad_steps = 1059, loss = 0.07037659734487534
In grad_steps = 1060, loss = 0.11463053524494171
In grad_steps = 1061, loss = 0.023048801347613335
In grad_steps = 1062, loss = 0.04455951601266861
In grad_steps = 1063, loss = 0.04158490523695946
In grad_steps = 1064, loss = 0.11843762546777725
In grad_steps = 1065, loss = 0.03512568026781082
In grad_steps = 1066, loss = 0.051661767065525055
In grad_steps = 1067, loss = 0.023729603737592697
In grad_steps = 1068, loss = 0.025874434038996696
In grad_steps = 1069, loss = 0.06853247433900833
In grad_steps = 1070, loss = 0.08640285581350327
In grad_steps = 1071, loss = 0.21118377149105072
In grad_steps = 1072, loss = 0.002376638352870941
In grad_steps = 1073, loss = 0.03391677141189575
In grad_steps = 1074, loss = 0.49507948756217957
In grad_steps = 1075, loss = 0.012633373029530048
In grad_steps = 1076, loss = 0.006072999909520149
In grad_steps = 1077, loss = 0.2788817882537842
In grad_steps = 1078, loss = 0.13714773952960968
In grad_steps = 1079, loss = 0.0067828926257789135
In grad_steps = 1080, loss = 0.08089357614517212
In grad_steps = 1081, loss = 0.19099797308444977
In grad_steps = 1082, loss = 0.09390991181135178
In grad_steps = 1083, loss = 0.02257915586233139
In grad_steps = 1084, loss = 0.8656202554702759
In grad_steps = 1085, loss = 0.16408464312553406
In grad_steps = 1086, loss = 0.25175803899765015
In grad_steps = 1087, loss = 0.3193536102771759
In grad_steps = 1088, loss = 0.020733308047056198
In grad_steps = 1089, loss = 0.030668947845697403
In grad_steps = 1090, loss = 0.010767634958028793
In grad_steps = 1091, loss = 0.02309689112007618
In grad_steps = 1092, loss = 0.14353039860725403
In grad_steps = 1093, loss = 0.1800694316625595
In grad_steps = 1094, loss = 0.03303155675530434
In grad_steps = 1095, loss = 0.33190399408340454
In grad_steps = 1096, loss = 0.013861645944416523
In grad_steps = 1097, loss = 0.0797957181930542
In grad_steps = 1098, loss = 0.12431801855564117
In grad_steps = 1099, loss = 0.0437527671456337
In grad_steps = 1100, loss = 0.03018101304769516
In grad_steps = 1101, loss = 0.0267663411796093
In grad_steps = 1102, loss = 0.12693388760089874
In grad_steps = 1103, loss = 0.04647137597203255
In grad_steps = 1104, loss = 0.045221827924251556
In grad_steps = 1105, loss = 0.007827038876712322
In grad_steps = 1106, loss = 0.02120831049978733
In grad_steps = 1107, loss = 0.09746556729078293
In grad_steps = 1108, loss = 0.07189515233039856
In grad_steps = 1109, loss = 0.010132862254977226
In grad_steps = 1110, loss = 0.005763442255556583
Beginning epoch 12
In grad_steps = 1111, loss = 0.007940883748233318
In grad_steps = 1112, loss = 0.009591341018676758
In grad_steps = 1113, loss = 0.02378113567829132
In grad_steps = 1114, loss = 0.014432479627430439
In grad_steps = 1115, loss = 0.04336680471897125
In grad_steps = 1116, loss = 0.12005771696567535
In grad_steps = 1117, loss = 0.012186544016003609
In grad_steps = 1118, loss = 0.035825926810503006
In grad_steps = 1119, loss = 0.001838230062276125
In grad_steps = 1120, loss = 0.08561749756336212
In grad_steps = 1121, loss = 0.06065439432859421
In grad_steps = 1122, loss = 0.1363850086927414
In grad_steps = 1123, loss = 0.007594243157655001
In grad_steps = 1124, loss = 0.015245123766362667
In grad_steps = 1125, loss = 0.031911998987197876
In grad_steps = 1126, loss = 0.002264464506879449
In grad_steps = 1127, loss = 0.7980506420135498
In grad_steps = 1128, loss = 0.02779671736061573
In grad_steps = 1129, loss = 0.013130683451890945
In grad_steps = 1130, loss = 0.004234012681990862
In grad_steps = 1131, loss = 0.003580409102141857
In grad_steps = 1132, loss = 0.01842840015888214
In grad_steps = 1133, loss = 0.0008918570238165557
In grad_steps = 1134, loss = 0.47487521171569824
In grad_steps = 1135, loss = 0.003938970156013966
In grad_steps = 1136, loss = 0.03510187938809395
In grad_steps = 1137, loss = 0.002986836014315486
In grad_steps = 1138, loss = 0.006287775002419949
In grad_steps = 1139, loss = 0.14713048934936523
In grad_steps = 1140, loss = 0.01193438284099102
In grad_steps = 1141, loss = 0.001758505473844707
In grad_steps = 1142, loss = 0.013871550559997559
In grad_steps = 1143, loss = 0.006935334764420986
In grad_steps = 1144, loss = 0.005849773529917002
In grad_steps = 1145, loss = 0.021762743592262268
In grad_steps = 1146, loss = 0.004615010228008032
In grad_steps = 1147, loss = 0.09458925575017929
In grad_steps = 1148, loss = 0.03390870615839958
In grad_steps = 1149, loss = 0.009631592780351639
In grad_steps = 1150, loss = 0.006791575346142054
In grad_steps = 1151, loss = 0.05112495273351669
In grad_steps = 1152, loss = 0.0574716217815876
In grad_steps = 1153, loss = 0.02099555730819702
In grad_steps = 1154, loss = 0.025653915479779243
In grad_steps = 1155, loss = 0.7455682754516602
In grad_steps = 1156, loss = 0.007679324131458998
In grad_steps = 1157, loss = 0.0485580749809742
In grad_steps = 1158, loss = 0.0033419169485569
In grad_steps = 1159, loss = 0.03260984644293785
In grad_steps = 1160, loss = 0.0034073900897055864
In grad_steps = 1161, loss = 0.003095120657235384
In grad_steps = 1162, loss = 0.01370322797447443
In grad_steps = 1163, loss = 0.006032231729477644
In grad_steps = 1164, loss = 0.009012024849653244
In grad_steps = 1165, loss = 0.15492525696754456
In grad_steps = 1166, loss = 0.007915494963526726
In grad_steps = 1167, loss = 0.014847337268292904
In grad_steps = 1168, loss = 0.0033977660350501537
In grad_steps = 1169, loss = 0.010501370765268803
In grad_steps = 1170, loss = 0.007237215526401997
In grad_steps = 1171, loss = 0.004087910056114197
In grad_steps = 1172, loss = 0.0025990428403019905
In grad_steps = 1173, loss = 0.0058634537272155285
In grad_steps = 1174, loss = 0.12892086803913116
In grad_steps = 1175, loss = 0.17466947436332703
In grad_steps = 1176, loss = 0.006486381869763136
In grad_steps = 1177, loss = 0.5093160271644592
In grad_steps = 1178, loss = 0.03906967490911484
In grad_steps = 1179, loss = 0.00635333638638258
In grad_steps = 1180, loss = 0.31169798970222473
In grad_steps = 1181, loss = 0.011745183728635311
In grad_steps = 1182, loss = 0.02333507500588894
In grad_steps = 1183, loss = 0.011437181383371353
In grad_steps = 1184, loss = 0.004383791703730822
In grad_steps = 1185, loss = 0.03951480612158775
In grad_steps = 1186, loss = 0.915678858757019
In grad_steps = 1187, loss = 0.5345889925956726
In grad_steps = 1188, loss = 0.023178845643997192
In grad_steps = 1189, loss = 0.02378908172249794
In grad_steps = 1190, loss = 0.516483724117279
In grad_steps = 1191, loss = 0.09958183765411377
In grad_steps = 1192, loss = 0.06074749678373337
In grad_steps = 1193, loss = 0.3443320095539093
In grad_steps = 1194, loss = 0.06374151259660721
In grad_steps = 1195, loss = 0.11460194736719131
In grad_steps = 1196, loss = 0.0217184666544199
In grad_steps = 1197, loss = 0.011138702742755413
In grad_steps = 1198, loss = 0.02952675335109234
In grad_steps = 1199, loss = 0.028856411576271057
In grad_steps = 1200, loss = 0.030188005417585373
In grad_steps = 1201, loss = 0.040478747338056564
In grad_steps = 1202, loss = 0.01910402998328209
In grad_steps = 1203, loss = 0.06519358605146408
In grad_steps = 1204, loss = 0.04793109744787216
In grad_steps = 1205, loss = 0.008530126884579659
In grad_steps = 1206, loss = 0.03385030850768089
In grad_steps = 1207, loss = 0.03288736566901207
In grad_steps = 1208, loss = 0.03187328949570656
In grad_steps = 1209, loss = 0.004233621060848236
In grad_steps = 1210, loss = 0.013214847072958946
In grad_steps = 1211, loss = 0.004933998920023441
Elapsed time: 2138.315280199051 seconds for ensemble 2 with 12 epochs
LoRA instance 2 evaluation complete. Data saved to /pscratch/sd/t/tianle/lucid/other_source/SURP_2024/results/tmp_lora_ensemble/Llama3-8B-set-1.1/test_data_instance_2_seed_20187.npz.
lora instance i = 2 Successfully finished.
Training lora instance 3
DEBUG: Total parameters: 8037076992
DEBUG: Trainable parameters: 6815744
Beginning epoch 1
In grad_steps = 0, loss = 1.6091442108154297
In grad_steps = 1, loss = 0.957466185092926
In grad_steps = 2, loss = 0.7128434181213379
In grad_steps = 3, loss = 0.7431908249855042
In grad_steps = 4, loss = 1.2033777236938477
In grad_steps = 5, loss = 0.4167872667312622
In grad_steps = 6, loss = 0.8720705509185791
In grad_steps = 7, loss = 0.6929490566253662
In grad_steps = 8, loss = 0.8127872347831726
In grad_steps = 9, loss = 0.6494787335395813
In grad_steps = 10, loss = 0.7080276012420654
In grad_steps = 11, loss = 0.7036644220352173
In grad_steps = 12, loss = 0.6988185048103333
In grad_steps = 13, loss = 0.639197826385498
In grad_steps = 14, loss = 0.7478581070899963
In grad_steps = 15, loss = 0.8327322006225586
In grad_steps = 16, loss = 0.7007396221160889
In grad_steps = 17, loss = 0.6820142269134521
In grad_steps = 18, loss = 0.6978896856307983
In grad_steps = 19, loss = 0.7139507532119751
In grad_steps = 20, loss = 0.6748600602149963
In grad_steps = 21, loss = 0.7120272517204285
In grad_steps = 22, loss = 0.6768981218338013
In grad_steps = 23, loss = 0.703326940536499
In grad_steps = 24, loss = 0.7088193893432617
In grad_steps = 25, loss = 0.7563011646270752
In grad_steps = 26, loss = 0.642318844795227
In grad_steps = 27, loss = 0.7323673963546753
In grad_steps = 28, loss = 0.6522791385650635
In grad_steps = 29, loss = 0.706612765789032
In grad_steps = 30, loss = 0.6820746660232544
In grad_steps = 31, loss = 0.6969813704490662
In grad_steps = 32, loss = 0.749485194683075
In grad_steps = 33, loss = 0.685335099697113
In grad_steps = 34, loss = 0.7955358028411865
In grad_steps = 35, loss = 0.6876762509346008
In grad_steps = 36, loss = 0.7266853451728821
In grad_steps = 37, loss = 0.673139214515686
In grad_steps = 38, loss = 0.7323631048202515
In grad_steps = 39, loss = 0.7020781636238098
In grad_steps = 40, loss = 0.6857712268829346
In grad_steps = 41, loss = 0.7058578729629517
In grad_steps = 42, loss = 0.6991680264472961
In grad_steps = 43, loss = 0.68282151222229
In grad_steps = 44, loss = 0.6909339427947998
In grad_steps = 45, loss = 0.6821203827857971
In grad_steps = 46, loss = 0.7025095224380493
In grad_steps = 47, loss = 0.7289540767669678
In grad_steps = 48, loss = 0.6844247579574585
In grad_steps = 49, loss = 0.6753890514373779
In grad_steps = 50, loss = 0.6612244248390198
In grad_steps = 51, loss = 0.723198652267456
In grad_steps = 52, loss = 0.7276812791824341
In grad_steps = 53, loss = 0.6540317535400391
In grad_steps = 54, loss = 0.5685514807701111
In grad_steps = 55, loss = 0.6044359803199768
In grad_steps = 56, loss = 0.566623330116272
In grad_steps = 57, loss = 0.5548868179321289
In grad_steps = 58, loss = 0.7570667862892151
In grad_steps = 59, loss = 0.6875876784324646
In grad_steps = 60, loss = 0.8141831159591675
In grad_steps = 61, loss = 0.670660674571991
In grad_steps = 62, loss = 0.9880180954933167
In grad_steps = 63, loss = 0.7585501074790955
In grad_steps = 64, loss = 0.5892637968063354
In grad_steps = 65, loss = 0.5794134140014648
In grad_steps = 66, loss = 0.8080084323883057
In grad_steps = 67, loss = 0.6611700654029846
In grad_steps = 68, loss = 0.737520694732666
In grad_steps = 69, loss = 0.6824076175689697
In grad_steps = 70, loss = 0.7196962237358093
In grad_steps = 71, loss = 0.7026943564414978
In grad_steps = 72, loss = 0.6890276670455933
In grad_steps = 73, loss = 0.7129513025283813
In grad_steps = 74, loss = 0.6684303283691406
In grad_steps = 75, loss = 0.6742442846298218
In grad_steps = 76, loss = 0.6586834192276001
In grad_steps = 77, loss = 1.0371681451797485
In grad_steps = 78, loss = 0.8206705451011658
In grad_steps = 79, loss = 0.6335383653640747
In grad_steps = 80, loss = 0.6890882253646851
In grad_steps = 81, loss = 0.6763890385627747
In grad_steps = 82, loss = 0.72380131483078
In grad_steps = 83, loss = 0.7007768154144287
In grad_steps = 84, loss = 0.7073128819465637
In grad_steps = 85, loss = 0.7058573961257935
In grad_steps = 86, loss = 0.651633083820343
In grad_steps = 87, loss = 0.6905529499053955
In grad_steps = 88, loss = 0.7158000469207764
In grad_steps = 89, loss = 0.6779940724372864
In grad_steps = 90, loss = 0.7134093046188354
In grad_steps = 91, loss = 0.6608016490936279
In grad_steps = 92, loss = 0.6776382327079773
In grad_steps = 93, loss = 0.7167579531669617
In grad_steps = 94, loss = 0.7324685454368591
In grad_steps = 95, loss = 0.6497916579246521
In grad_steps = 96, loss = 0.8459463715553284
In grad_steps = 97, loss = 0.7584047913551331
In grad_steps = 98, loss = 0.6420841813087463
In grad_steps = 99, loss = 0.6262120008468628
In grad_steps = 100, loss = 0.6860294342041016
Beginning epoch 2
In grad_steps = 101, loss = 0.6404083371162415
In grad_steps = 102, loss = 0.6407984495162964
In grad_steps = 103, loss = 0.6579409241676331
In grad_steps = 104, loss = 0.680461049079895
In grad_steps = 105, loss = 0.7107528448104858
In grad_steps = 106, loss = 0.6223384737968445
In grad_steps = 107, loss = 0.6433699727058411
In grad_steps = 108, loss = 0.6306508183479309
In grad_steps = 109, loss = 0.7057310342788696
In grad_steps = 110, loss = 0.6393879055976868
In grad_steps = 111, loss = 0.7354276776313782
In grad_steps = 112, loss = 0.6512937545776367
In grad_steps = 113, loss = 0.6983077526092529
In grad_steps = 114, loss = 0.7464801669120789
In grad_steps = 115, loss = 0.7090198397636414
In grad_steps = 116, loss = 0.6632166504859924
In grad_steps = 117, loss = 0.6421248912811279
In grad_steps = 118, loss = 0.6673664450645447
In grad_steps = 119, loss = 0.7113817930221558
In grad_steps = 120, loss = 0.6661219596862793
In grad_steps = 121, loss = 0.5823341608047485
In grad_steps = 122, loss = 0.6978946328163147
In grad_steps = 123, loss = 0.7203119397163391
In grad_steps = 124, loss = 0.7063068747520447
In grad_steps = 125, loss = 0.6162969470024109
In grad_steps = 126, loss = 0.5393980145454407
In grad_steps = 127, loss = 0.681454062461853
In grad_steps = 128, loss = 0.6532049775123596
In grad_steps = 129, loss = 0.5192635655403137
In grad_steps = 130, loss = 0.7165447473526001
In grad_steps = 131, loss = 0.5432745814323425
In grad_steps = 132, loss = 0.7157351970672607
In grad_steps = 133, loss = 0.9452154040336609
In grad_steps = 134, loss = 0.705132782459259
In grad_steps = 135, loss = 0.7771623730659485
In grad_steps = 136, loss = 0.7164932489395142
In grad_steps = 137, loss = 0.664875864982605
In grad_steps = 138, loss = 0.645993709564209
In grad_steps = 139, loss = 0.7677392959594727
In grad_steps = 140, loss = 0.8271836042404175
In grad_steps = 141, loss = 0.6163517832756042
In grad_steps = 142, loss = 0.6735303401947021
In grad_steps = 143, loss = 0.6798787117004395
In grad_steps = 144, loss = 0.7056392431259155
In grad_steps = 145, loss = 0.6599692702293396
In grad_steps = 146, loss = 0.6421293616294861
In grad_steps = 147, loss = 0.6473284959793091
In grad_steps = 148, loss = 0.6230860352516174
In grad_steps = 149, loss = 0.6247815489768982
In grad_steps = 150, loss = 0.6691622734069824
In grad_steps = 151, loss = 0.5850763320922852
In grad_steps = 152, loss = 0.7619316577911377
In grad_steps = 153, loss = 0.7330019474029541
In grad_steps = 154, loss = 0.6397844552993774
In grad_steps = 155, loss = 0.5516220927238464
In grad_steps = 156, loss = 0.5923988819122314
In grad_steps = 157, loss = 0.5304973721504211
In grad_steps = 158, loss = 0.5203632116317749
In grad_steps = 159, loss = 0.7519076466560364
In grad_steps = 160, loss = 0.6955933570861816
In grad_steps = 161, loss = 0.7904919385910034
In grad_steps = 162, loss = 0.6353601813316345
In grad_steps = 163, loss = 0.8886868953704834
In grad_steps = 164, loss = 0.7173501253128052
In grad_steps = 165, loss = 0.580924928188324
In grad_steps = 166, loss = 0.586700439453125
In grad_steps = 167, loss = 0.675773024559021
In grad_steps = 168, loss = 0.6370311379432678
In grad_steps = 169, loss = 0.6802021861076355
In grad_steps = 170, loss = 0.7119350433349609
In grad_steps = 171, loss = 0.6145005226135254
In grad_steps = 172, loss = 0.6782162189483643
In grad_steps = 173, loss = 0.6708099246025085
In grad_steps = 174, loss = 0.6992639899253845
In grad_steps = 175, loss = 0.6696507930755615
In grad_steps = 176, loss = 0.6575177311897278
In grad_steps = 177, loss = 0.6400966644287109
In grad_steps = 178, loss = 0.8861740827560425
In grad_steps = 179, loss = 0.7443516254425049
In grad_steps = 180, loss = 0.5555924773216248
In grad_steps = 181, loss = 0.5688363313674927
In grad_steps = 182, loss = 0.6560403108596802
In grad_steps = 183, loss = 0.7202160954475403
In grad_steps = 184, loss = 0.7066648602485657
In grad_steps = 185, loss = 0.7014144062995911
In grad_steps = 186, loss = 0.7090195417404175
In grad_steps = 187, loss = 0.5003715753555298
In grad_steps = 188, loss = 0.6443637609481812
In grad_steps = 189, loss = 0.6958258748054504
In grad_steps = 190, loss = 0.6903756260871887
In grad_steps = 191, loss = 0.6403777003288269
In grad_steps = 192, loss = 0.5836987495422363
In grad_steps = 193, loss = 0.5975512266159058
In grad_steps = 194, loss = 0.695162296295166
In grad_steps = 195, loss = 0.7896575927734375
In grad_steps = 196, loss = 0.5869103670120239
In grad_steps = 197, loss = 0.5735145211219788
In grad_steps = 198, loss = 0.694527268409729
In grad_steps = 199, loss = 0.46998274326324463
In grad_steps = 200, loss = 0.5961121320724487
In grad_steps = 201, loss = 0.6729137897491455
Beginning epoch 3
In grad_steps = 202, loss = 0.5906118154525757
In grad_steps = 203, loss = 0.5360826253890991
In grad_steps = 204, loss = 0.67403644323349
In grad_steps = 205, loss = 0.7657696604728699
In grad_steps = 206, loss = 0.6582520604133606
In grad_steps = 207, loss = 0.558904767036438
In grad_steps = 208, loss = 0.6182941198348999
In grad_steps = 209, loss = 0.5041465163230896
In grad_steps = 210, loss = 0.7985588908195496
In grad_steps = 211, loss = 0.5324133038520813
In grad_steps = 212, loss = 0.7564488649368286
In grad_steps = 213, loss = 0.5996077656745911
In grad_steps = 214, loss = 0.653396487236023
In grad_steps = 215, loss = 0.7391597032546997
In grad_steps = 216, loss = 0.6973564028739929
In grad_steps = 217, loss = 0.6698402166366577
In grad_steps = 218, loss = 0.6764875054359436
In grad_steps = 219, loss = 0.6210253834724426
In grad_steps = 220, loss = 0.5759880542755127
In grad_steps = 221, loss = 0.5659441947937012
In grad_steps = 222, loss = 0.5303845405578613
In grad_steps = 223, loss = 0.6716628074645996
In grad_steps = 224, loss = 0.7491637468338013
In grad_steps = 225, loss = 0.7072317004203796
In grad_steps = 226, loss = 0.542399525642395
In grad_steps = 227, loss = 0.40757209062576294
In grad_steps = 228, loss = 0.6396158933639526
In grad_steps = 229, loss = 0.7004222869873047
In grad_steps = 230, loss = 0.3826148211956024
In grad_steps = 231, loss = 0.5939745903015137
In grad_steps = 232, loss = 0.4884254038333893
In grad_steps = 233, loss = 0.6521006226539612
In grad_steps = 234, loss = 0.5880661606788635
In grad_steps = 235, loss = 0.5622215867042542
In grad_steps = 236, loss = 0.7612534761428833
In grad_steps = 237, loss = 0.4094221591949463
In grad_steps = 238, loss = 0.5497357845306396
In grad_steps = 239, loss = 0.6116225719451904
In grad_steps = 240, loss = 0.9863401055335999
In grad_steps = 241, loss = 0.9602755308151245
In grad_steps = 242, loss = 0.5672561526298523
In grad_steps = 243, loss = 0.8368372321128845
In grad_steps = 244, loss = 0.6296436190605164
In grad_steps = 245, loss = 0.6913868188858032
In grad_steps = 246, loss = 0.6055166721343994
In grad_steps = 247, loss = 0.527004063129425
In grad_steps = 248, loss = 0.5949038863182068
In grad_steps = 249, loss = 0.5555766820907593
In grad_steps = 250, loss = 0.5458714962005615
In grad_steps = 251, loss = 0.6033288240432739
In grad_steps = 252, loss = 0.5737611055374146
In grad_steps = 253, loss = 0.6667031049728394
In grad_steps = 254, loss = 0.7363864183425903
In grad_steps = 255, loss = 0.595430850982666
In grad_steps = 256, loss = 0.5677220821380615
In grad_steps = 257, loss = 0.5484185218811035
In grad_steps = 258, loss = 0.4521367847919464
In grad_steps = 259, loss = 0.4393357038497925
In grad_steps = 260, loss = 0.9102444648742676
In grad_steps = 261, loss = 0.7980301380157471
In grad_steps = 262, loss = 0.8039215803146362
In grad_steps = 263, loss = 0.569436252117157
In grad_steps = 264, loss = 0.7441057562828064
In grad_steps = 265, loss = 0.6551603078842163
In grad_steps = 266, loss = 0.5898601412773132
In grad_steps = 267, loss = 0.6210579872131348
In grad_steps = 268, loss = 0.6174028515815735
In grad_steps = 269, loss = 0.6515052318572998
In grad_steps = 270, loss = 0.6365241408348083
In grad_steps = 271, loss = 0.7173394560813904
In grad_steps = 272, loss = 0.588473379611969
In grad_steps = 273, loss = 0.6611889600753784
In grad_steps = 274, loss = 0.651878833770752
In grad_steps = 275, loss = 0.6533994078636169
In grad_steps = 276, loss = 0.6385916471481323
In grad_steps = 277, loss = 0.6174564957618713
In grad_steps = 278, loss = 0.607327938079834
In grad_steps = 279, loss = 0.7976130843162537
In grad_steps = 280, loss = 0.6890448927879333
In grad_steps = 281, loss = 0.5188578963279724
In grad_steps = 282, loss = 0.564495861530304
In grad_steps = 283, loss = 0.6350870728492737
In grad_steps = 284, loss = 0.7154004573822021
In grad_steps = 285, loss = 0.7204847931861877
In grad_steps = 286, loss = 0.6946772336959839
In grad_steps = 287, loss = 0.6440510749816895
In grad_steps = 288, loss = 0.4775710701942444
In grad_steps = 289, loss = 0.6168246865272522
In grad_steps = 290, loss = 0.6398990750312805
In grad_steps = 291, loss = 0.6384384036064148
In grad_steps = 292, loss = 0.546829879283905
In grad_steps = 293, loss = 0.46824732422828674
In grad_steps = 294, loss = 0.537516176700592
In grad_steps = 295, loss = 0.6902045011520386
In grad_steps = 296, loss = 0.7747050523757935
In grad_steps = 297, loss = 0.566990852355957
In grad_steps = 298, loss = 0.48754745721817017
In grad_steps = 299, loss = 0.5233683586120605
In grad_steps = 300, loss = 0.6484422087669373
In grad_steps = 301, loss = 0.7416544556617737
In grad_steps = 302, loss = 0.5919265151023865
Beginning epoch 4
In grad_steps = 303, loss = 0.6355569362640381
In grad_steps = 304, loss = 0.5172241926193237
In grad_steps = 305, loss = 0.6246418356895447
In grad_steps = 306, loss = 0.6429394483566284
In grad_steps = 307, loss = 0.6347796320915222
In grad_steps = 308, loss = 0.5229858160018921
In grad_steps = 309, loss = 0.5532057285308838
In grad_steps = 310, loss = 0.49599191546440125
In grad_steps = 311, loss = 0.7716259360313416
In grad_steps = 312, loss = 0.5288004875183105
In grad_steps = 313, loss = 0.76793372631073
In grad_steps = 314, loss = 0.5590606927871704
In grad_steps = 315, loss = 0.6725130081176758
In grad_steps = 316, loss = 0.7846453189849854
In grad_steps = 317, loss = 0.6334084272384644
In grad_steps = 318, loss = 0.6092523336410522
In grad_steps = 319, loss = 0.6324726343154907
In grad_steps = 320, loss = 0.5751099586486816
In grad_steps = 321, loss = 0.5138984322547913
In grad_steps = 322, loss = 0.5116379261016846
In grad_steps = 323, loss = 0.4657403230667114
In grad_steps = 324, loss = 0.619857907295227
In grad_steps = 325, loss = 0.7328352928161621
In grad_steps = 326, loss = 0.6498503684997559
In grad_steps = 327, loss = 0.4225212335586548
In grad_steps = 328, loss = 0.4071146249771118
In grad_steps = 329, loss = 0.5671247243881226
In grad_steps = 330, loss = 0.6960002779960632
In grad_steps = 331, loss = 0.28601688146591187
In grad_steps = 332, loss = 0.4445040822029114
In grad_steps = 333, loss = 0.35021746158599854
In grad_steps = 334, loss = 0.6099359393119812
In grad_steps = 335, loss = 0.37251976132392883
In grad_steps = 336, loss = 0.3431245982646942
In grad_steps = 337, loss = 0.5165107250213623
In grad_steps = 338, loss = 0.2469327747821808
In grad_steps = 339, loss = 0.2008897364139557
In grad_steps = 340, loss = 0.09915764629840851
In grad_steps = 341, loss = 0.35871702432632446
In grad_steps = 342, loss = 0.684145450592041
In grad_steps = 343, loss = 0.5897916555404663
In grad_steps = 344, loss = 0.9604129195213318
In grad_steps = 345, loss = 0.5747081637382507
In grad_steps = 346, loss = 0.724599301815033
In grad_steps = 347, loss = 0.5057346820831299
In grad_steps = 348, loss = 0.3776630163192749
In grad_steps = 349, loss = 0.4798932671546936
In grad_steps = 350, loss = 0.5427948832511902
In grad_steps = 351, loss = 0.4678300619125366
In grad_steps = 352, loss = 0.48808756470680237
In grad_steps = 353, loss = 0.5496017932891846
In grad_steps = 354, loss = 0.6267828941345215
In grad_steps = 355, loss = 0.7497538328170776
In grad_steps = 356, loss = 0.5699187517166138
In grad_steps = 357, loss = 0.4497995376586914
In grad_steps = 358, loss = 0.3532053232192993
In grad_steps = 359, loss = 0.39883020520210266
In grad_steps = 360, loss = 0.42204123735427856
In grad_steps = 361, loss = 0.9495478272438049
In grad_steps = 362, loss = 0.717960774898529
In grad_steps = 363, loss = 0.7163835763931274
In grad_steps = 364, loss = 0.4344513416290283
In grad_steps = 365, loss = 0.4416311979293823
In grad_steps = 366, loss = 0.4845449924468994
In grad_steps = 367, loss = 0.8647028803825378
In grad_steps = 368, loss = 0.5716433525085449
In grad_steps = 369, loss = 0.5168362259864807
In grad_steps = 370, loss = 0.7513732314109802
In grad_steps = 371, loss = 0.6934151649475098
In grad_steps = 372, loss = 0.648650586605072
In grad_steps = 373, loss = 0.6365548968315125
In grad_steps = 374, loss = 0.6536529064178467
In grad_steps = 375, loss = 0.6751072406768799
In grad_steps = 376, loss = 0.5328142642974854
In grad_steps = 377, loss = 0.7221943140029907
In grad_steps = 378, loss = 0.5442453622817993
In grad_steps = 379, loss = 0.61982262134552
In grad_steps = 380, loss = 0.7322858572006226
In grad_steps = 381, loss = 0.6485507488250732
In grad_steps = 382, loss = 0.469730406999588
In grad_steps = 383, loss = 0.47401779890060425
In grad_steps = 384, loss = 0.5672503709793091
In grad_steps = 385, loss = 0.7378442883491516
In grad_steps = 386, loss = 0.5083350539207458
In grad_steps = 387, loss = 0.5728548765182495
In grad_steps = 388, loss = 0.6032865643501282
In grad_steps = 389, loss = 0.3768230676651001
In grad_steps = 390, loss = 0.5041781663894653
In grad_steps = 391, loss = 0.6808035969734192
In grad_steps = 392, loss = 0.5393783450126648
In grad_steps = 393, loss = 0.5568690299987793
In grad_steps = 394, loss = 0.388753741979599
In grad_steps = 395, loss = 0.4199398159980774
In grad_steps = 396, loss = 0.5454742312431335
In grad_steps = 397, loss = 0.56553715467453
In grad_steps = 398, loss = 0.4426353871822357
In grad_steps = 399, loss = 0.6407662034034729
In grad_steps = 400, loss = 0.5224602222442627
In grad_steps = 401, loss = 0.37960630655288696
In grad_steps = 402, loss = 0.507167398929596
In grad_steps = 403, loss = 0.21621374785900116
Beginning epoch 5
In grad_steps = 404, loss = 0.7194785475730896
In grad_steps = 405, loss = 0.3825274109840393
In grad_steps = 406, loss = 0.5664311051368713
In grad_steps = 407, loss = 0.8347560167312622
In grad_steps = 408, loss = 0.5518164038658142
In grad_steps = 409, loss = 0.5449634790420532
In grad_steps = 410, loss = 0.4010612666606903
In grad_steps = 411, loss = 0.39054909348487854
In grad_steps = 412, loss = 0.6196262836456299
In grad_steps = 413, loss = 0.42387688159942627
In grad_steps = 414, loss = 0.5562199354171753
In grad_steps = 415, loss = 0.37493565678596497
In grad_steps = 416, loss = 0.5619539618492126
In grad_steps = 417, loss = 0.8711058497428894
In grad_steps = 418, loss = 0.5849077105522156
In grad_steps = 419, loss = 0.6089861989021301
In grad_steps = 420, loss = 0.578967809677124
In grad_steps = 421, loss = 0.5025604367256165
In grad_steps = 422, loss = 0.4167217016220093
In grad_steps = 423, loss = 0.4353554844856262
In grad_steps = 424, loss = 0.438711941242218
In grad_steps = 425, loss = 0.5429601073265076
In grad_steps = 426, loss = 0.5900698304176331
In grad_steps = 427, loss = 0.6375230550765991
In grad_steps = 428, loss = 0.23025977611541748
In grad_steps = 429, loss = 0.46066442131996155
In grad_steps = 430, loss = 0.30957257747650146
In grad_steps = 431, loss = 0.5977028012275696
In grad_steps = 432, loss = 0.2261463701725006
In grad_steps = 433, loss = 0.41427847743034363
In grad_steps = 434, loss = 0.18628279864788055
In grad_steps = 435, loss = 0.47595614194869995
In grad_steps = 436, loss = 0.3792363405227661
In grad_steps = 437, loss = 0.24594184756278992
In grad_steps = 438, loss = 0.2453320026397705
In grad_steps = 439, loss = 0.17833706736564636
In grad_steps = 440, loss = 0.12899775803089142
In grad_steps = 441, loss = 0.0945814698934555
In grad_steps = 442, loss = 0.13713063299655914
In grad_steps = 443, loss = 0.04200369119644165
In grad_steps = 444, loss = 0.014165774919092655
In grad_steps = 445, loss = 0.1334892362356186
In grad_steps = 446, loss = 0.728843092918396
In grad_steps = 447, loss = 0.5639768838882446
In grad_steps = 448, loss = 1.4244588613510132
In grad_steps = 449, loss = 0.2645115554332733
In grad_steps = 450, loss = 0.08460458368062973
In grad_steps = 451, loss = 0.49141964316368103
In grad_steps = 452, loss = 0.1782887727022171
In grad_steps = 453, loss = 0.38088685274124146
In grad_steps = 454, loss = 0.4150668680667877
In grad_steps = 455, loss = 0.6622471213340759
In grad_steps = 456, loss = 0.4593614935874939
In grad_steps = 457, loss = 0.37229254841804504
In grad_steps = 458, loss = 0.3239256739616394
In grad_steps = 459, loss = 0.24596525728702545
In grad_steps = 460, loss = 0.33556944131851196
In grad_steps = 461, loss = 0.3941107392311096
In grad_steps = 462, loss = 1.1856719255447388
In grad_steps = 463, loss = 0.8097774386405945
In grad_steps = 464, loss = 0.8825131058692932
In grad_steps = 465, loss = 0.4170370399951935
In grad_steps = 466, loss = 0.2731595039367676
In grad_steps = 467, loss = 0.3450050950050354
In grad_steps = 468, loss = 0.5975348949432373
In grad_steps = 469, loss = 0.5553796887397766
In grad_steps = 470, loss = 0.47012025117874146
In grad_steps = 471, loss = 0.7907736897468567
In grad_steps = 472, loss = 0.4857018291950226
In grad_steps = 473, loss = 0.567590594291687
In grad_steps = 474, loss = 0.8159224987030029
In grad_steps = 475, loss = 0.7072343826293945
In grad_steps = 476, loss = 0.6067419648170471
In grad_steps = 477, loss = 0.4713127911090851
In grad_steps = 478, loss = 0.6252154111862183
In grad_steps = 479, loss = 0.3354633152484894
In grad_steps = 480, loss = 0.5084424018859863
In grad_steps = 481, loss = 0.6869123578071594
In grad_steps = 482, loss = 0.746366024017334
In grad_steps = 483, loss = 0.2941123843193054
In grad_steps = 484, loss = 0.3885333836078644
In grad_steps = 485, loss = 0.5011345744132996
In grad_steps = 486, loss = 0.6429117321968079
In grad_steps = 487, loss = 0.5516197085380554
In grad_steps = 488, loss = 0.44778722524642944
In grad_steps = 489, loss = 0.48307543992996216
In grad_steps = 490, loss = 0.26577824354171753
In grad_steps = 491, loss = 0.3429942727088928
In grad_steps = 492, loss = 0.588797390460968
In grad_steps = 493, loss = 0.3631189465522766
In grad_steps = 494, loss = 0.3173903822898865
In grad_steps = 495, loss = 0.2905009090900421
In grad_steps = 496, loss = 0.45570725202560425
In grad_steps = 497, loss = 0.7684335708618164
In grad_steps = 498, loss = 0.17924991250038147
In grad_steps = 499, loss = 0.19201084971427917
In grad_steps = 500, loss = 0.4356979429721832
In grad_steps = 501, loss = 0.23289747536182404
In grad_steps = 502, loss = 0.20845670998096466
In grad_steps = 503, loss = 0.44259724020957947
In grad_steps = 504, loss = 0.027216646820306778
Beginning epoch 6
In grad_steps = 505, loss = 0.42328616976737976
In grad_steps = 506, loss = 0.19536948204040527
In grad_steps = 507, loss = 0.30164581537246704
In grad_steps = 508, loss = 0.508104145526886
In grad_steps = 509, loss = 0.22780455648899078
In grad_steps = 510, loss = 0.17867332696914673
In grad_steps = 511, loss = 0.3209339380264282
In grad_steps = 512, loss = 0.10958286374807358
In grad_steps = 513, loss = 0.378265917301178
In grad_steps = 514, loss = 0.4096786677837372
In grad_steps = 515, loss = 0.4506452679634094
In grad_steps = 516, loss = 0.05531376972794533
In grad_steps = 517, loss = 0.2613219618797302
In grad_steps = 518, loss = 0.6375740766525269
In grad_steps = 519, loss = 0.3400278091430664
In grad_steps = 520, loss = 0.4161810278892517
In grad_steps = 521, loss = 0.4133079946041107
In grad_steps = 522, loss = 0.3396730422973633
In grad_steps = 523, loss = 0.301832377910614
In grad_steps = 524, loss = 0.03404323384165764
In grad_steps = 525, loss = 0.13118089735507965
In grad_steps = 526, loss = 0.5586349368095398
In grad_steps = 527, loss = 0.3251033425331116
In grad_steps = 528, loss = 0.2233571708202362
In grad_steps = 529, loss = 0.03204062581062317
In grad_steps = 530, loss = 0.1557958573102951
In grad_steps = 531, loss = 0.06717421859502792
In grad_steps = 532, loss = 0.38284826278686523
In grad_steps = 533, loss = 0.3052467107772827
In grad_steps = 534, loss = 0.26235753297805786
In grad_steps = 535, loss = 0.041142988950014114
In grad_steps = 536, loss = 0.2357357144355774
In grad_steps = 537, loss = 0.07493951171636581
In grad_steps = 538, loss = 0.6863200664520264
In grad_steps = 539, loss = 0.3858456611633301
In grad_steps = 540, loss = 0.08157847076654434
In grad_steps = 541, loss = 0.31751886010169983
In grad_steps = 542, loss = 0.07769987732172012
In grad_steps = 543, loss = 0.1271486133337021
In grad_steps = 544, loss = 0.2947899103164673
In grad_steps = 545, loss = 0.10320895165205002
In grad_steps = 546, loss = 0.0525854229927063
In grad_steps = 547, loss = 0.31899553537368774
In grad_steps = 548, loss = 0.10454632341861725
In grad_steps = 549, loss = 0.3789145350456238
In grad_steps = 550, loss = 0.09534814953804016
In grad_steps = 551, loss = 0.013089990243315697
In grad_steps = 552, loss = 0.24399001896381378
In grad_steps = 553, loss = 0.40197911858558655
In grad_steps = 554, loss = 0.07775131613016129
In grad_steps = 555, loss = 0.2508944869041443
In grad_steps = 556, loss = 0.2165779024362564
In grad_steps = 557, loss = 1.1331902742385864
In grad_steps = 558, loss = 0.49571260809898376
In grad_steps = 559, loss = 0.24546130001544952
In grad_steps = 560, loss = 0.1695249229669571
In grad_steps = 561, loss = 0.3221297562122345
In grad_steps = 562, loss = 0.3245866596698761
In grad_steps = 563, loss = 0.8910629749298096
In grad_steps = 564, loss = 0.7035115361213684
In grad_steps = 565, loss = 0.7768954634666443
In grad_steps = 566, loss = 0.4543026387691498
In grad_steps = 567, loss = 0.4067816436290741
In grad_steps = 568, loss = 0.4604651927947998
In grad_steps = 569, loss = 0.6465460658073425
In grad_steps = 570, loss = 0.5580321550369263
In grad_steps = 571, loss = 0.34698593616485596
In grad_steps = 572, loss = 0.5705816149711609
In grad_steps = 573, loss = 0.5165746808052063
In grad_steps = 574, loss = 0.6314674615859985
In grad_steps = 575, loss = 0.43177828192710876
In grad_steps = 576, loss = 0.4393436312675476
In grad_steps = 577, loss = 0.5519017577171326
In grad_steps = 578, loss = 0.2345452606678009
In grad_steps = 579, loss = 0.44730865955352783
In grad_steps = 580, loss = 0.36227381229400635
In grad_steps = 581, loss = 0.7205181121826172
In grad_steps = 582, loss = 0.3802293837070465
In grad_steps = 583, loss = 0.30219757556915283
In grad_steps = 584, loss = 0.1323733627796173
In grad_steps = 585, loss = 0.24138300120830536
In grad_steps = 586, loss = 0.4477640986442566
In grad_steps = 587, loss = 0.7647549510002136
In grad_steps = 588, loss = 0.45908239483833313
In grad_steps = 589, loss = 0.5939421653747559
In grad_steps = 590, loss = 0.19974026083946228
In grad_steps = 591, loss = 0.047520026564598083
In grad_steps = 592, loss = 0.1613520383834839
In grad_steps = 593, loss = 0.4570186734199524
In grad_steps = 594, loss = 0.34707528352737427
In grad_steps = 595, loss = 0.3232751190662384
In grad_steps = 596, loss = 0.11368697136640549
In grad_steps = 597, loss = 0.2046642005443573
In grad_steps = 598, loss = 0.41851621866226196
In grad_steps = 599, loss = 0.3078712224960327
In grad_steps = 600, loss = 0.34672802686691284
In grad_steps = 601, loss = 0.5480015277862549
In grad_steps = 602, loss = 0.5164253115653992
In grad_steps = 603, loss = 0.16557084023952484
In grad_steps = 604, loss = 0.25045713782310486
In grad_steps = 605, loss = 0.026940232142806053
Beginning epoch 7
In grad_steps = 606, loss = 0.29390615224838257
In grad_steps = 607, loss = 0.21234753727912903
In grad_steps = 608, loss = 0.23956814408302307
In grad_steps = 609, loss = 0.4372716248035431
In grad_steps = 610, loss = 0.45597508549690247
In grad_steps = 611, loss = 0.2972009479999542
In grad_steps = 612, loss = 0.041741617023944855
In grad_steps = 613, loss = 0.09678773581981659
In grad_steps = 614, loss = 0.16689768433570862
In grad_steps = 615, loss = 0.12727667391300201
In grad_steps = 616, loss = 0.10899874567985535
In grad_steps = 617, loss = 0.09012073278427124
In grad_steps = 618, loss = 0.30861324071884155
In grad_steps = 619, loss = 0.760052502155304
In grad_steps = 620, loss = 0.6905092597007751
In grad_steps = 621, loss = 0.1724637746810913
In grad_steps = 622, loss = 0.25842276215553284
In grad_steps = 623, loss = 0.042771488428115845
In grad_steps = 624, loss = 0.0925174281001091
In grad_steps = 625, loss = 0.07481600344181061
In grad_steps = 626, loss = 0.22602611780166626
In grad_steps = 627, loss = 0.4000544250011444
In grad_steps = 628, loss = 0.6921785473823547
In grad_steps = 629, loss = 0.36961179971694946
In grad_steps = 630, loss = 0.04859839007258415
In grad_steps = 631, loss = 0.0445295087993145
In grad_steps = 632, loss = 0.1165667176246643
In grad_steps = 633, loss = 0.17795410752296448
In grad_steps = 634, loss = 0.12549076974391937
In grad_steps = 635, loss = 0.3186257481575012
In grad_steps = 636, loss = 0.24965958297252655
In grad_steps = 637, loss = 0.7427334785461426
In grad_steps = 638, loss = 0.09719344973564148
In grad_steps = 639, loss = 0.3542969822883606
In grad_steps = 640, loss = 0.07854011654853821
In grad_steps = 641, loss = 0.13135962188243866
In grad_steps = 642, loss = 0.15582138299942017
In grad_steps = 643, loss = 0.11165225505828857
In grad_steps = 644, loss = 0.4396360516548157
In grad_steps = 645, loss = 0.14851392805576324
In grad_steps = 646, loss = 1.3029839992523193
In grad_steps = 647, loss = 0.15239164233207703
In grad_steps = 648, loss = 0.12106039375066757
In grad_steps = 649, loss = 0.1356709599494934
In grad_steps = 650, loss = 0.7750795483589172
In grad_steps = 651, loss = 0.33338692784309387
In grad_steps = 652, loss = 0.20983798801898956
In grad_steps = 653, loss = 0.37711310386657715
In grad_steps = 654, loss = 0.19959627091884613
In grad_steps = 655, loss = 0.24764931201934814
In grad_steps = 656, loss = 0.1333152949810028
In grad_steps = 657, loss = 0.29010602831840515
In grad_steps = 658, loss = 0.18805666267871857
In grad_steps = 659, loss = 0.05408782511949539
In grad_steps = 660, loss = 0.2296425849199295
In grad_steps = 661, loss = 0.1450311690568924
In grad_steps = 662, loss = 0.18866749107837677
In grad_steps = 663, loss = 0.04937295243144035
In grad_steps = 664, loss = 0.4132031500339508
In grad_steps = 665, loss = 0.5793924927711487
In grad_steps = 666, loss = 1.111335039138794
In grad_steps = 667, loss = 0.5277794599533081
In grad_steps = 668, loss = 0.06516557186841965
In grad_steps = 669, loss = 0.37529027462005615
In grad_steps = 670, loss = 0.5510295033454895
In grad_steps = 671, loss = 0.1260356456041336
In grad_steps = 672, loss = 0.09180125594139099
In grad_steps = 673, loss = 0.6563247442245483
In grad_steps = 674, loss = 0.24805119633674622
In grad_steps = 675, loss = 0.8410725593566895
In grad_steps = 676, loss = 0.5850377082824707
In grad_steps = 677, loss = 0.3594907820224762
In grad_steps = 678, loss = 0.34965062141418457
In grad_steps = 679, loss = 0.13224153220653534
In grad_steps = 680, loss = 0.40084782242774963
In grad_steps = 681, loss = 0.30355462431907654
In grad_steps = 682, loss = 0.5639314651489258
In grad_steps = 683, loss = 0.28966835141181946
In grad_steps = 684, loss = 0.15761443972587585
In grad_steps = 685, loss = 0.19484350085258484
In grad_steps = 686, loss = 0.24131682515144348
In grad_steps = 687, loss = 0.1966383159160614
In grad_steps = 688, loss = 0.0760447159409523
In grad_steps = 689, loss = 0.10565666854381561
In grad_steps = 690, loss = 0.2857901453971863
In grad_steps = 691, loss = 0.07828046381473541
In grad_steps = 692, loss = 0.03380124270915985
In grad_steps = 693, loss = 0.12232401221990585
In grad_steps = 694, loss = 0.037953220307826996
In grad_steps = 695, loss = 0.020016584545373917
In grad_steps = 696, loss = 0.10118936747312546
In grad_steps = 697, loss = 0.4535084664821625
In grad_steps = 698, loss = 0.06918585300445557
In grad_steps = 699, loss = 0.008315378800034523
In grad_steps = 700, loss = 0.01119245495647192
In grad_steps = 701, loss = 0.00428701750934124
In grad_steps = 702, loss = 0.04599594697356224
In grad_steps = 703, loss = 0.06508408486843109
In grad_steps = 704, loss = 0.44256392121315
In grad_steps = 705, loss = 0.0888633280992508
In grad_steps = 706, loss = 0.04471193253993988
Beginning epoch 8
In grad_steps = 707, loss = 1.025968313217163
In grad_steps = 708, loss = 0.006496081128716469
In grad_steps = 709, loss = 0.4788975119590759
In grad_steps = 710, loss = 0.06855720281600952
In grad_steps = 711, loss = 0.09659118950366974
In grad_steps = 712, loss = 0.2547760605812073
In grad_steps = 713, loss = 0.49171027541160583
In grad_steps = 714, loss = 0.14987681806087494
In grad_steps = 715, loss = 0.07678977400064468
In grad_steps = 716, loss = 0.40139347314834595
In grad_steps = 717, loss = 0.09341971576213837
In grad_steps = 718, loss = 0.14984701573848724
In grad_steps = 719, loss = 0.18564020097255707
In grad_steps = 720, loss = 0.16911059617996216
In grad_steps = 721, loss = 0.13586032390594482
In grad_steps = 722, loss = 0.3206726908683777
In grad_steps = 723, loss = 0.7013298869132996
In grad_steps = 724, loss = 0.6211073398590088
In grad_steps = 725, loss = 0.4160764515399933
In grad_steps = 726, loss = 0.2952275276184082
In grad_steps = 727, loss = 0.041477225720882416
In grad_steps = 728, loss = 0.48812028765678406
In grad_steps = 729, loss = 0.09324511140584946
In grad_steps = 730, loss = 0.14026972651481628
In grad_steps = 731, loss = 0.10433518886566162
In grad_steps = 732, loss = 0.22883550822734833
In grad_steps = 733, loss = 0.35374972224235535
In grad_steps = 734, loss = 0.12364905327558517
In grad_steps = 735, loss = 0.36729079484939575
In grad_steps = 736, loss = 0.06628147512674332
In grad_steps = 737, loss = 0.018324904143810272
In grad_steps = 738, loss = 0.05245641991496086
In grad_steps = 739, loss = 0.11456041038036346
In grad_steps = 740, loss = 0.06755302101373672
In grad_steps = 741, loss = 0.1528286188840866
In grad_steps = 742, loss = 0.0485723540186882
In grad_steps = 743, loss = 0.5867828726768494
In grad_steps = 744, loss = 0.0787278488278389
In grad_steps = 745, loss = 0.028957465663552284
In grad_steps = 746, loss = 0.062260545790195465
In grad_steps = 747, loss = 0.03363293036818504
In grad_steps = 748, loss = 0.014420152641832829
In grad_steps = 749, loss = 0.026678098365664482
In grad_steps = 750, loss = 0.10519184917211533
In grad_steps = 751, loss = 0.020485714077949524
In grad_steps = 752, loss = 0.008679128251969814
In grad_steps = 753, loss = 0.007007500622421503
In grad_steps = 754, loss = 0.025071902200579643
In grad_steps = 755, loss = 0.04658546298742294
In grad_steps = 756, loss = 0.025737132877111435
In grad_steps = 757, loss = 0.3330199420452118
In grad_steps = 758, loss = 0.32549238204956055
In grad_steps = 759, loss = 0.09575765579938889
In grad_steps = 760, loss = 0.03531312942504883
In grad_steps = 761, loss = 0.4645805358886719
In grad_steps = 762, loss = 0.234164297580719
In grad_steps = 763, loss = 0.17898833751678467
In grad_steps = 764, loss = 0.009590212255716324
In grad_steps = 765, loss = 0.4691474139690399
In grad_steps = 766, loss = 0.3126298785209656
In grad_steps = 767, loss = 0.7380490303039551
In grad_steps = 768, loss = 0.029271073639392853
In grad_steps = 769, loss = 0.10210385918617249
In grad_steps = 770, loss = 0.32377633452415466
In grad_steps = 771, loss = 0.6675280332565308
In grad_steps = 772, loss = 0.05195154994726181
In grad_steps = 773, loss = 0.46185317635536194
In grad_steps = 774, loss = 0.4660760462284088
In grad_steps = 775, loss = 0.22785235941410065
In grad_steps = 776, loss = 0.22159788012504578
In grad_steps = 777, loss = 0.27558448910713196
In grad_steps = 778, loss = 0.5493768453598022
In grad_steps = 779, loss = 0.546984076499939
In grad_steps = 780, loss = 0.3535870909690857
In grad_steps = 781, loss = 0.7178345918655396
In grad_steps = 782, loss = 0.1824498474597931
In grad_steps = 783, loss = 0.3916735053062439
In grad_steps = 784, loss = 0.3380485773086548
In grad_steps = 785, loss = 0.18526896834373474
In grad_steps = 786, loss = 0.21225085854530334
In grad_steps = 787, loss = 0.17721697688102722
In grad_steps = 788, loss = 0.16901999711990356
In grad_steps = 789, loss = 0.15860670804977417
In grad_steps = 790, loss = 0.28736430406570435
In grad_steps = 791, loss = 0.27357664704322815
In grad_steps = 792, loss = 0.14057408273220062
In grad_steps = 793, loss = 0.10465529561042786
In grad_steps = 794, loss = 0.10468190163373947
In grad_steps = 795, loss = 0.11786503344774246
In grad_steps = 796, loss = 0.24174092710018158
In grad_steps = 797, loss = 0.03589942306280136
In grad_steps = 798, loss = 0.01148788258433342
In grad_steps = 799, loss = 0.013489038683474064
In grad_steps = 800, loss = 0.057064238935709
In grad_steps = 801, loss = 0.07193692773580551
In grad_steps = 802, loss = 0.0021863600704818964
In grad_steps = 803, loss = 0.1066228523850441
In grad_steps = 804, loss = 0.002261795336380601
In grad_steps = 805, loss = 0.0021265989635139704
In grad_steps = 806, loss = 0.00290834647603333
In grad_steps = 807, loss = 0.0003387239994481206
Beginning epoch 9
In grad_steps = 808, loss = 0.006975885946303606
In grad_steps = 809, loss = 0.0020855115726590157
In grad_steps = 810, loss = 0.04650300741195679
In grad_steps = 811, loss = 0.01271391473710537
In grad_steps = 812, loss = 0.7732672691345215
In grad_steps = 813, loss = 0.021313179284334183
In grad_steps = 814, loss = 0.28372764587402344
In grad_steps = 815, loss = 0.00844371598213911
In grad_steps = 816, loss = 0.001804141909815371
In grad_steps = 817, loss = 0.639422595500946
In grad_steps = 818, loss = 0.25565698742866516
In grad_steps = 819, loss = 0.20139290392398834
In grad_steps = 820, loss = 0.7086653709411621
In grad_steps = 821, loss = 0.062296152114868164
In grad_steps = 822, loss = 0.05265144258737564
In grad_steps = 823, loss = 0.07330984622240067
In grad_steps = 824, loss = 0.09584131836891174
In grad_steps = 825, loss = 0.029230834916234016
In grad_steps = 826, loss = 0.057295944541692734
In grad_steps = 827, loss = 0.11544623970985413
In grad_steps = 828, loss = 0.0302753746509552
In grad_steps = 829, loss = 0.4717745780944824
In grad_steps = 830, loss = 0.47232115268707275
In grad_steps = 831, loss = 0.3434499502182007
In grad_steps = 832, loss = 0.044730089604854584
In grad_steps = 833, loss = 0.07556580007076263
In grad_steps = 834, loss = 0.037317413836717606
In grad_steps = 835, loss = 0.05765257030725479
In grad_steps = 836, loss = 0.02813682146370411
In grad_steps = 837, loss = 0.11387704312801361
In grad_steps = 838, loss = 0.03369168937206268
In grad_steps = 839, loss = 0.08581899851560593
In grad_steps = 840, loss = 0.2548966705799103
In grad_steps = 841, loss = 0.04771502688527107
In grad_steps = 842, loss = 0.082567498087883
In grad_steps = 843, loss = 0.013630309142172337
In grad_steps = 844, loss = 0.03522424399852753
In grad_steps = 845, loss = 0.03125434368848801
In grad_steps = 846, loss = 0.11089659482240677
In grad_steps = 847, loss = 0.01626511476933956
In grad_steps = 848, loss = 0.08401403576135635
In grad_steps = 849, loss = 0.04156236723065376
In grad_steps = 850, loss = 0.009323382750153542
In grad_steps = 851, loss = 0.20546606183052063
In grad_steps = 852, loss = 0.48135656118392944
In grad_steps = 853, loss = 0.019676560536026955
In grad_steps = 854, loss = 0.020613189786672592
In grad_steps = 855, loss = 0.07469487935304642
In grad_steps = 856, loss = 0.006879724562168121
In grad_steps = 857, loss = 0.013378230854868889
In grad_steps = 858, loss = 0.007192191202193499
In grad_steps = 859, loss = 0.012746582739055157
In grad_steps = 860, loss = 0.02000439539551735
In grad_steps = 861, loss = 0.013262325897812843
In grad_steps = 862, loss = 0.7621227502822876
In grad_steps = 863, loss = 0.060080636292696
In grad_steps = 864, loss = 0.1749374270439148
In grad_steps = 865, loss = 0.15211661159992218
In grad_steps = 866, loss = 0.17447718977928162
In grad_steps = 867, loss = 0.024219203740358353
In grad_steps = 868, loss = 0.8039930462837219
In grad_steps = 869, loss = 0.21613633632659912
In grad_steps = 870, loss = 0.12743496894836426
In grad_steps = 871, loss = 0.02687227539718151
In grad_steps = 872, loss = 0.5394933223724365
In grad_steps = 873, loss = 0.28706011176109314
In grad_steps = 874, loss = 0.5576043128967285
In grad_steps = 875, loss = 0.8114029765129089
In grad_steps = 876, loss = 0.48377886414527893
In grad_steps = 877, loss = 0.2409050613641739
In grad_steps = 878, loss = 0.4112192988395691
In grad_steps = 879, loss = 0.30844905972480774
In grad_steps = 880, loss = 0.23341768980026245
In grad_steps = 881, loss = 0.1056889072060585
In grad_steps = 882, loss = 0.6025192141532898
In grad_steps = 883, loss = 0.12860912084579468
In grad_steps = 884, loss = 0.3240426480770111
In grad_steps = 885, loss = 0.7492004632949829
In grad_steps = 886, loss = 0.37622737884521484
In grad_steps = 887, loss = 0.1724984347820282
In grad_steps = 888, loss = 0.14950668811798096
In grad_steps = 889, loss = 0.24045686423778534
In grad_steps = 890, loss = 0.23645588755607605
In grad_steps = 891, loss = 0.30458876490592957
In grad_steps = 892, loss = 0.3949040174484253
In grad_steps = 893, loss = 0.07497883588075638
In grad_steps = 894, loss = 0.012801505625247955
In grad_steps = 895, loss = 0.10006722807884216
In grad_steps = 896, loss = 0.2958090603351593
In grad_steps = 897, loss = 0.04970477521419525
In grad_steps = 898, loss = 0.055167749524116516
In grad_steps = 899, loss = 0.005289295222610235
In grad_steps = 900, loss = 0.021070396527647972
In grad_steps = 901, loss = 0.04858902841806412
In grad_steps = 902, loss = 0.06507766991853714
In grad_steps = 903, loss = 0.0016298412811011076
In grad_steps = 904, loss = 0.0586993433535099
In grad_steps = 905, loss = 0.07549865543842316
In grad_steps = 906, loss = 0.0009198148618452251
In grad_steps = 907, loss = 0.011162406764924526
In grad_steps = 908, loss = 7.557450589956716e-05
Beginning epoch 10
In grad_steps = 909, loss = 0.005968250334262848
In grad_steps = 910, loss = 0.00026876459014602005
In grad_steps = 911, loss = 0.3915427029132843
In grad_steps = 912, loss = 0.17093637585639954
In grad_steps = 913, loss = 0.012266689911484718
In grad_steps = 914, loss = 0.2087971270084381
In grad_steps = 915, loss = 0.24604661762714386
In grad_steps = 916, loss = 0.0008751403074711561
In grad_steps = 917, loss = 0.05851386487483978
In grad_steps = 918, loss = 0.04692636802792549
In grad_steps = 919, loss = 0.47936880588531494
In grad_steps = 920, loss = 0.019659442827105522
In grad_steps = 921, loss = 0.020349599421024323
In grad_steps = 922, loss = 0.03924217447638512
In grad_steps = 923, loss = 0.05958330258727074
In grad_steps = 924, loss = 0.2183455377817154
In grad_steps = 925, loss = 0.23613731563091278
In grad_steps = 926, loss = 0.010590891353785992
In grad_steps = 927, loss = 0.039339564740657806
In grad_steps = 928, loss = 0.015293775126338005
In grad_steps = 929, loss = 0.018422458320856094
In grad_steps = 930, loss = 0.3105025887489319
In grad_steps = 931, loss = 0.04802235960960388
In grad_steps = 932, loss = 0.18990471959114075
In grad_steps = 933, loss = 0.0061821588315069675
In grad_steps = 934, loss = 0.010334030725061893
In grad_steps = 935, loss = 0.06716574728488922
In grad_steps = 936, loss = 0.10874928534030914
In grad_steps = 937, loss = 0.48446139693260193
In grad_steps = 938, loss = 0.09205718338489532
In grad_steps = 939, loss = 0.020917003974318504
In grad_steps = 940, loss = 0.05260375887155533
In grad_steps = 941, loss = 0.032526660710573196
In grad_steps = 942, loss = 0.08510629087686539
In grad_steps = 943, loss = 0.15436908602714539
In grad_steps = 944, loss = 0.07056377828121185
In grad_steps = 945, loss = 0.03368520364165306
In grad_steps = 946, loss = 0.38829731941223145
In grad_steps = 947, loss = 0.23997607827186584
In grad_steps = 948, loss = 0.3004646301269531
In grad_steps = 949, loss = 0.10254984349012375
In grad_steps = 950, loss = 0.07704757899045944
In grad_steps = 951, loss = 0.21776074171066284
In grad_steps = 952, loss = 0.34147781133651733
In grad_steps = 953, loss = 0.2521837055683136
In grad_steps = 954, loss = 0.05523607134819031
In grad_steps = 955, loss = 0.1205897182226181
In grad_steps = 956, loss = 0.04877007007598877
In grad_steps = 957, loss = 0.00822113361209631
In grad_steps = 958, loss = 0.012599688023328781
In grad_steps = 959, loss = 0.024062389507889748
In grad_steps = 960, loss = 0.018160490319132805
In grad_steps = 961, loss = 0.5795707106590271
In grad_steps = 962, loss = 0.010697522200644016
In grad_steps = 963, loss = 0.3063300549983978
In grad_steps = 964, loss = 0.02909470722079277
In grad_steps = 965, loss = 0.15345510840415955
In grad_steps = 966, loss = 0.027196956798434258
In grad_steps = 967, loss = 0.03706987574696541
In grad_steps = 968, loss = 0.2784917950630188
In grad_steps = 969, loss = 0.20569361746311188
In grad_steps = 970, loss = 0.0018294458277523518
In grad_steps = 971, loss = 0.005561506375670433
In grad_steps = 972, loss = 0.08569107949733734
In grad_steps = 973, loss = 0.5394247174263
In grad_steps = 974, loss = 0.005929118022322655
In grad_steps = 975, loss = 0.021200861781835556
In grad_steps = 976, loss = 0.877652108669281
In grad_steps = 977, loss = 0.2774902880191803
In grad_steps = 978, loss = 0.09122319519519806
In grad_steps = 979, loss = 0.24189935624599457
In grad_steps = 980, loss = 0.1512971818447113
In grad_steps = 981, loss = 0.12294837087392807
In grad_steps = 982, loss = 0.029771912842988968
In grad_steps = 983, loss = 0.05510127544403076
In grad_steps = 984, loss = 0.20283114910125732
In grad_steps = 985, loss = 0.2146480530500412
In grad_steps = 986, loss = 0.11837779730558395
In grad_steps = 987, loss = 0.19095537066459656
In grad_steps = 988, loss = 0.0469796247780323
In grad_steps = 989, loss = 0.27136924862861633
In grad_steps = 990, loss = 0.06277574598789215
In grad_steps = 991, loss = 0.5950803756713867
In grad_steps = 992, loss = 0.5602307915687561
In grad_steps = 993, loss = 0.22033943235874176
In grad_steps = 994, loss = 0.23753061890602112
In grad_steps = 995, loss = 0.014683914370834827
In grad_steps = 996, loss = 0.013568122871220112
In grad_steps = 997, loss = 0.13479439914226532
In grad_steps = 998, loss = 0.6037434339523315
In grad_steps = 999, loss = 0.637052595615387
In grad_steps = 1000, loss = 0.5761712789535522
In grad_steps = 1001, loss = 0.26482969522476196
In grad_steps = 1002, loss = 0.2799503803253174
In grad_steps = 1003, loss = 0.11553920060396194
In grad_steps = 1004, loss = 0.0929410383105278
In grad_steps = 1005, loss = 0.17605850100517273
In grad_steps = 1006, loss = 0.20181937515735626
In grad_steps = 1007, loss = 0.06129591912031174
In grad_steps = 1008, loss = 0.12325158715248108
In grad_steps = 1009, loss = 0.015648353844881058
Beginning epoch 11
In grad_steps = 1010, loss = 0.07545184344053268
In grad_steps = 1011, loss = 0.11449990421533585
In grad_steps = 1012, loss = 0.0885840356349945
In grad_steps = 1013, loss = 0.5495740175247192
In grad_steps = 1014, loss = 0.5705823302268982
In grad_steps = 1015, loss = 0.01818285696208477
In grad_steps = 1016, loss = 0.06384631991386414
In grad_steps = 1017, loss = 0.054927416145801544
In grad_steps = 1018, loss = 0.06057290732860565
In grad_steps = 1019, loss = 0.03760085999965668
In grad_steps = 1020, loss = 0.07817452400922775
In grad_steps = 1021, loss = 0.014071342535316944
In grad_steps = 1022, loss = 0.054189883172512054
In grad_steps = 1023, loss = 0.05418691784143448
In grad_steps = 1024, loss = 0.05394774675369263
In grad_steps = 1025, loss = 0.034713082015514374
In grad_steps = 1026, loss = 0.04835851490497589
In grad_steps = 1027, loss = 0.015190379694104195
In grad_steps = 1028, loss = 0.003107955912128091
In grad_steps = 1029, loss = 0.002595210447907448
In grad_steps = 1030, loss = 0.0032443292438983917
In grad_steps = 1031, loss = 0.010472469963133335
In grad_steps = 1032, loss = 0.0014649060321971774
In grad_steps = 1033, loss = 0.007667087018489838
In grad_steps = 1034, loss = 0.0013659142423421144
In grad_steps = 1035, loss = 0.004008593037724495
In grad_steps = 1036, loss = 0.012950746342539787
In grad_steps = 1037, loss = 0.0018327297875657678
In grad_steps = 1038, loss = 0.0018854755908250809
In grad_steps = 1039, loss = 0.022344088181853294
In grad_steps = 1040, loss = 0.00028823106549680233
In grad_steps = 1041, loss = 0.003561600111424923
In grad_steps = 1042, loss = 0.00042231695260852575
In grad_steps = 1043, loss = 0.001682364963926375
In grad_steps = 1044, loss = 0.0007439888431690633
In grad_steps = 1045, loss = 0.01729796826839447
In grad_steps = 1046, loss = 0.002375173382461071
In grad_steps = 1047, loss = 0.08391169458627701
In grad_steps = 1048, loss = 0.0002602785243652761
In grad_steps = 1049, loss = 0.0008769892738200724
In grad_steps = 1050, loss = 0.4597982168197632
In grad_steps = 1051, loss = 0.009785594418644905
In grad_steps = 1052, loss = 0.0007324416073970497
In grad_steps = 1053, loss = 0.028939509764313698
In grad_steps = 1054, loss = 0.0006157446186989546
In grad_steps = 1055, loss = 0.002339468337595463
In grad_steps = 1056, loss = 0.0048166257329285145
In grad_steps = 1057, loss = 0.000533178448677063
In grad_steps = 1058, loss = 0.13496851921081543
In grad_steps = 1059, loss = 0.0016092445002868772
In grad_steps = 1060, loss = 0.0005640690797008574
In grad_steps = 1061, loss = 0.00433546444401145
In grad_steps = 1062, loss = 0.00812138058245182
In grad_steps = 1063, loss = 0.0004939159844070673
In grad_steps = 1064, loss = 0.025527460500597954
In grad_steps = 1065, loss = 0.0007377698784694076
In grad_steps = 1066, loss = 0.03516256436705589
In grad_steps = 1067, loss = 0.0005118632689118385
In grad_steps = 1068, loss = 0.0017338076140731573
In grad_steps = 1069, loss = 0.17599524557590485
In grad_steps = 1070, loss = 0.1306944340467453
In grad_steps = 1071, loss = 0.00034659539232961833
In grad_steps = 1072, loss = 0.0006608020048588514
In grad_steps = 1073, loss = 0.0005517337704077363
In grad_steps = 1074, loss = 0.39877307415008545
In grad_steps = 1075, loss = 0.0007806483190506697
In grad_steps = 1076, loss = 0.0003950770478695631
In grad_steps = 1077, loss = 0.7666340470314026
In grad_steps = 1078, loss = 0.002538888016715646
In grad_steps = 1079, loss = 0.001897623180411756
In grad_steps = 1080, loss = 0.05591631308197975
In grad_steps = 1081, loss = 0.38371801376342773
In grad_steps = 1082, loss = 0.02212812937796116
In grad_steps = 1083, loss = 0.03637215495109558
In grad_steps = 1084, loss = 1.2582149505615234
In grad_steps = 1085, loss = 0.01883300580084324
In grad_steps = 1086, loss = 0.060814227908849716
In grad_steps = 1087, loss = 0.2746208906173706
In grad_steps = 1088, loss = 0.0868704542517662
In grad_steps = 1089, loss = 0.11974671483039856
In grad_steps = 1090, loss = 0.05914214998483658
In grad_steps = 1091, loss = 0.017375309020280838
In grad_steps = 1092, loss = 0.07169939577579498
In grad_steps = 1093, loss = 0.040548134595155716
In grad_steps = 1094, loss = 0.09730386734008789
In grad_steps = 1095, loss = 0.09099344909191132
In grad_steps = 1096, loss = 0.14929190278053284
In grad_steps = 1097, loss = 0.052979178726673126
In grad_steps = 1098, loss = 0.11626562476158142
In grad_steps = 1099, loss = 0.02076643332839012
In grad_steps = 1100, loss = 0.04846182465553284
In grad_steps = 1101, loss = 0.02920345962047577
In grad_steps = 1102, loss = 0.04565118998289108
In grad_steps = 1103, loss = 0.007531474810093641
In grad_steps = 1104, loss = 0.005725407041609287
In grad_steps = 1105, loss = 0.02259599044919014
In grad_steps = 1106, loss = 0.009602298028767109
In grad_steps = 1107, loss = 0.0034377933479845524
In grad_steps = 1108, loss = 0.00433627562597394
In grad_steps = 1109, loss = 0.19596409797668457
In grad_steps = 1110, loss = 0.0006847712211310863
Beginning epoch 12
In grad_steps = 1111, loss = 0.010536991991102695
In grad_steps = 1112, loss = 0.014601740054786205
In grad_steps = 1113, loss = 0.002457955153658986
In grad_steps = 1114, loss = 0.023873817175626755
In grad_steps = 1115, loss = 0.11478737741708755
In grad_steps = 1116, loss = 0.0020103855058550835
In grad_steps = 1117, loss = 0.004957180470228195
In grad_steps = 1118, loss = 0.001359828980639577
In grad_steps = 1119, loss = 0.0007669375627301633
In grad_steps = 1120, loss = 0.004581643268465996
In grad_steps = 1121, loss = 0.0029984754510223866
In grad_steps = 1122, loss = 0.0011360228527337313
In grad_steps = 1123, loss = 0.007002537604421377
In grad_steps = 1124, loss = 0.005773419514298439
In grad_steps = 1125, loss = 0.029017260298132896
In grad_steps = 1126, loss = 0.017591850832104683
In grad_steps = 1127, loss = 0.42937880754470825
In grad_steps = 1128, loss = 0.0021304150577634573
In grad_steps = 1129, loss = 0.002201069612056017
In grad_steps = 1130, loss = 0.0004398275341372937
In grad_steps = 1131, loss = 0.0009745840798132122
In grad_steps = 1132, loss = 0.0025118908379226923
In grad_steps = 1133, loss = 0.00114718871191144
In grad_steps = 1134, loss = 0.564175546169281
In grad_steps = 1135, loss = 0.0009182526264339685
In grad_steps = 1136, loss = 0.0021490934304893017
In grad_steps = 1137, loss = 0.034948959946632385
In grad_steps = 1138, loss = 0.001922760857269168
In grad_steps = 1139, loss = 0.031462837010622025
In grad_steps = 1140, loss = 0.056754741817712784
In grad_steps = 1141, loss = 0.002897722879424691
In grad_steps = 1142, loss = 0.7708950042724609
In grad_steps = 1143, loss = 0.08339545130729675
In grad_steps = 1144, loss = 0.05809536948800087
In grad_steps = 1145, loss = 0.02443987876176834
In grad_steps = 1146, loss = 0.0042916941456496716
In grad_steps = 1147, loss = 0.02861584909260273
In grad_steps = 1148, loss = 0.09490436315536499
In grad_steps = 1149, loss = 0.030753118917346
In grad_steps = 1150, loss = 0.1402062177658081
In grad_steps = 1151, loss = 0.0047320290468633175
In grad_steps = 1152, loss = 0.09500987827777863
In grad_steps = 1153, loss = 0.1387530416250229
In grad_steps = 1154, loss = 0.08208158612251282
In grad_steps = 1155, loss = 0.028262095525860786
In grad_steps = 1156, loss = 0.012732382863759995
In grad_steps = 1157, loss = 0.0864928588271141
In grad_steps = 1158, loss = 0.01002947986125946
In grad_steps = 1159, loss = 0.054279860109090805
In grad_steps = 1160, loss = 0.13908430933952332
In grad_steps = 1161, loss = 0.061123479157686234
In grad_steps = 1162, loss = 0.04863383620977402
In grad_steps = 1163, loss = 0.9347856044769287
In grad_steps = 1164, loss = 0.07591290026903152
In grad_steps = 1165, loss = 0.31316256523132324
In grad_steps = 1166, loss = 0.04785272851586342
In grad_steps = 1167, loss = 0.013168616220355034
In grad_steps = 1168, loss = 0.011765613220632076
In grad_steps = 1169, loss = 0.13467252254486084
In grad_steps = 1170, loss = 0.31499484181404114
In grad_steps = 1171, loss = 0.2720796763896942
In grad_steps = 1172, loss = 0.006335797253996134
In grad_steps = 1173, loss = 0.0038493555039167404
In grad_steps = 1174, loss = 0.007135936524719
In grad_steps = 1175, loss = 0.5682461857795715
In grad_steps = 1176, loss = 0.0193023681640625
In grad_steps = 1177, loss = 0.12379232048988342
In grad_steps = 1178, loss = 0.5059511065483093
In grad_steps = 1179, loss = 0.916214644908905
In grad_steps = 1180, loss = 0.08452034741640091
In grad_steps = 1181, loss = 1.068390965461731
In grad_steps = 1182, loss = 0.41623052954673767
In grad_steps = 1183, loss = 0.12729504704475403
In grad_steps = 1184, loss = 0.0357942096889019
In grad_steps = 1185, loss = 0.23996157944202423
In grad_steps = 1186, loss = 0.05512773618102074
In grad_steps = 1187, loss = 0.14006318151950836
In grad_steps = 1188, loss = 0.11387085914611816
In grad_steps = 1189, loss = 0.11190861463546753
In grad_steps = 1190, loss = 0.08476082235574722
In grad_steps = 1191, loss = 0.02184896171092987
In grad_steps = 1192, loss = 0.31518760323524475
In grad_steps = 1193, loss = 0.043830614537000656
In grad_steps = 1194, loss = 0.13433837890625
In grad_steps = 1195, loss = 0.03463718295097351
In grad_steps = 1196, loss = 0.09306593239307404
In grad_steps = 1197, loss = 0.015211213380098343
In grad_steps = 1198, loss = 0.009014997631311417
In grad_steps = 1199, loss = 0.029022878035902977
In grad_steps = 1200, loss = 0.018073800951242447
In grad_steps = 1201, loss = 0.15863895416259766
In grad_steps = 1202, loss = 0.01030556671321392
In grad_steps = 1203, loss = 0.14514444768428802
In grad_steps = 1204, loss = 0.010173817165195942
In grad_steps = 1205, loss = 0.0024687706027179956
In grad_steps = 1206, loss = 0.03939167410135269
In grad_steps = 1207, loss = 0.016932746395468712
In grad_steps = 1208, loss = 0.005858138669282198
In grad_steps = 1209, loss = 0.07621638476848602
In grad_steps = 1210, loss = 0.002911078277975321
In grad_steps = 1211, loss = 0.00034457657602615654
Elapsed time: 2138.1133975982666 seconds for ensemble 3 with 12 epochs
LoRA instance 3 evaluation complete. Data saved to /pscratch/sd/t/tianle/lucid/other_source/SURP_2024/results/tmp_lora_ensemble/Llama3-8B-set-1.1/test_data_instance_3_seed_30280.npz.
lora instance i = 3 Successfully finished.
Training lora instance 4
DEBUG: Total parameters: 8037076992
DEBUG: Trainable parameters: 6815744
Beginning epoch 1
In grad_steps = 0, loss = 1.6091442108154297
In grad_steps = 1, loss = 0.9641543030738831
In grad_steps = 2, loss = 0.7029867172241211
In grad_steps = 3, loss = 0.7475348711013794
In grad_steps = 4, loss = 1.2049369812011719
In grad_steps = 5, loss = 0.4255390763282776
In grad_steps = 6, loss = 0.8697996735572815
In grad_steps = 7, loss = 0.6950026154518127
In grad_steps = 8, loss = 0.8067558407783508
In grad_steps = 9, loss = 0.6503450870513916
In grad_steps = 10, loss = 0.7116668820381165
In grad_steps = 11, loss = 0.7009565830230713
In grad_steps = 12, loss = 0.7014205455780029
In grad_steps = 13, loss = 0.6353293657302856
In grad_steps = 14, loss = 0.7466102242469788
In grad_steps = 15, loss = 0.8368707895278931
In grad_steps = 16, loss = 0.7024217247962952
In grad_steps = 17, loss = 0.6869763731956482
In grad_steps = 18, loss = 0.6897785663604736
In grad_steps = 19, loss = 0.7113821506500244
In grad_steps = 20, loss = 0.6752366423606873
In grad_steps = 21, loss = 0.7086371779441833
In grad_steps = 22, loss = 0.6788898706436157
In grad_steps = 23, loss = 0.7095733880996704
In grad_steps = 24, loss = 0.7105892896652222
In grad_steps = 25, loss = 0.7497092485427856
In grad_steps = 26, loss = 0.6464962959289551
In grad_steps = 27, loss = 0.7349886894226074
In grad_steps = 28, loss = 0.6602288484573364
In grad_steps = 29, loss = 0.707525908946991
In grad_steps = 30, loss = 0.6821393966674805
In grad_steps = 31, loss = 0.6966301798820496
In grad_steps = 32, loss = 0.7452805042266846
In grad_steps = 33, loss = 0.6849069595336914
In grad_steps = 34, loss = 0.791020393371582
In grad_steps = 35, loss = 0.689459502696991
In grad_steps = 36, loss = 0.7243209481239319
In grad_steps = 37, loss = 0.6747035980224609
In grad_steps = 38, loss = 0.7350582480430603
In grad_steps = 39, loss = 0.7027584314346313
In grad_steps = 40, loss = 0.6897611021995544
In grad_steps = 41, loss = 0.7033859491348267
In grad_steps = 42, loss = 0.6977401375770569
In grad_steps = 43, loss = 0.6876600384712219
In grad_steps = 44, loss = 0.6910909414291382
In grad_steps = 45, loss = 0.6784057021141052
In grad_steps = 46, loss = 0.6992440223693848
In grad_steps = 47, loss = 0.729114294052124
In grad_steps = 48, loss = 0.6869679093360901
In grad_steps = 49, loss = 0.6742786765098572
In grad_steps = 50, loss = 0.6624910831451416
In grad_steps = 51, loss = 0.7177363634109497
In grad_steps = 52, loss = 0.725895345211029
In grad_steps = 53, loss = 0.6584261655807495
In grad_steps = 54, loss = 0.5731360912322998
In grad_steps = 55, loss = 0.6064969897270203
In grad_steps = 56, loss = 0.5685116052627563
In grad_steps = 57, loss = 0.5604944229125977
In grad_steps = 58, loss = 0.7507339119911194
In grad_steps = 59, loss = 0.6854657530784607
In grad_steps = 60, loss = 0.8079444169998169
In grad_steps = 61, loss = 0.6730048656463623
In grad_steps = 62, loss = 0.9943134784698486
In grad_steps = 63, loss = 0.7648921012878418
In grad_steps = 64, loss = 0.5896234512329102
In grad_steps = 65, loss = 0.5765172243118286
In grad_steps = 66, loss = 0.8089020848274231
In grad_steps = 67, loss = 0.6613726019859314
In grad_steps = 68, loss = 0.7319605350494385
In grad_steps = 69, loss = 0.6892827749252319
In grad_steps = 70, loss = 0.7237343192100525
In grad_steps = 71, loss = 0.6976888179779053
In grad_steps = 72, loss = 0.6837806701660156
In grad_steps = 73, loss = 0.710118293762207
In grad_steps = 74, loss = 0.674807071685791
In grad_steps = 75, loss = 0.6737714409828186
In grad_steps = 76, loss = 0.6565332412719727
In grad_steps = 77, loss = 1.0441224575042725
In grad_steps = 78, loss = 0.8162971138954163
In grad_steps = 79, loss = 0.6356152296066284
In grad_steps = 80, loss = 0.6865203976631165
In grad_steps = 81, loss = 0.6753931045532227
In grad_steps = 82, loss = 0.7247303128242493
In grad_steps = 83, loss = 0.7053264379501343
In grad_steps = 84, loss = 0.7131560444831848
In grad_steps = 85, loss = 0.7085406184196472
In grad_steps = 86, loss = 0.6555290222167969
In grad_steps = 87, loss = 0.6910114288330078
In grad_steps = 88, loss = 0.7102881073951721
In grad_steps = 89, loss = 0.6780403852462769
In grad_steps = 90, loss = 0.7077736854553223
In grad_steps = 91, loss = 0.6553855538368225
In grad_steps = 92, loss = 0.6774899959564209
In grad_steps = 93, loss = 0.7252544164657593
In grad_steps = 94, loss = 0.7426137924194336
In grad_steps = 95, loss = 0.6484520435333252
In grad_steps = 96, loss = 0.8646092414855957
In grad_steps = 97, loss = 0.7630027532577515
In grad_steps = 98, loss = 0.6445063352584839
In grad_steps = 99, loss = 0.6259499192237854
In grad_steps = 100, loss = 0.6852797269821167
Beginning epoch 2
In grad_steps = 101, loss = 0.639564037322998
In grad_steps = 102, loss = 0.6465347409248352
In grad_steps = 103, loss = 0.6597388982772827
In grad_steps = 104, loss = 0.682340145111084
In grad_steps = 105, loss = 0.7100517749786377
In grad_steps = 106, loss = 0.629658579826355
In grad_steps = 107, loss = 0.6466046571731567
In grad_steps = 108, loss = 0.6365653872489929
In grad_steps = 109, loss = 0.7033022046089172
In grad_steps = 110, loss = 0.6441724300384521
In grad_steps = 111, loss = 0.7313238382339478
In grad_steps = 112, loss = 0.6572451591491699
In grad_steps = 113, loss = 0.7029618620872498
In grad_steps = 114, loss = 0.7460903525352478
In grad_steps = 115, loss = 0.7044411301612854
In grad_steps = 116, loss = 0.659945011138916
In grad_steps = 117, loss = 0.644482433795929
In grad_steps = 118, loss = 0.6655094623565674
In grad_steps = 119, loss = 0.7150828838348389
In grad_steps = 120, loss = 0.6680563688278198
In grad_steps = 121, loss = 0.5940631628036499
In grad_steps = 122, loss = 0.7002521753311157
In grad_steps = 123, loss = 0.7179383039474487
In grad_steps = 124, loss = 0.7049360871315002
In grad_steps = 125, loss = 0.6243208646774292
In grad_steps = 126, loss = 0.5454502105712891
In grad_steps = 127, loss = 0.7023849487304688
In grad_steps = 128, loss = 0.6463085412979126
In grad_steps = 129, loss = 0.5519851446151733
In grad_steps = 130, loss = 0.7068057060241699
In grad_steps = 131, loss = 0.5551912784576416
In grad_steps = 132, loss = 0.7053157091140747
In grad_steps = 133, loss = 0.9167384505271912
In grad_steps = 134, loss = 0.6948192715644836
In grad_steps = 135, loss = 0.7986589670181274
In grad_steps = 136, loss = 0.6751919984817505
In grad_steps = 137, loss = 0.6641610264778137
In grad_steps = 138, loss = 0.6194564700126648
In grad_steps = 139, loss = 0.7579468488693237
In grad_steps = 140, loss = 0.8359670639038086
In grad_steps = 141, loss = 0.6047598719596863
In grad_steps = 142, loss = 0.6719024777412415
In grad_steps = 143, loss = 0.679311215877533
In grad_steps = 144, loss = 0.69667649269104
In grad_steps = 145, loss = 0.6606432199478149
In grad_steps = 146, loss = 0.6341488361358643
In grad_steps = 147, loss = 0.6515897512435913
In grad_steps = 148, loss = 0.6218616962432861
In grad_steps = 149, loss = 0.6190651655197144
In grad_steps = 150, loss = 0.6577808260917664
In grad_steps = 151, loss = 0.5825963020324707
In grad_steps = 152, loss = 0.760250449180603
In grad_steps = 153, loss = 0.7383773922920227
In grad_steps = 154, loss = 0.6308437585830688
In grad_steps = 155, loss = 0.5444794297218323
In grad_steps = 156, loss = 0.5891106128692627
In grad_steps = 157, loss = 0.5091882348060608
In grad_steps = 158, loss = 0.5008180737495422
In grad_steps = 159, loss = 0.7444998621940613
In grad_steps = 160, loss = 0.7106223702430725
In grad_steps = 161, loss = 0.7899032831192017
In grad_steps = 162, loss = 0.623755931854248
In grad_steps = 163, loss = 0.8667433261871338
In grad_steps = 164, loss = 0.7033680081367493
In grad_steps = 165, loss = 0.5764404535293579
In grad_steps = 166, loss = 0.5647844076156616
In grad_steps = 167, loss = 0.6671271920204163
In grad_steps = 168, loss = 0.6309067010879517
In grad_steps = 169, loss = 0.6816503405570984
In grad_steps = 170, loss = 0.7222221493721008
In grad_steps = 171, loss = 0.6050998568534851
In grad_steps = 172, loss = 0.6772510409355164
In grad_steps = 173, loss = 0.6725885272026062
In grad_steps = 174, loss = 0.6902183890342712
In grad_steps = 175, loss = 0.6735693216323853
In grad_steps = 176, loss = 0.6547434329986572
In grad_steps = 177, loss = 0.6242563724517822
In grad_steps = 178, loss = 0.8826047778129578
In grad_steps = 179, loss = 0.7323246598243713
In grad_steps = 180, loss = 0.5512793064117432
In grad_steps = 181, loss = 0.5687106251716614
In grad_steps = 182, loss = 0.6461549401283264
In grad_steps = 183, loss = 0.7302280068397522
In grad_steps = 184, loss = 0.7159072160720825
In grad_steps = 185, loss = 0.7104130983352661
In grad_steps = 186, loss = 0.6972192525863647
In grad_steps = 187, loss = 0.5104218125343323
In grad_steps = 188, loss = 0.6577816009521484
In grad_steps = 189, loss = 0.6788122653961182
In grad_steps = 190, loss = 0.6964854001998901
In grad_steps = 191, loss = 0.6378511786460876
In grad_steps = 192, loss = 0.5729770660400391
In grad_steps = 193, loss = 0.5932130813598633
In grad_steps = 194, loss = 0.7110288739204407
In grad_steps = 195, loss = 0.7959887981414795
In grad_steps = 196, loss = 0.5772101879119873
In grad_steps = 197, loss = 0.572160542011261
In grad_steps = 198, loss = 0.6838212609291077
In grad_steps = 199, loss = 0.47465261816978455
In grad_steps = 200, loss = 0.6129968166351318
In grad_steps = 201, loss = 0.6828500628471375
Beginning epoch 3
In grad_steps = 202, loss = 0.6003671288490295
In grad_steps = 203, loss = 0.5392613410949707
In grad_steps = 204, loss = 0.6706958413124084
In grad_steps = 205, loss = 0.7399210929870605
In grad_steps = 206, loss = 0.6583914756774902
In grad_steps = 207, loss = 0.5479702949523926
In grad_steps = 208, loss = 0.6064990758895874
In grad_steps = 209, loss = 0.49682024121284485
In grad_steps = 210, loss = 0.7829744815826416
In grad_steps = 211, loss = 0.5317654609680176
In grad_steps = 212, loss = 0.752521276473999
In grad_steps = 213, loss = 0.6080420613288879
In grad_steps = 214, loss = 0.6720594167709351
In grad_steps = 215, loss = 0.7598885893821716
In grad_steps = 216, loss = 0.688174843788147
In grad_steps = 217, loss = 0.6512847542762756
In grad_steps = 218, loss = 0.6662919521331787
In grad_steps = 219, loss = 0.6325523853302002
In grad_steps = 220, loss = 0.5720966458320618
In grad_steps = 221, loss = 0.581338107585907
In grad_steps = 222, loss = 0.5211683511734009
In grad_steps = 223, loss = 0.6634436845779419
In grad_steps = 224, loss = 0.7500561475753784
In grad_steps = 225, loss = 0.6941391229629517
In grad_steps = 226, loss = 0.5373560190200806
In grad_steps = 227, loss = 0.41818392276763916
In grad_steps = 228, loss = 0.6280953884124756
In grad_steps = 229, loss = 0.7147946357727051
In grad_steps = 230, loss = 0.37841781973838806
In grad_steps = 231, loss = 0.6142895221710205
In grad_steps = 232, loss = 0.4938364624977112
In grad_steps = 233, loss = 0.6385255455970764
In grad_steps = 234, loss = 0.6206623911857605
In grad_steps = 235, loss = 0.5493994951248169
In grad_steps = 236, loss = 0.818924605846405
In grad_steps = 237, loss = 0.397787868976593
In grad_steps = 238, loss = 0.5738697052001953
In grad_steps = 239, loss = 0.668101966381073
In grad_steps = 240, loss = 0.8965451717376709
In grad_steps = 241, loss = 0.9226710200309753
In grad_steps = 242, loss = 0.5421429872512817
In grad_steps = 243, loss = 0.6850422620773315
In grad_steps = 244, loss = 0.6504127979278564
In grad_steps = 245, loss = 0.6771810054779053
In grad_steps = 246, loss = 0.6163692474365234
In grad_steps = 247, loss = 0.5286883115768433
In grad_steps = 248, loss = 0.5991395711898804
In grad_steps = 249, loss = 0.5279395580291748
In grad_steps = 250, loss = 0.538038969039917
In grad_steps = 251, loss = 0.5965296030044556
In grad_steps = 252, loss = 0.536171555519104
In grad_steps = 253, loss = 0.7057198286056519
In grad_steps = 254, loss = 0.7624898552894592
In grad_steps = 255, loss = 0.5842186808586121
In grad_steps = 256, loss = 0.5613337159156799
In grad_steps = 257, loss = 0.5198962688446045
In grad_steps = 258, loss = 0.4575631618499756
In grad_steps = 259, loss = 0.4393360912799835
In grad_steps = 260, loss = 0.8231564164161682
In grad_steps = 261, loss = 0.7651278376579285
In grad_steps = 262, loss = 0.7346052527427673
In grad_steps = 263, loss = 0.5210570693016052
In grad_steps = 264, loss = 0.6341598629951477
In grad_steps = 265, loss = 0.6430985331535339
In grad_steps = 266, loss = 0.6109737753868103
In grad_steps = 267, loss = 0.6773451566696167
In grad_steps = 268, loss = 0.5818724036216736
In grad_steps = 269, loss = 0.6298820376396179
In grad_steps = 270, loss = 0.6243937611579895
In grad_steps = 271, loss = 0.6851085424423218
In grad_steps = 272, loss = 0.5961362719535828
In grad_steps = 273, loss = 0.6672840118408203
In grad_steps = 274, loss = 0.6604849696159363
In grad_steps = 275, loss = 0.6354145407676697
In grad_steps = 276, loss = 0.6417220234870911
In grad_steps = 277, loss = 0.6003384590148926
In grad_steps = 278, loss = 0.5940165519714355
In grad_steps = 279, loss = 0.7865800857543945
In grad_steps = 280, loss = 0.6813502907752991
In grad_steps = 281, loss = 0.49113261699676514
In grad_steps = 282, loss = 0.5353267192840576
In grad_steps = 283, loss = 0.5997440218925476
In grad_steps = 284, loss = 0.7291143536567688
In grad_steps = 285, loss = 0.6557748317718506
In grad_steps = 286, loss = 0.6256626844406128
In grad_steps = 287, loss = 0.6028810143470764
In grad_steps = 288, loss = 0.4203171133995056
In grad_steps = 289, loss = 0.5729293823242188
In grad_steps = 290, loss = 0.6822178959846497
In grad_steps = 291, loss = 0.6525814533233643
In grad_steps = 292, loss = 0.5667675733566284
In grad_steps = 293, loss = 0.43532097339630127
In grad_steps = 294, loss = 0.6139988899230957
In grad_steps = 295, loss = 0.7195835709571838
In grad_steps = 296, loss = 0.7153756022453308
In grad_steps = 297, loss = 0.5346363186836243
In grad_steps = 298, loss = 0.5148357152938843
In grad_steps = 299, loss = 0.5125433206558228
In grad_steps = 300, loss = 0.7018886804580688
In grad_steps = 301, loss = 0.7575603127479553
In grad_steps = 302, loss = 0.5900039672851562
Beginning epoch 4
In grad_steps = 303, loss = 0.6470476388931274
In grad_steps = 304, loss = 0.5495635271072388
In grad_steps = 305, loss = 0.6400783061981201
In grad_steps = 306, loss = 0.6282726526260376
In grad_steps = 307, loss = 0.6764004826545715
In grad_steps = 308, loss = 0.48902177810668945
In grad_steps = 309, loss = 0.6574714183807373
In grad_steps = 310, loss = 0.5144227147102356
In grad_steps = 311, loss = 0.8449944257736206
In grad_steps = 312, loss = 0.5173541307449341
In grad_steps = 313, loss = 0.7578583359718323
In grad_steps = 314, loss = 0.5452607274055481
In grad_steps = 315, loss = 0.6424118876457214
In grad_steps = 316, loss = 0.7118908762931824
In grad_steps = 317, loss = 0.6323902010917664
In grad_steps = 318, loss = 0.65782630443573
In grad_steps = 319, loss = 0.6616168022155762
In grad_steps = 320, loss = 0.5814403295516968
In grad_steps = 321, loss = 0.46470412611961365
In grad_steps = 322, loss = 0.5003507733345032
In grad_steps = 323, loss = 0.41471922397613525
In grad_steps = 324, loss = 0.5671555995941162
In grad_steps = 325, loss = 0.7054190039634705
In grad_steps = 326, loss = 0.7113881707191467
In grad_steps = 327, loss = 0.36616480350494385
In grad_steps = 328, loss = 0.3838461637496948
In grad_steps = 329, loss = 0.4004307687282562
In grad_steps = 330, loss = 0.8827887177467346
In grad_steps = 331, loss = 0.2145427018404007
In grad_steps = 332, loss = 0.28581205010414124
In grad_steps = 333, loss = 0.2954665720462799
In grad_steps = 334, loss = 0.5541055798530579
In grad_steps = 335, loss = 0.3505164384841919
In grad_steps = 336, loss = 0.40403228998184204
In grad_steps = 337, loss = 0.301831990480423
In grad_steps = 338, loss = 0.4053654670715332
In grad_steps = 339, loss = 0.23362472653388977
In grad_steps = 340, loss = 0.0984383299946785
In grad_steps = 341, loss = 0.397972971200943
In grad_steps = 342, loss = 0.26869693398475647
In grad_steps = 343, loss = 0.6486392021179199
In grad_steps = 344, loss = 1.3024065494537354
In grad_steps = 345, loss = 0.789377748966217
In grad_steps = 346, loss = 0.7271526455879211
In grad_steps = 347, loss = 0.511994481086731
In grad_steps = 348, loss = 0.40651845932006836
In grad_steps = 349, loss = 0.5199687480926514
In grad_steps = 350, loss = 0.4923872947692871
In grad_steps = 351, loss = 0.4440590739250183
In grad_steps = 352, loss = 0.5190061926841736
In grad_steps = 353, loss = 0.4952070116996765
In grad_steps = 354, loss = 0.692290723323822
In grad_steps = 355, loss = 0.714259147644043
In grad_steps = 356, loss = 0.5740766525268555
In grad_steps = 357, loss = 0.48317083716392517
In grad_steps = 358, loss = 0.4151489734649658
In grad_steps = 359, loss = 0.4279187321662903
In grad_steps = 360, loss = 0.38880211114883423
In grad_steps = 361, loss = 0.8199381232261658
In grad_steps = 362, loss = 0.6498727798461914
In grad_steps = 363, loss = 0.72237229347229
In grad_steps = 364, loss = 0.44901812076568604
In grad_steps = 365, loss = 0.5723042488098145
In grad_steps = 366, loss = 0.51909339427948
In grad_steps = 367, loss = 0.6521354913711548
In grad_steps = 368, loss = 0.5708999633789062
In grad_steps = 369, loss = 0.4827609360218048
In grad_steps = 370, loss = 0.6739842295646667
In grad_steps = 371, loss = 0.7055891752243042
In grad_steps = 372, loss = 0.6402509212493896
In grad_steps = 373, loss = 0.6093400716781616
In grad_steps = 374, loss = 0.593926727771759
In grad_steps = 375, loss = 0.6340975761413574
In grad_steps = 376, loss = 0.4942625164985657
In grad_steps = 377, loss = 0.7163836359977722
In grad_steps = 378, loss = 0.4663952589035034
In grad_steps = 379, loss = 0.5242960453033447
In grad_steps = 380, loss = 0.7440788745880127
In grad_steps = 381, loss = 0.5705042481422424
In grad_steps = 382, loss = 0.36951738595962524
In grad_steps = 383, loss = 0.36967241764068604
In grad_steps = 384, loss = 0.498766154050827
In grad_steps = 385, loss = 0.6592535972595215
In grad_steps = 386, loss = 0.46632835268974304
In grad_steps = 387, loss = 0.5575101375579834
In grad_steps = 388, loss = 0.3033938705921173
In grad_steps = 389, loss = 0.24894699454307556
In grad_steps = 390, loss = 0.36964792013168335
In grad_steps = 391, loss = 0.6038615107536316
In grad_steps = 392, loss = 0.2121822088956833
In grad_steps = 393, loss = 0.2810882329940796
In grad_steps = 394, loss = 0.20288704335689545
In grad_steps = 395, loss = 0.4295673072338104
In grad_steps = 396, loss = 0.8428682684898376
In grad_steps = 397, loss = 1.0226770639419556
In grad_steps = 398, loss = 0.9875421524047852
In grad_steps = 399, loss = 0.764190137386322
In grad_steps = 400, loss = 0.3986407518386841
In grad_steps = 401, loss = 0.6731833815574646
In grad_steps = 402, loss = 0.6717067360877991
In grad_steps = 403, loss = 0.4201298952102661
Beginning epoch 5
In grad_steps = 404, loss = 0.45256972312927246
In grad_steps = 405, loss = 0.4296629726886749
In grad_steps = 406, loss = 0.6048515439033508
In grad_steps = 407, loss = 0.6406185626983643
In grad_steps = 408, loss = 0.8620162606239319
In grad_steps = 409, loss = 0.4414660632610321
In grad_steps = 410, loss = 0.6164505481719971
In grad_steps = 411, loss = 0.5836718082427979
In grad_steps = 412, loss = 0.6267302632331848
In grad_steps = 413, loss = 0.5549854040145874
In grad_steps = 414, loss = 0.6494848728179932
In grad_steps = 415, loss = 0.5483263731002808
In grad_steps = 416, loss = 0.5380303263664246
In grad_steps = 417, loss = 0.6835331320762634
In grad_steps = 418, loss = 0.7422664165496826
In grad_steps = 419, loss = 0.6512947082519531
In grad_steps = 420, loss = 0.6175105571746826
In grad_steps = 421, loss = 0.5369898676872253
In grad_steps = 422, loss = 0.49292707443237305
In grad_steps = 423, loss = 0.6121910810470581
In grad_steps = 424, loss = 0.2592610716819763
In grad_steps = 425, loss = 0.565827488899231
In grad_steps = 426, loss = 0.6186797618865967
In grad_steps = 427, loss = 0.4947911202907562
In grad_steps = 428, loss = 0.21319574117660522
In grad_steps = 429, loss = 0.409282386302948
In grad_steps = 430, loss = 0.3034842312335968
In grad_steps = 431, loss = 0.8038614392280579
In grad_steps = 432, loss = 0.22150272130966187
In grad_steps = 433, loss = 0.2385215014219284
In grad_steps = 434, loss = 0.10804666578769684
In grad_steps = 435, loss = 0.23661737143993378
In grad_steps = 436, loss = 1.048230767250061
In grad_steps = 437, loss = 0.3702303469181061
In grad_steps = 438, loss = 0.322640985250473
In grad_steps = 439, loss = 0.16223928332328796
In grad_steps = 440, loss = 0.11099676042795181
In grad_steps = 441, loss = 0.4389021694660187
In grad_steps = 442, loss = 0.4375891387462616
In grad_steps = 443, loss = 0.2607145607471466
In grad_steps = 444, loss = 0.3174213469028473
In grad_steps = 445, loss = 0.4716981053352356
In grad_steps = 446, loss = 0.4846198558807373
In grad_steps = 447, loss = 0.6129095554351807
In grad_steps = 448, loss = 0.38262543082237244
In grad_steps = 449, loss = 0.32794448733329773
In grad_steps = 450, loss = 0.622612476348877
In grad_steps = 451, loss = 0.2561829090118408
In grad_steps = 452, loss = 0.26015588641166687
In grad_steps = 453, loss = 0.42418450117111206
In grad_steps = 454, loss = 0.7434560656547546
In grad_steps = 455, loss = 0.5116748809814453
In grad_steps = 456, loss = 0.49745169281959534
In grad_steps = 457, loss = 0.5307592749595642
In grad_steps = 458, loss = 0.4172207713127136
In grad_steps = 459, loss = 0.18959224224090576
In grad_steps = 460, loss = 0.24945074319839478
In grad_steps = 461, loss = 0.23810285329818726
In grad_steps = 462, loss = 1.136834979057312
In grad_steps = 463, loss = 0.5489792823791504
In grad_steps = 464, loss = 0.8514677882194519
In grad_steps = 465, loss = 0.35495632886886597
In grad_steps = 466, loss = 0.29734453558921814
In grad_steps = 467, loss = 0.2920481264591217
In grad_steps = 468, loss = 0.7169406414031982
In grad_steps = 469, loss = 0.5991750955581665
In grad_steps = 470, loss = 0.4434519410133362
In grad_steps = 471, loss = 0.8142354488372803
In grad_steps = 472, loss = 0.5255208611488342
In grad_steps = 473, loss = 0.6058968305587769
In grad_steps = 474, loss = 0.6016924977302551
In grad_steps = 475, loss = 0.42243844270706177
In grad_steps = 476, loss = 0.6590622663497925
In grad_steps = 477, loss = 0.4258289933204651
In grad_steps = 478, loss = 0.5902283787727356
In grad_steps = 479, loss = 0.4900408983230591
In grad_steps = 480, loss = 0.541728138923645
In grad_steps = 481, loss = 0.5907853841781616
In grad_steps = 482, loss = 0.49433258175849915
In grad_steps = 483, loss = 0.2630542814731598
In grad_steps = 484, loss = 0.2791714072227478
In grad_steps = 485, loss = 0.42317745089530945
In grad_steps = 486, loss = 0.679171085357666
In grad_steps = 487, loss = 0.3486567735671997
In grad_steps = 488, loss = 0.4181442856788635
In grad_steps = 489, loss = 0.25216734409332275
In grad_steps = 490, loss = 0.14039048552513123
In grad_steps = 491, loss = 0.3430480360984802
In grad_steps = 492, loss = 0.5200108885765076
In grad_steps = 493, loss = 0.13056521117687225
In grad_steps = 494, loss = 0.1687989979982376
In grad_steps = 495, loss = 0.1024671345949173
In grad_steps = 496, loss = 0.14317640662193298
In grad_steps = 497, loss = 0.541938304901123
In grad_steps = 498, loss = 0.31903889775276184
In grad_steps = 499, loss = 0.6555183529853821
In grad_steps = 500, loss = 0.45015206933021545
In grad_steps = 501, loss = 0.09685762226581573
In grad_steps = 502, loss = 0.8949877619743347
In grad_steps = 503, loss = 1.1195122003555298
In grad_steps = 504, loss = 0.25973397493362427
Beginning epoch 6
In grad_steps = 505, loss = 0.7043542265892029
In grad_steps = 506, loss = 0.42454245686531067
In grad_steps = 507, loss = 0.4519238770008087
In grad_steps = 508, loss = 0.5238747000694275
In grad_steps = 509, loss = 0.6743919253349304
In grad_steps = 510, loss = 0.3361593782901764
In grad_steps = 511, loss = 0.5444023609161377
In grad_steps = 512, loss = 0.4567461609840393
In grad_steps = 513, loss = 0.5162610411643982
In grad_steps = 514, loss = 0.4676756262779236
In grad_steps = 515, loss = 0.5192772150039673
In grad_steps = 516, loss = 0.3897570073604584
In grad_steps = 517, loss = 0.4478588402271271
In grad_steps = 518, loss = 0.6241081953048706
In grad_steps = 519, loss = 0.5922035574913025
In grad_steps = 520, loss = 0.7933353185653687
In grad_steps = 521, loss = 0.2886500358581543
In grad_steps = 522, loss = 0.34055498242378235
In grad_steps = 523, loss = 0.20241302251815796
In grad_steps = 524, loss = 0.2180040180683136
In grad_steps = 525, loss = 0.395541787147522
In grad_steps = 526, loss = 0.5130772590637207
In grad_steps = 527, loss = 0.4130678176879883
In grad_steps = 528, loss = 0.4058762192726135
In grad_steps = 529, loss = 0.07161951810121536
In grad_steps = 530, loss = 0.6214435696601868
In grad_steps = 531, loss = 0.0779213011264801
In grad_steps = 532, loss = 0.16420519351959229
In grad_steps = 533, loss = 0.07703998684883118
In grad_steps = 534, loss = 0.05612830072641373
In grad_steps = 535, loss = 0.23232322931289673
In grad_steps = 536, loss = 0.16511878371238708
In grad_steps = 537, loss = 0.0648999884724617
In grad_steps = 538, loss = 0.09759829938411713
In grad_steps = 539, loss = 0.15847808122634888
In grad_steps = 540, loss = 0.1340639293193817
In grad_steps = 541, loss = 0.057950928807258606
In grad_steps = 542, loss = 0.04996882379055023
In grad_steps = 543, loss = 0.0582280158996582
In grad_steps = 544, loss = 1.4381344318389893
In grad_steps = 545, loss = 0.4906378984451294
In grad_steps = 546, loss = 0.023629041388630867
In grad_steps = 547, loss = 0.1075650304555893
In grad_steps = 548, loss = 0.4917495548725128
In grad_steps = 549, loss = 1.1887192726135254
In grad_steps = 550, loss = 0.4273410141468048
In grad_steps = 551, loss = 0.15394994616508484
In grad_steps = 552, loss = 0.25172099471092224
In grad_steps = 553, loss = 0.1620989739894867
In grad_steps = 554, loss = 0.35717448592185974
In grad_steps = 555, loss = 0.4861757159233093
In grad_steps = 556, loss = 0.43596217036247253
In grad_steps = 557, loss = 0.5054426193237305
In grad_steps = 558, loss = 0.5951210260391235
In grad_steps = 559, loss = 0.5263322591781616
In grad_steps = 560, loss = 0.38861557841300964
In grad_steps = 561, loss = 0.461737722158432
In grad_steps = 562, loss = 0.13855844736099243
In grad_steps = 563, loss = 0.5631411671638489
In grad_steps = 564, loss = 0.4924638569355011
In grad_steps = 565, loss = 0.9921548366546631
In grad_steps = 566, loss = 0.5063937306404114
In grad_steps = 567, loss = 0.7485629320144653
In grad_steps = 568, loss = 0.5243745446205139
In grad_steps = 569, loss = 0.5089312791824341
In grad_steps = 570, loss = 0.24887670576572418
In grad_steps = 571, loss = 0.2506032884120941
In grad_steps = 572, loss = 0.7157918810844421
In grad_steps = 573, loss = 0.4089876711368561
In grad_steps = 574, loss = 0.7188268303871155
In grad_steps = 575, loss = 0.5531467795372009
In grad_steps = 576, loss = 0.43122774362564087
In grad_steps = 577, loss = 0.6376689672470093
In grad_steps = 578, loss = 0.3371279835700989
In grad_steps = 579, loss = 0.44209426641464233
In grad_steps = 580, loss = 0.43152284622192383
In grad_steps = 581, loss = 0.8333698511123657
In grad_steps = 582, loss = 0.46808362007141113
In grad_steps = 583, loss = 0.3681485056877136
In grad_steps = 584, loss = 0.29250073432922363
In grad_steps = 585, loss = 0.2931910455226898
In grad_steps = 586, loss = 0.4564272463321686
In grad_steps = 587, loss = 0.33998820185661316
In grad_steps = 588, loss = 0.4515005052089691
In grad_steps = 589, loss = 0.5999437570571899
In grad_steps = 590, loss = 0.22643297910690308
In grad_steps = 591, loss = 0.13502594828605652
In grad_steps = 592, loss = 0.288371205329895
In grad_steps = 593, loss = 0.1842719316482544
In grad_steps = 594, loss = 0.15347950160503387
In grad_steps = 595, loss = 0.28771570324897766
In grad_steps = 596, loss = 0.28191906213760376
In grad_steps = 597, loss = 0.0769246444106102
In grad_steps = 598, loss = 0.12345153093338013
In grad_steps = 599, loss = 0.14617139101028442
In grad_steps = 600, loss = 0.15833622217178345
In grad_steps = 601, loss = 0.8307688236236572
In grad_steps = 602, loss = 1.6906356811523438
In grad_steps = 603, loss = 0.35284486413002014
In grad_steps = 604, loss = 0.15236186981201172
In grad_steps = 605, loss = 0.01913745142519474
Beginning epoch 7
In grad_steps = 606, loss = 0.7084649801254272
In grad_steps = 607, loss = 0.37786388397216797
In grad_steps = 608, loss = 0.5930604338645935
In grad_steps = 609, loss = 0.8538901805877686
In grad_steps = 610, loss = 0.28781619668006897
In grad_steps = 611, loss = 0.7061367034912109
In grad_steps = 612, loss = 0.5546483993530273
In grad_steps = 613, loss = 0.48454540967941284
In grad_steps = 614, loss = 0.37470290064811707
In grad_steps = 615, loss = 0.32243111729621887
In grad_steps = 616, loss = 0.5285682082176208
In grad_steps = 617, loss = 0.39565667510032654
In grad_steps = 618, loss = 0.5869304537773132
In grad_steps = 619, loss = 0.724902331829071
In grad_steps = 620, loss = 0.5750732421875
In grad_steps = 621, loss = 0.7396804690361023
In grad_steps = 622, loss = 0.6390206217765808
In grad_steps = 623, loss = 0.3456396758556366
In grad_steps = 624, loss = 0.28501689434051514
In grad_steps = 625, loss = 0.23918847739696503
In grad_steps = 626, loss = 0.26327019929885864
In grad_steps = 627, loss = 0.4119284152984619
In grad_steps = 628, loss = 0.4822312593460083
In grad_steps = 629, loss = 0.28232502937316895
In grad_steps = 630, loss = 0.2627246379852295
In grad_steps = 631, loss = 0.20301508903503418
In grad_steps = 632, loss = 0.280868262052536
In grad_steps = 633, loss = 0.20616205036640167
In grad_steps = 634, loss = 0.2053719013929367
In grad_steps = 635, loss = 0.2104012668132782
In grad_steps = 636, loss = 0.15551462769508362
In grad_steps = 637, loss = 0.44721323251724243
In grad_steps = 638, loss = 0.1220550611615181
In grad_steps = 639, loss = 0.12654539942741394
In grad_steps = 640, loss = 0.08127318322658539
In grad_steps = 641, loss = 0.06120606139302254
In grad_steps = 642, loss = 0.13511525094509125
In grad_steps = 643, loss = 0.34964510798454285
In grad_steps = 644, loss = 0.7075138092041016
In grad_steps = 645, loss = 0.04005018621683121
In grad_steps = 646, loss = 0.2535775899887085
In grad_steps = 647, loss = 0.03135896474123001
In grad_steps = 648, loss = 0.12602275609970093
In grad_steps = 649, loss = 0.23172463476657867
In grad_steps = 650, loss = 0.5674740672111511
In grad_steps = 651, loss = 0.11604922264814377
In grad_steps = 652, loss = 0.08319780975580215
In grad_steps = 653, loss = 0.02827155962586403
In grad_steps = 654, loss = 0.02516208216547966
In grad_steps = 655, loss = 0.061187744140625
In grad_steps = 656, loss = 0.1182289719581604
In grad_steps = 657, loss = 0.15825363993644714
In grad_steps = 658, loss = 0.25796428322792053
In grad_steps = 659, loss = 0.11188887059688568
In grad_steps = 660, loss = 0.4997606873512268
In grad_steps = 661, loss = 0.12250948697328568
In grad_steps = 662, loss = 0.18468131124973297
In grad_steps = 663, loss = 0.14578913152217865
In grad_steps = 664, loss = 0.38983336091041565
In grad_steps = 665, loss = 0.1657855063676834
In grad_steps = 666, loss = 1.157348394393921
In grad_steps = 667, loss = 0.4692290723323822
In grad_steps = 668, loss = 0.11988019943237305
In grad_steps = 669, loss = 0.042868029326200485
In grad_steps = 670, loss = 0.5541311502456665
In grad_steps = 671, loss = 0.13715726137161255
In grad_steps = 672, loss = 0.19937962293624878
In grad_steps = 673, loss = 0.6676402688026428
In grad_steps = 674, loss = 0.23057393729686737
In grad_steps = 675, loss = 0.270826131105423
In grad_steps = 676, loss = 0.4634513556957245
In grad_steps = 677, loss = 0.17878934741020203
In grad_steps = 678, loss = 0.3625088334083557
In grad_steps = 679, loss = 0.17042312026023865
In grad_steps = 680, loss = 0.33471786975860596
In grad_steps = 681, loss = 0.23712962865829468
In grad_steps = 682, loss = 0.45535069704055786
In grad_steps = 683, loss = 0.9161062836647034
In grad_steps = 684, loss = 0.35281020402908325
In grad_steps = 685, loss = 0.19193397462368011
In grad_steps = 686, loss = 0.16191835701465607
In grad_steps = 687, loss = 0.17151501774787903
In grad_steps = 688, loss = 0.4039061963558197
In grad_steps = 689, loss = 0.2482350468635559
In grad_steps = 690, loss = 0.4299014210700989
In grad_steps = 691, loss = 0.17696483433246613
In grad_steps = 692, loss = 0.09417841583490372
In grad_steps = 693, loss = 0.21693155169487
In grad_steps = 694, loss = 0.2754446566104889
In grad_steps = 695, loss = 0.38463398814201355
In grad_steps = 696, loss = 0.16220533847808838
In grad_steps = 697, loss = 0.2582067847251892
In grad_steps = 698, loss = 0.031194226816296577
In grad_steps = 699, loss = 0.1596563309431076
In grad_steps = 700, loss = 0.02798764780163765
In grad_steps = 701, loss = 0.025132717564702034
In grad_steps = 702, loss = 0.199848473072052
In grad_steps = 703, loss = 0.07674580067396164
In grad_steps = 704, loss = 0.15868686139583588
In grad_steps = 705, loss = 0.31953760981559753
In grad_steps = 706, loss = 0.015536125749349594
Beginning epoch 8
In grad_steps = 707, loss = 0.40496379137039185
In grad_steps = 708, loss = 0.2146502584218979
In grad_steps = 709, loss = 0.5709726214408875
In grad_steps = 710, loss = 0.42800119519233704
In grad_steps = 711, loss = 0.05123324692249298
In grad_steps = 712, loss = 0.6000990867614746
In grad_steps = 713, loss = 0.8401153087615967
In grad_steps = 714, loss = 0.5328583717346191
In grad_steps = 715, loss = 0.30008813738822937
In grad_steps = 716, loss = 0.3664901852607727
In grad_steps = 717, loss = 0.2760489583015442
In grad_steps = 718, loss = 0.27580052614212036
In grad_steps = 719, loss = 0.35952356457710266
In grad_steps = 720, loss = 0.31801754236221313
In grad_steps = 721, loss = 0.26610666513442993
In grad_steps = 722, loss = 0.46010711789131165
In grad_steps = 723, loss = 0.36376824975013733
In grad_steps = 724, loss = 0.6085520386695862
In grad_steps = 725, loss = 0.3923151195049286
In grad_steps = 726, loss = 0.13690431416034698
In grad_steps = 727, loss = 0.134390190243721
In grad_steps = 728, loss = 0.25598713755607605
In grad_steps = 729, loss = 0.33607369661331177
In grad_steps = 730, loss = 0.23808498680591583
In grad_steps = 731, loss = 0.13612984120845795
In grad_steps = 732, loss = 0.39519259333610535
In grad_steps = 733, loss = 0.295157253742218
In grad_steps = 734, loss = 0.15231332182884216
In grad_steps = 735, loss = 0.18464075028896332
In grad_steps = 736, loss = 0.13704609870910645
In grad_steps = 737, loss = 0.11230304837226868
In grad_steps = 738, loss = 0.33717378973960876
In grad_steps = 739, loss = 0.07717200368642807
In grad_steps = 740, loss = 0.2258497178554535
In grad_steps = 741, loss = 0.08974441885948181
In grad_steps = 742, loss = 0.12671515345573425
In grad_steps = 743, loss = 0.03162739798426628
In grad_steps = 744, loss = 0.021334674209356308
In grad_steps = 745, loss = 0.1107013002038002
In grad_steps = 746, loss = 0.1219693124294281
In grad_steps = 747, loss = 0.12086722999811172
In grad_steps = 748, loss = 0.14106173813343048
In grad_steps = 749, loss = 0.5460625290870667
In grad_steps = 750, loss = 0.052581436932086945
In grad_steps = 751, loss = 0.12375736236572266
In grad_steps = 752, loss = 0.004663451109081507
In grad_steps = 753, loss = 0.09682375192642212
In grad_steps = 754, loss = 0.01920895464718342
In grad_steps = 755, loss = 0.06904608756303787
In grad_steps = 756, loss = 0.018750322982668877
In grad_steps = 757, loss = 0.016489019617438316
In grad_steps = 758, loss = 0.013186875730752945
In grad_steps = 759, loss = 0.04613161087036133
In grad_steps = 760, loss = 0.03774765878915787
In grad_steps = 761, loss = 0.9487003087997437
In grad_steps = 762, loss = 0.10459563881158829
In grad_steps = 763, loss = 0.13402283191680908
In grad_steps = 764, loss = 0.003029672894626856
In grad_steps = 765, loss = 0.10765255242586136
In grad_steps = 766, loss = 0.324937641620636
In grad_steps = 767, loss = 0.6916839480400085
In grad_steps = 768, loss = 0.037848297506570816
In grad_steps = 769, loss = 0.36297154426574707
In grad_steps = 770, loss = 0.4347454905509949
In grad_steps = 771, loss = 0.6695054173469543
In grad_steps = 772, loss = 0.0582050159573555
In grad_steps = 773, loss = 0.4494730234146118
In grad_steps = 774, loss = 0.6557029485702515
In grad_steps = 775, loss = 0.05370929092168808
In grad_steps = 776, loss = 0.19535090029239655
In grad_steps = 777, loss = 0.2821645736694336
In grad_steps = 778, loss = 0.09791548550128937
In grad_steps = 779, loss = 0.11501927673816681
In grad_steps = 780, loss = 0.13708625733852386
In grad_steps = 781, loss = 0.3984348475933075
In grad_steps = 782, loss = 0.1125512570142746
In grad_steps = 783, loss = 0.2890065312385559
In grad_steps = 784, loss = 0.36101585626602173
In grad_steps = 785, loss = 0.21215470135211945
In grad_steps = 786, loss = 0.16731399297714233
In grad_steps = 787, loss = 0.08104976266622543
In grad_steps = 788, loss = 0.3206906318664551
In grad_steps = 789, loss = 0.34830132126808167
In grad_steps = 790, loss = 0.3739442527294159
In grad_steps = 791, loss = 0.2720513939857483
In grad_steps = 792, loss = 0.0281249787658453
In grad_steps = 793, loss = 0.11396507918834686
In grad_steps = 794, loss = 0.13718339800834656
In grad_steps = 795, loss = 0.43774574995040894
In grad_steps = 796, loss = 0.39018696546554565
In grad_steps = 797, loss = 1.284630298614502
In grad_steps = 798, loss = 0.3446688652038574
In grad_steps = 799, loss = 0.35760462284088135
In grad_steps = 800, loss = 0.06953654438257217
In grad_steps = 801, loss = 0.19056959450244904
In grad_steps = 802, loss = 0.028813151642680168
In grad_steps = 803, loss = 0.10518383979797363
In grad_steps = 804, loss = 0.08335240185260773
In grad_steps = 805, loss = 0.03234785050153732
In grad_steps = 806, loss = 0.02097293548285961
In grad_steps = 807, loss = 0.008053128607571125
Beginning epoch 9
In grad_steps = 808, loss = 0.5169945359230042
In grad_steps = 809, loss = 0.28051242232322693
In grad_steps = 810, loss = 0.4748697876930237
In grad_steps = 811, loss = 0.8282822370529175
In grad_steps = 812, loss = 0.6354454159736633
In grad_steps = 813, loss = 0.08149540424346924
In grad_steps = 814, loss = 0.20375394821166992
In grad_steps = 815, loss = 0.09258881956338882
In grad_steps = 816, loss = 0.11701463162899017
In grad_steps = 817, loss = 1.0560812950134277
In grad_steps = 818, loss = 0.27336227893829346
In grad_steps = 819, loss = 0.4515010714530945
In grad_steps = 820, loss = 0.47066783905029297
In grad_steps = 821, loss = 0.18213783204555511
In grad_steps = 822, loss = 0.19472768902778625
In grad_steps = 823, loss = 0.2919684648513794
In grad_steps = 824, loss = 0.1763213723897934
In grad_steps = 825, loss = 0.08223311603069305
In grad_steps = 826, loss = 0.19261513650417328
In grad_steps = 827, loss = 0.0490885004401207
In grad_steps = 828, loss = 0.023301921784877777
In grad_steps = 829, loss = 0.14116275310516357
In grad_steps = 830, loss = 0.10495034605264664
In grad_steps = 831, loss = 0.16499063372612
In grad_steps = 832, loss = 0.023496678099036217
In grad_steps = 833, loss = 0.2060774564743042
In grad_steps = 834, loss = 0.018907580524683
In grad_steps = 835, loss = 0.08418139815330505
In grad_steps = 836, loss = 0.005985458381474018
In grad_steps = 837, loss = 0.12090130895376205
In grad_steps = 838, loss = 0.00824080128222704
In grad_steps = 839, loss = 0.014548216946423054
In grad_steps = 840, loss = 0.033124469220638275
In grad_steps = 841, loss = 0.03716891258955002
In grad_steps = 842, loss = 0.046696994453668594
In grad_steps = 843, loss = 0.0141608826816082
In grad_steps = 844, loss = 0.01987987942993641
In grad_steps = 845, loss = 0.05595873296260834
In grad_steps = 846, loss = 0.23613421618938446
In grad_steps = 847, loss = 0.02483031153678894
In grad_steps = 848, loss = 0.2390267550945282
In grad_steps = 849, loss = 0.0062971473671495914
In grad_steps = 850, loss = 0.14753809571266174
In grad_steps = 851, loss = 0.03880361095070839
In grad_steps = 852, loss = 0.006971413269639015
In grad_steps = 853, loss = 0.005997312720865011
In grad_steps = 854, loss = 0.1053398996591568
In grad_steps = 855, loss = 0.8378489017486572
In grad_steps = 856, loss = 0.00896604172885418
In grad_steps = 857, loss = 0.029363274574279785
In grad_steps = 858, loss = 0.017316564917564392
In grad_steps = 859, loss = 0.01147823128849268
In grad_steps = 860, loss = 0.01935090869665146
In grad_steps = 861, loss = 0.014584758318960667
In grad_steps = 862, loss = 0.37058568000793457
In grad_steps = 863, loss = 0.08764699101448059
In grad_steps = 864, loss = 0.26771965622901917
In grad_steps = 865, loss = 0.039941150695085526
In grad_steps = 866, loss = 0.018745237961411476
In grad_steps = 867, loss = 0.47737592458724976
In grad_steps = 868, loss = 0.43758073449134827
In grad_steps = 869, loss = 0.008288503624498844
In grad_steps = 870, loss = 0.005026028025895357
In grad_steps = 871, loss = 0.009625865146517754
In grad_steps = 872, loss = 0.43985578417778015
In grad_steps = 873, loss = 0.05353288725018501
In grad_steps = 874, loss = 0.6230605244636536
In grad_steps = 875, loss = 0.7995452880859375
In grad_steps = 876, loss = 0.6956335306167603
In grad_steps = 877, loss = 0.2082008719444275
In grad_steps = 878, loss = 0.5005800127983093
In grad_steps = 879, loss = 0.24858428537845612
In grad_steps = 880, loss = 0.15272367000579834
In grad_steps = 881, loss = 0.03883729875087738
In grad_steps = 882, loss = 0.27925121784210205
In grad_steps = 883, loss = 0.2823961675167084
In grad_steps = 884, loss = 0.15055853128433228
In grad_steps = 885, loss = 0.449695348739624
In grad_steps = 886, loss = 0.2263091802597046
In grad_steps = 887, loss = 0.03939865157008171
In grad_steps = 888, loss = 0.11803990602493286
In grad_steps = 889, loss = 0.5915914177894592
In grad_steps = 890, loss = 0.45009663701057434
In grad_steps = 891, loss = 0.09297195076942444
In grad_steps = 892, loss = 0.08659940958023071
In grad_steps = 893, loss = 0.09526129066944122
In grad_steps = 894, loss = 0.04742725193500519
In grad_steps = 895, loss = 0.06757421791553497
In grad_steps = 896, loss = 0.07208316773176193
In grad_steps = 897, loss = 0.05986236035823822
In grad_steps = 898, loss = 0.06287392973899841
In grad_steps = 899, loss = 0.12603504955768585
In grad_steps = 900, loss = 0.14038017392158508
In grad_steps = 901, loss = 0.08481395244598389
In grad_steps = 902, loss = 0.02717513218522072
In grad_steps = 903, loss = 0.04115945100784302
In grad_steps = 904, loss = 0.021104831248521805
In grad_steps = 905, loss = 0.015527824871242046
In grad_steps = 906, loss = 0.009884065017104149
In grad_steps = 907, loss = 0.057726312428712845
In grad_steps = 908, loss = 0.00013278778351377696
Beginning epoch 10
In grad_steps = 909, loss = 0.10621394217014313
In grad_steps = 910, loss = 0.002486457349732518
In grad_steps = 911, loss = 0.007698396220803261
In grad_steps = 912, loss = 0.645545482635498
In grad_steps = 913, loss = 0.12063916027545929
In grad_steps = 914, loss = 0.002184665994718671
In grad_steps = 915, loss = 0.09212201833724976
In grad_steps = 916, loss = 0.005646232515573502
In grad_steps = 917, loss = 0.0422770231962204
In grad_steps = 918, loss = 0.1244974210858345
In grad_steps = 919, loss = 0.38686254620552063
In grad_steps = 920, loss = 0.010009650141000748
In grad_steps = 921, loss = 0.002779719652608037
In grad_steps = 922, loss = 0.017060399055480957
In grad_steps = 923, loss = 0.015748031437397003
In grad_steps = 924, loss = 0.014557975344359875
In grad_steps = 925, loss = 0.006880291737616062
In grad_steps = 926, loss = 0.004557029809802771
In grad_steps = 927, loss = 0.005618664436042309
In grad_steps = 928, loss = 0.0009329804452136159
In grad_steps = 929, loss = 0.0026308023370802402
In grad_steps = 930, loss = 0.36610153317451477
In grad_steps = 931, loss = 0.020923389121890068
In grad_steps = 932, loss = 0.0038707128260284662
In grad_steps = 933, loss = 0.005486906971782446
In grad_steps = 934, loss = 0.008628281764686108
In grad_steps = 935, loss = 0.0012036508414894342
In grad_steps = 936, loss = 0.029476774856448174
In grad_steps = 937, loss = 0.000439463765360415
In grad_steps = 938, loss = 0.03769417479634285
In grad_steps = 939, loss = 0.061097562313079834
In grad_steps = 940, loss = 0.0031553246080875397
In grad_steps = 941, loss = 0.0041557070799171925
In grad_steps = 942, loss = 0.04988890886306763
In grad_steps = 943, loss = 0.17109300196170807
In grad_steps = 944, loss = 0.01582448184490204
In grad_steps = 945, loss = 0.0076142773032188416
In grad_steps = 946, loss = 0.01947113685309887
In grad_steps = 947, loss = 0.026754558086395264
In grad_steps = 948, loss = 0.00502229668200016
In grad_steps = 949, loss = 0.15740202367305756
In grad_steps = 950, loss = 0.4820271134376526
In grad_steps = 951, loss = 0.0013563028769567609
In grad_steps = 952, loss = 0.028042292222380638
In grad_steps = 953, loss = 0.0025398104917258024
In grad_steps = 954, loss = 1.0852634906768799
In grad_steps = 955, loss = 0.2976674437522888
In grad_steps = 956, loss = 0.014583148062229156
In grad_steps = 957, loss = 0.006192873232066631
In grad_steps = 958, loss = 0.012187115848064423
In grad_steps = 959, loss = 0.02307155914604664
In grad_steps = 960, loss = 0.03684108704328537
In grad_steps = 961, loss = 0.01155623234808445
In grad_steps = 962, loss = 0.10197186470031738
In grad_steps = 963, loss = 0.1026543527841568
In grad_steps = 964, loss = 0.28932568430900574
In grad_steps = 965, loss = 0.020799126476049423
In grad_steps = 966, loss = 0.01051761768758297
In grad_steps = 967, loss = 0.04021807387471199
In grad_steps = 968, loss = 0.49736058712005615
In grad_steps = 969, loss = 0.1814757138490677
In grad_steps = 970, loss = 0.08116891235113144
In grad_steps = 971, loss = 0.006752150133252144
In grad_steps = 972, loss = 0.017657289281487465
In grad_steps = 973, loss = 0.40191060304641724
In grad_steps = 974, loss = 0.09583134949207306
In grad_steps = 975, loss = 0.020454635843634605
In grad_steps = 976, loss = 0.5500376224517822
In grad_steps = 977, loss = 0.08819414675235748
In grad_steps = 978, loss = 0.05949355289340019
In grad_steps = 979, loss = 0.2660488188266754
In grad_steps = 980, loss = 0.029814524576067924
In grad_steps = 981, loss = 0.12337255477905273
In grad_steps = 982, loss = 0.1247166097164154
In grad_steps = 983, loss = 0.40949001908302307
In grad_steps = 984, loss = 0.06452535092830658
In grad_steps = 985, loss = 0.31601446866989136
In grad_steps = 986, loss = 0.4181530177593231
In grad_steps = 987, loss = 0.03714660555124283
In grad_steps = 988, loss = 0.21590735018253326
In grad_steps = 989, loss = 0.08213900029659271
In grad_steps = 990, loss = 0.08203403651714325
In grad_steps = 991, loss = 0.11421617120504379
In grad_steps = 992, loss = 0.016224967315793037
In grad_steps = 993, loss = 0.024901634082198143
In grad_steps = 994, loss = 0.028939783573150635
In grad_steps = 995, loss = 0.037048257887363434
In grad_steps = 996, loss = 0.5947036743164062
In grad_steps = 997, loss = 0.14588671922683716
In grad_steps = 998, loss = 0.031294479966163635
In grad_steps = 999, loss = 0.009157934226095676
In grad_steps = 1000, loss = 0.020210962742567062
In grad_steps = 1001, loss = 0.3481732904911041
In grad_steps = 1002, loss = 0.05468759313225746
In grad_steps = 1003, loss = 0.06844837218523026
In grad_steps = 1004, loss = 0.023754168301820755
In grad_steps = 1005, loss = 0.029982201755046844
In grad_steps = 1006, loss = 0.03621157258749008
In grad_steps = 1007, loss = 0.04173437878489494
In grad_steps = 1008, loss = 0.030704481527209282
In grad_steps = 1009, loss = 0.002175606321543455
Beginning epoch 11
In grad_steps = 1010, loss = 0.13976240158081055
In grad_steps = 1011, loss = 0.00779590243473649
In grad_steps = 1012, loss = 0.11130610853433609
In grad_steps = 1013, loss = 0.07075734436511993
In grad_steps = 1014, loss = 0.04123665764927864
In grad_steps = 1015, loss = 0.011669948697090149
In grad_steps = 1016, loss = 0.10114649683237076
In grad_steps = 1017, loss = 0.012570513412356377
In grad_steps = 1018, loss = 0.0011311157140880823
In grad_steps = 1019, loss = 0.09283195436000824
In grad_steps = 1020, loss = 0.008438220247626305
In grad_steps = 1021, loss = 0.010363451205193996
In grad_steps = 1022, loss = 0.007769621443003416
In grad_steps = 1023, loss = 0.03139534220099449
In grad_steps = 1024, loss = 0.2858431339263916
In grad_steps = 1025, loss = 0.01674947515130043
In grad_steps = 1026, loss = 0.009041397832334042
In grad_steps = 1027, loss = 0.007196083199232817
In grad_steps = 1028, loss = 0.0027232228312641382
In grad_steps = 1029, loss = 0.002280494896695018
In grad_steps = 1030, loss = 0.0023614438250660896
In grad_steps = 1031, loss = 0.058684516698122025
In grad_steps = 1032, loss = 0.03136509656906128
In grad_steps = 1033, loss = 0.0031217443756759167
In grad_steps = 1034, loss = 0.001688814372755587
In grad_steps = 1035, loss = 0.405631422996521
In grad_steps = 1036, loss = 0.010054076090455055
In grad_steps = 1037, loss = 0.017131716012954712
In grad_steps = 1038, loss = 0.0023986324667930603
In grad_steps = 1039, loss = 1.0554773807525635
In grad_steps = 1040, loss = 0.001434984034858644
In grad_steps = 1041, loss = 0.45376718044281006
In grad_steps = 1042, loss = 0.2990410327911377
In grad_steps = 1043, loss = 0.1478729099035263
In grad_steps = 1044, loss = 0.0747215673327446
In grad_steps = 1045, loss = 0.005942299030721188
In grad_steps = 1046, loss = 0.06279515475034714
In grad_steps = 1047, loss = 0.04249696061015129
In grad_steps = 1048, loss = 0.851365864276886
In grad_steps = 1049, loss = 0.02856503427028656
In grad_steps = 1050, loss = 0.09249895066022873
In grad_steps = 1051, loss = 0.3333580195903778
In grad_steps = 1052, loss = 0.04285111650824547
In grad_steps = 1053, loss = 0.1738850474357605
In grad_steps = 1054, loss = 0.0860280692577362
In grad_steps = 1055, loss = 0.26290783286094666
In grad_steps = 1056, loss = 0.1294264793395996
In grad_steps = 1057, loss = 0.05842263624072075
In grad_steps = 1058, loss = 0.02653582952916622
In grad_steps = 1059, loss = 0.038565050810575485
In grad_steps = 1060, loss = 0.055296026170253754
In grad_steps = 1061, loss = 0.03361887484788895
In grad_steps = 1062, loss = 0.28919586539268494
In grad_steps = 1063, loss = 0.03771296143531799
In grad_steps = 1064, loss = 0.16734369099140167
In grad_steps = 1065, loss = 0.06176750361919403
In grad_steps = 1066, loss = 0.051237188279628754
In grad_steps = 1067, loss = 0.03920278325676918
In grad_steps = 1068, loss = 0.09377175569534302
In grad_steps = 1069, loss = 0.016768306493759155
In grad_steps = 1070, loss = 0.11782206594944
In grad_steps = 1071, loss = 0.008631349541246891
In grad_steps = 1072, loss = 0.0030592973344027996
In grad_steps = 1073, loss = 0.0544101819396019
In grad_steps = 1074, loss = 0.23441846668720245
In grad_steps = 1075, loss = 0.011228111572563648
In grad_steps = 1076, loss = 0.007037803065031767
In grad_steps = 1077, loss = 0.2661516070365906
In grad_steps = 1078, loss = 0.004196588881313801
In grad_steps = 1079, loss = 0.010134268552064896
In grad_steps = 1080, loss = 0.07733873277902603
In grad_steps = 1081, loss = 0.009839234873652458
In grad_steps = 1082, loss = 0.008454712107777596
In grad_steps = 1083, loss = 0.02183673530817032
In grad_steps = 1084, loss = 0.01248473022133112
In grad_steps = 1085, loss = 0.06150667369365692
In grad_steps = 1086, loss = 0.00949816219508648
In grad_steps = 1087, loss = 0.012262480333447456
In grad_steps = 1088, loss = 0.0054988013580441475
In grad_steps = 1089, loss = 0.00725599005818367
In grad_steps = 1090, loss = 0.006177866365760565
In grad_steps = 1091, loss = 0.002331832190975547
In grad_steps = 1092, loss = 0.003824990475550294
In grad_steps = 1093, loss = 0.0023897034116089344
In grad_steps = 1094, loss = 0.005562428385019302
In grad_steps = 1095, loss = 0.002314903773367405
In grad_steps = 1096, loss = 0.0011746255913749337
In grad_steps = 1097, loss = 0.006577189080417156
In grad_steps = 1098, loss = 0.0023348957765847445
In grad_steps = 1099, loss = 0.0009429739438928664
In grad_steps = 1100, loss = 0.22527767717838287
In grad_steps = 1101, loss = 0.004366986453533173
In grad_steps = 1102, loss = 0.0012482134625315666
In grad_steps = 1103, loss = 0.005975741893053055
In grad_steps = 1104, loss = 0.005356788635253906
In grad_steps = 1105, loss = 0.0023012799210846424
In grad_steps = 1106, loss = 0.008322186768054962
In grad_steps = 1107, loss = 0.008198700845241547
In grad_steps = 1108, loss = 0.003903955454006791
In grad_steps = 1109, loss = 0.001781066064722836
In grad_steps = 1110, loss = 0.0005823459359817207
Beginning epoch 12
In grad_steps = 1111, loss = 0.5953382849693298
In grad_steps = 1112, loss = 0.0028754991944879293
In grad_steps = 1113, loss = 0.0021367964800447226
In grad_steps = 1114, loss = 0.022882768884301186
In grad_steps = 1115, loss = 0.8275043368339539
In grad_steps = 1116, loss = 0.0030135917477309704
In grad_steps = 1117, loss = 0.1784832626581192
In grad_steps = 1118, loss = 0.011301917023956776
In grad_steps = 1119, loss = 0.3544793725013733
In grad_steps = 1120, loss = 0.15233202278614044
In grad_steps = 1121, loss = 0.011062192730605602
In grad_steps = 1122, loss = 0.02793015167117119
In grad_steps = 1123, loss = 0.033355601131916046
In grad_steps = 1124, loss = 0.04560932517051697
In grad_steps = 1125, loss = 0.25981199741363525
In grad_steps = 1126, loss = 0.07203367352485657
In grad_steps = 1127, loss = 0.09774341434240341
In grad_steps = 1128, loss = 0.044947557151317596
In grad_steps = 1129, loss = 0.014560466632246971
In grad_steps = 1130, loss = 0.013194046914577484
In grad_steps = 1131, loss = 0.009884699247777462
In grad_steps = 1132, loss = 0.01735355332493782
In grad_steps = 1133, loss = 0.04550030082464218
In grad_steps = 1134, loss = 0.03282716125249863
In grad_steps = 1135, loss = 0.10671234875917435
In grad_steps = 1136, loss = 0.49341848492622375
In grad_steps = 1137, loss = 0.015351331792771816
In grad_steps = 1138, loss = 0.16699229180812836
In grad_steps = 1139, loss = 0.017232650890946388
In grad_steps = 1140, loss = 0.22605794668197632
In grad_steps = 1141, loss = 0.03425998613238335
In grad_steps = 1142, loss = 0.13090431690216064
In grad_steps = 1143, loss = 0.033358655869960785
In grad_steps = 1144, loss = 0.029206249862909317
In grad_steps = 1145, loss = 0.0649835392832756
In grad_steps = 1146, loss = 0.08693758398294449
In grad_steps = 1147, loss = 0.09093683958053589
In grad_steps = 1148, loss = 0.08073175698518753
In grad_steps = 1149, loss = 0.35674309730529785
In grad_steps = 1150, loss = 0.006873238831758499
In grad_steps = 1151, loss = 0.008536688052117825
In grad_steps = 1152, loss = 0.006978312041610479
In grad_steps = 1153, loss = 0.01356955710798502
In grad_steps = 1154, loss = 0.0131809301674366
In grad_steps = 1155, loss = 0.004393904469907284
In grad_steps = 1156, loss = 0.02858208306133747
In grad_steps = 1157, loss = 0.012519474141299725
In grad_steps = 1158, loss = 0.0032879638019949198
In grad_steps = 1159, loss = 0.004294521640986204
In grad_steps = 1160, loss = 0.008514964021742344
In grad_steps = 1161, loss = 0.010224025696516037
In grad_steps = 1162, loss = 0.006113313138484955
In grad_steps = 1163, loss = 0.00976226944476366
In grad_steps = 1164, loss = 0.0058234333992004395
In grad_steps = 1165, loss = 0.058213528245687485
In grad_steps = 1166, loss = 0.0066338395699858665
In grad_steps = 1167, loss = 0.01464575994759798
In grad_steps = 1168, loss = 0.0011391820153221488
In grad_steps = 1169, loss = 0.39294639229774475
In grad_steps = 1170, loss = 0.08059192448854446
In grad_steps = 1171, loss = 0.017471740022301674
In grad_steps = 1172, loss = 0.0008534950320608914
In grad_steps = 1173, loss = 0.0007462615612894297
In grad_steps = 1174, loss = 0.002679535187780857
In grad_steps = 1175, loss = 0.2054533213376999
In grad_steps = 1176, loss = 0.0007074796012602746
In grad_steps = 1177, loss = 0.0025328779593110085
In grad_steps = 1178, loss = 0.25856587290763855
In grad_steps = 1179, loss = 0.004791101906448603
In grad_steps = 1180, loss = 0.003225754015147686
In grad_steps = 1181, loss = 0.0851949080824852
In grad_steps = 1182, loss = 0.2039508819580078
In grad_steps = 1183, loss = 0.02266284078359604
In grad_steps = 1184, loss = 0.0050062984228134155
In grad_steps = 1185, loss = 0.7409514784812927
In grad_steps = 1186, loss = 0.015615303069353104
In grad_steps = 1187, loss = 0.20067594945430756
In grad_steps = 1188, loss = 0.04368598759174347
In grad_steps = 1189, loss = 0.004014862235635519
In grad_steps = 1190, loss = 0.015645841136574745
In grad_steps = 1191, loss = 0.06783314794301987
In grad_steps = 1192, loss = 0.014572161249816418
In grad_steps = 1193, loss = 0.01071764063090086
In grad_steps = 1194, loss = 0.012006026692688465
In grad_steps = 1195, loss = 0.10769559442996979
In grad_steps = 1196, loss = 0.14442825317382812
In grad_steps = 1197, loss = 0.033604420721530914
In grad_steps = 1198, loss = 0.052101291716098785
In grad_steps = 1199, loss = 0.02769610844552517
In grad_steps = 1200, loss = 0.01491905190050602
In grad_steps = 1201, loss = 0.006609579548239708
In grad_steps = 1202, loss = 0.035144247114658356
In grad_steps = 1203, loss = 0.011680858209729195
In grad_steps = 1204, loss = 0.013448270969092846
In grad_steps = 1205, loss = 0.009649470448493958
In grad_steps = 1206, loss = 0.017828527837991714
In grad_steps = 1207, loss = 0.026105979457497597
In grad_steps = 1208, loss = 0.01085323840379715
In grad_steps = 1209, loss = 0.015073221176862717
In grad_steps = 1210, loss = 0.015616357326507568
In grad_steps = 1211, loss = 0.002528091426938772
Elapsed time: 2137.9748520851135 seconds for ensemble 4 with 12 epochs
LoRA instance 4 evaluation complete. Data saved to /pscratch/sd/t/tianle/lucid/other_source/SURP_2024/results/tmp_lora_ensemble/Llama3-8B-set-1.1/test_data_instance_4_seed_40373.npz.
lora instance i = 4 Successfully finished.
Final, Test average ensemble probabilities = 
[[5.40492117e-01 4.59507883e-01]
 [6.00448430e-01 3.99551600e-01]
 [2.94058412e-01 7.05941558e-01]
 [3.53005826e-01 6.46994233e-01]
 [9.94380593e-01 5.61939972e-03]
 [8.82114530e-01 1.17885515e-01]
 [3.40317070e-01 6.59682870e-01]
 [9.97451603e-01 2.54842709e-03]
 [9.52601910e-01 4.73980606e-02]
 [5.09257138e-01 4.90742862e-01]
 [9.44289029e-01 5.57110310e-02]
 [7.19759345e-01 2.80240685e-01]
 [3.10105999e-04 9.99689877e-01]
 [3.91503662e-01 6.08496308e-01]
 [2.77431346e-02 9.72256839e-01]
 [3.86371166e-01 6.13628745e-01]
 [8.13785255e-01 1.86214715e-01]
 [6.14606738e-01 3.85393202e-01]
 [7.62243152e-01 2.37756774e-01]
 [5.10122955e-01 4.89877045e-01]
 [8.61375630e-01 1.38624430e-01]
 [8.50479782e-01 1.49520233e-01]
 [9.87429976e-01 1.25700040e-02]
 [6.75104856e-01 3.24895144e-01]
 [1.48856714e-01 8.51143241e-01]
 [2.75632232e-01 7.24367738e-01]
 [8.26653659e-01 1.73346311e-01]
 [8.95421326e-01 1.04578711e-01]
 [5.74032426e-01 4.25967604e-01]
 [8.80983472e-01 1.19016543e-01]
 [7.55997002e-01 2.44002983e-01]
 [5.36078095e-01 4.63921934e-01]
 [9.98258293e-01 1.74168090e-03]
 [8.80584419e-01 1.19415537e-01]
 [1.36128724e-01 8.63871276e-01]
 [6.82320476e-01 3.17679524e-01]
 [9.19687271e-01 8.03127587e-02]
 [8.42792168e-02 9.15720761e-01]
 [4.79078829e-01 5.20921230e-01]
 [6.29245222e-01 3.70754808e-01]
 [5.85697711e-01 4.14302260e-01]
 [9.31107819e-01 6.88922554e-02]
 [1.37926072e-01 8.62074018e-01]
 [5.01925824e-03 9.94980693e-01]
 [9.19049561e-01 8.09503943e-02]
 [8.65672112e-01 1.34327918e-01]
 [1.55349951e-02 9.84464943e-01]
 [3.45916331e-01 6.54083610e-01]
 [4.46365118e-01 5.53634942e-01]
 [7.40020216e-01 2.59979784e-01]
 [3.08401622e-02 9.69159901e-01]
 [9.21293855e-01 7.87061602e-02]
 [3.49572957e-01 6.50426984e-01]
 [3.55152130e-01 6.44847929e-01]
 [1.12125300e-01 8.87874603e-01]
 [2.72600263e-01 7.27399707e-01]
 [9.68932986e-01 3.10669094e-02]
 [9.98563588e-01 1.43645681e-03]
 [2.78850436e-01 7.21149564e-01]
 [3.38596463e-01 6.61403537e-01]
 [6.78408600e-04 9.99321580e-01]
 [9.77720141e-01 2.22799424e-02]
 [8.34092498e-01 1.65907502e-01]
 [6.82274640e-01 3.17725360e-01]
 [9.21348095e-01 7.86518753e-02]
 [9.91683364e-01 8.31660815e-03]
 [7.68328726e-01 2.31671289e-01]
 [9.02222276e-01 9.77777094e-02]
 [8.38407502e-03 9.91615891e-01]
 [4.11642827e-02 9.58835781e-01]
 [1.06636301e-01 8.93363655e-01]
 [7.86070585e-01 2.13929415e-01]
 [9.56316173e-01 4.36839014e-02]
 [8.73283386e-01 1.26716673e-01]
 [3.49645838e-02 9.65035439e-01]
 [7.79283047e-01 2.20716953e-01]
 [4.33882296e-01 5.66117644e-01]
 [4.62011248e-01 5.37988782e-01]
 [6.54817939e-01 3.45182002e-01]
 [5.88125765e-01 4.11874235e-01]
 [9.94460225e-01 5.53974183e-03]
 [9.76040959e-01 2.39591189e-02]
 [9.81653512e-01 1.83464605e-02]
 [7.84697413e-01 2.15302557e-01]
 [1.05028436e-01 8.94971550e-01]
 [7.77431607e-01 2.22568318e-01]
 [5.48130155e-01 4.51869875e-01]
 [9.65875626e-01 3.41244414e-02]
 [3.36945474e-01 6.63054585e-01]
 [9.28080440e-01 7.19195530e-02]]
Accuracy: 0.5556
MCC: 0.1404
AUC: 0.6043
Confusion Matrix:
tensor([[29, 12],
        [28, 21]])
Specificity: 0.7073
Precision (Macro): 0.5726
F1 Score (Macro): 0.5520
Expected Calibration Error (ECE): 0.2754
NLL loss: 1.1348
Ensemble evaluation complete.
